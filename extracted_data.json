[
  {
    "source": "2012.00364v4.pdf",
    "content": "Pre-Trained Image Processing Transformer\nHanting Chen1,2, Yunhe Wang2*, Tianyu Guo1,2, Chang Xu3, Yiping Deng4,\nZhenhua Liu2,5,6, Siwei Ma5,6, Chunjing Xu2, Chao Xu1, Wen Gao5,6\n1 Key Lab of Machine Perception (MOE), Dept. of Machine Intelligence, Peking University. 2 Noah’s Ark Lab, Huawei Technologies.\n3 School of Computer Science, Faculty of Engineering, The University of Sydney. 4 Central Software Institution, Huawei Technologies.\n5 Institute of Digital Media, School of Electronic Engineering and Computer Science, Peking University. 6 Peng Cheng Laboratory.\nhtchen@pku.edu.cn, yunhe.wang@huawei.com\nAbstract\nAs the computing power of modern hardware is in-\ncreasing strongly, pre-trained deep learning models ( e.g.,\nBERT, GPT-3) learned on large-scale datasets have shown\ntheir effectiveness over conventional methods. The big\nprogress is mainly contributed to the representation abil-\nity of transformer and its variant architectures. In this\npaper, we study the low-level computer vision task ( e.g.,\ndenoising, super-resolution and deraining) and develop a\nnew pre-trained model, namely, image processing trans-\nformer (IPT). To maximally excavate the capability of trans-\nformer, we present to utilize the well-known ImageNet\nbenchmark for generating a large amount of corrupted\nimage pairs. The IPT model is trained on these images\nwith multi-heads and multi-tails. In addition, the con-\ntrastive learning is introduced for well adapting to differ-\nent image processing tasks. The pre-trained model can\ntherefore efﬁciently employed on desired task after ﬁne-\ntuning. With only one pre-trained model, IPT outperforms\nthe current state-of-the-art methods on various low-level\nbenchmarks. Code is available at https://github.\ncom/huawei-noah/Pretrained-IPT and https:\n//gitee.com/mindspore/mindspore/tree/\nmaster/model_zoo/research/cv/IPT\n1. Introduction\nImage processing is one component of the low-level part\nof a more global image analysis or computer vision system.\nResults from the image processing can largely inﬂuence the\nsubsequent high-level part to perform recognition and un-\nderstanding of the image data. Recently, deep learning has\nbeen widely applied to solve low-level vision tasks, such as\nimage super-resolution, inpainting, deraining and coloriza-\ntion. As many image processing tasks are related, it is nat-\n*Corresponding author\n26.6\n26.7\n26.8\n26.9\n27\n27.1\n27.2\n27.3\nHAN\n(ECCV 2020)\nIPT\n31.5\n31.6\n31.7\n31.8\n31.9\n32\n32.1\nRDN\n(CVPR 2018)\nIPT\n0.3dB↑\n0.4dB↑\nDenoising (30) Denoising (50) Deraining\n33.1\n33.2\n33.3\n33.4\n33.5\n33.6\n33.7\n33.8\nHAN\n(ECCV 2020)\nIPT\n39\n39.5\n40\n40.5\n41\n41.5\n42\nRCDNet\n(CVPR 2020)\nIPT\n28.9\n29\n29.1\n29.2\n29.3\n29.4\n29.5\n29.6\nHAN\n(ECCV 2020)\nIPT\n28.95\n29.1\n29.25\n29.4\n29.55\n29.7\n29.85\nRDN\n(CVPR 2018)\nIPT\nSISR x2 SISR x3 SISR x4\n0.4dB↑ 0.4dB↑\n0.4dB↑ 1.6dB↑\nFigure 1. Comparison on the performance of the proposed IPT and\nthe state-of-the-art image processing models on different tasks.\nural to expect a model pre-trained on one dataset can be\nhelpful for another. But few studies have generalized pre-\ntraining across image processing tasks.\nPre-training has the potential to provide an attractive so-\nlution to image processing tasks by addressing the follow-\ning two challenges: First, task-speciﬁc data can be limited.\nThis problem is exacerbated in image processing task that\ninvolves the paid-for data or data privacy, such as medical\nimages [8] and satellite images [83]. Various inconsistent\nfactors (e.g. camera parameter, illumination and weather)\ncan further perturb the distribution of the captured data for\ntraining. Second, it is unknown which type of image pro-\ncessing job will be requested until the test image is pre-\nsented. We therefore have to prepare a series of image pro-\ncessing modules at hand. They have distinct aims, but some\nunderlying operations could be shared.\nIt is now common to have pre-training in natural lan-\nguage processing and computer vision [12]. For example,\nthe backbones of object detection models [98, 97] are of-\nten pre-trained on ImageNet classiﬁcation [18]. A num-\narXiv:2012.00364v4  [cs.CV]  8 Nov 2021\nber of well-trained networks can now be easily obtained\nfrom the Internet, including AlexNet [43], VGGNet [63]\nand ResNet [34]. The seminal work Transformers [70]\nhave been widely used in many natural language process-\ning (NLP) tasks, such as translation [73] and question-\nanswering [66]. The secret of its success is to pre-train\ntransformer-based models on a large text corpus and ﬁne-\ntune them on the task-speciﬁc dataset. Variants of Trans-\nformers, like BERT [19] and GPT-3 [5], further enriched\nthe training data and improved the pre-training skills. There\nhave been interesting attempts on extending the success of\nTransformers to the computer vision ﬁeld. For example,\nWang et al. [71] and Fu et al. [25] applied the self-attention\nbased models to capture global information on images. Car-\nion et al. [7] proposed DERT to use transformer architec-\ntures for an end-to-end object detection. Most recently,\nDosovitskiy et al. [22] introduced Vision Transformer (ViT)\nto treat input images as16×16 words and attained excellent\nresults on image recognition.\nThe aforementioned pre-training in computer vision and\nnatural language mostly investigate a pretest classiﬁcation\ntask, but both the input and the output in an image pro-\ncessing task are images. A straightforward application of\nthese existing pre-training strategies might not be feasible.\nFurther, how to effectively address different target image\nprocessing tasks in the pre-training stage remains a hard\nchallenge. It is also instructive to note that the pre-training\nof image processing models enjoys a convenience of self-\ngenerating training instances based on the original real im-\nages. The synthetically manipulated images are taken for\ntraining, while the original image itself is the ground-truth\nto be reconstructed.\nIn this paper, we develop a pre-trained model for im-\nage processing using the transformer architecture, namely,\nImage Processing Transformer (IPT). As the pre-trained\nmodel needs to be compatible with different image process-\ning tasks, including super-resolution, denoising, and derain-\ning, the entire network is composed of multiple pairs of\nhead and tail corresponding to different tasks and a sin-\ngle shared body. Since the potential of transformer needs\nto be excavated using large-scale dataset, we should pre-\npair a great number of images with considerable diversity\nfor training the IPT model. To this end, we select the Im-\nageNet benchmark which contains various high-resolution\nwith 1,000 categories. For each image in the ImageNet,\nwe generate multiple corrupted counterparts using several\ncarefully designed operations to serve different tasks. For\nexample, training samples for the super-resolution task are\ngenerated by downsampling original images. The entired\ndataset we used for training IPT contains about over 10 mil-\nlions of images.\nThen, the transformer architecture is trained on the huge\ndataset as follows. The training images are input to the\nspeciﬁc head, and the generated features are cropped into\npatches ( i.e., “words”) and ﬂattened to sequences subse-\nquently. The transformer body is employed to process the\nﬂattened features in which position and task embedding are\nutilized for encoder and decoder, respectively. In addition,\ntails are forced to predict the original images with differ-\nent output sizes according to the speciﬁc task. Moreover,\na contrastive loss on the relationship between patches of\ndifferent inputs is introduced for well adopting to differ-\nent image processing tasks. The proposed image processing\ntransformer is learned in an end-to-end manner. Experimen-\ntal results conducted on several benchmarks show that the\npre-trained IPT model can surpass most of existing meth-\nods on their own tasks by a signiﬁcant enhancement after\nﬁne-tuning.\n2. Related Works\n2.1. Image Processing\nImage processing consists of the manipulation of im-\nages, including super-resolution, denoising, dehazing, de-\nraining, debluring, etc. There are a variety of deep-learning-\nbased methods proposed to conduct on one or many kinds of\nimage processing tasks. For the super-resolution, Dong et\nal. propose SRCNN [20, 21] which are considered as pio-\nneering works introducing end-to-end models that recon-\nstructs HR images from their LR counterparts. Kim et\nal. [41] further explore the capacity of deep neural network\nwith a more deeper convolutional network. Ahn et al. [2]\nand Lim et al. [50] propose introduce residual block into\nSR task. Zhang et al. [92] and Anwar and Barnes [3] utilize\nthe power of attention to enhance the performance on SR\ntask. A various excellent works are also proposed for the\nother tasks, such as denoising [68, 32, 37, 45, 24], dehaz-\ning [6, 46, 85, 80], deraining [36, 78, 62, 29, 74, 47], and\ndebluring [67, 53, 23, 10]. Different from above methods,\nwe dig the capacity of both big models and huge volume\nof data. Then a pre-training model handling several image\nprocessing tasks is introduced.\n2.2. Transformer\nTransformer [70] and its variants have proven its suc-\ncess being powerful unsupervised or self-supervised pre-\ntraining frameworks in various natural language processing\ntasks. For example, GPTs [59, 60, 5] are pre-trained in a\nautoregressive way that predicting next word in huge text\ndatasets. BERT [19] learns from data without explicit su-\npervision and predicts a masking word based on context.\nColin et al . [61] proposes a universal pre-training frame-\nwork for several downstream tasks. Yinhan et al. [52] pro-\nposes a robust variant for original BERT.\nDue to the success of Transformer-based models in the\nNLP ﬁeld, there are many attempts to explore the beneﬁts\nReshape\nTransformer Encoder\nMulti-head Multi-tail\nFeatures\nFeatures\nFlatten features\nTask embedding\n…\nDenoising \nHead\nDeraining \nHead\nx2 Up \nHead\nx4 Up \nHead\n…\nx4 Up \nTail\nDenoising\nTail\nDeraining\nTail\nx2 Up \nTail…\n…\nTransformer Decoder\nFigure 2. The diagram of the proposed image processing transformer (IPT). The IPT model consists of multi-head and multi-tail for\ndifferent tasks and a shared transformer body including encoder and decoder. The input images are ﬁrst converted to visual features and\nthen divided into patches as visual words for subsequent processing. The resulting images with high visual quality are reconstructed by\nensembling output patches.\nof Transformer in computer vision tasks. These attempts\ncan be roughly divided into two types. The ﬁrst is to intro-\nduce self-attention into the traditional convolutional neural\nnetwork. Yuan et al. [82] introduce spatial attention for im-\nage segmentation. Fu et al. [26] proposes DANET utiliz-\ning the context information by combining spatial and chan-\nnel attention. Wang et al. [75], Chen et al. [15], Jiang et\nal. [38] and Zhang et al. [91] also augment features by self-\nattention to enhance model performance on several high-\nlevel vision tasks. The other type is to replace convolu-\ntional neural network with self-attention block. For in-\nstance, Kolesnikov et al . [42] and Dosovitskiy [22] con-\nduct image classiﬁcation with transformer block. Carion et\nal. [7] and Zhu et al . [100] implement transformer-based\nmodels in detection. Chen et al. [11] proposes a pre-trained\nGPT model for generative and classiﬁcation tasks. Wu et\nal. [77] and Zhao et al. [96] propose pre-training methods\nfor teansformer-based models for image recognition task.\nJiang et al. [39] propose the TransGAN to generate images\nusing Transformer. However, few related works focus on\nlow-level vision tasks. In this paper, we explore a universal\npre-training approach for image processing tasks.\n3. Image Processing Transformer\nTo excavate the potential use of transformer on im-\nage processing tasks for achieving better results, here we\npresent the image processing transformer by pre-training on\nlarge-scale dataset.\n3.1. IPT architecture\nThe overall architecture of our IPT consists of four com-\nponents: heads for extracting features from the input cor-\nrupted images ( e.g., images with noise and low-resolution\nimages), an encoder-decoder transformer is established for\nrecovering the missing information in input data, and tails\nare used formapping the features into restored images. Here\nwe brieﬂy introduce our architecture, details can be found\nin the supplementary material.\nHeads. To adjust different image processing task, we use\na multi-head architecture to deal with each task separately,\nwhere each head consists of three convolutional layers. De-\nnote the input image as x ∈R3×H×W (3 means R, G, and\nB), the head generates a feature map fH ∈RC×H×W with\nCchannels and same height and width (typical we useC =\n64). The calculation can be formulated as fH = Hi(x),\nwhere Hi (i = {1,...,N t}) denote the head for the ith\ntask and Nt denotes the number of tasks.\nTransformer encoder. Before input features into the\ntransformer body, we split the given features into patches\nand each patch is regarded as a ”word”. Speciﬁcally, the\nfeatures fH ∈ RC×H×W are reshaped into a sequence\nof patches, i.e., fpi ∈ RP2×C,i = {1,...,N }, where\nN = HW\nP2 is the number of patches ( i.e., the length of se-\nquence) and P is patch size. To maintain the position in-\nformation of each patch, we add learnable position encod-\nings Epi ∈RP2×C for each patch of feature fpi follow-\ning [22, 7], and Epi + fpi will be directly input into the\ntransformer encoder. The architecture of encoder layer is\nfollowing the original structure in [70], which has a multi-\nhead self-attention module and a feed forward network. The\noutput of encoder fEi ∈ RP2×C for each patch has the\nsame size to that of the input patch fpi. The calculation can\nbe formulated as\ny0 = [Ep1 + fp1 ,Ep2 + fp2 ,...,E pN + fpN ] ,\nqi = ki = vi = LN(yi−1),\ny′\ni = MSA(qi,ki,vi) +yi−1,\nyi = FFN(LN(y′\ni)) +y′\ni, i = 1,...,l\n[fE1 ,fE2 ,...,f EN ] =yl,\n(1)\nwhere ldenotes the number of layers in the encoder, MSA\ndenotes the multi-head self-attention module in the conven-\ntional transformer model [70], LN denotes the layer nor-\nmalization [4] and FFN denotes the feed forward network,\nwhich contains two fully connected layers.\nTransformer decoder. The decoder also follows the\nsame architecture and takes the output of decoder as input\nin the transformer body, which consists of two multi-head\nself-attention (MSA) layers and one feed forward network\n(FFN). The difference to that of the original transformer\nhere is that we utilize a task-speciﬁc embedding as an addi-\ntional input of the decoder. These task-speciﬁc embeddings\nEi\nt ∈RP2×C,i = {1,...,N t}are learned to decode fea-\ntures for different tasks. The calculation of decoder can be\nformulated as:\nz0 = [fE1 ,fE2 ,...,f EN ] ,\nqi = ki = LN(zi−1) +Et,vi = LN(zi−1),\nz′\ni = MSA(qi,ki,vi) +zi−1,\nq′\ni = LN(z′\ni) +Et,k′\ni = v′\ni = LN(z0),\nz′′\ni = MSA(q′\ni,k′\ni,v′\ni) +z′\ni,\nzi = FFN(LN(z′′\ni )) +z′′\ni , i = 1,...,l\n[fD1 ,fD2 ,...,f DN ] =yl,\n(2)\nwhere fDi ∈RP2×C denotes the outputs of decoder. The\ndecoded N patched features with size P2 ×C are then re-\nshaped into the features fD with size C×H×W.\nTails. The properties of tails are same as those of heads,\nwe use multi tails to deal with different tasks. The cal-\nculation can be formulated as fT = Ti(fD), where Ti\n(i = {1,...,N t}) denote the head for the ith task and Nt\ndenotes the number of tasks. The output fT is the resulted\nimages size of 3 ×H′×W′ which is determined by the\nspeciﬁc task. For example, H′ = 2H,W = 2W for a 2×\nsuper-resolution task.\n3.2. Pre-training on ImageNet\nBesides the architecture of transformer itself, one of\nthe key factors for successfully training an excellent trans-\nformer is that the well use of large-scale datasets. Compared\nwith image classiﬁcation, the number of available data used\nfor image processing task is relatively small (e.g., only 2000\nimages on DIV2K dataset for the image super-resolution\ntask), we propose to utilize the well-known ImageNet as\nthe baseline dataset for pre-training our IPT model, then\nwe generate the entire dataset for several tasks (e.g., super-\nresolution and denosing) as follows.\nAs the images in the ImageNet benchmark are of high\ndiversity, which contains over 1 million of natural images\nfrom 1,000 different categories. These images have abun-\ndant texture and color information. We ﬁrst remove the\nsemantic label and manually synthesize a variety of cor-\nrupted images from these unlabeled images with a variety\nof degradation models for different tasks. Note that synthe-\nsized dataset is also usually used in these image processing\ntasks and we use the same degeneration methods as sug-\ngested in [31, 1]. For example, super-resolution tasks often\ntake bicubic degradation to generate low-resolution images,\ndenoising tasks add Gaussian noise in clean images with\ndifferent noise level to generate the noisy images. These\nsynthesized images can signiﬁcantly improve the perfor-\nmance of learned deep networks including both CNN and\ntransformer architectures, which will be shown in the exper-\niment part. Basically, the corrupted images are synthesized\nas:\nIcorrupted = f(Iclean), (3)\nwhere f denotes the degradation transformation, which is\ndepended on the speciﬁc task: for the super-resolution task,\nfsr is exactly the bicubic interpolation; for image denois-\ning, fnoise(I) = I + η, where η is the additive Gaussian\nnoise; for deraining, frain(I) =I+rin which ris a hand-\ncrafted rain streak. The loss function for learning our IPT\nin the supervised fashion can be formulated as:\nLsupervised =\nNt∑\ni=1\nL1(IPT(Ii\ncorrupted),Iclean), (4)\nwhere L1 denote the conventional L1 loss for reconstructing\ndesired images and Ii\ncorrupted denote the corrupted image\nfor task i, respectively. In addition, Eq. 4 implies that the\nproposed framework is trained with multiple image process\ntasks simultaneously. Speciﬁcally, for each batch, we ran-\ndomly select one task from Nt supervised tasks for train-\ning and each task will be processed using the correspond-\ning head, tail and task embedding, simultaneously. After\nthe pre-training the IPT model, it will capture the intrin-\nsic features and transformations for a large variety of image\nprocessing tasks thus can be further ﬁne-tuned to apply on\nthe desired task using the new provided dataset. Moreover,\nother heads and tails will be dropped for saving the compu-\ntation costs and parameters in the remained head, tail and\nbody will be updated according to the back-propagation.\nHowever, due to the variety of degradation models, we\ncannot synthesize images for all image processing tasks.\nFor example, there is a wide range of possible noise lev-\nels in practice. Therefore, the generalization ability of\nthe resulting IPT should be further enhanced. Similar to\nthe pre-training natural language processing models, the\nrelationship between patches of images is also informa-\ntive. The patch in image scenario can be considered as a\nword in natural language processing. For example, patches\ncropped from the same feature map are more likely to ap-\npear together, which should be embedded into similar posi-\ntions. Therefore, we introduce contrastive learning [13, 33]\nfor learning universal features so that the pre-trained IPT\nmodel can be utilized to unseen tasks. In practice, denote\nthe output patched features generated by IPT decoder for\nthe given input xj as fj\nDi ∈ RP2×C,i = {1,...,N },\nwhere xj is selected from a batch of training images X =\n{x1,x2,...,x B}. We aims to minimize the distance be-\ntween patched features from the same images while max-\nimize the distance between patches from different images.\nThe loss function for contrastive learning is formulated as:\nl(fj\nDi1\n,fj\nDi2\n) =−log\nexp(d(fj\nDi1\n,fj\nDi2\n))\n∑B\nk=1 Ik̸=jexp(d(fj\nDi1\n,fk\nDi2\n))\n,\nLconstrastive = 1\nBN2\nN∑\ni1=1\nN∑\ni2=1\nB∑\nj=1\nl(fj\nDi1\n,fj\nDi2\n),\n(5)\nwhere d(a,b) = aT b\n∥a∥∥b∥ denotes the cosine similarity.\nMoreover, to make fully usage of both supervised and self-\nsupervised information, we reformulate the loss function as:\nLIPT = λ·Lcontrastive + Lsupervised. (6)\nWherein, we combine the λ-balanced contrastive loss with\nthe supervised loss as the ﬁnal objective function of IPT.\nThus, the proposed transformer network trained using Eq. 6\ncan be effectively exploited on various existing image pro-\ncessing tasks.\n4. Experiments\nIn this section, we evaluate the performance of the pro-\nposed IPT on various image processing tasks including\nsuper-resolution and image denoising. We show that the\npre-trained IPT model can achieve state-of-the-art perfor-\nmance on these tasks. Moreover, extensive experiments for\nablation study show that the transformer-based models per-\nform better than convolutional neural networks when us-\ning the large-scale dataset for solving the image processing\nproblem.\nDatasets. To obtain better pre-trained results of the IPT\nmodel, we use the well-known ImageNet dataset, which\nconsists of over 1M color images of high diversity. The\ntraining images are cropped into 48 ×48 patches with 3\nchannels for training, i.e., there are over 10M patches for\ntraining the IPT model. We then generate the corrupted im-\nages with 6 types of degradation: 2×,3×,4×bicubic inter-\npolation, 30,50 noise level Gaussian noise and adding rain-\nstreaks, respectively. For the rain-streak generation, we fol-\nlow the method described in [79]. During the test, we crop\nthe images in the test set into 48 ×48 patches with a 10\npixels overlap. Note that the same testing strategy is also\nadopted for CNN based models for a fair comparison, and\nthe resulting PSNR values of CNN models are the same as\nthat of their baselines.\nTraining & Fine-tuning. We use 32 Nvidia NVIDIA\nTesla V100 cards to train our IPT model using the conven-\ntional Adam optimizer with β1 = 0.9,β2 = 0.999 for 300\nepochs on the modiﬁed ImageNet dataset. The initial learn-\ning rate is set as 5e−5 and decayed to 2e−5 in 200 epoch\nwith 256 batch size. Since the training set consists of dif-\nferent tasks, we cannot input all of them in a single batch\ndue to the expensive memory cost. Therefore, we stack a\nbatch of images from a randomly selected task in each iter-\nation. After pre-training on the entire synthesized dataset,\nwe ﬁne-tune the IPT model on the desired task ( e.g., ×3\nsingle image super-resolution) for 30 epochs with a learn-\ning rate of 2e−5. Note that SRCNN [20] also found that\nusing ImageNet training can bring up the performance of\nthe super-resolution task, while we propose a model ﬁtting\ngeneral low-level vision tasks.\n4.1. Super-resolution\nWe compare our model with several state-of-the-art\nCNN-based SR methods. As shown in Table 1, our pre-\ntrained IPT outperforms all the other methods and achieves\nthe best performance in ×2,×3,×4 scale on all datasets.\nIt is worth to highlight that our model achieves 33.76dB\nPSNR on the ×2 scale Urban100 dataset, which surpasses\nother methods with more than ∼0.4dB, while previous\nSOTA methods can only achieve a <0.2dB improvement\ncompared with others, which indicates the superiority of the\nproposed model by utilizing large scale pre-training.\nWe further present the visualization results on our model\nin 4×scale on Urban100 dataset. As shown in Figure 3,\nit is difﬁcult for recover the original high resolution images\nsince lots of information are lost due to the high scaling\nfactor. Previous methods generated blurry images, while the\nsuper-resolution images produced by our model can well\nrecover the details from the low-resolution images.\n4.2. Denoising\nSince our pre-trained model can be well adapt to many\ntasks, we then evaluate the performance of our model on\nimage denoising task. The training and testing data is gen-\nerated by adding Gaussian noise with σ = 30,50 to the\nclean images.\nTo verify the effectiveness of the proposed method,\nUrban100 (×4): img004\nHR VDSR [41] EDSR [51]\nRDN [94] OISR [35] SAN [17]\nRNAN [93] IGNN [99] IPT (ours)\nUrban100 (4×):img012\nHR Bicubic VDSR [41] EDSR [51] RDN [94]\nOISR [35] SAN [17] RNAN [93] IGNN [99] IPT (ours)\nUrban100 (4×): img044\nHR Bicubic VDSR [41] EDSR [51] RDN [94]\nOISR [35] SAN [17] RNAN [93] IGNN [99] IPT (ours)\nFigure 3. Visual results with bicubic downsampling (×4) from Urban100. The proposed method recovers more details. Compared images\nare derived from [99].\nBSD68: 163085\nGT Noisy ( σ=50) CBM3D [16] TNRD [14] RDN [94]\nDnCNN [87] MemNet [65] IRCNN [88] FFDNet [89] IPT (ours)\nFigure 4. Color image denoising results with noise level σ = 50. Compared images are derived from [90].\nwe compare our results with various state-of-the-art mod-\nels. Table 2 reported the color image denoising results\non BSD68 and Urban100 dataset. As a result, our IPT\nachieves the best results among all denoising methods on\ndifferent Gaussian noise level. Moreover, we surprisingly\nfound that our model improve the state-of-the-art perfor-\nmance by ∼0.3dB on the Urban100 dataset, which demon-\nstrate the effectiveness of pre-training and the superiority of\nour transformer-based model.\nFigure 4 shows the visualization of the resulted images.\nAs shown in the ﬁgure, noisy images are hard to be recog-\nnized and it is difﬁcult to recover the clean images. There-\nfore, existing methods fail to reconstruct enough details and\ngenerate abnormal pixels. As a result, our pre-trained model\ncan well recover several details in the hair of this cat and our\nvisual quality beats all the previous models obviously.\n4.3. Deraining\nFor the image deraining task, we evaluate our model on\nthe synthesized Rain100L dataset [79], which consists of\n100 rainy images. Quantitative results can be viewed in\nTable 3. Compared with the state-of-the-art methods, we\nachieve the best performance (41.62dB) with an 1.62dB im-\nprovement.\nFigure 5 shows the visualization results. Previous meth-\nods are failed to reconstruct the original clean images since\nthey lack of image prior. As a result, our IPT model can\npresent exactly the same image as the ground-truth and sur-\nInput / Groundtruth\n27.37 / 0.8154\nDSC\n29.34 / 0.8479\nGMM\n32.38 / 0.9306\nJCAS\n31.45 / 0.9151\nClear\n31.59 / 0.9380\nRESCAN\n41.26 / 0.9887\nPReNet\n37.27 / 0.9793\nSPANet\n35.67 / 0.9700\nJORDER_E\n41.11 / 0.9894\nSIRR\n36.99 / 0.9692\nRCDNet \n42.15 / 0.9912\nIPT (ours)\n43.91 / 0.9922\nFigure 5. Image deraining results on the Rain100L dataset. Compared images are derived from [72].\nTable 1. Quantitative results on image super-resolution. Best and\nsecond best results are highlighted and underlined.\nMethod Scale Set5 Set14 B100 Urban100\nVDSR [41] ×2 37.53 33.05 31.90 30.77\nEDSR [51] ×2 38.11 33.92 32.32 32.93\nRCAN [92] ×2 38.27 34.12 32.41 33.34\nRDN [94] ×2 38.24 34.01 32.34 32.89\nOISR-RK3 [35] ×2 38.21 33.94 32.36 33.03\nRNAN [93] ×2 38.17 33.87 32.32 32.73\nSAN [17] ×2 38.31 34.07 32.42 33.10\nHAN [55] ×2 38.27 34.16 32.41 33.35\nIGNN [99] ×2 38.24 34.07 32.41 33.23\nIPT (ours) ×2 38.37 34.43 32.48 33.76\nVDSR [41] ×3 33.67 29.78 28.83 27.14\nEDSR [51] ×3 34.65 30.52 29.25 28.80\nRCAN [92] ×3 34.74 30.65 29.32 29.09\nRDN [94] ×3 34.71 30.57 29.26 28.80\nOISR-RK3 [35] ×3 34.72 30.57 29.29 28.95\nRNAN [93] ×3 34.66 30.52 29.26 28.75\nSAN [17] ×3 34.75 30.59 29.33 28.93\nHAN [55] ×3 34.75 30.67 29.32 29.10\nIGNN [99] ×3 34.72 30.66 29.31 29.03\nIPT (ours) ×3 34.81 30.85 29.38 29.49\nVDSR [41] ×4 31.35 28.02 27.29 25.18\nEDSR [51] ×4 32.46 28.80 27.71 26.64\nRCAN [92] ×4 32.63 28.87 27.77 26.82\nSAN [17] ×4 32.64 28.92 27.78 26.79\nRDN [94] ×4 32.47 28.81 27.72 26.61\nOISR-RK3 [35] ×4 32.53 28.86 27.75 26.79\nRNAN [93] ×4 32.49 28.83 27.72 26.61\nHAN [55] ×4 32.64 28.90 27.80 26.85\nIGNN [99] ×4 32.57 28.85 27.77 26.84\nIPT (ours) ×4 32.64 29.01 27.82 27.26\npasses all the previous algorithms in visual quality. This\nresult substantiates the generality of the proposed model.\nTable 2. Quantitative results on color image denoising. Best and\nsecond best results are highlighted and underlined.\nMethod BSD68 Urban100\n30 50 30 50\nCBM3D [16] 29.73 27.38 30.36 27.94\nTNRD [14] 27.64 25.96 27.40 25.52\nDnCNN [87] 30.40 28.01 30.28 28.16\nMemNet [65] 28.39 26.33 28.93 26.53\nIRCNN [88] 30.22 27.86 30.28 27.69\nFFDNet [89] 30.31 27.96 30.53 28.05\nSADNet [9] 30.64 28.32 N/A N/A\nRDN [95] 30.67 28.31 31.69 29.29\nIPT (ours) 30.75 28.39 32.00 29.71\n4.4. Generalization Ability\nAlthough we can generate various corrupted images, nat-\nural images are of high complexity and we cannot syn-\nthesize all possible images for pre-training the transformer\nmodel. However, a good pre-trained model should have the\ncapacity for well adapting other tasks as those in the ﬁeld of\nNLP. To this end, we then conduct several experiments to\nverify the generalization ability of our model. In practice,\nwe test corrupted images that did not include in our syn-\nthesized ImageNet dataset, i.e., image denoising with noisy\nlevel 10 and 70, respectively. We use the heads and tails for\nimage denoising tasks as the pre-trained model.\nThe detailed results are shown in Table 4, we compare\nthe performance of using the pre-trained IPT model and the\nstate-of-the-art methods for image denoising. Obviously,\nIPT model outperforms other conventional methods, which\nTable 3. Quantitative results of image deraining on the Rain100L dataset. Best and second best results are highlighted and underlined.\nMethod Input DSC [28] GMM [49] JCAS [31] Clear [27] DDN [28]\nPSNR 26.90 27.34 29.05 28.54 30.24 32.38\nSSIM 0.8384 0.8494 0.8717 0.8524 0.9344 0.9258\nRESCAN [48] PReNet [62] JORDER E [79] SPANet [74] SSIR [76] RCDNet [72] IPT (ours)\n38.52 37.45 38.59 35.33 32.37 40.00 41.62\n0.9812 0.9790 0.9834 0.9694 0.9258 0.9860 0.9880\nTable 4. Generation ability of our IPT model on color image de-\nnoising with different noise levels. Best and second best results\nare highlighted and underlined.\nMethod BSD68 Urban100\n10 70 10 70\nCBM3D [16] 35.91 26.00 36.00 26.31\nTNRD [14] 33.36 23.83 33.60 22.63\nDnCNN [87] 36.31 26.56 36.21 26.17\nMemNet [65] N/A 25.08 N/A 24.96\nIRCNN [88] 36.06 N/A 35.81 N/A\nFFDNet [89] 36.14 26.53 35.77 26.39\nRDN [95] 36.47 26.85 36.69 27.63\nIPT (ours) 36.53 26.92 36.99 27.90\n0.0 0.2 0.4 0.6 0.8 1.0\nPercentage of Usage of ImageNet (1.1M Images)\n32.8\n33.0\n33.2\n33.4\n33.6\n33.8PSNR (dB)\nIPT\nEDSR\nIGNN\nRDN\nFigure 6. The performance of CNN and IPT models using different\npercentages of data.\ndemonstrates that the pre-trained model can capture more\nuseful information and features from the large-scale dataset.\n4.5. Ablation Study\nImpact of data percentage.To evaluate the effective-\nness of the transformer architecture, we conduct experi-\nments to analyse the improvement of pre-training on CNN-\nbased model and transformer-based model. We use 20%,\n40%, 60%, 80% and 100% percentages of the synthesized\nImageNet dataset to analyse the impact on the number of\nused data for resulting performance. Figure 6 shows the\nresults of different pre-trained models. When the models\nare not pre-trained or pre-trained with small amount ( <\n60%) of the entire dataset, the CNN models achieve bet-\nter performance. In contrast, when using large-scale data,\nthe transformer-based models overwhelming CNN models,\nwhich demonstrates that the effectiveness of our IPT model\nfor pre-training.\nTable 5. Impact of λ for contrastive learning.\nλ 0 0.05 0.1 0.2 0.5\nPSNR 38.27 38.32 38.37 38.33 38.26\nImpact of contrastive learning.As discussed above, to\nimprove the representation ability of our pre-trained model,\nwe embed the contrastive learning loss (Eq. 6) into the train-\ning procedure. We then evaluate its effectiveness on the×2\nscale super-resolution task using the Set4 dataset. Table 5\nshows the impact of the hyper-parameter λ for balancing\nthe two terms in Eq. 6. When λ=0, the IPT model is trained\nusing only a supervised learning approach, the resulting\nPSNR value is 38.27dB. When employing the contrastive\nloss for self-supervised learning, the model can achieve a\n38.37dB PSNR value (λ= 0.1), which is about 0.1dB higher\nthan that of the model trained withλ= 0. These results fur-\nther demonstrate the effectiveness of the contrastive learn-\ning for learning better pre-trained IPT model.\n5. Conclusions and Discussions\nThis paper aims to address the image processing prob-\nlems using a pre-trained transformer model (IPT). The IPT\nmodel is designed with multi-heads,multi-tails a shared\ntransformer body for serving different image processing\ntask such as image super-resolution and denoising. To max-\nimally excavate the performance of the transformer archi-\ntecture on various tasks, we explore a synthesized ImageNet\ndatesets. Wherein, each original image will be degraded to\na series of counterparts as paired training data. The IPT\nmodel is then trained using supervised and self-supervised\napproaches which shows strong ability for capturing intrin-\nsic features for low-level image processing. Experimental\nresults demonstrate that our IPT can outperform the state-\nof-the-art methods using only one pre-trained model after a\nquickly ﬁne-tuning. In the future work, we will extend our\nIPT model to more tasks such as inpainting, dehazing, etc.\nAcknowledgment This work is supported by National\nNatural Science Foundation of China under Grant No.\n61876007, and Australian Research Council under Project\nDE180101438 and DP210101859.\nA. Results on Deblurring\nWe further evaluate the performance of our model on im-\nage deblurring task. We use the GoPro dataset [54] to ﬁne-\ntune and test our model. We modify the patch size as 256,\npatch dim as 8 and number of features as 9 to achieve a\nhigher receptive ﬁeld. Table 6 reported deblurring results,\nwhere + denotes applying self-ensemble technique. As a re-\nsult, our IPT achieves the best results among all deblurring\nmethods. Figure 8 shows the visualization of the resulted\nimages. As shown in the ﬁgure, our pre-trained model can\nwell achieve the best visual quality among all the previous\nmodels obviously.\nB. Architecture of IPT\nIn the main paper, we propose the image processing\ntransformer (IPT). Here we show the detailed architecture\nof IPT, which consists of heads, body and tails. Each head\nhas one convolutional layer (with 3 ×3 kernel size, 3 in-\nput channels and 64 output channels) and two ResBlock.\nEach ResBlock consists of two convolutional layers (with\n5×5 kernel size, 64 input channels and 64 output channels)\nwhich involved by a single shortcut. The body has 12 en-\ncoder layers and 12 decoder layers. The tail of denoising or\nderaining is a convolutional layer with 3 ×3 kernel size, 64\ninput channels and 3 output channels. For super-resolution,\nthe tail consists of one pixelshufﬂe layer with upsampling\nscale 2 and 3 for×2 and ×3 SR, two pixelshufﬂe layer with\nupsampling scale 2 for ×4 SR.\nThe whole IPT has 114M parameters and 33G FLOPs,\nwhich have more parameters while fewer FLOPs compared\nwith traditional CNN models (e.g., EDSR has 43M param-\neters and 99G FLOPs).\nC. Impact of Multi-task Training\nWe train IPT following a multi-task manner and then\nﬁne-tune it on 6 different tasks including×2,×3,×4 super-\nresolution, denoising with noise level 30,50 and deraining.\nWe ﬁnd that this training strategy would not harm the per-\nformance on these tasks which have been pre-trained on\nlarge scale dataset (ImageNet). In other words, the per-\nformance of multi-task training and single-task training re-\nmains almost the same. However, when transferring to other\ntasks (e.g., Section 4.4 in the main paper), the pre-trained\nmodel using multi-task training is better than that of single-\ntask training for about 0.3dB, which suggests the multi-task\ntraining would learn universal representation of image pro-\ncessing tasks.\nD. Visualization of Embeddings\nWe visualize the learned embeddings of IPT. Figure 7\nshows the visualization results of position embeddings. We\nFigure 7. Visualization of cosine similarity of position embed-\ndings.\nﬁnd that patches with similar columns or rows have similar\nembeddings, which indicate that they learn useful informa-\ntion for discovering the position on image processing. We\nalso test to use ﬁxed embeddings or do not use embeddings,\nwhose performance are lower than that of using learnable\nposition embeddings (vary from 0.2dB to 0.3dB for differ-\nent tasks).\nMoreover, we visualize the task embeddings in ﬁgure 9.\nWe can ﬁnd that for ×2 super-resolution task, the simi-\nlarity between the embeddings on each position and their\nneighbours are higher than ×3 super-resolution, while that\nof ×4 super-resolution is the smallest. This results indi-\ncates that each patches in ×2 super-resolution can focus\non other patches with farther distance than ×3 and ×4,\nsince their downsampling scale are smaller and the rela-\ntionship between different patches are closer. The similar-\nity of task embedding for deraining in ﬁgure 9 (d) shows\nthat the patches pay more attention on the vertical direc-\ntion than horizontal direction, which is reasonable as the\nrain is dropped vertically. The similarity of task embedding\nfor denoising is similar with Gaussian noise, and ﬁgure 9\n(f) with higher (50) noise level shows higher similarity be-\ntween neighbours than ﬁgure 9 (e) with 30 noise level. The\nvisualization results suggests that our task embeddings can\nindeed learn some information for different tasks. We also\ntest to not use task embeddings, which results in signiﬁ-\ncant accuracy drop (vary from 0.1dB to 0.5dB for different\ntasks).\nInputRADNBANet\nIPT(Ours)\nFigure 8. Image deblurring results on the GoPro dataset. Compared images are derived from [69].\nTable 6. Quantitative results on image deblurring. Best and second best results are highlighted and underlined.\nMethod MSCNN [54] SRN [67] DSD [30] DeblurGANv2 [44] DMPHN [84] LEBMD [40] EDSD [81]\nPSNR 30.40 30.25 30.96 29.55 31.36 31.79 29.81\nDBGAN [86] MTRNN [57] RADN [58] SAPHN [64] BANET [69] MB2D [56] IPT (Ours) IPT+ (Ours)\n31.10 31.13 31.85 32.02 32.44 32.16 32.58 32.91\nReferences\n[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge\non single image super-resolution: Dataset and study. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition Workshops, pages 126–135, 2017.\n4\n[2] Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn.\nFast, accurate, and lightweight super-resolution with cas-\ncading residual network. In Proceedings of the European\nConference on Computer Vision (ECCV) , pages 252–268,\n2018. 2\n[3] Saeed Anwar and Nick Barnes. Densely residual laplacian\nsuper-resolution. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2020. 2\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv preprint arXiv:1607.06450 ,\n2016. 4\n[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 2\n[6] Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, and\nDacheng Tao. Dehazenet: An end-to-end system for single\nimage haze removal. IEEE Transactions on Image Process-\ning, 25(11):5187–5198, 2016. 2\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. arXiv\npreprint arXiv:2005.12872, 2020. 2, 3\n[8] Gabriella Castellano, Leonardo Bonilha, LM Li, and Fer-\nnando Cendes. Texture analysis of medical images. Clini-\ncal radiology, 59(12):1061–1069, 2004. 1\n[9] Meng Chang, Qi Li, Huajun Feng, and Zhihai Xu. Spatial-\nadaptive network for single image denoising.arXiv preprint\narXiv:2001.10291, 2020. 7\n[10] Liang Chen, Faming Fang, Shen Lei, Fang Li, and Guixu\nZhang. Enhanced sparse model for blind deblurring. 2020.\n2\n[11] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo\nJun, Prafulla Dhariwal, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In Proceedings of the\n37th International Conference on Machine Learning , vol-\nume 1, 2020. 3\n(a) ×2 super-resolution (b) ×3 super-resolution (c) ×4 super-resolution\n(d) deraining (e) denoising with 30 noise level (f) denoising with 50 noise level\nFigure 9. Visualization of six different task embeddings.\n[12] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,\nYang Zhang, Michael Carbin, and Zhangyang Wang. The\nlottery tickets hypothesis for supervised and self-supervised\npre-training in computer vision models. arXiv preprint\narXiv:2012.06908, 2020. 1\n[13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. arXiv preprint arXiv:2002.05709,\n2020. 5\n[14] Yunjin Chen and Thomas Pock. Trainable nonlinear reac-\ntion diffusion: A ﬂexible framework for fast and effective\nimage restoration. IEEE transactions on pattern analysis\nand machine intelligence, 39(6):1256–1272, 2016. 6, 7, 8\n[15] Yunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Yan\nShuicheng, Jiashi Feng, and Yannis Kalantidis. Graph-\nbased global reasoning networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 433–442, 2019. 3\n[16] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and\nKaren Egiazarian. Color image denoising via sparse 3d col-\nlaborative ﬁltering with grouping constraint in luminance-\nchrominance space. In 2007 IEEE International Confer-\nence on Image Processing , volume 1, pages I–313. IEEE,\n2007. 6, 7, 8\n[17] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and\nLei Zhang. Second-order attention network for single im-\nage super-resolution. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition , pages\n11065–11074, 2019. 6, 7\n[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In 2009 IEEE conference on computer vision\nand pattern recognition, pages 248–255. Ieee, 2009. 1\n[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 2\n[20] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang. Learning a deep convolutional network for image\nsuper-resolution. In European conference on computer vi-\nsion, pages 184–199. Springer, 2014. 2, 5\n[21] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang. Image super-resolution using deep convolutional net-\nworks. IEEE transactions on pattern analysis and machine\nintelligence, 38(2):295–307, 2015. 2\n[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 3\n[23] Thomas Eboli, Jian Sun, and Jean Ponce. End-to-end in-\nterpretable learning of non-blind image deblurring. arXiv\npreprint arXiv:2007.01769, 2020. 2\n[24] Yuchen Fan, Honghui Shi, Jiahui Yu, Ding Liu, Wei\nHan, Haichao Yu, Zhangyang Wang, Xinchao Wang, and\nThomas S Huang. Balanced two-stage residual networks\nfor image super-resolution. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\nWorkshops, pages 161–168, 2017. 2\n[25] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhi-\nwei Fang, and Hanqing Lu. Dual attention network for\nscene segmentation. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition , pages\n3146–3154, 2019. 2\n[26] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhi-\nwei Fang, and Hanqing Lu. Dual attention network for\nscene segmentation. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition , pages\n3146–3154, 2019. 3\n[27] Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao,\nand John Paisley. Clearing the skies: A deep network archi-\ntecture for single-image rain removal. IEEE Transactions\non Image Processing, 26(6):2944–2956, 2017. 8\n[28] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xing-\nhao Ding, and John Paisley. Removing rain from single im-\nages via a deep detail network. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 3855–3863, 2017. 8\n[29] Xueyang Fu, Borong Liang, Yue Huang, Xinghao Ding,\nand John Paisley. Lightweight pyramid networks for im-\nage deraining. IEEE transactions on neural networks and\nlearning systems, 2019. 2\n[30] Hongyun Gao, Xin Tao, Xiaoyong Shen, and Jiaya Jia.\nDynamic scene deblurring with parameter selective shar-\ning and nested skip connections. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3848–3856, 2019. 10\n[31] Shuhang Gu, Deyu Meng, Wangmeng Zuo, and Lei Zhang.\nJoint convolutional analysis and synthesis sparse represen-\ntation for single image layer separation. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 1708–1716, 2017. 4, 8\n[32] Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei\nZhang. Toward convolutional blind denoising of real pho-\ntographs. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 1712–1722,\n2019. 2\n[33] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9729–9738, 2020. 5\n[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 2\n[35] Xiangyu He, Zitao Mo, Peisong Wang, Yang Liu,\nMingyuan Yang, and Jian Cheng. Ode-inspired network\ndesign for single image super-resolution. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1732–1741, 2019. 6, 7\n[36] Xiaowei Hu, Chi-Wing Fu, Lei Zhu, and Pheng-Ann Heng.\nDepth-attentional features for single-image rain removal. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 8022–8031, 2019. 2\n[37] Xixi Jia, Sanyang Liu, Xiangchu Feng, and Lei Zhang. Foc-\nnet: A fractional optimal control network for image denois-\ning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 6054–6063, 2019. 2\n[38] Peng-Tao Jiang, Qibin Hou, Yang Cao, Ming-Ming Cheng,\nYunchao Wei, and Hong-Kai Xiong. Integral object mining\nvia online attention accumulation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 2070–2079, 2019. 3\n[39] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan:\nTwo transformers can make one strong gan. arXiv preprint\narXiv:2102.07074, 2021. 3\n[40] Zhe Jiang, Yu Zhang, Dongqing Zou, Jimmy Ren,\nJiancheng Lv, and Yebin Liu. Learning event-based motion\ndeblurring. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3320–\n3329, 2020. 10\n[41] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accu-\nrate image super-resolution using very deep convolutional\nnetworks. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition , pages 1646–1654,\n2016. 2, 6, 7\n[42] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning.\narXiv preprint arXiv:1912.11370, 6, 2019. 3\n[43] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. Communications of the ACM , 60(6):84–90, 2017.\n2\n[44] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and\nZhangyang Wang. Deblurgan-v2: Deblurring (orders-\nof-magnitude) faster and better. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 8878–8887, 2019. 10\n[45] Stamatios Lefkimmiatis. Non-local color image denoising\nwith convolutional neural networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 3587–3596, 2017. 2\n[46] Boyi Li, Xiulian Peng, Zhangyang Wang, Jizheng Xu, and\nDan Feng. An all-in-one network for dehazing and beyond.\narXiv preprint arXiv:1707.06543, 2017. 2\n[47] Siyuan Li, Wenqi Ren, Feng Wang, Iago Breno Araujo,\nEric K Tokuda, Roberto Hirata Junior, Roberto M Cesar-\nJr, Zhangyang Wang, and Xiaochun Cao. A comprehen-\nsive benchmark analysis of single image deraining: Current\nchallenges and future perspectives. International Journal\nof Computer Vision, pages 1–22, 2021. 2\n[48] Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, and Hong-\nbin Zha. Recurrent squeeze-and-excitation context aggre-\ngation net for single image deraining. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV) , pages\n254–269, 2018. 8\n[49] Yu Li, Robby T Tan, Xiaojie Guo, Jiangbo Lu, and\nMichael S Brown. Rain streak removal using layer priors.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 2736–2744, 2016. 8\n[50] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah,\nand Kyoung Mu Lee. Enhanced deep residual networks\nfor single image super-resolution. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion workshops, pages 136–144, 2017. 2\n[51] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah,\nand Kyoung Mu Lee. Enhanced deep residual networks\nfor single image super-resolution. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion workshops, pages 136–144, 2017. 6, 7\n[52] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A ro-\nbustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019. 2\n[53] Boyu Lu, Jun-Cheng Chen, and Rama Chellappa. Unsu-\npervised domain-speciﬁc deblurring via disentangled rep-\nresentations. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , pages 10225–\n10234, 2019. 2\n[54] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep\nmulti-scale convolutional neural network for dynamic scene\ndeblurring. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3883–3891,\n2017. 9, 10\n[55] Ben Niu, Weilei Wen, Wenqi Ren, Xiangde Zhang, Lian-\nping Yang, Shuzhen Wang, Kaihao Zhang, Xiaochun Cao,\nand Haifeng Shen. Single image super-resolution via a\nholistic attention network. In European Conference on\nComputer Vision, pages 191–207. Springer, 2020. 7\n[56] Dongwon Park, Dong Un Kang, and Se Young Chun. Blur\nmore to deblur better: Multi-blur2deblur for efﬁcient video\ndeblurring. arXiv preprint arXiv:2012.12507, 2020. 10\n[57] Dongwon Park, Dong Un Kang, Jisoo Kim, and Se Young\nChun. Multi-temporal recurrent neural networks for pro-\ngressive non-uniform single image deblurring with incre-\nmental temporal training. InEuropean Conference on Com-\nputer Vision, pages 327–343. Springer, 2020. 10\n[58] Kuldeep Purohit and AN Rajagopalan. Region-adaptive\ndense network for efﬁcient motion deblurring. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence, vol-\nume 34, pages 11882–11889, 2020. 10\n[59] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by genera-\ntive pre-training, 2018. 2\n[60] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. OpenAI blog , 1(8):9,\n2019. 2\n[61] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J Liu. Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer. Journal of Machine\nLearning Research, 21(140):1–67, 2020. 2\n[62] Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu,\nand Deyu Meng. Progressive image deraining networks: A\nbetter and simpler baseline. In Proceedings of the IEEE\nconference on computer vision and pattern recognition ,\npages 3937–3946, 2019. 2, 8\n[63] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 2\n[64] Maitreya Suin, Kuldeep Purohit, and AN Rajagopalan.\nSpatially-attentive patch-hierarchical network for adaptive\nmotion deblurring. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3606–3615, 2020. 10\n[65] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu.\nMemnet: A persistent memory network for image restora-\ntion. In Proceedings of the IEEE international conference\non computer vision, pages 4539–4547, 2017. 6, 7, 8\n[66] Hao Tan and Mohit Bansal. Lxmert: Learning cross-\nmodality encoder representations from transformers. arXiv\npreprint arXiv:1908.07490, 2019. 2\n[67] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Ji-\naya Jia. Scale-recurrent network for deep image deblurring.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 8174–8182, 2018. 2, 10\n[68] Chunwei Tian, Yong Xu, Zuoyong Li, Wangmeng Zuo,\nLunke Fei, and Hong Liu. Attention-guided cnn for image\ndenoising. Neural Networks, 124:117–129, 2020. 2\n[69] Fu-Jen Tsai, Yan-Tsung Peng, Yen-Yu Lin, Chung-Chi\nTsai, and Chia-Wen Lin. Banet: Blur-aware attention\nnetworks for dynamic scene deblurring. arXiv preprint\narXiv:2101.07518, 2021. 10\n[70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Ad-\nvances in neural information processing systems , pages\n5998–6008, 2017. 2, 4\n[71] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng\nLi, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.\nResidual attention network for image classiﬁcation. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pages 3156–3164, 2017. 2\n[72] Hong Wang, Qi Xie, Qian Zhao, and Deyu Meng. A model-\ndriven deep neural network for single image rain removal.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 3103–3112, 2020. 7,\n8\n[73] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang\nLi, Derek F Wong, and Lidia S Chao. Learning deep\ntransformer models for machine translation. arXiv preprint\narXiv:1906.01787, 2019. 2\n[74] Tianyu Wang, Xin Yang, Ke Xu, Shaozhe Chen, Qiang\nZhang, and Rynson WH Lau. Spatial attentive single-image\nderaining with a high quality real rain dataset. In Proceed-\nings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 12270–12279, 2019. 2, 8\n[75] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794–7803, 2018. 3\n[76] Wei Wei, Deyu Meng, Qian Zhao, Zongben Xu, and Ying\nWu. Semi-supervised transfer learning for image rain re-\nmoval. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 3877–3886,\n2019. 8\n[77] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan,\nPeizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer, and\nPeter Vajda. Visual transformers: Token-based image rep-\nresentation and processing for computer vision. arXiv\npreprint arXiv:2006.03677, 2020. 3\n[78] Wenhan Yang, Jiaying Liu, Shuai Yang, and Zongming\nGuo. Scale-free single image deraining via visibility-\nenhanced recurrent wavelet learning. IEEE Transactions\non Image Processing, 28(6):2948–2961, 2019. 2\n[79] Wenhan Yang, Robby T Tan, Jiashi Feng, Zongming Guo,\nShuicheng Yan, and Jiaying Liu. Joint rain detection and\nremoval from a single image with contextualized deep net-\nworks. IEEE transactions on pattern analysis and machine\nintelligence, 42(6):1377–1393, 2019. 5, 6, 8\n[80] Xitong Yang, Zheng Xu, and Jiebo Luo. Towards percep-\ntual image dehazing by physics-based disentanglement and\nadversarial training. In AAAI, pages 7485–7492, 2018. 2\n[81] Yuan Yuan, Wei Su, and Dandan Ma. Efﬁcient dynamic\nscene deblurring using spatially variant deconvolution net-\nwork with optical ﬂow guided training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3555–3564, 2020. 10\n[82] Yuhui Yuan and Jingdong Wang. Ocnet: Object context net-\nwork for scene parsing. arXiv preprint arXiv:1809.00916,\n2018. 3\n[83] Yongnian Zeng, Wei Huang, Maoguo Liu, Honghui Zhang,\nand Bin Zou. Fusion of satellite images in urban area: As-\nsessing the quality of resulting images. In 2010 18th Inter-\nnational Conference on Geoinformatics, pages 1–4. IEEE,\n2010. 1\n[84] Hongguang Zhang, Yuchao Dai, Hongdong Li, and Piotr\nKoniusz. Deep stacked hierarchical multi-patch network for\nimage deblurring. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pages\n5978–5986, 2019. 10\n[85] He Zhang and Vishal M Patel. Densely connected pyramid\ndehazing network. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages 3194–\n3203, 2018. 2\n[86] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn\nStenger, Wei Liu, and Hongdong Li. Deblurring by realis-\ntic blurring. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2737–\n2746, 2020. 10\n[87] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and\nLei Zhang. Beyond a gaussian denoiser: Residual learning\nof deep cnn for image denoising. IEEE Transactions on\nImage Processing, 26(7):3142–3155, 2017. 6, 7, 8\n[88] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.\nLearning deep cnn denoiser prior for image restoration. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 3929–3938, 2017. 6, 7, 8\n[89] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet:\nToward a fast and ﬂexible solution for cnn-based im-\nage denoising. IEEE Transactions on Image Processing ,\n27(9):4608–4622, 2018. 6, 7, 8\n[90] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet:\nToward a fast and ﬂexible solution for cnn-based im-\nage denoising. IEEE Transactions on Image Processing ,\n27(9):4608–4622, 2018. 6\n[91] Songyang Zhang, Xuming He, and Shipeng Yan. Latent-\ngnn: Learning efﬁcient non-local relations for visual recog-\nnition. In International Conference on Machine Learning,\npages 7374–7383, 2019. 3\n[92] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng\nZhong, and Yun Fu. Image super-resolution using very deep\nresidual channel attention networks. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV) , pages\n286–301, 2018. 2, 7\n[93] Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and\nYun Fu. Residual non-local attention networks for image\nrestoration. arXiv preprint arXiv:1903.10082, 2019. 6, 7\n[94] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong,\nand Yun Fu. Residual dense network for image super-\nresolution. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition , pages 2472–2481,\n2018. 6, 7\n[95] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and\nYun Fu. Residual dense network for image restoration.\nIEEE Transactions on Pattern Analysis and Machine Intel-\nligence, 2020. 7, 8\n[96] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor-\ning self-attention for image recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10076–10085, 2020. 3\n[97] Jia-Xing Zhao, Yang Cao, Deng-Ping Fan, Ming-Ming\nCheng, Xuan-Yi Li, and Le Zhang. Contrast prior and ﬂuid\npyramid integration for rgbd salient object detection. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3927–3936, 2019. 1\n[98] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao,\nJufeng Yang, and Ming-Ming Cheng. Egnet: Edge guid-\nance network for salient object detection. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, pages 8779–8788, 2019. 1\n[99] Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, and\nChen Change Loy. Cross-scale internal graph neural net-\nwork for image super-resolution. Advances in Neural In-\nformation Processing Systems, 33, 2020. 6, 7\n[100] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 3"
  },
  {
    "source": "131805D.pdf",
    "content": "Integrating 3D convolutional neural networks and transformer for \nvideo action recognition \nYan Cheng*, Lingfeng Wan \nCollege of Computer Information Engineering, Jiangxi Normal University, Nanchang 330022, China \n* chyan88888@jxnu.edu.cn \nABSTRACT \nIn the field of video action recognition, the challenge of effi ciently extracting video features while ensuring \ncomputational efficiency has been addressed in our study. We propose a novel video action recognition model named 3D \nResNet-Transformer that integrates 3D ResNet (Residual Networks) with Transformer architecture. Utilizing 3D ResNet \nas the foundation, our model effectively captures spatial featu res of videos through its deep network structure. \nAdditionally, the integration of Transformer encoding layers en hances temporal-spatial correlations between video \nfeatures via its self-attention mechanism, thereby improving recognition accuracy. Our design synergizes the strengths of \n3D ResNet and Transformer, combining their powerful capabilitie s effectively. Experimental results on standard video \naction recognition datasets, HMDB51 and UCF101, demonstrate sup erior performance of o ur model, with accuracy \nimprovements of 3.4% and 0.4% over baseline models, achieving T OP-1 accuracies of 82.1% a nd 97.4%, respectively. \nThis research validates the effectiveness and innovation of our  integrated 3D ResNet and Transformer model in \nenhancing video recognition accuracy. \nKeywords: video action recognition,3D ResNet, Transformer encoding layer, self-attention mechanism  \n1. INTRODUCTION \nVideo action recognition is an important research topic in the field of computer vision, whose core task is to \nautomatically parse and understand the actions in videos throug h algorithms, and carry out corresponding tasks such as \nclassification, detection, and tracking. Images or videos gener ated by RGB cameras aim to mimic the human eye's \nperception of real scenes, thus capturing rich visual informati on to reflect the context of the recorded environment. At \nthe same time, RGB-based human action recognition has wide appl ications in many fields, such as classroom video \nsurveillance[1], video retrieval[2], and human-computer interaction[3]. \nIn recent years, with the rapid development of deep learning te chnologies, a series of deep learning-based action \nrecognition models have been proposed, such as C3D [4], I3D[5], SlowFast[6], and VideoSwin [7]. Although Convolutional \nNeural Networks (CNNs) excel in modeling spatial information, their ability to model temporal sequences is lacking. On \nthe other hand, using Recurrent Neural Networks, especially LSTM[8], to model the temporal information of videos faces \nissues such as high computational costs and difficulty in paral lel computing. Therefore, how to effectively extract \nspatiotemporal information from videos while reducing compu tational costs has become a key issue in video action \nrecognition. \nTo address the aforementioned issues, we propose a 3D ResNet-Transformer video action recognition model. This model \nleverages the 3D ResNet network to extract video features and s ignificantly enhances the correlation between video \nfeatures through the self-attention mechanism of the Transforme r encoding layer, providing valuable improvements and \nattempts in video action recognition methods. \nThe main contributions of this paper are as follows: \ni. We propose a video action recognition model that integrates the  3D ResNet network with Transformer encoding \nlayers. The model utilizes the efficient performance of the 3D ResNet network in processing spatial and temporal \nfeatures of videos while enhancing the correlation between vide o features through the sel f-attention mechanism of \nthe Transformer encoding layer. \nii. Extensive experiments were conducted on the action recognition datasets UCF101 and HMDB51. The experimental \nresults show that, compared to related baseline models, the mod el proposed in this paper achieves improvements in \nTOP-1 accuracy. \nInternational Conference on Image, Signal Processing, and Pattern Recognition (ISPP 2024), \nedited by Ram Bilas Pachori, Lei Chen, Proc. of SPIE Vol. 13180, 131805D \n© 2024 SPIE · 0277-786X · doi: 10.1117/12.3033760\nProc. of SPIE Vol. 13180  131805D-1\n \n2. RELATED WORK \nBehavior recognition is one of the important research areas in the field of computer vision. In recent years, with the rapid \ndevelopment of deep learning technologies, various deep learnin g architectures have also emerged. As a classic \ndual-stream framework, Simonyan and Zisserman [9] proposed a dual-stream CNN model consisting of a spatial networ k \nand a temporal network, these two streams learn the appearance and motion features of actions, finally, the classification \nscores of these two streams are fused to obtain the final class ification result. However, such architectures still have \nlimitations in modeling temporal information, especially in dea ling with long-term dependen cies. Recurrent Neural \nNetworks (RNN) are suitable for analyzing sequential data, as t here are recurrent connections in their hidden layers. \nHowever, traditional RNNs have the problem of vanishing gradien ts, making it difficult to effectively model long-term \ntemporal dependencies. Therefore, most existing methods have ad opted gated RNN architectures, such as Long \nShort-Term Memory (LSTM) [8], to model the long-term temporal dynamics in video sequences. Some studies[10] have \nextended 2D CNNs to 3D structures to model both spatial and tem poral context information in videos, which is crucial \nfor video behavior recognition. Methods for behavior recognitio n based on three-dimensional convolutional neural \nnetworks typically use window-based three-dimensional convoluti on operations to process spatio-temporal information \nwithin a limited time interval,  where each convolution operatio n only processes shorter context content in videos. The \nscalable self-attention mechanism in Transformer structures effectively learns long-distance spatio-temporal relationships \nin videos. Therefore, many recent studies have also begun to ex plore behavior recognition methods based on \ntransformers in RGB videos. Transformer [11] is an innovative deep learning model that has achieved notable results in the \nfield of deep learning in recent years, thanks to its powerful performance and broad application prospects. Transformer \nconsists of encoders and decoders. The encoder is mainly compos ed of multiple self-attention modules, used for \nencoding input sequences. The decoder has a structure similar to the encoder, but an additional encoder-decoder attention \nmechanism is added in each module. This design makes the Transf ormer excel in modeling long-term dependencies, \nmultimodal fusion, and multitasking [12]. To combine the advantages of 3D convolutional neural networks  a n d  \nTransformers, we propose a new solution: the fusion of 3D ResNe t-Transformer for video behavior recognition. In the \nfollowing sections, we will detail our model. \n3. MODEL \nWe propose a hybrid model combining 3D ResNet and Transformer f or video behavior recognition, aiming to leverage \nthe complementary strengths of b oth architectures. This mixed a pproach utilizes the robust spatio-temporal feature \ncapturing capability of 3D ResNet, along with the Transformer's  efficient self-attention mechanism for handling \nlong-distance dependencies. Specifically, the model integrates 3D ResNet-34 and T ransformer encoding layers. It first \nextracts spatio-temporal features  from videos using 3D ResNet-3 4, then further enhances these features through 3D \naverage pooling, before feeding them into the Transformer encod ing layer for behavior classification. In the pre-training \nphase, we employ models pre-trained on the large-scale dataset IG-65M, endowing our model with the ability to \nrecognize and extract features from the outset. The model diagram is shown in Figure 1: \n \nFigure 1. 3D ResNet-transformer model diagram. \nProc. of SPIE Vol. 13180  131805D-2\n \n3.1 Task description \nIn this study, we define the video behavior recognition task as the problem of predicting specific behaviors contained in a \nseries of videos. Specifically, given a video dataset 𝐷 ൌ  ሼሺ𝑥ଵ,𝑦ଵሻ, ሺ𝑥ଶ,𝑦 ଶሻ ,...,ሺ 𝑥௡,𝑦௡ሻሽ，where 𝑛 represents the total \nnumber of videos in the dataset, each  𝑥௜ represents a video segment containing a time series, and 𝑦௜ represents the \nbehavior category corresponding to that video segment. Our goal , through our model  𝑔，is to accurately predict the \nbehavior category 𝑦 of an input video segment 𝑥，i.e., 𝑔ሺ𝑥ሻ  ൌ  𝑦. \n3.2 Spatiotemporal feature extraction with 3D ResNet \n 3D ResNet-34 effectively understands video content through spat iotemporal feature extraction and residual network \ndesign. The model employs 3D convolutional layers to extract in itial spatiotemporal features from input data \n(N,3,64,112,112), gradually transforming to (N,512,8,7,7). Thro ughout this process, the issue of vanishing gradients in \ndeep model training is addressed via skip connections within re sidual blocks, while preserving important features. This \nstructure not only deepens the ne twork, enhancing the complexit y and richness of features, but also enables precise \nspatiotemporal analysis, allowing the model to capture dynamic changes and spatial details in videos, thus effectively \nsupporting complex tasks such as video behavior understanding. \n3.3 Transformer encoding layer \nIn the video recognition process based on the 3D ResNet-Transfo rmer model, original video features are first extracted \nthrough the 3D ResNet-34 module, generating a five-dimensional tensor X with dimensions (N,512,8,7,7), where N \nrepresents the number of video sequences in a batch. Subsequent ly, the dimensions of the tensor are simplified to \n(N,512,8,1,1) through 3D average pooling. Further, these features are transformed into a new tensor Y in the Transformer \nencoding layer, with dimensions  ሺ𝑁, 𝐶𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑐𝑎𝑡𝑖𝑜𝑛_𝑁𝑢𝑚ሻ, where 𝐶𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑐𝑎𝑡𝑖𝑜𝑛_𝑁𝑢𝑚 represents the total number \nof classifications. During this process, the self-attention mec hanism plays a core role by updating features through \ncalculating the relative weights between features, where Q, K, V represent Query, Key, a nd Value, respectively. As \nshown in Equationሺ1ሻ \n                                      𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛ሺ𝑄, 𝐾, 𝑉ሻ ൌ 𝑠𝑜𝑓𝑡𝑚𝑎𝑥 ቆ𝑄𝐾்\nඥ𝑑௞\nቇ 𝑉                                                                        ሺ1ሻ    \nTo adapt to the encoding layer, video features are divided into  fixed-size sequences, with each sequence's relative \nposition in the time series preserved through positional encodi ng, allowing the model to globally perceive video content \nand extract complex and rich behavioral features. This feature transformation strategy significantly improves the model's \nperformance in video recognition tasks, enhancing accuracy and robustness. \n3.4 Loss Function  \n                                          𝐿 ൌെ ෍𝑦 ௜\n௄\n𝑖 ൌ1\nlog 𝑦𝑖\n′                                                                                         ሺ2ሻ   \nEquation (2) shows the loss function, where 𝐿 represents the loss, 𝑦௜ denotes the true sample value, 𝑦௜\nᇱ  signifies the \npredicted sample value,  and 𝐾 stands for the number of classes. Finally, 𝑠𝑜𝑓𝑡𝑚𝑎𝑥 is utilized as the classifier. The \nmodel's final output vector 𝐶  is inputted into the 𝑠𝑜𝑓𝑡𝑚𝑎𝑥 classifier to obtain the classification result, as depicted in  \nEquation ሺ3ሻ \n                                                                        𝑦ᇱ ൌ 𝑠𝑜𝑓𝑡𝑚𝑎𝑥ሺ𝐶ሻ                                                                                           ሺ3ሻ  \n4. EXPERIMENTS \n4.1 Dataset \nTo evaluate the performance of the behavior recognition algorit hm of the 3D ResNet-Transformer model based on \nadaptive loss scheduling proposed in this paper, we conducted e xperiments on two public behavior recognition \nbenchmark datasets: UCF101 and HMDB51. These two datasets cover  a variety of behavior recognition scenarios, \ncontaining different numbers of videos and different categories  of behaviors, providing a rich sample for our evaluation. \nDetailed descriptions of the datasets are as follows, and dataset information is shown in Table 1. \nProc. of SPIE Vol. 13180  131805D-3\n \nTable 1. Dataset information. \nDataset Categories Training Set Test Set \nUCF101 101 9537 3783 \nHMDB51 51 3570 1530 \n4.2 Experimental results \nWe selected accuracy as the main evaluation metric for the model, with its calculation formula as follows: \n                                                              𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 ൌ       𝑇௖௢௥௥௘௖௧\n𝑇௧௢௧௔௟\n ൈ 100%                                                                             ሺ4ሻ  \nWhere 𝑇௖௢௥௥௘௖௧ represents the number of samples correctly classified in the t est set, and 𝑇௧௢௧௔௟ represents the total \nnumber of samples in the test set. We conducted experimental co mparisons of our proposed 3D ResNet-Transformer \nmodel on two public datasets, UCF-101 and HMDB-51, against vari ous baseline models (see Table 2 for details). In \nsummary, the 3D ResNet-Transformer model demonstrated superior performance on both UCF-101 and HMDB-51 \ndatasets compared to other models, effectively proving the effectiveness and superiority of our model.  \nTable 2. Comparison of accuracy (%) of methods on UCF-101 and HMDB-51 datasets. \nGroup Comparison Method UCF-101 HMDB-51 \nTwo stream Network Two-Stream CNN [9]  88.0 59.4 \nRNN RNN-FV+iDT [13] 94.1 67.7 \n3D CNN STC-ResNext 101[14] 96.5 72.7 \nD3D[15] 97.0 78.7 \nTransformer V i d T r[16] 96.7 74.4 \n3D CNN+ Transformer 3D ResNet-Transformer（Ours） 97.4 82.1 \nThe experimental results of the comparison models are from the corresponding referenced literature. \n4.3 Hyperparameter selection: number of heads in transformer multi-head attention \nAccording to our experimental results (see Figures 2), the numb er of heads in multi-head attention significantly impacts \nthe model's performance. On the UCF101 and HMDB51 datasets, we adjusted the number of heads in multi-head \nattention, including 1, 2, 4, 8, and 16. The results show that as the number of heads increases, the model's classification \naccuracy also improves. However, w hen the number of heads reach es 8, the performance of the model peaks on both \ndatasets, with accuracy rates r eaching 97.4% and 82.1%, respect ively. But when we further increased the number of \nheads to 16, the classificatio n accuracy actually decreased. Th ese results suggest that using 8 attention heads is the \noptimal choice for this task. This might be because more heads can capture more feature correlations, but beyond a \ncertain number, it may introduce some unnecessary information, leading to performance degradation.  \n \nFigure 2. Performance of selecting different numbers of attention heads on UCF101 and HMDB51. \nProc. of SPIE Vol. 13180  131805D-4\n \n4.4 Ablation experiment \nFrom the results shown in Table 3, it is evident that the 3D ResNet component significantly impacts model performance. \nSpecifically, on the UCF101 and HMDB51 datasets, models that in corporate the 3D ResNet component demonstrate a \nsubstantial increase in accuracy compared to models that rely s olely on the Transformer architecture. This outcome \nfurther validates the effectiveness of integrating the 3D ResNet component. \nTable 3. Impact of 3D ResNet components on Acc1 (%). \ncomponent UCF101 HMDB51 \nTransformer 96.3 80.6 \n3D ResNet-Transformer 97.4 82.1 \n5. CONCLUSION \nIn addressing the challenges of complex feature extraction and computational efficiency in video action recognition tasks, \nwe have designed and validated a  model named 3D ResNet-Transfor mer. This model integrates the 3D ResNet network \nwith Transformer encoding layers, achieving strong spatial and temporal feature extraction capabilities through the \nincorporation of 3D ResNet convolutions. With the addition of t he Transformer encoding layers, the model further \nrefines these features and enhances the inter-feature relationships. Experiments conducted on the HMDB51 and UCF101 \ndatasets have confirmed the superiority of our model over basel ine models, with specific improvements in TOP-1 \naccuracy of 3.4% and 0.4%, respectively. \nOverall, this research offers a valuable solution for video act ion recognition tasks. Looking forward, we plan to continue \noptimizing the model and exploring more advanced self-attention  mechanisms, aiming to further enhance the model's \ngeneralizability and robustness. Such improvements will make th e model more adaptable to handling various complex \nscenes and tasks. \nREFERENCES \n[1] Lin W, Sun M T, Poovandran R, et al. Human activity recognition  for video surveillance[C]//2008 IEEE \nInternational Symposium on Circuits and Systems (ISCAS). IEEE, 2008: 2737-2740. \n[2] Hu W, Xie D, Fu Z, et al. Semantic-based surveillance video ret rieval[J]. IEEE Transactions on image \nprocessing, 2007, 16(4): 1168-1181. \n[3] Rodomagoulakis I, Kardaris N, Pitsikalis V, et al. Multimodal h uman action recognition in assistive \nhuman-robot interaction[C]//2016  IEEE international conference on acoustics, speech and signal processing \n(ICASSP). IEEE, 2016: 2702-2706. \n[4] Tran D, Bourdev L, Fergus R, et al. Learning spatiotemporal fea tures with 3d convolutional \nnetworks[C]//Proceedings of the IEEE international conference on computer vision. 2015: 4489-4497. \n[5] Carreira J, Zisserman A. Quo vadis, action recognition? a new m odel and the kinetics dataset[C]//proceedings \nof the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 6299-6308. \n[6] Feichtenhofer C, Fan H, Malik J, et al. Slowfast networks for v ideo recognition[C]//Proceedings of the \nIEEE/CVF international conference on computer vision. 2019: 6202-6211. \n[7] Liu Z, Ning J, Cao Y, et al. V ideo swin transformer[C]//Proceedings of the IEEE/CVF conference on computer \nvision and pattern recognition. 2022: 3202-3211. \n[8] Sun L, Jia K, Chen K, et al. Lattice long short-term memory for  human action recognition[C]//Proceedings of \nthe IEEE international conference on computer vision. 2017: 2147-2156. \n[9] Simonyan K, Zisserman A. Two-stream convolutional networks for action recognition in videos[J]. Advances \nin neural information processing systems, 2014, 27. \n[10] Li X, Shuai B, Tighe J. Directional temporal modeling for actio n recognition[C]//Computer Vision–ECCV \n2020: 16th European Conference, Glasgow, UK, August 23–28, 2020 , Proceedings, Part VI 16. Springer \nInternational Publishing, 2020: 275-291. \n[11] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing \nsystems, 2017, 30. \nProc. of SPIE Vol. 13180  131805D-5\n \n[12] Han K, Wang Y, Chen H, et al. A survey on vision transformer[J] . IEEE transactions on pattern analysis and \nmachine intelligence, 2022, 45(1): 87-110. \n[13] Lev G, Sadeh G, Klein B, et al. Rnn fisher vectors for action r ecognition and image annotation[C]//Computer \nVision–ECCV 2016: 14th European Conference, Amsterdam, The Neth erlands, October 11-14, 2016, \nProceedings, Part VI 14. Springer International Publishing, 2016: 833-850. \n[14] Li J, Liu X, Zhang M, et al. Spa tio-temporal deformable 3d conv nets with attention for action recognition[J]. \nPattern Recognition, 2020, 98: 107037. \n[15] Stroud J, Ross D, Sun C, et al. D3d: Distilled 3d networks for video action recognition[C]//Proceedings of the \nIEEE/CVF Winter Conference on Applications of Computer Vision. 2020: 625-634. \n[16] Y. Zhang, X. Li, C. Liu, B. Shuai, Y. Zhu, B. Brattoli, H. Chen ,  I .  M a r s i c ,  a n d  J .  T i g h e ,  “ V i d t r :  V i d e o  \ntransformer without convolutions,” in ICCV, 2021. \nProc. of SPIE Vol. 13180  131805D-6"
  },
  {
    "source": "Lee_BAAM_Monocular_3D_Pose_and_Shape_Reconstruction_With_Bi-Contextual_Attention_CVPR_2023_paper.pdf",
    "content": "BAAM: Monocular 3D pose and shape reconstruction with bi-contextual\nattention module and attention-guided modeling\nHyo-Jun Lee1† Hanul Kim3† Su-Min Choi2 Seong-Gyun Jeong2 Yeong Jun Koh1∗\n1Chungnam National University 242dot Inc.\n3Seoul National University of Science and Technology\ngywns6287@gmail.com, hukim@seoultech.ac.kr, sumin.choi@42dot.ai,\nseonggyun.jeong@42dot.ai, yjkoh@cnu.ac.kr\nAbstract\n3D trafﬁc scene comprises various 3D information about\ncar objects, including their pose and shape. However,\nmost recent studies pay relatively less attention to recon-\nstructing detailed shapes. Furthermore, most of them treat\neach 3D object as an independent one, resulting in losses\nof relative context inter-objects and scene context reﬂect-\ning road circumstances. A novel monocular 3D pose and\nshape reconstruction algorithm, based on bi-contextual at-\ntention and attention-guided modeling (BAAM), is proposed\nin this work. First, given 2D primitives, we reconstruct\n3D object shape based on attention-guided modeling that\nconsiders the relevance between detected objects and ve-\nhicle shape priors. Next, we estimate 3D object pose\nthrough bi-contextual attention, which leverages relation-\ncontext inter objects and scene-context between an object\nand road environment. Finally, we propose a 3D non-\nmaximum suppression algorithm to eliminate spurious ob-\njects based on their Bird-Eye-View distance. Extensive\nexperiments demonstrate that the proposed BAAM yields\nstate-of-the-art performance on ApolloCar3D. Also, they\nshow that the proposed BAAM can be plugged into any\nmature monocular 3D object detector on KITTI and sig-\nniﬁcantly boost their performance. Code is available at\nhttps://github.com/gywns6287/BAAM.\n1. Introduction\n3D trafﬁc scene understanding provides enriched de-\nscriptions of the dynamic objects, e.g., 3D shape, pose,\nand location, compared to representing objects as bounding\nboxes. 3D visual perception is crucial for the autonomous\ndriving system to develop downstream tasks such as mo-\ntion prediction and planning, and aids to faithfully recon-\n∗Corresponding author\n†These authors contributed equally to this work.\n3D SceneInput image\nFigure 1. Reconstructed 3D scene with rough bounding box (right\nup) and with detailed shape (right down). For better 3D recon-\nstruction, detailed 3D shapes are needed rather than the simple 3D\nbounding boxes.\nstruct the trafﬁc scene from recorded data. To acquire pre-\ncise 3D information, some prior arts have relied on speciﬁc\ndevices such as LiDAR [3,10,42] and stereo vision [26,44].\nHowever, as the system becomes complex and costly, it\nquickly reaches the limit to scalability. To contrary, areas\nof study about 3D perception using monocular vision have\nbeen receiving attention due to its simplicity and cost efﬁ-\nciency [4, 7, 19, 29, 33, 50, 51].\nMonocular 3D perception is an ill-posed problem in that\nprojective geometry inherently loses depth information. In\nparticular, trafﬁc scene contains partially observable ob-\njects, and shows ﬁne-grained classes which are visually\nconfusing. Pseudo-LiDAR [49] presents a feasible solution\nof the image based 3D object detection. To reconstruct 3D\nposes of the objects, many studies [25, 27, 30, 33, 40, 41, 50,\n51] focus on using geometry constraints between 2D and\n3D. Yet, it is less studied in the line of research that leverage\nrelative context among the objects and global scene context\ndepending on road environment.\nFigure 1 compares the reconstructed 3D scene with 3D\nbounding boxes and detailed 3D shapes. With a detailed 3D\nshape, we render the trafﬁc scene in realistic and provide\nintuitive representations of the objects. Despite scale ambi-\nguity of the monocular 3D perception, 3D mesh provides a\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n9011\n\nstrong clue to align instances’ scales and orientations. Con-\ncurrently, there have been many attempts [8, 20, 22, 31, 45,\n46] to reconstruct the 3D shape of human objects. These\nmethods mainly focus on learning PCA-basis to represent\nhuman shapes. Inspired by human shape reconstruction,\nrecent methods [21, 24] also design PCA-basis for vehicle\nshape reconstruction. However, as pointed out in [1, 34],\nPCA-basis often loses object details and thus leads to un-\nsatisfactory reconstruction.\nIn this work, we propose a novel 3D pose and shape\nestimation algorithm, utilizing bi-contextual attention and\nattention-guided modeling (BAAM). Given a monocular\nRGB image, the proposed BAAM ﬁrst extracts various 2D\nprimitive features such as appearance, position, and size.\nAnd it constructs object features to embed internal object\nstructures by aggregating primitive features. For detailed\nobject shapes, we introduce shape priors consisting of the\nmean shape and various template offsets to represent de-\ntails of vehicle shapes. Then, BAAM reconstructs objects’\n3D shapes as mesh structures with attention-guided mod-\neling, which combines shape prior and individual object\nfeatures based on their relevance. For accurate pose es-\ntimation, we present the notion of bi-contextual attention\nconsisting of relation-context and scene-context, which de-\nscribe the relationship inter objects and between object and\nroad environment, respectively. Based on this rich infor-\nmation, BAAM integrates object features to predict ob-\njects’ 3D poses through a carefully designed bi-contextual\nattention module. Finally, we proposed a novel 3D non-\nmaximum suppression (NMS) algorithm that effectually re-\nmoves spurious objects based on Bird-Eye-view (BEV) ge-\nometry. Extensive experiments on Apollocar3D [43] and\nKITTI [12] datasets demonstrate the effectiveness of the\nproposed BAAM algorithm. Also, experiments show that\nthe proposed method signiﬁcantly outperforms state-of-the\narts [21, 43] in both pose and shape estimation. The main\ncontributions of our work are four folds:\n•We propose the attention-guided modeling that recon-\nstructs objects’ shapes based on the relevance between\nobjects and vehicle shape priors.\n•We proposed the bi-contextual attention module that\nestimates objects’ pose by exploiting relation-context\ninter objects and scene-context between an object and\nroad environment.\n•We also develop the novel 3D non-maximum suppres-\nsion algorithm to remove spurious objects based on\ntheir Bird-Eye-view distance.\n•The proposed BAAM algorithm achieves the state-of\nthe art performance on ApolloCar3D [43]. Also, ex-\nperiments on KITTI [12] show that the proposed al-\ngorithm can signiﬁcantly improve the performance of\nexisting monocular 3D detectors.\n2. Related Work\nMonocular 3D object detection.Monocular 3D object de-\ntection aims to estimate 3D bounding boxes of objects in a\ngiven image. Existing monocular 3D object detection meth-\nods are roughly categorized into depth-assisted and image-\nonly methods. The depth-assisted approach uses a pixel-\nwise depth map to aid 3D object detection by training a\nmonocular depth estimator. Pseudo-LiDAR [36, 49] meth-\nods transform estimated depth maps into 3D point clouds\nand feed them into the existing LiDAR-based 3D detec-\ntors. PatchNet [35] takes advantage of CNNs by represent-\ning transformed 3D information in images. DDMP-3D de-\nsigns the message passing block to transfer depth informa-\ntion to 3D detectors. DD3D [39] pre-train depth estimator\nand ﬁne-tune it for 3D object detection. In [40], DID-M3D\ndecouples object depth into visual and attribute depth. Al-\nthough these depth-assisted methods [35,36,39,40,49] have\nthe advantage of improving 3D detection quality, they have\nlimitations in that they require additional information.\nDue to the lack of depth information, many image-\nonly methods focus on exploiting geometry priors.\nDeep3DBox [38] introduces MultiBin loss for rotation es-\ntimation and solves the translation by geometrical relation-\nship between 2D-3D boxes. GUPNet [33] combines ge-\nometry priors and uncertainty modeling to infer depth dis-\ntribution. MonoFlex [50] decouples truncated objects and\nformulates the depth estimation as an uncertainty-guided\ndepth ensemble. MonoGround [41] introduced the notion\nof ground plane to convert the ill-posed 2D to 3D mapping\ninto a well-posed problem with a unique solution. In [51],\nZhang et al. developed the dimension-aware embedding for\nmore accurate geometric constraints. However, these meth-\nods are limited in that they do not fully consider rich context\nin monocular images, which gives additional cues for depth\nestimation. In contrast, MonoPair [7] adopts the pair-wise\nrelationship between neighboring objects to post-optimize\nobject translation. In [14], Gu et al. presented the homogra-\nphy loss to constrain mutual locations of objects in the 3D\nscene. Both methods concentrate on the geometrical asso-\nciation between predicted objects. On the contrary, the pro-\nposed BAAM algorithm considers object relation in feature\nspace, which implies various cues for 3D pose.\nMonocular 3D pose and shape reconstruction.Joint 3D\npose and shape regression has been actively studied for hu-\nman objects [8, 20, 22, 31, 45, 46]. Inspired by human shape\nreconstruction, there have been many attempts to restore\nvehicle shape for 3D trafﬁc scene understanding. Deep-\nMANTA [5] adopts a coarse-to-ﬁne retrieval strategy to re-\nconstruct pose and skeleton shape. 3D-RCNN [24] and Roi-\n10D [37] render 3D shapes as PCA parameters, which are\ndecoded to coarse voxel shape representation. In [43], the\ndirect-based approach extends 3D-RCNN to utilize atten-\ntion mask and offset ﬂow. GSNet [21] presents a divide-\n9012\n\nFigure 2. Overview of the proposed BAAM. The input image is sent to the network to extract box features, 2D box, and keypoints. Given,\nthe informative 2D representation, BAAM estimates the shapes using attention-guided modeling. 3D rotation is directly regressed by the\nfully connected layers and 3D translation is faithfully predicted through the bi-contextual attention module.\nand-conquer strategy, which generates 3D shapes by blend-\ning multiple meshes from different PCA-basis. However,\nthe PCA-basis often lose the details of object shape [1, 34].\nTo overcome these limitations, we construct shape priors\nto hold these details. We then propose an attention-guided\nmodeling, which can reconstruct object shapes adopting rel-\nevance between object and shape prior.\n3. Methods\n3.1. Problem Statement\nSuppose that there exist nobjects in an RGB image. We\ndeﬁne each object pose as 3D translation pt = {x,y,z }\nand 3D rotation pr = {α,β,γ }. Also, we represent an ob-\nject shape as 3D mesh m ∈R3v in [21], where v is the\nnumber of vertices, and each vertex has its 3D coordinates.\nTherefore, a monocular 3D pose and shape reconstruction\naims to estimate all objects’ translation Pt ∈Rn×3, rota-\ntion Pr ∈Rn×3, and their shape M ∈Rn×3v.\n3.2. Model Overview\nFigure 2 illustrates the overall architecture of BAAM.\nThe proposed BAAM extends Mask R-CNN [16] to extract\n2D primitive features: bounding box (region of interest,\nRoI), bounding box feature, keypoints, and keypoint visi-\nbility. Given primitive 2D features, it ﬁrst conveys visibil-\nity information to keypoints. Then, it concatenates three\ntypes of features to construct object features for 3D pose\nand shape estimation. Also, it extracts the global feature to\nencode scene context indicating the road environment. Af-\nter that, the proposed BAAM predicts objects’ poses and\nshapes based on global and object features. First, it uses\nattention-guided modeling (AGM) which leverages the rel-\nevance between objects and the shape prior about vehicles\nto estimate object shapes. Second, it faithfully predicts ob-\nject translation through bi-contextual attention (BCA) in-\ncluding relation-aware attention between each object and\nscene-aware attention between objects and the road scene.\nThird, it takes object features to regress their rotation.\n3.3. 2D Feature Construction\nObject feature.A bounding box denotes object center co-\nordinates (bx,by) and its width bw and height bh in image\nspace. Since image formation is closely related to camera\nparameters [15], it is essential to aggregate camera informa-\ntion into bounding boxes for 3D pose and shape estimation.\nHence, we transform bx, by, bw, bh from image space to ˜bx,\n˜by, ˜bw, ˜bh in camera space:\n˜bx = bx −cx\nfx\n, ˜by = by −cy\nfy\n, ˜bw = bw\nfx\n, ˜bh = bh\nfy\n(1)\nwhere fx, fy are camera focal lengths and cx, cy are cam-\nera principal points. We then feed the transformed boxes\ninto two fully-connected layers. A bounding box feature,\nobtained using RoIAlign [16], contains useful information\nabout object appearance. We further process this feature\nusing three convolution operations to reduce its spatial res-\nolution for the object feature construction.\nWe estimate keypoint coordinates and visibility for each\nobject deﬁned in [43]. Here, the visibility of each point\nis a Bernoulli variable that indicates whether that point is\nvisible in the input image. Since keypoints are in image\nspace, we transform them into camera space in the same\nway as Equation (1), and process them using two different\nfully-connected layers. Then, we multiply their elements\nto aggregate visibility information to keypoints. Note that\nkeypoints give not only location and size cues but also shape\ncues, which are essential in shape estimation, because they\ninvolve local structural information of vehicles. Finally,\nwe construct object features Xo ∈ Rn×c by concatenat-\ning bounding boxes, bounding box features, and keypoints,\n9013\n\nObject shape, \nintermediate\nShape-aware attention\nTemplate offsets, \nAttention score\nMean shape, \nObject features Template features\nMLP\nMulti-head \ncross-attention\nObject offset\nqueryq\nkey-valuek-v\nq k-v\nFigure 3. Attention-guided modeling for 3D shape estimation. The\nshape-aware attention can explore the relevance between the ob-\nject features and learnable template features to estimate object off-\nset and to generate attention score.\nwhere cis concatenated feature dimension. As a combina-\ntion of primitive features, object features can provide rich\n2D information including shape, position, and size about\nindividual objects for faithful 3D estimation.\nGlobal feature. Different from object features, which de-\nscribe individual objects, global features represent various\nscene contexts. Speciﬁcally, we feed backbone feature\nmaps into three convolutional layers. We then sequentially\napply global average pooling and reshape operator to con-\nstruct global features Xg ∈ Rg×c. Here, the number of\nglobal contexts gis ﬁxed to 8.\n3.4. Attention-guided modeling\nShape prior.Since a vehicle is a rigid object, we take ad-\nvantage of prior knowledge about vehicle shapes for 3D\nshape estimation. To build shape prior, we adopt p = 79\nmesh templates in [21], in which each template is composed\nof v = 1352 vertices to describe a representative vehicle\nshape in ApolloCar3D dataset [43]. As shown in Figure 3,\nwe divide them into the mean shape ¯ms ∈R3v and template\noffsets Os ∈Rp×3v indicating the difference between the\nmean shape and templates.\nShape-aware attention. We decompose an object shape\ninto three components: mean shape, template offsets, and\nobject offset. Let ¯Ms ∈Rn×3v be the mean shape matrix,\nwhose rows are ¯ms, and Oo ∈Rn×3v be the object offset\nmatrix, whose rows contain offset for each object. Then We\ndeﬁne all objects’ shapes as\nM = ¯Ms + AOs + Oo (2)\nwhere A ∈Rn×p denotes all objects’ attention scores, of\nwhich rows indicate templates’ contribution to represent-\ning each object shape. Therefore, the shape-aware attention\naims to estimate all objects’ attention scores A and object\noffsets Oo ∈Rn×3v by exploiting the relationship between\nobjects and shape priors. This decomposition simpliﬁes the\noriginal problem, which should directly determine vertex\ncoordinates. Note that since cars are rigid bodies, the range\nof possible object offsets is limited.\nAs shown in Figure 3, we ﬁrst represent template off-\nsets to template features Xs ∈ Rp×c using the standard\nlearnable embedding scheme [9, 48], to measure relevance\nbetween an object and templates. We then predict ob-\nject offsets using multi-head cross-attention (MCA). MCA’s\nmechanism is similar to standard multi-head self-attention\n(MSA) [48]. The only difference is that MCA takes queries\nand key-value pairs from different sources. Speciﬁcally, we\nproject object features to queries and template features into\nkeys and values. Thus, object offsets are given by\n˜Oo = Xo + MCA(LN(Xo),Xs) (3)\nOo = MLP( ˜Oo) (4)\nwhere LN(·) is a layer normalization [2], MLP(·) contains\ntwo fully-connected layers with GELU non-linearity [18].\nNote that attention scores A, which present the similarities\nbetween objects and shape priors, are available in the MCA\nblock for object offsets.\n3.5. Bi-contextual attention\nGiven the object features Xo, we directly regress it to\nthe 3D rotation Pr with 3 fully connected layers. This is be-\ncause the object feature encodes the internal object structure\ngiving meaningful cues for rotation estimation. On the other\nhand, it is not straightforward to restore 3D translation from\nthe object feature. Note that 2D images have already lost\ndepth due to the image formation process. Thus, we attempt\nto use an external object structure to compensate for it. Fig-\nure 4 illustrates the bi-contextual attention (BCA) module to\nestimate 3D translation. Speciﬁcally, relation-aware atten-\ntion is designed to consider the relevance between objects\nbased on a multi-head self-attention mechanism. Thus, the\nrelation-aware feature Xr is given by\nXr = MSA(LN(Xo)) (5)\nOn the other hand, scene-aware attention is presented to ex-\nploit scene context from global features. More precisely,\nwe design it based on the MCA mechanism in which ob-\nject features give queries and global features give keys and\nvalues:\nXc = MCA(LN(Xo),LN(Xg)) (6)\n9014\n\nFigure 4. Bi-contextual attention module for 3D translation esti-\nmation. Relation-aware attention extracts object’s relative infor-\nmation from the object’s individual features, while scene-aware\nattention interfuses various scene contexts into each object.\nwhere Xc is the scene-aware feature. Next, we construct\ntranslation features Xt by integrating these external cues\ninto object features:\n˜Xt = Xo + XrΛr + XcΛc (7)\nXt = ˜Xt + MLP(LN( ˜Xt))Λt (8)\nwhere Λr, Λc, Λt are c×clearnable diagonal matrices [47]\nto scale the contribution of feature channels. Given transla-\ntion features Xt, the last fully-connected layer regresses 3D\ntranslations Pt.\n3.6. 3D Non-Maximum Suppression\nFigure 5 shows a failure example of standard non-\nmaximum suppression (NMS) in 2D image space. How-\never, it also shows that spurious detection can be removed\nin Bird’s eye view (BEV). Here, we propose a simple 3D\nNMS algorithm working on BEV as a post-processing step.\nAs shown in Figure 5, our NMS identiﬁes duplicated de-\ntection by comparing the xand zdistances with thresholds\nλx and λz. It then iterates the procedure, which selects one\nFigure 5. 3D non-maximum suppression to remove spurious ob-\njects based on their Bird-Eye-view distance.\ndetection with the highest detection score and removes du-\nplicated detections.\nFor accurate 3D NMS, we introduce the 3D detection\nscore based on depth uncertainty [33]. Note that restoring\ndepth by inverting the image formation process is an ill-\nposed problem. And there exists inevitable uncertainty in\nestimated depth. Thus, it is essential to consider this un-\ncertainty to make a certain decision using estimated depth.\nSpeciﬁcally, we predict depth uncertainty uthrough the last\nfully-connected layer in Figure 4. We then deﬁne the 3D\ndetection score considering depth uncertainty as\ns3D = s2D ·exp(−u) (9)\nwhere s2D is 2D detection score by Mask R-CNN [16].\n3.7. Loss functions\nRegression losses.We deﬁne the translation loss Ltrans as\nLtrans = |x−ˆx|+ |y−ˆy|+\n√\n2\nu |z−ˆz|+ log(u) (10)\nwhere ˆx, ˆy, ˆz are ground-truth translation coordinate. The\nlast two terms are the uncertainty regression loss [7, 33].\nNote that due to inevitable uncertainty in depth estimation,\ndifﬁcult or noise-labeled objects often produce large errors\ncausing unstable training. The uncertainty regression loss\nprevents it by inducing larger uncertainty for such cases.\nAlso, we employ the rotation loss Lrot in [21], which con-\nstrains the range of rotation pr to [−π,π].\nLrot =\n{\n|pr −ˆpr| if |pr −ˆpr|≤ π\n|2π−|pr −ˆpr|| if |pr −ˆpr|>π (11)\nwhere ˆpr is ground-truth rotation vector. For the shape\nLshape loss, we simply adopt L2-loss between predicted and\nground-truth ones.\nDetection loss. The detection loss Ldet constrains inaccu-\nrate bounding boxes and keypoints. More speciﬁcally, we\ndeﬁne it as\nLdet = Lrpn + Lbbox + Lkpts (12)\n9015\n\nA3DP-Abs A3DP-Rel\nMethod Detailed shape\nmean c-l c-s mean c-l c-s\nDeepMANTA [5] \u0017 20.10 30.69 23.76 16.04 23.76 19.80\nKeypoints-based [43] \u0017 20.40 31.68 24.75 16.53 24.75 19.80\n3D-RCNN [24] \u0013 16.44 29.70 19.80 10.79 17.82 11.88\nDirect-based [43] \u0013 15.15 28.71 17.82 11.49 17.82 11.88\nGSNet [21] \u0013 18.91 37.42 18.36 20.21 40.50 19.85\nBAAM-ResNet101 \u0013 23.80 45.62 21.92 21.00 44.24 17.88\nBAAM-Res2Net \u0013 25.19 47.31 23.13 22.85 46.21 20.31\nTable 1. Performance comparison with state-of-the-art methods for the monocular 3D pose and shape reconstruction on ApolloCar3D\ndataset [43]. No detailed shape methods use retrieval strategy, which searches a 3D shape to best match its 2D observation. We highlight\nthe best and second best results in bold and underline.\nwhere Lrpn, Lbbox, Lkpts are standard losses in [16] for RPN,\n2D box head, and 2D keypoints head, respectively.\n3D space loss.Even though objects’ translation, rotation,\nand shape are interdependent in 3D space, the regression\nlosses consider them independently. To emphasize their\nstructure, we employ the 3D loss [13] that penalizes the\nmean vertex error between predicted and ground-truth ver-\ntices on the world space. Let R ∈R3×3 be the rotation ma-\ntrix corresponding to the estimated object rotationpr. Using\nthe rotation matrix and the translation vectort = pt, we can\ntransform 3D mesh vertices in the camera space to the world\nspace by mworld = Rm∗+ t. Here, m∗ ∈R3×v is the\nreshaped matrix of the estimated 3D mesh m. Moreover,\nwe consider additional 3D spaces, rotation, and translation\nspaces, to clarify the ambiguous contributions of the trans-\nlation and rotation. Thus, we deﬁne 3D mesh vertices in\nthese spaces by mrot = Rm∗and mtrans = m∗+ t. Then,\nthe 3D loss is given by\nL3D =\n∑\nl∈S\n|ml −ˆml| (13)\nwhere S= {world,trans,rot}is the 3D space set.\n4. Experiments\n4.1. Datasets and Metrics\nApolloCar3D [43]. It contains 4036, 200, and 1041 high-\nresolution images for training, validation, and testing. How-\never, we only use the training and validation sets for our\nexperiments because the ApolloCar3D test server is not ser-\nviced. ApolloCar3D images contain an average of 11.7 car\nobjects described by 2D keypoints, 3D translation, and rota-\ntion labels. Each object is one of 79 car classes ( e.g. sedan,\ncoupe, SUV , and so on). For the ground-truth car shape,\nwe adopt 3D mesh models in [21]. For the detection loss,\nwe deﬁne a pseudo 2D bounding box since ApolloCar3D\ndoes not provide a 2D bounding box label. Speciﬁcally, we\nproject object meshes to image space using camera param-\neters and their 3D translation and rotation. We then deﬁne\na tight 2D box surrounding a projected mesh mask, as the\npseudo 2D bounding box.\nKITTI [12].It is the widely used dataset in monocular 3D\nobject detection. It consists of 7,481 images for training and\n7,518 images for testing. As done in [6], we split the train-\ning data into a training set (3,712 images) and a validation\nset (3,769 images). Then, we conduct experiments on this\nsplit to validate the scalability of the bi-contextual attention\nmodule on 3D object detection.\nEvaluation Metrics. For ApolloCar3D experiments, we\nadopt the average 3D precision (A3DP) [43], which jointly\nmeasures 3D translation, rotation, and shape reconstruc-\ntion accuracy. According to the 3D translation error mea-\nsurement scheme, we denote A3DP metrics with an abso-\nlute translation error and a relative one as A3DP-Abs and\nA3DP-Rel, respectively.\n4.2. Implementation Details\nInference. We employ Res2Net [11] as our backbone, pre-\ntrained on the COCO 2017 dataset [28]. We then perform\n2D box and keypoints detection through Mask R-CNN [16].\nAfter that, we estimate the 3D pose and shape for all detec-\ntions. Finally, we conduct the proposed 3D NMS to remove\nspurious detections.\nTraining. We train the proposed BAAM network in two\nstages. First, we train BAAM with the detection loss Ldet\nfor 10 epochs. For this stage, we employ AdamW opti-\nmizer [32] with a learning rate of 0.0001. Second, we train\nBAAM with the total lossL= Ldet +Ltrans +Lrot +Lshape +\nL3D for 30 epochs. Here, we balance the contribution of\nlosses with scales of 1, 0.5, 1, 3, and 0.01, respectively.\nAlso, we use AdamW optimizer with an initial learning rate\nof 0.0001 and divide it by 10 at the 20 epoch. For the train-\ning, we use a mini-batch of size 4. The training is performed\nwith an RTX A6000 GPU.\n9016\n\nFigure 6. Qualitative results of BAAM on ApolloCar3D. Note how precisely BAAM estimates car 3D pose and shape.\nMethod\n3D@IOU=0.7 BEV@IOU=0.7 3D@IOU=0.5 BEV@IOU=0.5\nEasy Mod. Hard Easy Mod. Hard Easy Mod. Hard Easy Mod. Hard\nGUPNet [33] 23.18 16.24 13.57 30.18 22.42 19.31 58.99 43.64 39.34 65.16 48.88 42.93\n+ BCA 24.23 16.48 13.60 30.90 22.58 19.21 60.32 45.67 39.51 66.23 49.73 43.23\n+ BCA + 3D NMS 24.33 16.55 13.66 31.00 22.66 19.26 60.60 44.66 38.29 65.36 49.99 43.40\nDID-M3D [40] 25.41 17.07 14.05 33.94 23.22 19.52 64.47 48.32 41.75 70.34 52.34 45.47\n+ BCA 25.96 17.66 14.57 33.97 24.21 20.69 64.56 48.82 42.49 70.73 53.00 47.75\n+ BCA + 3D NMS 26.02 17.72 14.62 34.04 24.31 19.87 64.69 48.94 42.52 70.85 53.12 46.36\nDEVIANT [23] 24.58 16.52 14.50 32.63 23.05 20.00 60.97 44.78 40.18 65.31 49.63 43.50\n+ BCA 25.34 16.98 14.93 32.77 23.21 20.12 61.11 46.13 40.31 65.45 49.80 43.74\n+ BCA + 3D NMS 25.40 17.03 14.95 32.80 23.32 20.15 61.29 45.01 40.34 65.64 49.99 43.73\nTable 2. The effectiveness of bi-contextual attention (BCA) and 3D non-maximum suppression (NMS). We add BCA module before the\ndepth bias module of GUPNet [33] and DEVIANT [23], and attribute depth module of DID-M3D [40]. Also, we adopt 3D NMS in the\npost-processing steps of all. For the baselines, we reproduce results with the ofﬁcially released code and parameters. We highlight the best\nand second best results in bold and underline.\n4.3. Main Results\nResults on ApolloCar3D. Table 1 compares the pro-\nposed BAAM with recent state-of-the-art algorithms on\nApollo3D: DeepMANTA [5], Keypoints-based [43], 3D-\nRCNN [24], Direct-based [43], and GSNet [21]. Note that\nthe proposed BAAM signiﬁcantly outperforms the exist-\ning methods across the evaluation metrics. Compared to\nGSNet, BAAM improves A3DP-Abs and A3DP-Rel scores\nby 33% and 13%, respectively. For fair comparisons, we\nalso report the performance of BAAM with ResNet101 [17]\nbackbone, which is the same one with GSNet [21]. As\nshown in Table 1, BAAM-ResNet101 still exceeds GSNet\nwith the improvement of 26% and 4% in terms of A3DP-\nAbs mean and A3DP-Rel mean. Figure 6 shows qualitative\nresults on ApolloCar3D. The ﬁrst row is the input images,\nand the second row is the result of our BAAM. The third\nrow shows the reconstructed 3D space of BAAM in a dif-\nferent viewpoint. Corresponding car instances are depicted\nin the same color. We see that BAAM faithfully places car\nobjects onto 3D space and estimates detailed object shapes.\nResults on KITTI. We verify the scalability of the pro-\nposed BCA module and 3D NMS algorithm. To this end,\nwe integrate our module into state-of-the-art algorithms\nfor monocular 3D object detection: GUPNet [33], DID-\nM3D [40], and DEVIANT [23]. Table 2 reports the perfor-\nmance of these algorithms on KITTI validation set. Note\nthat our BCA module improves their performances in all\nsettings with only one exception. In addition, our 3D\nNMS algorithm further increases the overall performance\nof monocular 3D object detectors.\n4.4. Ablation Study\nNext, we study the contribution of three components:\nAttention-guided modeling (AGM); Bi-contextual attention\n(BCA); 3D non-maximum suppression (3D NMS). For the\n9017\n\nMethod\nA3DP-Abs A3DP-Rel Rotate Trans Mesh\nMean c-l c-s Mean c-l c-s Error Error IOU\nBaseline 21.08 41.98 18.36 16.68 36.29 12.84 14.00 7.35 84.71\nBaseline + AGM 22.80 43.01 21.94 18.79 38.97 15.54 13.96 7.23 85.55\nBaseline + AGM + BCA 24.74 45.86 23.21 22.42 45.15 20.04 11.96 6.33 85.41\nBaseline + AGM + BCA + 3D NMS 25.19 47.31 23.13 22.85 46.21 20.31 11.96 6.33 85.41\nTable 3. Results on ApolloCar3D with different combinations of BAAM components: AGM, BCA, and 3D NMS. Baseline directly\nregresses mesh vertices and translation. We evaluate rotation error, translation error, and rendered mesh IOU with the ground truth boxes.\nMethod Mesh Error Mesh IOU\nRegression 8.58 82.61\nPCA-basis 7.25 85.08\nDivide-and-conquer [21] 7.71 84.60\nAGM 6.82 85.41\nTable 4. Results on ApolloCar3D with different shape estimation\nmethods. We measure rendered IOU, L1 distance error with re-\nspect to the ground truth mesh. The L1 distance error is in units of\n10−2.\nablation study, we design the baseline that excludes these\ncomponents from BAAM and directly regresses 3D pose\nand shape through a single fully-connected layer. Table 3\nreports the main results of ablation study on ApolloCar3D.\nImpacts of AGM.In Table 3, AGM generates more accu-\nrate 3D mesh by improving mesh IOU +0.76 from Baseline.\nWith a precise shape estimation, AGM increases A3DP-\nAbs mean, c-l, and c-s by +1.71, +3.25, and +2.8, respec-\ntively. Note that this result supports the importance of de-\ntailed shape estimation. For a comprehensive analysis, we\nreplace AGM from our BAAM with different shape esti-\nmation methods: Regression, PCA-basis, and divide-and-\nconquer method [21]. The implementation details about\nthese alternative methods are available in the supplementary\nmaterial. Table 4 shows that the proposed AGM consider-\nably exceeds the other methods with the highest mesh IOU\nand the lowest mesh error.\nImpacts of BCA.As shown in Table 3, BCA signiﬁcantly\nboosts A3DP-Abs and A3DP-Rel scores. This is because\nBCA effectively reduces translation errors by exploiting ex-\nternal object structures. Table 5 shows the contributions of\nrelation-aware attention and scene-aware attention. We ob-\nserve that both A3DP-Abs and A3DP-Rel scores are signif-\nicantly degraded if one of them is missing. This demon-\nstrates that the perception of both inter-object and road en-\nvironments is critical to the 3D translation reasoning.\nImpacts of 3D NMS.Table 3 shows that our 3D NMS fur-\nther improves A3DP scores on ApolloCar3D. Also, we see\nthat most best scores of state-of-the-arts on KITTI are ob-\ntained with our 3D NMS. Both experiments demonstrate the\nMethod\nA3DP-Abs A3DP-Rel\nMean c-l c-s Mean c-l c-s\nRA 23.95 46.29 21.59 20.59 43.20 17.15\nSA 23.66 45.59 21.84 20.59 42.64 17.15\nRA+SA 25.19 47.31 23.13 22.85 46.21 20.31\nTable 5. Results on ApolloCar3D with different combinations of\nBCA components. RA and SA mean the relation-aware attention\nand the scene-aware attention.\neffectiveness of our 3D NMS. However, the improvement\non ApolloCar3D is relatively larger than KITTI. This is be-\ncause the ApolloCar3D has far more cars (11.7) than the\nKITTI (4.8) per image.\n5. Conclusion\nWe proposed a novel algorithm, called BAAM, for\nmonocular 3D pose and shape reconstruction. The\nmain contributions of BAAM are bi-contextual attention,\nattention-guided modeling, and 3D NMS algorithm. Given\nvarious 2D primitives, BAAM reconstructs the object’s\nshape as a mesh based on attention-guided modeling, which\nexploits relevance between individual objects and vehicle\nshape priors. Then, BAAM estimates the object’s pose us-\ning the carefully designed bi-contextual attention module\nto consider relation-context inter objects and scene-context\nbetween the object and road environment. Finally, the\n3D NMS algorithm eliminates spurious objects based on\nBird-Eye-View geometry. Experiments demonstrated that\nBAAM signiﬁcantly outperforms conventional algorithms\non ApolloCar3D, and that BAAM improves monocular 3D\nobject detectors on KITTI as a plugged-in-plug module.\nAckowledgements. This work was supported partly by\nthe National Research Foundation of Korea (NRF) grants\n(NRF-2021R1A4A1031864, NRF-2022R1I1A3069113)\nand partly by Institute of Information & communications\nTechnology Planning & Evaluation (IITP) grant funded by\nthe Korea government (MSIT) (No.RS-2022-00155857,\nArtiﬁcial Intelligence Convergence Innovation Human\nResources Development (Chungnam National University)).\n9018\n\nReferences\n[1] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt,\nand Marcus Magnor. Tex2shape: Detailed full human body\ngeometry from a single image. In ICCV, pages 2293–2303,\n2019. 2, 3\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 4\n[3] Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun\nChen, Hongbo Fu, and Chiew-Lan Tai. Transfusion: Robust\nlidar-camera fusion for 3d object detection with transform-\ners. In CVPR, pages 1090–1099, 2022. 1\n[4] Garrick Brazil and Xiaoming Liu. M3d-rpn: Monocular 3d\nregion proposal network for object detection. InICCV, pages\n9287–9296, 2019. 1\n[5] Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa,\nC´eline Teuliere, and Thierry Chateau. Deep manta: A\ncoarse-to-ﬁne many-task network for joint 2d and 3d vehicle\nanalysis from monocular image. InCVPR, pages 2040–2049,\n2017. 2, 6, 7\n[6] Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Andrew G\nBerneshawi, Huimin Ma, Sanja Fidler, and Raquel Urta-\nsun. 3d object proposals for accurate object class detection.\nNeurIPS, 28, 2015. 6\n[7] Yongjian Chen, Lei Tai, Kai Sun, and Mingyang Li.\nMonopair: Monocular 3d object detection using pairwise\nspatial relationships. In CVPR, pages 12093–12102, 2020.\n1, 2, 5\n[8] Hongsuk Choi, Gyeongsik Moon, JoonKyu Park, and Ky-\noung Mu Lee. Learning to estimate robust 3d human mesh\nfrom in-the-wild crowded scenes. In CVPR, pages 1475–\n1484, 2022. 2\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2020. 4\n[10] Emec ¸ Erc ¸elik, Ekim Yurtsever, Mingyu Liu, Zhijie\nYang, Hanzhen Zhang, Pınar Topc ¸am, Maximilian Listl,\nYılmaz Kaan C ¸ aylı, and Alois Knoll. 3d object detection\nwith a self-supervised lidar scene ﬂow backbone. In ECCV,\n2022. 1\n[11] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu\nZhang, Ming-Hsuan Yang, and Philip Torr. Res2net: A new\nmulti-scale backbone architecture. IEEE TPAMI, 43(2):652–\n662, 2019. 6\n[12] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? the kitti vision benchmark\nsuite. In CVPR, pages 3354–3361, 2012. 2, 6\n[13] Qichuan Geng, Hong Zhang, Feixiang Lu, Xinyu Huang,\nSen Wang, Zhong Zhou, and Ruigang Yang. Part-level\ncar parsing and reconstruction in single street view images.\nIEEE TPAMI, 44(8):4291–4305, 2021. 6\n[14] Jiaqi Gu, Bojian Wu, Lubin Fan, Jianqiang Huang, Shen\nCao, Zhiyu Xiang, and Xian-Sheng Hua. Homography loss\nfor monocular 3d object detection. In CVPR, pages 1080–\n1089, 2022. 2\n[15] Richard Hartley and Andrew Zisserman. Multiple view ge-\nometry in computer vision . Cambridge university press,\n2003. 3\n[16] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In ICCV, pages 2961–2969, 2017. 3, 5,\n6\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\npages 770–778, 2016. 7\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). arXiv preprint arXiv:1606.08415, 2016. 4\n[19] Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, and Win-\nston H Hsu. Monodtr: Monocular 3d object detection with\ndepth-aware transformer. In CVPR, pages 4012–4021, 2022.\n1\n[20] Angjoo Kanazawa, Michael J Black, David W Jacobs, and\nJitendra Malik. End-to-end recovery of human shape and\npose. In CVPR, pages 7122–7131, 2018. 2\n[21] Lei Ke, Shichao Li, Yanan Sun, Yu-Wing Tai, and Chi-\nKeung Tang. Gsnet: Joint vehicle pose and shape recon-\nstruction with geometrical and scene-aware supervision. In\nECCV, pages 515–532, 2020. 2, 3, 4, 5, 6, 7, 8\n[22] Muhammed Kocabas, Chun-Hao P Huang, Joachim Tesch,\nLea M¨uller, Otmar Hilliges, and Michael J Black. Spec: See-\ning people in the wild with an estimated camera. In ICCV,\npages 11035–11045, 2021. 2\n[23] Abhinav Kumar, Garrick Brazil, Enrique Corona, Armin Par-\nchami, and Xiaoming Liu. Deviant: Depth equivariant net-\nwork for monocular 3d object detection. In ECCV, 2022.\n7\n[24] Abhijit Kundu, Yin Li, and James M Rehg. 3d-rcnn:\nInstance-level 3d object reconstruction via render-and-\ncompare. In CVPR, pages 3559–3568, 2018. 2, 6, 7\n[25] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and Xiao-\ngang Wang. Gs3d: An efﬁcient 3d object detection frame-\nwork for autonomous driving. In CVPR, pages 1019–1028,\n2019. 1\n[26] Peiliang Li, Xiaozhi Chen, and Shaojie Shen. Stereo r-cnn\nbased 3d object detection for autonomous driving. In CVPR,\npages 7644–7652, 2019. 1\n[27] Peixuan Li, Huaici Zhao, Pengfei Liu, and Feidao Cao.\nRtm3d: Real-time monocular 3d detection from object key-\npoints for autonomous driving. In ECCV, pages 644–660,\n2020. 1\n[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, pages 740–755, 2014. 6\n[29] Zechen Liu, Zizhang Wu, and Roland T ´oth. Smoke: Single-\nstage monocular 3d object detection via keypoint estimation.\nIn CVPRW, pages 996–997, 2020. 1\n[30] Zongdai Liu, Dingfu Zhou, Feixiang Lu, Jin Fang, and\nLiangjun Zhang. Autoshape: Real-time shape-aware monoc-\nular 3d object detection. InICCV, pages 15641–15650, 2021.\n1\n[31] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J Black. Smpl: A skinned multi-\nperson linear model. ACM TOG, 34(6):1–16, 2015. 2\n9019\n\n[32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 6\n[33] Yan Lu, Xinzhu Ma, Lei Yang, Tianzhu Zhang, Yating Liu,\nQi Chu, Junjie Yan, and Wanli Ouyang. Geometry uncer-\ntainty projection network for monocular 3d object detection.\nIn ICCV, pages 3111–3121, 2021. 1, 2, 5, 7\n[34] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades,\nGerard Pons-Moll, Siyu Tang, and Michael J Black. Learn-\ning to dress 3d people in generative clothing. InCVPR, pages\n6469–6478, 2020. 2, 3\n[35] Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu\nZeng, and Wanli Ouyang. Rethinking pseudo-lidar represen-\ntation. In ECCV, 2020. 2\n[36] Xinzhu Ma, Zhihui Wang, Haojie Li, Pengbo Zhang, Wanli\nOuyang, and Xin Fan. Accurate monocular 3d object detec-\ntion via color-embedded 3d reconstruction for autonomous\ndriving. In CVPR, 2019. 2\n[37] Fabian Manhardt, Wadim Kehl, and Adrien Gaidon. Roi-\n10d: Monocular lifting of 2d detection to 6d pose and metric\nshape. In CVPR, pages 2069–2078, 2019. 2\n[38] Arsalan Mousavian, Dragomir Anguelov, John Flynn, and\nJana Kosecka. 3d bounding box estimation using deep learn-\ning and geometry. In CVPR, 2017. 2\n[39] Dennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, and\nAdrien Gaidon. Is pseudo-lidar needed for monocular 3d\nobject detection? In ICCV, pages 3142–3152, 2021. 2\n[40] Liang Peng, Xiaopei Wu, Zheng Yang, Haifeng Liu, and\nDeng Cai. Did-m3d: Decoupling instance depth for monoc-\nular 3d object detection. In ECCV, 2022. 1, 2, 7\n[41] Zequn Qin and Xi Li. Monoground: Detecting monocular 3d\nobjects from the ground. In CVPR, pages 3793–3802, 2022.\n1, 2\n[42] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping\nShi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-\nvoxel feature set abstraction for 3d object detection. In\nCVPR, pages 10529–10538, 2020. 1\n[43] Xibin Song, Peng Wang, Dingfu Zhou, Rui Zhu, Chenye\nGuan, Yuchao Dai, Hao Su, Hongdong Li, and Ruigang\nYang. Apollocar3d: A large 3d car instance understanding\nbenchmark for autonomous driving. In CVPR, pages 5452–\n5462, 2019. 2, 3, 4, 6, 7\n[44] Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qin-\nhong Jiang, Xiaowei Zhou, and Hujun Bao. Disp r-cnn:\nStereo 3d object detection via shape prior guided instance\ndisparity estimation. In CVPR, pages 10548–10557, 2020. 1\n[45] Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J Black, and\nTao Mei. Monocular, one-stage, regression of multiple 3d\npeople. In ICCV, pages 11179–11188, 2021. 2\n[46] Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael J\nBlack. Putting people in their place: Monocular regression\nof 3d people in depth. In CVPR, pages 13243–13252, 2022.\n2\n[47] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv´e J´egou. Going deeper with im-\nage transformers. In ICCV, 2021. 5\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 4\n[49] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariha-\nran, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar\nfrom visual depth estimation: Bridging the gap in 3d object\ndetection for autonomous driving. In CVPR, 2019. 1, 2\n[50] Yunpeng Zhang, Jiwen Lu, and Jie Zhou. Objects are differ-\nent: Flexible monocular 3d object detection. In CVPR, pages\n3289–3298, 2021. 1, 2\n[51] Yunpeng Zhang, Wenzhao Zheng, Zheng Zhu, Guan Huang,\nDalong Du, Jie Zhou, and Jiwen Lu. Dimension embeddings\nfor monocular 3d object detection. In CVPR, pages 1589–\n1598, 2022. 1, 2\n9020\n"
  },
  {
    "source": "Riyaz Shaik.pdf",
    "content": "Riyaz Shaik Entrepreneur ● Data Scientist ● AI/ML Engineer ● Embedded Systems Engineer           520 E Washington St, Indianapolis, IN 46204 | (260) 804-3442 | riyazshaik.1104@gmail.com | LinkedIn | GitHub: riyaz-maker  \nWORK EXPERIENCE Founder – Sentient Notes                                             Present • AI powered mood journaling app using LLMs to provide companionship and therapeutic insights based on user entries, processing the data completely on device and relying on founding model run on cloud for complex understanding. Co-Founder – ChargeFetch                                             Present • EV charging delivery service provider application that lets users reserve a charging spot at desired locations and allots a representative to fetch the car, charge it and then return it to the user’s location.  Machine Learning Research Assistant            May 2023 - December 2024  • Developed and optimized custom deep learning algorithms and models trained on very large datasets on GPUs for autonomous drones using TensorFlow and TensorRT. • Implemented 3D ResNet + Transformer architectures for video classification and action recognition achieving a 17% increase in accuracy. • Deployed an optimized inference pipeline on Jetson GPUs using the DeepStream framework. • Optimized training and inference on GPU clusters and deployed for production.  SKILLS Languages: Python, C, C++, SQL, Verilog AI/ML Frameworks: PyTorch, Tensorflow, TensorRT, vLLM, DeepStream, Pandas, Scikit-Learn GPU Computing: CUDA, OpenCL, RAPIDS, HIP Distributed Computing: Multi GPU training, Multi node cluster deployment Tools: Git, Jira, LINUX, API, Bash, MongoDB, Tableau, Docker, UNIX, Google Analytics Deep Learning Domains: Computer Vision, Natural Language Processing, LLMs, GenAI, Recommendation Systems PROJECTS    Empathetic AI Chatbot (Detects Emotions and Consoles) • Developed an end-to-end Empathetic Adaptive Response Engine that leverages a fine-tuned RoBERTa-base transformer for real-time emotion detection, classifying user sentiment and contextual understanding. • Integrated Google’s Gemini API (Gemini 2.0 Flash) to generate empathetic, human-like responses, adapting conversation strategies to enhance user engagement and achieve high conversational quality. • Developed a fine-tuned recommendation layer for the LLM for the best and safe companion engagement.    Business Intelligence Platform • Developed an end-to-end Sports Business Intelligence Platform with scalable ETL pipelines and advanced ML models to optimize sports revenue and user engagement. Ready for deployment on cloud platforms such as AWS or GCP. • Designed and automated data workflows integrated with MLFlow for experiment tracking, ensuring model performance and accuracy align with real-world business challenges. Created dynamic SQL reporting for all types of stakeholders.    SQL Query Optimization and ETL Pipeline Design • Developed ETL pipelines and transformation processes that simulate real world AI database migration, managing databases for AI applications. Designed a star schema data model by transforming and optimizing raw user and financial data. • Automated end-to-end ETL process using Apache Airflow, featuring performance tuning and data quality checks.    Synthetic Data Generation with GANs for Imbalanced Datasets • Developed a GAN that synthesized 98.7% statistically indistinguishable fraud transaction samples, reducing class imbalance from 0.172% to 45.3% in credit card fraud detection dataset. • Implemented a custom deep learning architecture with dynamic batch processing and adversarial training, achieving 92.5% feature distribution similarity between original and synthetic datasets, using TensorFlow's gradient techniques.    GPU-Accelerated Customer Behavior Prediction • Built a 2-layer LSTM for time-series prediction using Optuna for Bayesian hyperparameter tuning. "
  },
  {
    "source": "2309.08365v1.pdf",
    "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nM3Net: Multilevel, Mixed and Multistage Attention\nNetwork for Salient Object Detection\nYao Yuan, Pan Gao, Member, IEEE, and Xiaoyang Tan\nAbstract—Most existing salient object detection methods\nmostly use U-Net or feature pyramid structure, which sim-\nply aggregates feature maps of different scales, ignoring the\nuniqueness and interdependence of them and their respective\ncontributions to the final prediction. To overcome these, we\npropose the M 3Net, i.e., the Multilevel, Mixed and Multistage\nattention network for Salient Object Detection (SOD). Firstly,\nwe propose Multiscale Interaction Block which innovatively\nintroduces the cross-attention approach to achieve the interaction\nbetween multilevel features, allowing high-level features to guide\nlow-level feature learning and thus enhancing salient regions.\nSecondly, considering the fact that previous Transformer based\nSOD methods locate salient regions only using global self-\nattention while inevitably overlooking the details of complex\nobjects, we propose the Mixed Attention Block. This block\ncombines global self-attention and window self-attention, aiming\nat modeling context at both global and local levels to further\nimprove the accuracy of the prediction map. Finally, we proposed\na multilevel supervision strategy to optimize the aggregated\nfeature stage-by-stage. Experiments on six challenging datasets\ndemonstrate that the proposed M 3Net surpasses recent CNN and\nTransformer-based SOD arts in terms of four metrics. Codes are\navailable at https://github.com/I2-Multimedia-Lab/M3Net.\nIndex Terms—Salient object detection, saliency prediction,\nmultilevel and multistage aggregation.\nI. I NTRODUCTION\nS\nALIENT object detection [1] (SOD) aims to identify\nthe most significant objects or regions in an image and\nsegment them. Given its widespread application in computer\nvision, SOD plays a critical role in various downstream tasks,\nsuch as object detection [2], semantic segmentation [3], image\nunderstanding [4], and object discovery [5]. Furthermore, SOD\nserves as a valuable reference for multimodal SOD tasks,\nincluding RGB-D SOD [6]–[8], RGB-T SOD [9], [10], and\nLight field SOD [11], [12].\nPrevious CNN-based saliency detection methods have\nachieved significant results in positioning salient regions. Most\nof them take U-shape [13] based structures as the encoder-\ndecoder architecture, and utilize multilevel features to recon-\nstruct high quality feature maps in the encoder side [14]–[19],\nor the decoder side [20]–[26]. For example, AADF [23] pro-\nposed the AD-ASPP module that combines the local saliency\ncues captured by dilated convolutions at a small rate and the\nglobal saliency cues captured by dilated convolutions at a large\nrate. DCENet [24] designed a dense context exploration mod-\nule to capture dense multi-scale contexts, thereby enhancing\nY . Yuan, P. Gao, and X. Tan are with College of Computer Science and\nTechnology, Nanjing University of Aeronautics and Astronautics, Nanjing\n211106, China.\nImage\n GT\n Level-1\n Level-2\n Level-3\n1.0\n0.0\nFig. 1. The difference between multilevel features. Levels 1, 2, and 3 represent\nthe multilevel features captured from different levels of our network. The color\ncloser to red indicates that the model pays more attention to this area.\nthe feature’s discriminability. Nevertheless, previously men-\ntioned methods mostly apply the same processing to features\nat different scales or levels, disregarding the uniqueness and\ninterdependence between different levels of features and their\ndistinct contributions to the final prediction. We argue that\nlow-level features contain more non-salient information, which\nmay harm the final prediction. As shown in Figure 1, lower-\nlevel feature generally contains more non-salient regions and\nbackground noises.\nRecently, following the Vision Transformer’s (ViT) [27]\nsuccess in image classification, some studies introduce the\nTransformer for dense prediction tasks, e.g., semantic segmen-\ntation [28]–[30] or SOD [31]–[33]. Thanks to the ability of\nthe Transformer to quickly establish long-term dependencies,\npreviously mentioned Transformer-based SOD methods excel\nin locating salient regions compared to CNN alternatives.\nHowever, the exclusive employment of global self-attention\ncan result in the loss of numerous local details, as depicted in\nFigure 2. In concealed object detection, UCT [34] employed\nCNNs to address the absence of detailed information. How-\never, in the field of SOD, compensating for the loss of detailed\ninformation in transformers has not been fully investigated.\nIn this paper, we propose the M 3Net for SOD. To facilitate\nthe use of complementarity between multilevel features, we\npropose the Multilevel Interaction Block (MIB), which intro-\nduces the cross-attention mechanism to achieve the interaction\nof multilevel features, letting high-level features guide low-\nlevel features to strengthen salient regions. Inspired by the\nsuccess of window self-attention [35], which computes self-\nattention within local windows, we propose the Mixed At-\ntention Block (MAB) that integrates global self-attention and\nwindow self-attention. On the basis of the localized salient\nregions, our MAB can model context at both global and\nlocal levels, thus enhancing final accuracy of the prediction.\nOur proposed MIB and MAB blocks work collaboratively\nto exploit multilevel features, which is different from prior\narXiv:2309.08365v1  [cs.CV]  15 Sep 2023\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\nImage\n GT\n M3Net\n SRformer\n[32]\nVST\n[31]\nFig. 2. Previous Transformer-based methods often suffer from a loss of local\ndetails, whereas our proposed M3Net excels at preserving these fine-grained\ndetails.\nworks that only rely on multilevel feature concentration or\nuse separate CNN networks for local detail learning.\nOur main contributions can be summarized as follows:\n• Based on the feature pyramid structure, we rethink the\nuniqueness and independence between multiscale features\nand their distinctive contributions to final prediction, and\naccordingly propose a multiscale and multistage decoder.\nThe decoder can optimize and integrate features step by\nstep, and progressively reconstruct the saliency map.\n• We introduce the Multilevel Interaction Block (MIB)\nto facilitate the interaction between features at different\nlevels, specifically utilizing high-level features to guide\nthe learning of low-level features. As demonstrated later,\nthe MIB effectively enhances the salient region within the\nlow-level features.\n• We propose the Mixed Attention Block (MAB), which\nintegrates self-attention and window self-attention mecha-\nnisms. The MAB enables improved localization of salient\nregions while preserving fine-grained object details.\nOur proposed method differs distinctly from existing works\nin two key aspects. Firstly, in our multistage decoder, we\nadopt a sequential processing approach for features instead\nof the conventional two-dimensional form. As a result, no\nconvolutional operations were incorporated into the decoder.\nSecondly, we capture the global and fine-grained features both\nthrough the attention mechanisms instead of adjusting the\nkernel size as done in many existing methods. Besides, We\nevaluate our model on six challenging datasets, and the results\nshow that the M3Net achieves state-of-the-art results compared\nto recent competitive models.\nII. R ELATED WORK\nA. Convolution Based Methods\nCNN-based methods are widely used in SOD and gain com-\nmendable performance, which usually takes the pre-trained\nnetworks [36], [37] as the encoder, and most of the efforts\nare then to design an effective decoder to conduct multilevel\nfeature aggregation. Most methods [14], [38], [39] use the U-\nshape [13] based structures as the encoder-decoder architecture\nand progressively aggregate the hierarchical features for final\nprediction, while DSS [21] follows the HED [40] structure to\nfully utilize multilevel and multiscale features. Some methods\ntend to utilize the attention mechanism to learn more discrim-\ninative features, such as pixel-wise contextual attention [41],\nspatial and channel attention [20]. Multi-task learning is also\nwidely used for SOD, including with the tasks, eye fixation\nprediction [42], [43], edge detection [18], [44]–[46], and image\ncaption [47].\nUnlike previous CNN-based methods, our M 3Net can han-\ndle features extracted by Transformer as the encoder and\nperforms sequential processing of feature maps in the decoder.\nWe adopt the U-shape structure and consider the uniqueness\nand interdependence of multilevel features by implementing\nthe proposed Multilevel Interaction Block, thus facilitating the\nsubsequent aggregation of multilevel features. In addition, we\nemploy the ”fold with overlap” as the upsampling method for\nour token-based model and perform a comparison with other\nexisting upsampling methods.\nB. Transformer Based Methods\nTransformer encoder-decoder architecture was first pro-\nposed by Vaswani et al. [48] for natural language processing.\nSince the success of ViT [27] in image classification, more\nand more works have introduced Transformer architecture\nto computer vision tasks. SETR [28] and PVT [49] use\nViT as the encoder for semantic segmentation. In SOD,\nVST [31] employs T2T-ViT [50] as the backbone and proposes\nan effective multi-task decoder for features in a sequence\nform. SRformer [32] adopted PVT as the encoder backbone\nand uses pixel shuffle as the upsampling method. Besides,\nHRTransNet [33] explored the application of transformer in\ntwo-modality SOD tasks, such as RGB-D SOD and RGB-T\nSOD.\nPrevious Transformer-based methods are superior in locat-\ning and capturing the salient areas in the images, while the\ndetails at the local level could be ignored. Inspired by the\nsuccess of window self-attention [35], we introduce window\nself-attention in our decoder to strengthen the ability to model\nlocal context. As shown in Figure 2, our M 3Net can model\ncontext at both global and local levels, thus effectively pre-\nserving local details on the basis of localized salient objects.\nC. Multilevel Feature Enhancement and Aggregation\nMultilevel feature enhancement and aggregation are crucial\nfor gaining a high-resolution saliency map, and numerous\nmethods have thoroughly investigated them. GateNet [39]\nadopted the gate mechanism to balance the contribution\nof each encoder block and reduce non-salient information.\nMiNet [38] proposed an interactive learning method for mul-\ntilevel features, aiming to minimize the discrepancies and\nimprove the spatial coherence of multilevel features. ICON\n[51] incorporated convolution kernels with different shapes\nto enhance the diversity of multilevel features. BBRF [52]\ndesigned a switch-path decoder with various receptive fields\nto deal with large or small-scale objects.\nTo fully utilize multi-scale features, we first design the\nproposed Multiscale Interaction Block which introduces the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\n...\n... ...\n...\nMIB\n𝐻 ×𝑊 ×3\n𝐹1\n𝐹2\n𝐹3\n𝐹4\nMIB\nMIB\nConcat\nUpsample\nEncoder Decoder\nMAB\nMAB\n2×\n2×\n4×\nSupervision\n𝐻\n4 ×𝑊\n4 ×𝐶\n𝐻\n8 ×𝑊\n8 ×2𝐶\n𝐻\n16× 𝑊\n16×4𝐶\n𝐻\n32× 𝑊\n32×8𝐶\n×r\n×r\nFig. 3. Overall architecture of our proposed M 3Net model for salient object detection. The backbone is defined as a hierarchical network structure (e.g.\nResNet [36], SwinTransformer [35]).\ncross-attention mechanism to achieve the interaction among\nmultilevel features. In this mechanism, high-level features\nguide low-level feature learning from top to bottom. Then,\nin order to effectively integrate salient information across\nvarious scales within the aggregated features, we propose the\nMixed Attention Block, which combines global self-attention\nand local window self-attention. This combination is aimed at\nmodeling context at both global and local levels for further\nimprovement in the accuracy of the prediction map.\nIII. P ROPOSED METHOD\nA. Overview\nThe overall architecture of our proposed M 3Net is given\nin Figure 3, which includes a Transformer encoder plus a\nmultistage decoder with mixed attention. The encoder uses\nthe Swin Transformer as the backbone for the demonstration\nof the proposed methodology, while any other hierarchical\nnetwork models are applicable to extract multilevel features.\nThe decoder optimizes and integrates multilevel features step\nby step, and gradually reconstructs the saliency map.\nAfter obtaining multilevel features by the backbone, we\noptimize the features by passing them through the multiscale\ninteraction block, which can enhance salient regions in low-\nlevel features. Then, on the basis of detected salient regions,\nwe further use the mixed attention block to improve the local-\nlevel fine details of the saliency map. In addition, supervision\nis applied to each decoder level, aiming to facilitate the model\ntraining and improve the performance of the model.\nB. Swin Transformer Encoder\nTransformer based on the standard architecture [27] has\nmade competitive achievements in image classification, where\nthe relationships between a token and all other tokens are\ncomputed. However, the vanilla Transformer has quadratic\ncomplexity. This limits it to be applied to many other vision\nproblems such as SOD, which requires tremendous tokens for\ndense prediction or gaining a high-resolution image.\nFor efficient modeling, Swin Transformer uses window\nself-attention to replace standard global self-attention, reduc-\ning complexity to linear. To achieve information exchange\namong non-overlapping windows, shifted window partitioning\nis adopted, and the successive Swin Transformer blocks can\nbe formulated as:\nˆzl = W-MSA(LN(zl−1)) +zl−1,\nzl = MLP(LN(ˆzl)) +ˆzl,\nˆzl+1 = SW-MSA(LN(zl)) +zl,\nzl+1 = MLP(LN(ˆzl+1)) +ˆzl+1,\n(1)\nwhere ˆzl and zl denote the output features of the (S)W-MSA\nmodule and the MLP module for block l, respectively.\nC. Multistage Decoder\nFigure 4 shows the detail of our multistage decoder. As\nshown in Figure 4 (a), in one stage, we first let multilevel\nfeatures interact across 2 scales to enhance the quality of low-\nlevel features. Then, we transform multilevel features to the\nsame resolution by upsampling method. To better integrate\nsalient information after feature fusion, we further perform\nmixed attention.\n1) Multilevel Interaction Block: We argue that low-level\nfeatures contain more non-salient information and noise, and\nusing an all-pass skip-layer structure to concatenate the low-\nlevel features of the encoder to the decoder may cause a\nnegative impact on the saliency map. However, low-level\nfeatures are rich in local spatial details, which is crucial to\ngain high quality saliency maps. Some methods [38], [51]\nrescale the adjacent levels of features and employ upsampling\nor downsampling operations to align them at the same spatial\nresolution, followed by concatenation for further processing.\nNevertheless, the non-learnable nature of commonly used\nsampling operations (e.g., bilinear) leads to information loss,\nthereby limiting the model’s performance. To solve the loss\nof information during sampling, we propose our MIB block,\nwhich introduces the cross-attention mechanism to achieve the\ninteraction of multilevel features, letting high-level features\nguide low-level features to strengthen salient regions and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\nWindow\nAttention\nGlobal\nAttention\nNorm\nMlp\nMAB\n𝐹𝐹𝑖𝑖\n𝐴𝐴\n𝐹𝐹𝑖𝑖+1\n𝐴𝐴\nMlp\nMAB\n𝐹𝐹𝑖𝑖\n𝑀𝑀\nMlp\n×r ..\n.\nUpsample\nElement-wise Addition\nMatrix Multiplication\nL1×d\nL1×d\nConcat\n𝐹𝐹𝑖𝑖 𝐹𝐹𝑖𝑖+1𝐹𝐹𝑖𝑖+2\nCross\nAttention\nCross\nAttention\nNorm\nMlp\n𝐹𝐹𝑖𝑖\n𝐼𝐼\n𝐹𝐹1 𝐹𝐹2\nV2K2Q1\nL1×C1 L2×C2\nL1×d L2×d L2×d\nL1 × L2\nSoftmax\nL1 × C1\nLinear\nF\nL1 × C1\nnext step\nsupervision\n(a) (c)\n(b)\nFig. 4. Details of our multistage decoder. (a) denotes one stage of the decoder, (b) denotes the structure of cross-attention, and (c) denotes the structure of\nthe Mixed Attention Block. r denotes the number of MAB stacked and we set it to 2. Fi+1 can be from FI\ni+1 or FM\ni+1.\nImage\n GT\n Before\n After\nFig. 5. Visual comparison of our MIB where ‘Before’ denotes the features\nbefore MIB, and ‘After’ denotes the features after processed. The salient area\nhas been observably enhanced, and meanwhile, local noises have also been\neffectively reduced.\neliminate non-salient information. Through cross-attention,\nwe can accomplish feature interaction across multiple scales\nwithout changing them in spatial resolution.\nIn the MIB block, given the features in a sequence form\nF1 ∈ Rl1×c1 , F2 ∈ Rl2×c2 , where F1 denotes low-level fea-\ntures and F2 denotes high-level features. We first change their\nchannel dimension and embed them to queries Q1 ∈ Rl1×d,\nkeys K2 ∈ Rl2×d, and values V2 ∈ Rl2×d through three linear\nprojections. Then, we compute attention between the queries\nfrom low-level features with the keys from high-level features.\nThe output is computed as a weighted sum of the values,\nformulated as:\nAttention(Q1, K2, V2) =Softmax(Q1KT\n2 /\n√\nd)V2. (2)\nAs shown in Figure 4 (a), our MIB makes features interact\nacross two scales. Two cross-attentions are performed in\nparallel and gathered by element-wise addition. We follow the\nstandard Transformer architecture [48] in subsequent struc-\nture, including layer normalization [53], residual connections,\nformulated as:\nˆFi = Fi + CA(Fi, Fi+1) +CA(Fi, Fi+2),\nFI\ni = MLP(LN( ˆFi)) +ˆFi,\n(3)\nwhere Fi+1, Fi+2 denote the high-level features. ˆFi, FI\ni denote\nthe output features of the cross-attention (CA) modules and the\nMLP module in the MIB, respectively.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nTABLE I\nQUANTITATIVE COMPARISON OF OUR PROPOSED M3NET WITH OTHER 16 SOTA SOD METHODS ON SIX BENCHMARK DATASETS . THE SYMBOLS\n“↑”/“↓” MEAN THAT A HIGHER /LOWER SCORE IS BETTER . THE BEST RESULTS ARE SHOWN IN BOLD . ‘-R’, ’-R2’, AND ‘-S’ MEANS THE RESNET50,\nRES2NET50 [54], AND SWIN TRANSFORMER BACKBONE .\ndataset DUT-O DUTS ECSSD HKU-IS PASCAL-S SOD\nMethod M ↓ Em\nξ ↑ Sm ↑ Fw\nβ ↑ M ↓ Em\nξ ↑ Sm ↑ Fw\nβ ↑ M ↓ Em\nξ ↑ Sm ↑ Fw\nβ ↑ M ↓ Em\nξ ↑ Sm ↑ Fw\nβ ↑ M ↓ Em\nξ ↑ Sm ↑ Fw\nβ ↑ M ↓ Em\nξ ↑ Sm ↑ Fw\nβ ↑\nCNN Methods\nPiCANet .054 .865 .826 .743 .04 .915 .863 .812 .035 .953 .916 .908 .031 .951 .905 .89 .064 .9 .846 .811 .094 .846 .78 .741\nBASNet .056 .871 .836 .751 .048 .903 .866 .803 .037 .951 .916 .904 .032 .951 .909 .889 .076 .886 .838 .793 .112 .832 .772 .728\nCPD-R .056 .868 .825 .719 .043 .914 .869 .795 .037 .951 .918 .898 .034 .95 .905 .875 .071 .891 .848 .794 .11 .849 .771 .713\nPoolNet .054 .867 .831 .725 .037 .926 .887 .817 .035 .956 .926 .904 .03 .958 .919 .888 .065 .907 .865 .809 .103 .867 .792 .746\nAFNet .057 .861 .826 .717 .046 .91 .867 .785 .042 .947 .913 .886 .036 .949 .905 .869 .07 .895 .849 .797 .108 .847 .78 .726\nEGNet .053 .878 .841 .738 .039 .927 .887 .816 .037 .955 .925 .903 .031 .958 .918 .887 .074 .892 .852 .795 .097 .873 .807 .767\nITSD-R .061 .88 .84 .75 .041 .929 .885 .824 .034 .959 .925 .91 .031 .96 .917 .894 .066 .908 .859 .812 .093 .873 .809 .777\nMINet-R .056 .869 .833 .738 .037 .927 .884 .825 .033 .957 .925 .911 .029 .96 .919 .897 .064 .903 .856 .809 .092 .87 .805 .768\nLDF .052 .869 .839 .752 .034 .93 .892 .845 .034 .954 .924 .915 .028 .958 .919 .904 .06 .908 .863 .822 .093 .866 .8 .765\nCSF-R2 .055 .775 .838 .733 .037 .93 .89 .823 .033 .96 .93 .91 .03 .96 .921 .891 .069 .899 .862 .807 .098 .872 .8 .757\nGateNet-R .055 .876 .838 .729 .04 .928 .885 .809 .04 .952 .92 .894 .033 .955 .915 .88 .067 .904 .858 .797 .098 .87 .801 .753\nPFSNet .055 .878 .842 .756 .036 .931 .892 .842 .031 .958 .93 .92 .026 .962 .924 .91 .063 .906 .86 .819 .089 .875 .81 .781\nICON-R .057 .884 .844 .761 .037 .932 .889 .837 .032 .96 .929 .918 .029 .96 .92 .902 .064 .908 .861 .818 .084 .882 .824 .794\nM3Net-R .061 .88 .848 .769 .036 .937 .897 .849 .029 .962 .931 .919 .026 .966 .929 .913 .06 .912 .868 .827 .084 .865 .819 .784\nTransformer Methods\nVST .058 .89 .852 .758 .037 .94 .897 .831 .032 .965 .934 .911 .029 .968 .928 .897 .06 .919 .874 .821 .085 .876 .82 .776\nICON-S .043 .906 .869 .804 .025 .96 .917 .886 .023 .972 .941 .936 .022 .974 .935 .925 .048 .93 .885 .854 .083 .885 .825 .802\nSRformer .043 .894 .86 .784 .027 .952 .91 .872 .028 .961 .932 .922 .025 .966 .928 .912 .051 .924 .878 .845 .088 .862 .809 .77\nBBRF .044 .899 .861 .803 .025 .952 .909 .886 .022 .972 .939 .944 .02 .972 .932 .932 .049 .927 .878 .856 .078 .868 .822 .802\nM3Net-S .045 .903 .872 .811 .024 .96 .927 .902 .021 .974 .948 .947 .019 .977 .943 .937 .047 .932 .889 .864 .073 .871 .838 .819\nBy using the proposed MIB, the salient region at the global\nlevel can be effectively enhanced. As can be seen in Figure 5,\nafter feeding the features into our MIB, the salient region is\nnoticeably distinguished from the non-salient areas.\n2) Mixed Attention Block: To facilitate the aggregation\nof multilevel features in the sequence form, [31] adopts\none Transformer layer with global self-attention, which may\nneglect local-level details and limit the fineness of the final pre-\ndiction. Inspired by the success of window self-attention [35],\nwhich computes self-attention within local windows, we com-\nbine global self-attention and window self-attention, aiming to\nmodel context at both global and local levels, further improve\nthe local accuracy of the prediction map. We use the same\nwindow size (set to 7 × 7 by default) as the encoder.\nAs can be seen in Figure 4 (a), given feature in a sequence\nform FC\ni ∈ Rl×c after feature fusion, we first increase\nits channel dimension to d = 384 by an MLP, and then\nlet it pass r mixed attention blocks. Similar to our MIB,\nwindow self-attention and global self-attention are performed\nin parallel, and gathered by element-wise addition, followed by\nstandard Transformer architecture. Finally, we apply another\nMLP to restore the channel dimension of the feature for\nsubsequent processing and supervision. The whole process can\nbe formulated as:\nFA\n0 = MLP1(FI\ni ),\nˆFA\n1 = W-MSA(FA\n0 ) +MSA(FA\n0 ),\nFA\n1 = MLP(LN( ˆFA\n1 )) +ˆFA\n1 ,\n...\nFM\ni = MLP2(FA\nr ),\n(4)\nwhere ˆFA\ni and FA\ni denote the output features of the mixed\nattention module and the MLP module in our MABs, respec-\ntively. r denotes the number of MAB stacked and we set it to\n2.\n3) Upsampling Methods and Multilevel Fusion: Most\nCNN-based methods [38], [51] adopt bilinear interpolation\nto re-scale high resolution feature maps, while upsampling\nfeatures in the sequence form remain under-studied. In this\nwork, we adopt the RT2T method developed in [31], [50] for\ntoken upsampling, where we found that fold with overlap leads\nto other upsampling methods not only in evaluation indicators\nbut also in the visual quality of saliency map.\nWe first use a linear projection to expand the channel\ndimension of FI\ni ∈ Rl×c to ck2. Each token is seen as a k ×k\nimage patch and neighboring patches have k − s overlapping,\nwhere s denotes the stride. Then we fold the tokens with p\nzero-padding. The arguments need to satisfy:\nl = h × w = ⌊ho + 2p − k\ns + 1⌋ × ⌊wo + 2p − k\ns + 1⌋, (5)\nwhere ho × wo denotes the spatial size of the feature maps\nafter folding. We reshape the feature map to the tokens with\nthe size lo × c, where lo = ho × wo, and define it as FU\ni . We\nfollow [31] to set k = [3, 3, 7], s = [2, 2, 4], and p = [1, 1, 2]\nto get the same resolution as the original input image after\nthree times upsampling. The upsampling ratio of each time is\nequal to the stride.\nAs can be seen in Figure 4 (a), we fuse multilevel features\nafter upsampling high-level features, and then use our MABs\nto further integrate salient information at different levels. One\nwhole stage of the decoder can be formulated as:\nFU\ni+1 = Upsample(FM\ni+1),\nFI\ni = MIB(Fi, Fi+1, Fi+2),\nFC\ni = Concat(FI\ni , FU\ni+1),\nFM\ni = MABs(FC\ni ),\n(6)\nwhere FM\ni+1 denotes the output of previous stage (F3 employed\nin the first stage) and FC\ni denotes the feature maps after fusing.\nFM\ni will be used for the next stage of the decoder and the\nmultilevel supervision.\n4) Multilevel Supervision Strategy: After each stage of our\nmultistage decoder, the channel of features will be reduced\nto 1-dimension, expressed as FP\ni ∈ Rl×1, for multilevel\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\nTABLE II\nCOMPARISON OF OUR PROPOSED MODEL WITH OTHER SOTA CNN- BASED SOD METHODS ON SOC TEST SET . THE BEST RESULTS ARE SHOWN IN BOLD .\nAttr Metrics Amulet DSS NLDF C2SNet SRM R3Net BMPM DGRL PiCA-R RANet AFNet CPD PoolNet EGNet BANet SCRN ICON-R Ours-R\nAC\nM ↓ .120 .113 .119 .109 .096 .135 .098 .081 .093 .132 .084 .083 .094 .085 .086 .078 .062 .071\nEm\nξ ↑ .791 .788 .784 .807 .824 .753 .815 .853 .815 .765 .852 .843 .846 .854 .858 .849 .891 .885\nSm ↑ .752 .753 .737 .755 .791 .713 .780 .790 .792 .708 .796 .799 .795 .806 .806 .809 .835 .824\nFw\nβ ↑ .620 .629 .620 .647 .690 .593 .680 .718 .682 .603 .712 .727 .713 .731 .740 .724 .784 .764\nBO\nM ↓ .346 .356 .354 .267 .306 .445 .303 .215 .200 .454 .245 .257 .353 .373 .271 .224 .200 .199\nEm\nξ ↑ .551 .537 .539 .661 .616 .419 .620 .725 .741 .404 .698 .665 .554 .528 .650 .706 .740 .763\nSm ↑ .574 .561 .568 .654 .614 .437 .604 .684 .729 .421 .658 .647 .561 .528 .645 .698 .714 .704\nFw\nβ ↑ .612 .614 .622 .730 .667 .456 .670 .786 .799 .453 .741 .739 .610 .585 .720 .778 .794 .783\nCL\nM ↓ .141 .153 .159 .144 .134 .182 .123 .119 .123 .188 .119 .114 .134 .139 .117 .113 .113 .112\nEm\nξ ↑ .789 .763 .764 .789 .793 .710 .801 .824 .794 .715 .802 .821 .801 .790 .824 .820 .829 .832\nSm ↑ .763 .722 .713 .742 .759 .659 .761 .770 .787 .624 .768 .773 .760 .757 .784 .795 .789 .791\nFw\nβ ↑ .663 .617 .614 .655 .665 .546 .678 .714 .692 .542 .696 .724 .681 .677 .726 .717 .732 .736\nHO\nM ↓ .119 .124 .126 .123 .115 .136 .116 .104 .108 .143 .103 .097 .100 .106 .094 .096 .092 .091\nEm\nξ ↑ .810 .796 .798 .805 .819 .782 .813 .833 .819 .777 .834 .845 .846 .829 .850 .842 .852 .865\nSm ↑ .791 .767 .755 .768 .794 .740 .781 .791 .809 .713 .798 .803 .815 .802 .819 .823 .818 .823\nFw\nβ ↑ ..688 .660 .661 .668 .696 .633 .684 .722 .704 .626 .722 .751 .739 .720 .754 .743 .752 .767\nMB\nM ↓ .142 .132 .138 .128 .115 .160 .105 .113 .099 .139 .111 .106 .121 .109 .104 .100 .100 .077\nEm\nξ ↑ .739 .753 .740 .778 .778 .697 .812 .823 .813 .761 .762 .804 .779 .789 .803 .817 .828 .88\nSm ↑ .712 .719 .685 .720 .742 .657 .762 .744 .775 .696 .734 .754 .751 .762 .764 .792 .774 .819\nFw\nβ ↑ .561 .577 .551 .593 .619 .489 .651 .655 .637 .576 .626 .679 .642 .649 .672 .690 .699 .74\nOC\nM ↓ .143 .144 .149 .130 .129 .168 .119 .116 .119 .169 .109 .115 .119 .121 .112 .111 .106 .99\nEm\nξ ↑ .763 .760 .755 .784 .780 .706 .800 .808 .784 .718 .820 .810 .801 .798 .809 .800 .817 .843\nSm ↑ .735 .718 .709 .738 .749 .653 .752 .747 .765 .641 .771 .750 .756 .754 .765 .775 .771 .786\nFw\nβ ↑ .607 .595 .593 .622 .630 .520 .644 .659 .638 .527 .680 .672 .659 .658 .678 .673 .683 .709\nOV\nM ↓ .173 .180 .184 .159 .150 .216 .136 .125 .127 .217 .129 .134 .148 .146 .119 .126 .120 .109\nEm\nξ ↑ .751 .737 .736 .790 .779 .663 .807 .828 .810 .664 .817 .803 .795 .802 .835 .808 .834 .838\nSm ↑ .721 .700 .688 .728 .745 .624 .751 .762 .781 .611 .761 .748 .747 .752 .779 .774 .779 .795\nFw\nβ ↑ .637 .622 .616 .671 .682 .527 .701 .733 .721 .529 .723 .721 .697 .707 .752 .723 .749 .759\nSC\nM ↓ .098 .098 .101 .100 .090 .114 .081 .087 .093 .110 .076 .080 .075 .083 .078 .078 .080 .078\nEm\nξ ↑ .794 .799 .788 .806 .814 .765 .841 .837 .799 .792 .854 .858 .856 .844 .851 .843 .860 .871\nSm ↑ .768 .761 .745 .756 .783 .716 .799 .772 .784 .724 .808 .793 .807 .793 .807 .809 .803 .808\nFw\nβ ↑ .608 .599 .593 .611 .638 .550 .677 .669 .627 .594 .696 .708 .695 .678 .706 .691 .714 .714\nSO\nM ↓ .119 .109 .115 .116 .099 .118 .096 .092 .095 .113 .089 .091 .087 .098 .090 .082 .087 .079\nEm\nξ ↑ .745 .756 .747 .752 .769 .732 .780 .802 .766 .759 .792 .804 .814 .784 .801 .797 .816 .835\nSm ↑ .718 .713 .703 .706 .737 .682 .732 .736 .748 .682 .746 .745 .768 .749 .755 .767 .763 .778\nFw\nβ ↑ .523 .524 .526 .531 .561 .487 .567 .602 .566 .518 .596 .623 .626 .594 .621 .614 .634 .66\nsupervision. Previous methods mostly apply BCE as the loss\nfunction, which treats every pixel equally and may neglect the\nrelation between pixels. We additionally use the IoU loss [18],\n[55] to supervise the structural inconsistency between the\nprediction and the ground truth. The BCE loss is defined as:\nLBCE = −\nHX\nx=1\nWX\ny=1\n[G(x, y)logP (x, y)\n+(1 − G(x, y))log(1 − P(x, y))],\n(7)\nwhere H, W are the height and width of the image, and\nP(x, y), G(x, y) denote the pixels of the prediction and the\nground truth at position (x, y), respectively. Meanwhile, the\nIoU loss is formulated as:\nLIoU = 1−\nPH\nx=1\nPW\ny=1(P(x, y)G(x, y))\nPH\nx=1\nPW\ny=1(P(x, y) +G(x, y) − P(x, y)G(x, y))\n,\n(8)\nThen the joint loss can be defined as:\nL(P, G) =LBCE + LIoU . (9)\nDuring training, we also use the multilevel supervision strategy\nwidely used in [31], [38], [56]–[58] to facilitate the training\nprocess.\nIV. E XPERIMENTS\nA. Implementation Details\nFor fair comparison, we follow recent methods to use\nthe DUTS-TR (10553 images) [59] to train our M 3Net and\nwe resize images to 224×224. Normalization, random 90◦\nrotation, and crop were applied as the data augmentation.\nAccording to [60], we set the batch size as 8 to avoid\nweakening the generalization ability of the model. We use the\nAdam optimizer to train our network for 120 epochs and set\nits initial learning rate to 0.0001. In order to make the model\nconverge better, we reduce the learning rate to 0.00002 in the\nlast 20 epochs. All experiments were implemented on an RTX\n3090 GPU.\nB. Evaluation Datasets and Metrics\nWe evaluate our M 3Net on six widely used bench-\nmark datasets, including DUT-OMORN [61] (5168 images),\nDUTS-TE [59] (5019 images), ECSSD [62] (1000 images),\nHKU-IS [63] (4447 images), PASCAL-S [64] (850 images),\nSOD [65] (300 images).\nWe use four metrics to evaluate our model and the existing\nstate-of-the-art algorithms:\n(1) MAE. MAE evaluates the average pixel-wise difference\nbetween the prediction P and the ground-truth G, and is\ncomputed as MAE = 1\nH×W\nPH\nx=1\nPW\ny=1 |P(x, y)−G(x, y)|.\n(2) E-measure . E-measure [66] considers the local pixel\nvalues with the image-level mean value simultaneously: Eξ =\n1\nH×W\nPH\nx=1\nPW\ny=1 ϕξ(i, j) where ϕξ denotes the enhanced\nalignment matrix.\n(3) S-measure. S-measure [67] aims to measure the struc-\ntural similarity between the prediction and ground truth, which\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nTABLE III\nTHE PERFORMANCE OF OUR MODEL UNDER DIFFERENT INPUTS AND BACKBONES WITH VARYING SCALES .\nMethod Backbone Input Size MACs Params DUTS ECSSD HKU-IS\nM ↓ Em\nξ ↑ Sm ↑ Fw\nβ ↑ M ↓ Em\nξ ↑ Sm ↑ Fw\nβ ↑ M ↓ Em\nξ ↑ Sm ↑ Fw\nβ ↑\nOurs-R ResNet50 224×224 18.83 34.61 0.036 0.937 0.897 0.849 0.029 0.962 0.931 0.919 0.026 0.966 0.929 0.913\nOurs-R ResNet50 352×352 46.49 34.61 0.039 0.931 0.898 0.856 0.03 0.959 0.932 0.923 0.025 0.964 0.93 0.916\nOurs-S SwinS 224×224 23.11 59.07 0.028 0.952 0.913 0.878 0.024 0.97 0.941 0.935 0.023 0.97 0.934 0.921\nOurs-S SwinB 224×224 39.51 102.62 0.026 0.955 0.918 0.885 0.022 0.974 0.945 0.941 0.021 0.975 0.939 0.928\nOurs-S SwinB 384×384 116.12 102.75 0.024 0.96 0.927 0.902 0.021 0.974 0.948 0.947 0.019 0.977 0.943 0.937\nThresholds\n0.0 0.2 0.4 0.6 0.8 1.0\nThresholds\n0.60\n0.65\n0.70\n0.75\n0.80F-measure\nDUT-O\nOurs-R\nCSF\nTSPOANet\nF3Net\nPoolNet\nCPD-R\nBASNet\nRAS\nMINet-R\nICON-R\nITSD-R\nGateNet-R\n0.0 0.2 0.4 0.6 0.8\nRecall\n0.70\n0.75\n0.80\n0.85\n0.90Precision\nDUT-O\nOurs-R\nCSF\nTSPOANet\nF3Net\nPoolNet\nCPD-R\nBASNet\nRAS\nMINet-R\nICON-R\nITSD-R\nGateNet-R\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.80\n0.85\n0.90\n0.95\nDUTS\nOurs-R\nCSF\nTSPOANet\nF3Net\nPoolNet\nCPD-R\nBASNet\nRAS\nMINet-R\nICON-R\nITSD-R\nGateNet-R\n0.0 0.2 0.4 0.6 0.8 1.0\nThresholds\n0.70\n0.75\n0.80\n0.85\n0.90\nDUTS\nOurs-R\nCSF\nTSPOANet\nF3Net\nPoolNet\nCPD-R\nBASNet\nRAS\nMINet-R\nICON-R\nITSD-R\nGateNet-R\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.80\n0.85\n0.90\n0.95\n1.00\nECSSD\nOurs-R\nCSF\nTSPOANet\nF3Net\nPoolNet\nCPD-R\nBASNet\nRAS\nMINet-R\nICON-R\nITSD-R\nGateNet-R\n0.0 0.2 0.4 0.6 0.8 1.0\nThresholds\n0.80\n0.85\n0.90\n0.95\nECSSD\nOurs-R\nCSF\nTSPOANet\nF3Net\nPoolNet\nCPD-R\nBASNet\nRAS\nMINet-R\nICON-R\nITSD-R\nGateNet-R\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.80\n0.85\n0.90\n0.95\n1.00\nHKU-IS\nOurs-R\nCSF\nTSPOANet\nF3Net\nPoolNet\nCPD-R\nBASNet\nRAS\nMINet-R\nICON-R\nITSD-R\nGateNet-R\n0.700.0 0.2 0.4 0.6 0.8 1.0\nThresholds\n0.75\n0.80\n0.85\n0.90\n0.95\nHKU-IS\nOurs-R\nCSF\nTSPOANet\nF3Net\nPoolNet\nCPD-R\nBASNet\nRAS\nMINet-R\nICON-R\nITSD-R\nGateNet-R\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.80\n0.85\n0.90\n0.95\nPASCAL-S\nOurs-R\nCSF\nTSPOANet\nF3Net\nPoolNet\nCPD-R\nBASNet\nRAS\nMINet-R\nICON-R\nITSD-R\nGateNet-R\n0.0 0.2 0.4 0.6 0.8 1.0\nThresholds\n0.70\n0.75\n0.80\n0.85\n0.90\nPASCAL-S\nOurs-R\nCSF\nTSPOANet\nF3Net\nPoolNet\nCPD-R\nBASNet\nRAS\nMINet-R\nICON-R\nITSD-R\nGateNet-R\nFig. 6. Precision-Recall curves (row 1) and F-measure curves (row 2) of our M 3Net-R and other CNN SOTA methods on five benchmark datasets.\nis computed as Sm = αSo + (1−α)Sr. So and Sr denote the\nobject-level and region-level structural similarity, respectively,\nand α is set to 0.5.\n(4) Weighted F-measure . Weighted F-measure [68] gives\nan intuitive generalization of Fβ, calculated as Fw\nβ =\n(1+β2)×Precision w×Recallw\nβ2×Precision w+Recallw . It extends the four basic quantities\nTP, TN, FP and FN to real values, and assigns different\nweights (w) to different errors based on different locations\nand neighborhood information.\nC. Comparison with State-of-the-Art\nWe compare our M 3Net with 30 state-of-the-art methods,\nincluding PiCANet [41], RAS [69], Amulet [14], DSS [21],\nNLDF [70], C2SNet [71], SRM [72], R3Net [73], BMPM\n[74], DGRL [75], RANet [76], BANet [77], BASNet [18],\nCPD [19], PoolNet [56], AFNet [78], TSPOANet [79], EG-\nNet [44], SCRN [80], F3Net [57], ITSD [81], MiNet [38],\nLDF [82], CSF [83], GateNet [39], PFSNet [84], ICON [51],\nVST [31], SRformer [32] and BBRF [52]. We also adopt\nResNet-50 as the backbone to compare with CNN-based\nmethods, in which case our model is referred to as M 3Net-R.\n1) Quantitative Comparison: Table I shows the quantitative\ncomparison results on six widely used benchmark datasets. We\ncompare our method with 16 state-of-the-art methods in terms\nof MAE, Eξ, Sm, Fw\nβ . Regarding M 3Net-R, an input size of\n224 x 224 is adopted. In the case of M 3Net-S, an input size of\n384 x 384 is used. The results show that our M 3Net not only\noutperforms all previous state-of-the-art Transformer based\nmethods but also achieves competitive results in CNN-based\nmethods. We also report the performance of our model under\ndifferent inputs and backbones with varying scales. As shown\nin Table III, with the increase in the size of the input image,\nboth the performance and computational cost of the model\nincrease simultaneously. Therefore, we can adjust the input\nsize of the model to accommodate different computational\nrequirements. In addition, we present the precision-recall [85]\nand F-measure curves [86] in Figure 6.\nRecently, Fan et al. proposed a challenging dataset known as\nSOC [87], which is based on real-world scenarios. In contrast\nto previous datasets, the SOC dataset comprises more complex\nscenes and is divided into nine distinct subsets based on image\nattributes, including AC (appearance change), BO (big object),\nCL (clutter), HO (heterogeneous object), MB (motion blur),\nOC (occlusion), OV (out-of-view), SC (shape complexity),\nand SO (small object). Evaluating the performance of our\nproposed model in comparison to previous methods on the\nSOC dataset provides a more comprehensive validation of\nits performance. Table II presents a comparison between our\nmodel and 17 recent state-of-the-art CNN-based methods,\nfocusing on attribute-based performance. Notably, our model\ndemonstrates significant performance improvement compared\nto the existing methods.\n2) Visual Comparison: Figure 7 provides visual compar-\nisons between our method and other methods. As can be\nseen, our M 3Net reconstructs more accurate saliency maps,\nwhile local fine-grained details are well preserved. Besides,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nImage\n GT\n M3Net-S\n BBRF\n[52]\nICON-S\n[51]\nSRformer\n[32]\nVST\n[31]\nGateNet\n[39]\nMiNet\n[38]\nF3Net\n[57]\nFig. 7. Visual comparisons between our M 3Net and other 7 state-of-the-art methods on various scenes. In contrast to previous methods, our M 3Net generates\nprediction maps with fewer shadows and undersaturated regions, thereby enhancing its reliability.\nTABLE IV\nABLATION STUDIES OF OUR PROPOSED MODEL . “B I” DENOTES BILINEAR UPSAMPLING , “F” DENOTES FOLD UPSAMPLING , “MIB” DENOTES OUR\nPROPOSED MULTILEVEL INTERACTION BLOCK , AND “MAB” DENOTES OUR PROPOSED MIXED ATTENTION BLOCK .\nID Component Settings DUTS ECSSD HKU-IS\nM ↓ Em\nξ ↑ Sm ↑ Fw\nβ ↑ M ↓ Em\nξ ↑ Sm ↑ Fw\nβ ↑ M ↓ Em\nξ ↑ Sm ↑ Fw\nβ ↑\n1 +Bi 0.046 0.9 0.851 0.78 0.046 0.936 0.897 0.879 0.035 0.945 0.896 0.872\n2 +F 0.042 0.911 0.868 0.802 0.042 0.939 0.906 0.888 0.031 0.953 0.911 0.888\n3 +F+MIB 0.038 0.93 0.891 0.84 0.035 0.953 0.923 0.907 0.028 0.96 0.923 0.904\n4 +F+MAB 0.037 0.933 0.892 0.842 0.035 0.953 0.924 0.908 0.028 0.961 0.923 0.904\n5 +F+MIB+MAB 0.036 0.937 0.897 0.849 0.029 0.962 0.931 0.919 0.026 0.966 0.929 0.913\nTABLE V\nABLATION STUDIES OF DIFFERENT FEATURE ENHANCEMENT METHODS\nCOMPARED WITH OUR MIB.\nID FEMs Settings DUTS ECSSD HKU-IS\nM ↓ Sm ↑ Fw\nβ ↑ M ↓ Sm ↑ Fw\nβ ↑ M ↓ Sm ↑ Fw\nβ ↑\n3 MIB .038 .891 .84 .035 .923 .907 .028 .923 .904\n6 DFA [51] .045 .878 .796 .04 .92 .892 .035 .915 .88\n7 RFB [88] .043 .882 .807 .039 .921 .893 .035 .915 .88\n8 AIM [38] .041 .888 .809 .037 .923 .898 .033 .919 .887\nTABLE VI\nDIFFERENT CROSS -LEVELS IN OUR MIB.\nID Across Levels DUTS ECSSD HKU-IS\nM ↓ Sm ↑ Fw\nβ ↑ M ↓ Sm ↑ Fw\nβ ↑ M ↓ Sm ↑ Fw\nβ ↑\n3 2 .038 .891 .84 .035 .923 .907 .028 .923 .904\n9 1 .039 .889 .837 .034 .923 .906 .029 .922 .904\n10 3 .039 .887 .835 .037 .92 .906 .03 .922 .903\nour method excels in dealing with challenging cases like small\nobjects (row 2), low-contrast (row 4), complex backgrounds\n(row 3), delicate structures (row 1), and multiple objects (rows\n4 and 5) integrally and noiselessly. The above results show the\nversatility and robustness of our M 3Net.\nD. Ablation Studies\nTo demonstrate the effectiveness of different modules in\nour M 3Net, we conduct the quantitative results of several\nsimplified versions of our method. The experimental results\nTABLE VII\nDIFFERENT INTERACTION MODES IN OUR MIB.\nID Interaction Settings DUTS ECSSD HKU-IS\nM ↓ Sm ↑ Fw\nβ ↑ M ↓ Sm ↑ Fw\nβ ↑ M ↓ Sm ↑ Fw\nβ ↑\n3 high to low .038 .891 .84 .035 .923 .907 .028 .923 .904\n11 low to high .042 .886 .836 .038 .911 .898 .031 .914 .891\n12 bi-directional .039 .889 .837 .036 .923 .907 .029 .922 .902\non three datasets including DUTS-TE, ECSSD, HKU-IS are\ngiven in Table IV. We start from a UNet-like structure with\nskip connections and bilinear upsampling as the baseline and\nprogressively incorporate the proposed modules, including\nMIB and MAB.\n1) Effectiveness of MIB: For enhancing multilevel features\nfrom the encoder, we use our MIB to strengthen salient re-\ngions and reduce non-salient information of low-level features,\nshown as “+MIB” in Table 2. The results show that our MIB\ngains notable improvement, demonstrating its effectiveness.\nTo verify the superiority of the proposed MIB, we compared\nit with existing feature enhancement methods, including DFA\n[51], RFB [88], and AIM [38]. As can be seen in Table V, the\nproposed MIB demonstrates advantages across all performance\nmetrics. This suggests that utilizing the complementarity of\nmultiscale features can lead to improved feature enhancement\neffects.\nTo further investigate the effectiveness of our MIB and the\ninteraction between multilevel features, we made adjustments\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\nImage\n GT\n Window\n Global\n Mixed\nFig. 8. Visual comparison of the ablation study on our MAB. It can be\nseen that the mixed attention, which combines window attention and global\nattention, can not only ensure accurate object localization but also effectively\npreserve local details.\nTABLE VIII\nABLATION STUDIES OF DIFFERENT SALIENCY INTEGRATION METHODS\nCOMPARED WITH OUR MAB.\nID Integration Methods DUTS ECSSD HKU-IS\nM ↓ Sm ↑ Fw\nβ ↑ M ↓ Sm ↑ Fw\nβ ↑ M ↓ Sm ↑ Fw\nβ ↑\n5 MAB .036 .897 .849 .029 .931 .919 .026 .929 .913\n13 SIM [38] .037 .895 .834 .035 .925 .904 .032 .921 .893\n14 AFM [89] .04 .894 .837 .036 .923 .901 .031 .923 .894\nto the scale range of the MIB. For instance, when we combine\ntwo scales, we employ features at 1/8 and 1/16 levels to\ninteract with features at the 1/4 level. Conversely, when we\nintersect two scales, we exclusively use features at the 1/8\nlevel to interact with features at the 1/4 level. The results are\npresented in Table VI.\nIt is worth mentioning that the interaction within our MIB\nis unidirectional, where the high-level features solely guide\nthe low-level features. Hence, we also endeavored to inte-\ngrate reverse and bidirectional interaction in our MIB, and\nthe outcomes are showcased in Table VII. The low-to-high\ninteraction is observed to result in performance degradation,\nwhich could be attributed to the presence of noise and non-\nsalient information in the low-level features.\n2) Effectiveness of MAB: To better integrate salient infor-\nmation from multilevel features after fusing them, we use our\nMABs to model context at both global and local levels to gain\nhigh quality salient map, shown as “+MAB” in Table IV. The\nresults demonstrate the effectiveness of our MAB.\nIn order to demonstrate the superior performance of the\nproposed MAB in saliency integration, we conduct a com-\nparative evaluation with existing methods, including SIM\n[38], AFM [89], shown as Table VIII. The effectiveness of\nthe proposed MAB in integrating salient information while\npreserving intricate local details is evident, resulting in the\ngeneration of saliency predictions of high quality.\nTo further explore the effectiveness of mixed attention, we\nconduct extra experiments, the results are shown in Table\nIX and Figure 8. We find that mixed attention can better\nretain local details with a slight additional computational cost.\nBesides, 7 × 7 is also a more suitable window size.\n3) Evaluation of upsampling method: Upsampling methods\nfor features in the form of a sequence remain under studied.\nWe compare several widely used upsampling methods with our\nM3Net, including bilinear, pixel shuffle, and fold. The results\nare shown in Table X and Fig 9. From the comparison result,\nTABLE IX\nABLATION STUDY OF DIFFERENT ATTENTION SETTINGS IN OUR MAB.\nh × w CAN BE SEEN AS GLOBAL SELF -ATTENTION .\nID Attention Settings DUTS ECSSD HKU-IS\nM ↓ Sm ↑ Fw\nβ ↑ M ↓ Sm ↑ Fw\nβ ↑ M ↓ Sm ↑ Fw\nβ ↑\n15 h × w .036 .894 .844 .03 .929 .918 .027 .927 .91\n16 7 × 7 .038 .895 .843 .029 .929 .917 .028 .927 .908\n5 7 × 7 +h × w .036 .897 .849 .029 .931 .919 .026 .929 .913\n17 4 × 4 +h × w .038 .891 .839 .03 .929 .917 .028 .926 .906\n18 14 × 14 +h × w .036 .896 .847 .029 .931 .918 .027 .928 .91\n19 7 × 7 + 14× 14 .036 .894 .845 .03 .929 .917 .026 .926 .909\nTABLE X\nABLATION STUDIES OF THE UPSAMPLING METHODS ADOPTED IN THE\nM3NET.\nID Upsampling Methods DUTS ECSSD HKU-IS\nM ↓ Sm ↑ Fw\nβ ↑ M ↓ Sm ↑ Fw\nβ ↑ M ↓ Sm ↑ Fw\nβ ↑\n20 pixel shuffle .036 .895 .845 .031 .928 .915 .027 .926 .907\n21 bilinear .037 .89 .837 .031 .927 .914 .028 .923 .903\n22 fold .037 .893 .843 .029 .93 .917 .026 .927 .91\n5 fold with overlap .036 .897 .849 .029 .931 .919 .026 .929 .913\nwe found that fold with overlap not only leads in evaluation\nmetrics but also makes the predicted saliency map closest to\nthe real one.\n4) Evaluation of Loss Function: We use different loss\nfunctions to train our M 3Net, and the results are shown in\nTable XI. We find that the model supervised by BCE loss is\nweak in Fw\nβ [68], which may be caused by BCE ignoring\nthe relations between pixels, while IoU loss based on the\nwhole region, effectively made up for the deficiencies of BCE\nloss. LwBCE and LwIoU [57] assign different weights to\ndifferent regions, aiming to let the model pay more attention\nto complex regions. However, as shown in Table XI, it has\nnot brought visible improvement to the performance of our\nmodel. We speculate that this lack of improvement may\nstem from the model’s pre-existing proficiency in attending\nto complex regions. Consequently, the emphasis on complex\nregions driven by weighted loss has the potential to impede\nthe model’s perception of non-complex regions.\nImage\n GT\n Fold with\noverlap\nFold\n Bilinear\n Pixel\nshuffle\nFig. 9. Visual comparisons of upsampling methods. As can be observed, the\nprediction imaps obtained through upsampling with Fold and Pixel Shuffle\nexhibit undersaturation issues, while Bilinear interpolation results in a notable\nloss of local details.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\nTABLE XI\nABLATION STUDY OF DIFFERENT LOSS FUNCTION SETTINGS WITH OUR\nM3NET.\nID Loss Settings DUTS ECSSD HKU-IS\nM ↓Sm ↑Fw\nβ ↑M ↓Sm ↑Fw\nβ ↑M ↓Sm ↑Fw\nβ ↑\n23 Lbce .039 .895 .835 .032 .93 .909 .029 .928 .903\n5 Lbce + Liou .036 .897 .849 .029 .931 .919 .026 .929 .913\n24Lwbce + Lwiou [57] .038 .895 .846 .03 .93 .917 .027 .927 .909\nV. C ONCLUSION\nIn this paper, we propose a novel Transformer based net-\nwork dubbed M3Net for SOD. Considering the uniqueness and\ninterdependence of multilevel features, we first propose the\nMIB to achieve the interaction between multilevel features\nand thus enhance the localized salient regions in low-level\nfeatures. Secondly, we design the MAB which integrates the\nglobal and window self-attentions, aiming at modeling local\ncontext to refine the fine-grained details of the objects. Finally,\nwe design a multistage decoder by employing the MIB and\nMAB blocks and optimize the multilevel features step by\nstep. Our M 3Net model achieves state-of-the-art results on\nsix challenging datasets without relying on heavy numerical\ncomputations, thus showing great potential for the SOD task\nin practical application.\nREFERENCES\n[1] Y . Liu, J. Han, Q. Zhang, and L. Wang, “Salient object detection via\ntwo-stage graphs,” IEEE Transactions on Circuits and Systems for Video\nTechnology, vol. 29, no. 4, pp. 1023–1037, 2019.\n[2] Z. Ren, S. Gao, L.-T. Chia, and I. W.-H. Tsang, “Region-based saliency\ndetection and its application in object recognition,” IEEE Transactions\non Circuits and Systems for Video Technology , vol. 24, no. 5, pp. 769–\n779, 2014.\n[3] G. Sun, W. Wang, J. Dai, and L. Van Gool, “Mining cross-image\nsemantics for weakly supervised semantic segmentation,” in ECCV,\n2020.\n[4] J.-Y . Zhu, J. Wu, Y . Xu, E. Chang, and Z. Tu, “Unsupervised object class\ndiscovery via saliency-guided multiple class learning,” IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence , vol. 37, no. 4, pp.\n862–875, 2015.\n[5] A. Karpathy, S. Miller, and L. Fei-Fei, “Object discovery in 3d scenes\nvia shape analysis,” in 2013 IEEE International Conference on Robotics\nand Automation, 2013, pp. 2088–2095.\n[6] A. Li, Y . Mao, J. Zhang, and Y . Dai, “Mutual information regularization\nfor weakly-supervised rgb-d salient object detection,” IEEE Transactions\non Circuits and Systems for Video Technology , 2023.\n[7] X. Jin, K. Yi, and J. Xu, “Moadnet: Mobile asymmetric dual-stream\nnetworks for real-time and lightweight rgb-d salient object detection,”\nIEEE Transactions on Circuits and Systems for Video Technology ,\nvol. 32, no. 11, pp. 7632–7645, 2022.\n[8] G. Chen, F. Shao, X. Chai, H. Chen, Q. Jiang, X. Meng, and Y .-S. Ho,\n“Modality-induced transfer-fusion network for rgb-d and rgb-t salient\nobject detection,” IEEE Transactions on Circuits and Systems for Video\nTechnology, vol. 33, no. 4, pp. 1787–1801, 2023.\n[9] G. Liao, W. Gao, G. Li, J. Wang, and S. Kwong, “Cross-collaborative\nfusion-encoder network for robust rgb-thermal salient object detection,”\nIEEE Transactions on Circuits and Systems for Video Technology ,\nvol. 32, no. 11, pp. 7646–7661, 2022.\n[10] Q. Zhang, T. Xiao, N. Huang, D. Zhang, and J. Han, “Revisiting feature\nfusion for rgb-t salient object detection,” IEEE Transactions on Circuits\nand Systems for Video Technology, vol. 31, no. 5, pp. 1804–1818, 2021.\n[11] Q. Zhang, S. Wang, X. Wang, Z. Sun, S. Kwong, and J. Jiang, “A multi-\ntask collaborative network for light field salient object detection,” IEEE\nTransactions on Circuits and Systems for Video Technology , vol. 31,\nno. 5, pp. 1849–1861, 2021.\n[12] Y . Chen, G. Li, P. An, Z. Liu, X. Huang, and Q. Wu, “Light field salient\nobject detection with sparse views via complementary and discriminative\ninteraction network,” IEEE Transactions on Circuits and Systems for\nVideo Technology, pp. 1–1, 2023.\n[13] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in Medical Image Computing\nand Computer-Assisted Intervention–MICCAI 2015: 18th International\nConference, Munich, Germany, October 5-9, 2015, Proceedings, Part III\n18. Springer, 2015, pp. 234–241.\n[14] P. Zhang, D. Wang, H. Lu, H. Wang, and X. Ruan, “Amulet: Aggre-\ngating multi-level convolutional features for salient object detection,” in\nProceedings of the IEEE International Conference on Computer Vision\n(ICCV), Oct 2017.\n[15] T. Wang, L. Zhang, S. Wang, H. Lu, G. Yang, X. Ruan, and A. Borji,\n“Detect globally, refine locally: A novel approach to saliency detection,”\nin 2018 IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, 2018, pp. 3127–3135.\n[16] L. Zhang, J. Dai, H. Lu, Y . He, and G. Wang, “A bi-directional\nmessage passing model for salient object detection,” in 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2018, pp.\n1741–1750.\n[17] W. Wang, S. Zhao, J. Shen, S. C. H. Hoi, and A. Borji, “Salient object\ndetection with pyramid attention and salient edges,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2019.\n[18] X. Qin, Z. Zhang, C. Huang, C. Gao, M. Dehghan, and M. Jagersand,\n“Basnet: Boundary-aware salient object detection,” in The IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) , June 2019.\n[19] Z. Wu, L. Su, and Q. Huang, “Cascaded partial decoder for fast and\naccurate salient object detection,” in The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , June 2019.\n[20] X. Zhang, T. Wang, J. Qi, H. Lu, and G. Wang, “Progressive attention\nguided recurrent network for salient object detection,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2018.\n[21] Q. Hou, M.-M. Cheng, X. Hu, A. Borji, Z. Tu, and P. H. S. Torr,\n“Deeply supervised salient object detection with short connections,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), July 2017.\n[22] N. Liu and J. Han, “Dhsnet: Deep hierarchical saliency network for\nsalient object detection,” in 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2016, pp. 678–686.\n[23] L. Zhu, J. Chen, X. Hu, C.-W. Fu, X. Xu, J. Qin, and P.-A. Heng,\n“Aggregating attentional dilated features for salient object detection,”\nIEEE Transactions on Circuits and Systems for Video Technology ,\nvol. 30, no. 10, pp. 3358–3371, 2020.\n[24] H. Mei, Y . Liu, Z. Wei, D. Zhou, X. Wei, Q. Zhang, and X. Yang,\n“Exploring dense context for salient object detection,” IEEE Transac-\ntions on Circuits and Systems for Video Technology , vol. 32, no. 3, pp.\n1378–1389, 2022.\n[25] C. Zhang, S. Gao, D. Mao, and Y . Zhou, “Dhnet: Salient object detection\nwith dynamic scale-aware learning and hard-sample refinement,” IEEE\nTransactions on Circuits and Systems for Video Technology , vol. 32,\nno. 11, pp. 7772–7782, 2022.\n[26] L. Zhang, Q. Zhang, and R. Zhao, “Progressive dual-attention residual\nnetwork for salient object detection,” IEEE Transactions on Circuits and\nSystems for Video Technology, vol. 32, no. 9, pp. 5902–5915, 2022.\n[27] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” in International\nConference on Learning Representations , 2021. [Online]. Available:\nhttps://openreview.net/forum?id=YicbFdNTTy\n[28] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\nT. Xiang, P. H. Torr, and L. Zhang, “Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers,” in CVPR,\n2021.\n[29] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n“Segformer: Simple and efficient design for semantic segmentation\nwith transformers,” Advances in Neural Information Processing Systems,\nvol. 34, pp. 12 077–12 090, 2021.\n[30] W. Zhang, Z. Huang, G. Luo, T. Chen, X. Wang, W. Liu, G. Yu, and\nC. Shen, “Topformer: Token pyramid transformer for mobile semantic\nsegmentation,” in Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , June 2022, pp. 12 083–\n12 093.\n[31] N. Liu, N. Zhang, K. Wan, L. Shao, and J. Han, “Visual saliency\ntransformer,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) , October 2021, pp. 4722–4732.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\n[32] Y . K. Yun and W. Lin, “Selfreformer: Self-refined network with\ntransformer for salient object detection,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2205.11283\n[33] B. Tang, Z. Liu, Y . Tan, and Q. He, “Hrtransnet: Hrformer-driven two-\nmodality salient object detection,” IEEE Transactions on Circuits and\nSystems for Video Technology, pp. 728–742, 2023.\n[34] H. Yang, Z. Yang, A. Hu, C. Liu, T. J. Cui, and J. Miao, “Unifying\nconvolution and transformer for efficient concealed object detection in\npassive millimeter-wave images,” IEEE Transactions on Circuits and\nSystems for Video Technology, vol. 33, no. 8, pp. 3872–3887, 2023.\n[35] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” in Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV) , 2021.\n[36] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 770–778.\n[37] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.\n[38] Y . Pang, X. Zhao, L. Zhang, and H. Lu, “Multi-scale interactive\nnetwork for salient object detection,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) , June\n2020.\n[39] X. Zhao, Y . Pang, L. Zhang, H. Lu, and L. Zhang, “Suppress and\nbalance: A simple gated network for salient object detection,” in ECCV,\n2020.\n[40] S. Xie and Z. Tu, “Holistically-nested edge detection,” in Proceedings of\nthe IEEE international conference on computer vision , 2015, pp. 1395–\n1403.\n[41] N. Liu, J. Han, and M.-H. Yang, “Picanet: Learning pixel-wise con-\ntextual attention for saliency detection,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , June\n2018.\n[42] W. Wang, J. Shen, X. Dong, and A. Borji, “Salient object detection\ndriven by fixation prediction,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , June 2018.\n[43] S. S. Kruthiventi, V . Gudisa, J. H. Dholakiya, and R. V . Babu, “Saliency\nunified: A deep architecture for simultaneous eye fixation prediction and\nsalient object segmentation,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , 2016, pp. 5781–5790.\n[44] J.-X. Zhao, J.-J. Liu, D.-P. Fan, Y . Cao, J. Yang, and M.-M. Cheng,\n“Egnet:edge guidance network for salient object detection,” in The IEEE\nInternational Conference on Computer Vision (ICCV) , Oct 2019.\n[45] J. Wei, S. Wang, Z. Wu, C. Su, Q. Huang, and Q. Tian, “Label\ndecoupling framework for salient object detection,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition ,\n2020, pp. 13 025–13 034.\n[46] Z. Tu, Y . Ma, C. Li, J. Tang, and B. Luo, “Edge-guided non-local fully\nconvolutional network for salient object detection,” IEEE Transactions\non Circuits and Systems for Video Technology , vol. 31, no. 2, pp. 582–\n593, 2021.\n[47] L. Zhang, J. Zhang, Z. Lin, H. Lu, and Y . He, “Capsal: Leveraging cap-\ntioning to boost semantics for salient object detection,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2019, pp. 6024–6033.\n[48] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you\nneed,” in Advances in Neural Information Processing Systems ,\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates,\nInc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/\n2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[49] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,\nand L. Shao, “Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , 2021, pp. 568–578.\n[50] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z.-H. Jiang, F. E. Tay, J. Feng,\nand S. Yan, “Tokens-to-token vit: Training vision transformers from\nscratch on imagenet,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV) , October 2021, pp. 558–567.\n[51] M. Zhuge, D.-P. Fan, N. Liu, D. Zhang, D. Xu, and L. Shao, “Salient\nobject detection via integrity learning,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 2022.\n[52] M. Ma, C. Xia, C. Xie, X. Chen, and J. Li, “Boosting broader\nreceptive fields for salient object detection,”IEEE Transactions on Image\nProcessing, vol. 32, pp. 1026–1038, 2023.\n[53] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” 2016.\n[Online]. Available: https://arxiv.org/abs/1607.06450\n[54] S.-H. Gao, M.-M. Cheng, K. Zhao, X.-Y . Zhang, M.-H. Yang, and\nP. Torr, “Res2net: A new multi-scale backbone architecture,” IEEE\nTPAMI, 2021.\n[55] G. M ´attyus, W. Luo, and R. Urtasun, “Deeproadmapper: Extracting road\ntopology from aerial images,” in 2017 IEEE International Conference\non Computer Vision (ICCV) , 2017, pp. 3458–3466.\n[56] J.-J. Liu, Q. Hou, M.-M. Cheng, J. Feng, and J. Jiang, “A simple pooling-\nbased design for real-time salient object detection,” in IEEE CVPR ,\n2019.\n[57] J. Wei, S. Wang, and Q. Huang, “F 3net: fusion, feedback and focus\nfor salient object detection,” in Proceedings of the AAAI conference on\nartificial intelligence, vol. 34, 2020, pp. 12 321–12 328.\n[58] Z. Chen, Q. Xu, R. Cong, and Q. Huang, “Global context-aware progres-\nsive aggregation network for salient object detection,” in Proceedings of\nthe AAAI conference on artificial intelligence, vol. 34, 2020, pp. 10 599–\n10 606.\n[59] L. Wang, H. Lu, Y . Wang, M. Feng, D. Wang, B. Yin, and X. Ruan,\n“Learning to detect salient objects with image-level supervision,” in The\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\n2017.\n[60] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P.\nTang, “On large-batch training for deep learning: Generalization gap\nand sharp minima,” 2016. [Online]. Available: https://arxiv.org/abs/\n1609.04836\n[61] C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang, “Saliency\ndetection via graph-based manifold ranking,” in 2013 IEEE Conference\non Computer Vision and Pattern Recognition , 2013, pp. 3166–3173.\n[62] Q. Yan, L. Xu, J. Shi, and J. Jia, “Hierarchical saliency detection,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2013.\n[63] G. Li and Y . Yu, “Visual saliency based on multiscale deep features,”\n2015 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 5455–5463, 2015.\n[64] Y . Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, “The secrets\nof salient object segmentation,” in 2014 IEEE Conference on Computer\nVision and Pattern Recognition , 2014, pp. 280–287.\n[65] V . Movahedi and J. H. Elder, “Design and perceptual validation of\nperformance measures for salient object segmentation,” in 2010 IEEE\nComputer Society Conference on Computer Vision and Pattern Recog-\nnition - Workshops, 2010, pp. 49–56.\n[66] D.-P. Fan, C. Gong, Y . Cao, B. Ren, M.-M. Cheng, and A. Borji,\n“Enhanced-alignment measure for binary foreground map evaluation,”\nin Proceedings of the Twenty-Seventh International Joint Conference\non Artificial Intelligence, IJCAI-18 . International Joint Conferences\non Artificial Intelligence Organization, 7 2018, pp. 698–704. [Online].\nAvailable: https://doi.org/10.24963/ijcai.2018/97\n[67] D.-P. Fan, M.-M. Cheng, Y . Liu, T. Li, and A. Borji, “Structure-measure:\nA new way to evaluate foreground maps,” in Proceedings of the IEEE\ninternational conference on computer vision , 2017, pp. 4548–4557.\n[68] R. Margolin, L. Zelnik-Manor, and A. Tal, “How to evaluate foreground\nmaps,” in 2014 IEEE Conference on Computer Vision and Pattern\nRecognition, 2014, pp. 248–255.\n[69] S. Chen, X. Tan, B. Wang, and X. Hu, “Reverse attention for salient\nobject detection,” in European Conference on Computer Vision , 2018.\n[70] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin, “Non-\nlocal deep features for salient object detection,” in Proceedings of the\nIEEE Conference on computer vision and pattern recognition , 2017, pp.\n6609–6617.\n[71] X. Li, F. Yang, H. Cheng, W. Liu, and D. Shen, “Contour knowledge\ntransfer for salient object detection,” in ECCV, 2018.\n[72] T. Wang, A. Borji, L. Zhang, P. Zhang, and H. Lu, “A stagewise\nrefinement model for detecting salient objects in images,” in 2017 IEEE\nInternational Conference on Computer Vision (ICCV) , 2017, pp. 4039–\n4048.\n[73] Z. Deng, X. Hu, L. Zhu, X. Xu, J. Qin, G. Han, and P.-A. Heng,\n“R3net: Recurrent residual refinement network for saliency detection,”\nin Proceedings of the 27th International Joint Conference on Artificial\nIntelligence, ser. IJCAI’18. AAAI Press, 2018, p. 684–690.\n[74] L. Zhang, J. Dai, H. Lu, Y . He, and G. Wang, “A bi-directional message\npassing model for salient object detection,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , June\n2018.\n[75] T. Wang, L. Zhang, S. Wang, H. Lu, G. Yang, X. Ruan, and A. Borji,\n“Detect globally, refine locally: A novel approach to saliency detection,”\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2018.\n[76] S. Chen, X. Tan, B. Wang, H. Lu, X. Hu, and Y . Fu, “Reverse attention-\nbased residual network for salient object detection,” IEEE Transactions\non Image Processing , vol. 29, pp. 3763–3776, 2020.\n[77] J. Su, J. Li, Y . Zhang, C. Xia, and Y . Tian, “Selectivity or in-\nvariance: Boundary-aware salient object detection,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV) ,\nOctober 2019.\n[78] M. Feng, H. Lu, and E. Ding, “Attentive feedback network for boundary-\naware salient object detection,” in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR) , June 2019.\n[79] Y . Liu, Q. Zhang, D. Zhang, and J. Han, “Employing deep part-\nobject relationships for salient object detection,” in 2019 IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , 2019, pp. 1232–\n1241.\n[80] Z. Wu, L. Su, and Q. Huang, “Stacked cross refinement network for\nedge-aware salient object detection,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , October 2019.\n[81] H. Zhou, X. Xie, J.-H. Lai, Z. Chen, and L. Yang, “Interactive two-\nstream decoder for accurate and fast saliency detection,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), June 2020.\n[82] J. Wei, S. Wang, Z. Wu, C. Su, Q. Huang, and Q. Tian, “Label decou-\npling framework for salient object detection,” in IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , June 2020.\n[83] S.-H. Gao, Y .-Q. Tan, M.-M. Cheng, C. Lu, Y . Chen, and S. Yan, “Highly\nefficient salient object detection with 100k parameters,” in ECCV, 2020.\n[84] M. Ma, C. Xia, and J. Li, “Pyramidal feature shrinking for salient\nobject detection,” in Proceedings of the AAAI conference on artificial\nintelligence, vol. 35, no. 3, 2021, pp. 2311–2318.\n[85] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. S. Torr, and S.-M. Hu,\n“Global contrast based salient region detection,” IEEE Transactions on\nPattern Analysis and Machine Intelligence , vol. 37, no. 3, pp. 569–582,\n2015.\n[86] R. Achanta, S. S. Hemami, F. J. Estrada, and S. S ¨usstrunk, “Frequency-\ntuned salient region detection,” 2009 IEEE Conference on Computer\nVision and Pattern Recognition , pp. 1597–1604, 2009.\n[87] D.-P. Fan, J. Zhang, G. Xu, M.-M. Cheng, and L. Shao, “Salient objects\nin clutter,” IEEE TPAMI, 2022.\n[88] S. Liu, D. Huang, and a. Wang, “Receptive field block net for accurate\nand fast object detection,” in The European Conference on Computer\nVision (ECCV), September 2018.\n[89] M. Ma, C. Xia, and J. Li, “Pyramidal feature shrinking for salient\nobject detection,” Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 35, no. 3, pp. 2311–2318, 2021. [Online]. Available:\nhttps://ojs.aaai.org/index.php/AAAI/article/view/16331"
  },
  {
    "source": "2211.12860v5.pdf",
    "content": "DETRs with Collaborative Hybrid Assignments Training\nZhuofan Zong Guanglu Song Yu Liu *\nSenseTime Research\n{zongzhuofan,liuyuisanai}@gmail.com\nsongguanglu@sensetime.com\nAbstract\nIn this paper, we provide the observation that too few\nqueries assigned as positive samples in DETR with one-\nto-one set matching leads to sparse supervision on the en-\ncoder’s output which considerably hurt the discriminative\nfeature learning of the encoder and vice visa for attention\nlearning in the decoder. To alleviate this, we present a novel\ncollaborative hybrid assignments training scheme, namely\nCo-DETR, to learn more efficient and effective DETR-based\ndetectors from versatile label assignment manners. This\nnew training scheme can easily enhance the encoder’s\nlearning ability in end-to-end detectors by training the mul-\ntiple parallel auxiliary heads supervised by one-to-many la-\nbel assignments such as ATSS and Faster RCNN. In addi-\ntion, we conduct extra customized positive queries by ex-\ntracting the positive coordinates from these auxiliary heads\nto improve the training efficiency of positive samples in the\ndecoder. In inference, these auxiliary heads are discarded\nand thus our method introduces no additional parameters\nand computational cost to the original detector while re-\nquiring no hand-crafted non-maximum suppression (NMS).\nWe conduct extensive experiments to evaluate the effective-\nness of the proposed approach on DETR variants, including\nDAB-DETR, Deformable-DETR, and DINO-Deformable-\nDETR. The state-of-the-art DINO-Deformable-DETR with\nSwin-L can be improved from 58.5% to 59.5% AP on COCO\nval. Surprisingly, incorporated with ViT-L backbone, we\nachieve 66.0% AP on COCO test-dev and 67.9% AP on\nLVIS val, outperforming previous methods by clear mar-\ngins with much fewer model sizes. Codes are available at\nhttps://github.com/Sense-X/Co-DETR.\n1. Introduction\nObject detection is a fundamental task in computer vi-\nsion, which requires us to localize the object and classify\nits category. The seminal R-CNN families [11, 14, 27] and\n*Corresponding author.\n0 20 40 60 80 100 120\nEpoch\n42\n44\n46\n48\n50\n52\n54AP\nCo-DETR\nDINO-Deformable-DETR\nH-Deformable-DETR\nDeformable-DETR\nDAB-DETR\nDN-DETR\nFaster-RCNN\nHTC\nFigure 1. Performance of models with ResNet-50 on COCO val.\nCo-DETR outperforms other counterparts by a large margin.\na series of variants [31, 37, 44] such as ATSS [41], Reti-\nnaNet [21], FCOS [32], and PAA [17] lead to the significant\nbreakthrough of object detection task. One-to-many label\nassignment is the core scheme of them, where each ground-\ntruth box is assigned to multiple coordinates in the detec-\ntor’s output as the supervised target cooperated with propos-\nals [11, 27], anchors [21] or window centers [32]. Despite\ntheir promising performance, these detectors heavily rely\non many hand-designed components like a non-maximum\nsuppression procedure or anchor generation [1]. To con-\nduct a more flexible end-to-end detector, DEtection TRans-\nformer (DETR) [1] is proposed to view the object detection\nas a set prediction problem and introduce the one-to-one set\nmatching scheme based on a transformer encoder-decoder\narchitecture. In this manner, each ground-truth box will\nonly be assigned to one specific query, and multiple hand-\ndesigned components that encode prior knowledge are no\nlonger needed. This approach introduces a flexible detec-\ntion pipeline and encourages many DETR variants to fur-\nther improve it. However, the performance of the vanilla\nend-to-end object detector is still inferior to the traditional\ndetectors with one-to-many label assignments.\n1\narXiv:2211.12860v5  [cs.CV]  9 Aug 2023\n0.2 0.4 0.6 0.8 1.0\nIoB\n0.2\n0.4\n0.6\n0.8\n1.0IoF\nATSS\nDeformable-DETR\nGroup-DETR\nCo-Deformable-DETR\n0.0 0.2 0.4 0.6 0.8 1.0\nIoB\n0.2\n0.4\n0.6\n0.8\n1.0IoF\nDeformable-DETR\nGroup-DETR\nCo-Deformable-DETR\nFigure 2. IoF-IoB curves for the feature discriminability score in\nthe encoder and attention discriminability score in the decoder.\nIn this paper, we try to make DETR-based detectors\nsuperior to conventional detectors while maintaining their\nend-to-end merit. To address this challenge, we focus on\nthe intuitive drawback of one-to-one set matching that it ex-\nplores less positive queries. This will lead to severe ineffi-\ncient training issues. We detailedly analyze this from two\naspects, the latent representation generated by the encoder\nand the attention learning in the decoder. We first compare\nthe discriminability score of the latent features between the\nDeformable-DETR [43] and the one-to-many label assign-\nment method where we simply replace the decoder with\nthe ATSS head. The feature l2-norm in each spatial co-\nordinate is utilized to represent the discriminability score.\nGiven the encoder’s output F ∈RC×H×W , we can obtain\nthe discriminability score map S ∈R1×H×W . The object\ncan be better detected when the scores in the correspond-\ning area are higher. As shown in Figure 2, we demonstrate\nthe IoF-IoB curve (IoF: intersection over foreground, IoB:\nintersection over background) by applying different thresh-\nolds on the discriminability scores (details in Section 3.4).\nThe higher IoF-IoB curve in ATSS indicates that it’s eas-\nier to distinguish the foreground and background. We fur-\nther visualize the discriminability score map S in Figure 3.\nIt’s obvious that the features in some salient areas are fully\nactivated in the one-to-many label assignment method but\nless explored in one-to-one set matching. For the explo-\nration of decoder training, we also demonstrate the IoF-IoB\ncurve of the cross-attention score in the decoder based on\nthe Deformable-DETR and the Group-DETR [5] which in-\ntroduces more positive queries into the decoder. The il-\nlustration in Figure 2 shows that too few positive queries\nalso influence attention learning and increasing more posi-\ntive queries in the decoder can slightly alleviate this.\nThis significant observation motivates us to present a\nsimple but effective method, a collaborative hybrid assign-\nment training scheme ( Co-DETR). The key insight of Co-\nDETR is to use versatile one-to-many label assignments to\nimprove the training efficiency and effectiveness of both the\nencoder and decoder. More specifically, we integrate the\nauxiliary heads with the output of the transformer encoder.\nThese heads can be supervised by versatile one-to-many la-\nbel assignments such as ATSS [41], FCOS [32], and Faster\nRCNN [27]. Different label assignments enrich the super-\nInput ImageATSSCo-Deformable-DETRDeformable-DETR\nFigure 3. Visualizations of discriminability scores in the encoder.\nvisions on the encoder’s output which forces it to be dis-\ncriminative enough to support the training convergence of\nthese heads. To further improve the training efficiency of\nthe decoder, we elaborately encode the coordinates of posi-\ntive samples in these auxiliary heads, including the positive\nanchors and positive proposals. They are sent to the origi-\nnal decoder as multiple groups of positive queries to predict\nthe pre-assigned categories and bounding boxes. Positive\ncoordinates in each auxiliary head serve as an independent\ngroup that is isolated from the other groups. Versatile one-\nto-many label assignments can introduce lavish (positive\nquery, ground-truth) pairs to improve the decoder’s train-\ning efficiency. Note that, only the original decoder is used\nduring inference, thus the proposed training scheme only\nintroduces extra overheads during training.\nWe conduct extensive experiments to evaluate the effi-\nciency and effectiveness of the proposed method. Illus-\ntrated in Figure 3, Co-DETR greatly alleviates the poorly\nencoder’s feature learning in one-to-one set matching. As a\nplug-and-play approach, we easily combine it with different\nDETR variants, including DAB-DETR [23], Deformable-\nDETR [43], and DINO-Deformable-DETR [39]. As shown\nin Figure 1, Co-DETR achieves faster training convergence\nand even higher performance. Specifically, we improve the\nbasic Deformable-DETR by 5.8% AP in 12-epoch train-\ning and 3.2% AP in 36-epoch training. The state-of-the-\nart DINO-Deformable-DETR with Swin-L [25] can still\nbe improved from 58.5% to 59.5% AP on COCO val.\nSurprisingly, incorporated with ViT-L [8] backbone, we\nachieve 66.0% AP on COCO test-dev and 67.9% AP\non LVIS val, establishing the new state-of-the-art detector\nwith much fewer model sizes.\n2. Related Works\nOne-to-many label assignment. For one-to-many label as-\nsignment in object detection, multiple box candidates can\nbe assigned to the same ground-truth box as positive sam-\nples in the training phase. In classic anchor-based detectors,\nsuch as Faster-RCNN [27] and RetinaNet [21], the sam-\nple selection is guided by the predefined IoU threshold and\nmatching IoU between anchors and annotated boxes. The\nanchor-free FCOS [32] leverages the center priors and as-\n2\n⋯⋯\nInput Image\nBackbone\nTransformer Decoder\nTransformer Decoder\nTransformer Encoder𝐐𝟏\nTransformer Decoder\nOne-to-One Set Matching \"𝓐\n𝐐𝒌\nOne-to-Many Label Assignments 𝓐𝟏 𝓐𝒌⋯⋯⋯⋯\nAuxiliary Head 1\nMulti-scale Adapter\n Auxiliary Head K𝐁𝟏{𝒑𝒐𝒔} 𝐁𝒌{𝒑𝒐𝒔}\n𝓐𝟏 𝓐𝒌\nTraining-only\nTraining-only\nFigure 4. Framework of our Collaborative Hybrid Assignment Training. The auxiliary branches are discarded during evaluation.\nsigns spatial locations near the center of each bounding box\nas positives. Moreover, the adaptive mechanism is incorpo-\nrated into one-to-many label assignments to overcome the\nlimitation of fixed label assignments. ATSS [41] performs\nadaptive anchor selection by the statistical dynamic IoU val-\nues of top-k closest anchors. PAA [17] adaptively separates\nanchors into positive and negative samples in a probabilis-\ntic manner. In this paper, we propose a collaborative hybrid\nassignment scheme to improve encoder representations via\nauxiliary heads with one-to-many label assignments.\nOne-to-one set matching. The pioneering transformer-\nbased detector, DETR [1], incorporates the one-to-one set\nmatching scheme into object detection and performs fully\nend-to-end object detection. The one-to-one set matching\nstrategy first calculates the global matching cost via Hun-\ngarian matching and assigns only one positive sample with\nthe minimum matching cost for each ground-truth box. DN-\nDETR [18] demonstrates the slow convergence results from\nthe instability of one-to-one set matching, thus introducing\ndenoising training to eliminate this issue. DINO [39] inher-\nits the advanced query formulation of DAB-DETR [23] and\nincorporates an improved contrastive denoising technique\nto achieve state-of-the-art performance. Group-DETR [5]\nconstructs group-wise one-to-many label assignment to ex-\nploit multiple positive object queries, which is similar to the\nhybrid matching scheme in H-DETR [16]. In contrast with\nthe above follow-up works, we present a new perspective of\ncollaborative optimization for one-to-one set matching.\n3. Method\n3.1. Overview\nFollowing the standard DETR protocol, the input image\nis fed into the backbone and encoder to generate latent fea-\ntures. Multiple predefined object queries interact with them\nin the decoder via cross-attention afterwards. We introduce\nCo-DETR to improve the feature learning in the encoder\nand the attention learning in the decoder via the collabora-\ntive hybrid assignments training scheme and the customized\npositive queries generation. We will detailedly describe\nthese modules and give insights why they can work well.\n3.2. Collaborative Hybrid Assignments Training\nTo alleviate the sparse supervision on the encoder’s out-\nput caused by the fewer positive queries in the decoder, we\nincorporate versatile auxiliary heads with different one-to-\nmany label assignment paradigms, e.g., ATSS, and Faster\nR-CNN. Different label assignments enrich the supervisions\non the encoder’s output which forces it to be discrimina-\ntive enough to support the training convergence of these\nheads. Specifically, given the encoder’s latent feature F,\nwe firstly transform it to the feature pyramid{F1, ··· , FJ}\nvia the multi-scale adapter where J indicates feature map\nwith 22+J downsampling stride. Similar to ViTDet [20],\nthe feature pyramid is constructed by a single feature map\nin the single-scale encoder, while we use bilinear interpo-\nlation and 3 × 3 convolution for upsampling. For instance,\nwith the single-scale feature from the encoder, we succes-\nsively apply downsampling (3×3 convolution with stride2)\nor upsampling operations to produce a feature pyramid. As\nfor the multi-scale encoder, we only downsample the coars-\nest feature in the multi-scale encoder features F to build\nthe feature pyramid. Defined K collaborative heads with\ncorresponding label assignment manners Ak, for the i-th\ncollaborative head, {F1, ··· , FJ} is sent to it to obtain the\npredictions ˆPi. At the i-th head, Ai is used to compute the\nsupervised targets for the positive and negative samples in\nPi. Denoted G as the ground-truth set, this procedure can\nbe formulated as:\nP{pos}\ni , B{pos}\ni , P{neg}\ni = Ai(ˆPi, G), (1)\nwhere {pos} and {neg} indicate the pair set of (j, positive\ncoordinates or negative coordinates in Fj) determined by\nAi. j means the feature index in {F1, ··· , FJ}. B{pos}\ni is\n3\nHeadi LossLi\nAssignmentAi\n{pos}, {neg}Generation Pi Generation B{pos}\ni Generation\nFaster-RCNN [27] cls: CE loss, {pos}: IoU(proposal, gt)>0.5 {pos}: gt labels, offset(proposal, gt) positive proposals\nreg: GIoU loss {neg}: IoU(proposal, gt)<0.5 {neg}: gt labels (x1, y1, x2, y2)\nATSS [41] cls: Focal loss {pos}:IoU(anchor, gt)>(mean+std){pos}: gt labels, offset(anchor, gt), centernesspositive anchors\nreg: GIoU, BCE loss{neg}: IoU(anchor, gt)<(mean+std) {neg}: gt labels (x1, y1, x2, y2)\nRetinaNet [21] cls: Focal loss {pos}: IoU(anchor, gt)>0.5 {pos}: gt labels, offset(anchor, gt) positive anchors\nreg: GIoU Loss {neg}: IoU(anchor, gt)<0.4 {neg}: gt labels (x1, y1, x2, y2)\nFCOS [32] cls: Focal Loss {pos}: points inside gt center area{pos}: gt labels, ltrb distance, centernessFCOS point(cx, cy)\nreg: GIoU, BCE loss{neg}: points outside gt center area {neg}: gt labels w=h= 8×22+j\nTable 1. Detailed information of auxiliary heads. The auxiliary heads include Faster-RCNN [27], ATSS [41], RetinaNet [21], and\nFCOS [32]. If not otherwise specified, we follow the original implementations, e.g., anchor generation.\nthe set of spatial positive coordinates. P{pos}\ni and P{neg}\ni\nare the supervised targets in the corresponding coordinates,\nincluding the categories and regressed offsets. To be spe-\ncific, we describe the detailed information about each vari-\nable in Table 1. The loss functions can be defined as:\nLenc\ni = Li(ˆP{pos}\ni , P{pos}\ni ) +Li(ˆP{neg}\ni , P{neg}\ni ), (2)\nNote that the regression loss is discarded for negative sam-\nples. The training objective of the optimization for K aux-\niliary heads is formulated as follows:\nLenc =\nKX\ni=1\nLenc\ni (3)\n3.3. Customized Positive Queries Generation\nIn the one-to-one set matching paradigm, each ground-\ntruth box will only be assigned to one specific query as the\nsupervised target. Too few positive queries lead to ineffi-\ncient cross-attention learning in the transformer decoder as\nshown in Figure 2. To alleviate this, we elaborately generate\nsufficient customized positive queries according to the label\nassignment Ai in each auxiliary head. Specifically, given\nthe positive coordinates set B{pos}\ni ∈ RMi×4 in the i-th\nauxiliary head, where Mi is the number of positive sam-\nples, the extra customized positive queries Qi ∈ RMi×C\ncan be generated by:\nQi = Linear(PE(B{pos}\ni )) + Linear(E({F∗}, {pos})).\n(4)\nwhere PE(·) stands for positional encodings and we select\nthe corresponding features from E(·) according to the index\npair (j, positive coordinates or negative coordinates in Fj).\nAs a result, there are K + 1groups of queries that con-\ntribute to a single one-to-one set matching branch and K\nbranches with one-to-many label assignments during train-\ning. The auxiliary one-to-many label assignment branches\nshare the same parameters with L decoders layers in the\noriginal main branch. All the queries in the auxiliary branch\nare regarded as positive queries, thus the matching process\nis discarded. To be specific, the loss of thel-th decoder layer\nin the i-th auxiliary branch can be formulated as:\nLdec\ni,l = eL(ePi,l, P{pos}\ni ). (5)\nePi,l refers to the output predictions of thel-th decoder layer\nin the i-th auxiliary branch. Finally, the training objective\nfor Co-DETR is:\nLglobal =\nLX\nl=1\n( eLdec\nl + λ1\nKX\ni=1\nLdec\ni,l + λ2Lenc), (6)\nwhere eLdec\nl stands for the loss in the original one-to-one set\nmatching branch [1],λ1 and λ2 are the coefficient balancing\nthe losses.\n3.4. Why Co-DETR works\nCo-DETR leads to evident improvement to the DETR-\nbased detectors. In the following, we try to investigate its\neffectiveness qualitatively and quantitatively. We conduct\ndetailed analysis based on Deformable-DETR with ResNet-\n50 [15] backbone using the 36-epoch setting.\nEnrich the encoder’s supervisions. Intuitively, too few\npositive queries lead to sparse supervisions as only one\nquery is supervised by regression loss for each ground-truth.\nThe positive samples in one-to-many label assignment man-\nners receive more localization supervisions to help enhance\nthe latent feature learning. To further explore how the sparse\nsupervisions impede the model training, we detailedly in-\nvestigate the latent features produced by the encoder. We\nintroduce the IoF-IoB curve to quantize the discriminabil-\nity score of the encoder’s output. Specifically, given the\nlatent feature F of the encoder, inspired by the feature visu-\nalization in Figure 3, we compute the IoF (intersection over\nforeground) and IoB (intersection over background). Given\nthe encoder’s feature Fj ∈ RC×Hj ×Wj at level j, we first\ncalculate the l2-norm bFj ∈ R1×Hj ×Wj and resize it to the\nimage size H × W. The discriminability score D(F) is\ncomputed by averaging the scores from all levels:\nD(F) = 1\nJ\nJX\nj=1\nbFj\nmax( bFj)\n, (7)\n4\n2 4 6 8 10 12\nEpoch\n10\n11\n12IS(Instability)\nDeformable-DETR\nCo-Deformable-DETR\nFigure 5. The instability (IS) [18] of Deformable-DETR and Co-\nDeformable-DETR on COCO dataset. These detectors are trained\nfor 12 epochs with ResNet-50 backbones.\nwhere the resize operation is omitted. We visualize the\ndiscriminability scores of ATSS, Deformable-DETR, and\nour Co-Deformable-DETR in Figure 3. Compared with\nDeformable-DETR, both ATSS and Co-Deformable-DETR\nown stronger ability to distinguish the areas of key objects,\nwhile Deformable-DETR is almost disturbed by the back-\nground. Consequently, we define the indicators for fore-\nground and background as 1 (D(F) > S) ∈ RH×W and\n1 (D(F) < S) ∈ RH×W , respectively. S is a predefined\nscore thresh, 1 (x) is 1 if x is true and 0 otherwise. As for\nthe mask of foreground Mfg ∈ RH×W , the element Mfg\nh,w\nis 1 if the point (h, w) is inside the foreground and 0 oth-\nerwise. The area of intersection over foreground (IoF) Ifg\ncan be computed as:\nIfg =\nPH\nh=1\nPW\nw=1(1 (D(Fh,w) > S) · Mfg\nh,w)\nPH\nh=1\nPW\nw=1 Mfg\nh,w\n. (8)\nConcretely, we compute the area of intersection over back-\nground areas (IoB) in a similar way and plot the curve\nIoF and IoB by varying S in Figure 2. Obviously, ATSS\nand Co-Deformable-DETR obtain higher IoF values than\nboth Deformable-DETR and Group-DETR under the same\nIoB values, which demonstrates the encoder representations\nbenefit from the one-to-many label assignment.\nImprove the cross-attention learning by reducing the in-\nstability of Hungarian matching. Hungarian matching is\nthe core scheme in one-to-one set matching. Cross-attention\nis an important operation to help the positive queries encode\nabundant object information. It requires sufficient training\nto achieve this. We observe that the Hungarian matching\nintroduces uncontrollable instability since the ground-truth\nassigned to a specific positive query in the same image is\nchanging during the training process. Following [18], we\npresent the comparison of instability in Figure 5, where\nwe find our approach contributes to a more stable matching\nprocess. Furthermore, in order to quantify how well cross-\nattention is being optimized, we also calculate the IoF-IoB\ncurve for attention score. Similar to the feature discrim-\ninability score computation, we set different thresholds for\nattention score to get multiple IoF-IoB pairs. The compar-\nisons between Deformable-DETR, Group-DETR, and Co-\nDeformable-DETR can be viewed in Figure 2. We find that\nthe IoF-IoB curves of DETRs with more positive queries\nare generally above Deformable-DETR, which is consistent\nwith our motivation.\n3.5. Comparison with other methods\nDifferences between our method and other counter-\nparts. Group-DETR, H-DETR, and SQR [2] perform one-\nto-many assignments by one-to-one matching with dupli-\ncate groups and repeated ground-truth boxes. Co-DETR ex-\nplicitly assigns multiple spatial coordinates as positives for\neach ground truth. Accordingly, these dense supervision\nsignals are directly applied to the latent feature map to en-\nable it more discriminative. By contrast, Group-DETR, H-\nDETR, and SQR lack this mechanism. Although more pos-\nitive queries are introduced in these counterparts, the one-\nto-many assignments implemented by Hungarian Matching\nstill suffer from the instability issues of one-to-one match-\ning. Our method benefits from the stability of off-the-\nshelf one-to-many assignments and inherits their specific\nmatching manner between positive queries and ground-truth\nboxes. Group-DETR and H-DETR fail to reveal the com-\nplementarities between one-to-one matching and traditional\none-to-many assignment. To our best knowledge, we are the\nfirst to give the quantitative and qualitative analysis on the\ndetectors with the traditional one-to-many assignment and\none-to-one matching. This helps us better understand their\ndifferences and complementarities so that we can naturally\nimprove the DETR’s learning ability by leveraging off-the-\nshelf one-to-many assignment designs without requiring ad-\nditional specialized one-to-many design experience.\nNo negative queries are introduced in the decoder. Du-\nplicate object queries inevitably bring large amounts of neg-\native queries for the decoder and a significant increase in\nGPU memory. However, our method only processes the\npositive coordinates in the decoder, thus consuming less\nmemory as shown in Table 7.\n4. Experiments\n4.1. Setup\nDatasets and Evaluation Metrics. Our experiments are\nconducted on the MS COCO 2017 dataset [22] and LVIS\nv1.0 dataset [12]. The COCO dataset consists of 115K\nlabeled images for training and 5K images for validation.\nWe report the detection results by default on the val sub-\nset. The results of our largest model evaluated on the\n5\nMethod K #epochs AP\nConditional DETR-C5 [26] 0 36 39.4\nConditional DETR-C5 [26] 1 36 41.5 (+2.1)\nConditional DETR-C5 [26] 2 36 41.8 (+2.4)\nDAB-DETR-C5 [23] 0 36 41.2\nDAB-DETR-C5 [23] 1 36 43.1 (+1.9)\nDAB-DETR-C5 [23] 2 36 43.5 (+2.3)\nDeformable-DETR [43] 0 12 37.1\nDeformable-DETR [43] 1 12 42.3 (+5.2)\nDeformable-DETR [43] 2 12 42.9 (+5.8)\nDeformable-DETR [43] 0 36 43.3\nDeformable-DETR [43] 1 36 46.8 (+3.5)\nDeformable-DETR [43] 2 36 46.5 (+3.2)\nTable 2. Results of plain baselines on COCO val.\ntest-dev (20K images) are also reported. LVIS v1.0 is\na large-scale and long-tail dataset with 1203 categories for\nlarge vocabulary instance segmentation. To verify the scal-\nability of Co-DETR, we further apply it to a large-scale ob-\nject detection benchmark, namely Objects365 [30]. There\nare 1.7M labeled images used for training and 80K images\nfor validation in the Objects365 dataset. All results follow\nthe standard mean Average Precision(AP) under IoU thresh-\nolds ranging from 0.5 to 0.95 at different object scales.\nImplementation Details. We incorporate our Co-DETR\ninto the current DETR-like pipelines and keep the train-\ning setting consistent with the baselines. We adopt ATSS\nand Faster-RCNN as the auxiliary heads for K = 2 and\nonly keep ATSS for K = 1. More details about our aux-\niliary heads can be found in the supplementary materials.\nWe choose the number of learnable object queries to 300\nand set {λ1, λ2} to {1.0, 2.0} by default. For Co-DINO-\nDeformable-DETR++, we use large-scale jitter with copy-\npaste [10].\n4.2. Main Results\nIn this section, we empirically analyze the effectiveness\nand generalization ability of Co-DETR on different DETR\nvariants in Table 2 and Table 3. All results are repro-\nduced using mmdetection [4]. We first apply the collabo-\nrative hybrid assignments training to single-scale DETRs\nwith C5 features. Surprisingly, both Conditional-DETR\nand DAB-DETR obtain 2.4% and 2.3% AP gains over the\nbaselines with a long training schedule. For Deformable-\nDETR with multi-scale features, the detection performance\nis significantly boosted from 37.1% to 42.9% AP. The over-\nall improvements (+3.2% AP) still hold when the training\ntime is increased to 36 epochs. Moreover, we conduct ex-\nperiments on the improved Deformable-DETR (denoted as\nDeformable-DETR++) following [16], where a +2.4% AP\ngain is observed. The state-of-the-art DINO-Deformable-\nMethod K #epochs AP\nDeformable-DETR++ [43] 0 12 47.1\nDeformable-DETR++ [43] 1 12 48.7(+1.6)\nDeformable-DETR++ [43] 2 12 49.5(+2.4)\nDINO-Deformable-DETR † [39] 0 12 49.4\nDINO-Deformable-DETR † [39] 1 12 51.0(+1.6)\nDINO-Deformable-DETR † [39] 2 12 51.2(+1.8)\nDeformable-DETR++ ‡ [43] 0 12 55.2\nDeformable-DETR++ ‡ [43] 1 12 56.4(+1.2)\nDeformable-DETR++ ‡ [43] 2 12 56.9(+1.7)\nDINO-Deformable-DETR †‡ [39] 0 12 58.5\nDINO-Deformable-DETR †‡ [39] 1 12 59.3(+0.8)\nDINO-Deformable-DETR †‡ [39] 2 12 59.5(+1.0)\nTable 3. Results of strong baselines on COCOval. Methods with\n† use 5 feature levels. ‡ refers to Swin-L backbone.\nDETR equipped with our method can achieve 51.2% AP,\nwhich is +1.8% AP higher than the competitive baseline.\nWe further scale up the backbone capacity from ResNet-\n50 to Swin-L [25] based on two state-of-the-art base-\nlines. As presented in Table 3, Co-DETR achieves 56.9%\nAP and surpasses the Deformable-DETR++ baseline by\na large margin (+1.7% AP). The performance of DINO-\nDeformable-DETR with Swin-L can still be boosted from\n58.5% to 59.5% AP.\n4.3. Comparisons with the state-of-the-art\nWe apply our method with K = 2 to Deformable-\nDETR++ and DINO. Besides, the quality focal loss [19] and\nNMS are adopted for our Co-DINO-Deformable-DETR.\nWe report the comparisons on COCOval in Table 4. Com-\npared with other competitive counterparts, our method con-\nverges much faster. For example, Co-DINO-Deformable-\nDETR readily achieves 52.1% AP when using only 12\nepochs with ResNet-50 backbone. Our method with Swin-\nL can obtain 58.9% AP for 1 × scheduler, even surpass-\ning other state-of-the-art frameworks on 3 × scheduler.\nMore importantly, our best model Co-DINO-Deformable-\nDETR++ achieves 54.8% AP with ResNet-50 and 60.7%\nAP with Swin-L under 36-epoch training, outperforming all\nexisting detectors with the same backbone by clear margins.\nTo further explore the scalability of our method, we ex-\ntend the backbone capacity to 304 million parameters. This\nlarge-scale backbone ViT-L [7] is pre-trained using a self-\nsupervised learning method (EV A-02 [8]). We first pre-train\nCo-DINO-Deformable-DETR with ViT-L on Objects365\nfor 26 epochs, then fine-tune it on the COCO dataset for\n12 epochs. In the fine-tuning stage, the input resolution\nis randomly selected between 480 ×2400 and 1536 ×2400.\nThe detailed settings are available in supplementary materi-\nals. Our results are evaluated with test-time augmentation.\nTable 5 presents the state-of-the-art comparisons on the\n6\nMethod Backbone Multi-scale #query #epochs AP AP 50 AP75 APS APM APL\nConditional-DETR [26] R50 ✗ 300 108 43.0 64.0 45.7 22.7 46.7 61.5\nAnchor-DETR [35] R50 ✗ 300 50 42.1 63.1 44.9 22.3 46.2 60.0\nDAB-DETR [23] R50 ✗ 900 50 45.7 66.2 49.0 26.1 49.4 63.1\nAdaMixer [9] R50 ✓ 300 36 47.0 66.0 51.1 30.1 50.2 61.8\nDeformable-DETR [43] R50 ✓ 300 50 46.9 65.6 51.0 29.6 50.1 61.6\nDN-Deformable-DETR [18] R50 ✓ 300 50 48.6 67.4 52.7 31.0 52.0 63.7\nDINO-Deformable-DETR† [39] R50 ✓ 900 12 49.4 66.9 53.8 32.3 52.5 63.9\nDINO-Deformable-DETR† [39] R50 ✓ 900 36 51.2 69.0 55.8 35.0 54.3 65.3\nDINO-Deformable-DETR† [39] Swin-L (IN-22K) ✓ 900 36 58.5 77.0 64.1 41.5 62.3 74.0\nGroup-DINO-Deformable-DETR [5] Swin-L (IN-22K) ✓ 900 36 58.4 - - 41.0 62.5 73.9\nH-Deformable-DETR [16] R50 ✓ 300 12 48.7 66.4 52.9 31.2 51.5 63.5\nH-Deformable-DETR [16] Swin-L (IN-22K) ✓ 900 36 57.9 76.8 63.6 42.4 61.9 73.4\nCo-Deformable-DETR R50 ✓ 300 12 49.5 67.6 54.3 32.4 52.7 63.7\nCo-Deformable-DETR Swin-L (IN-22K) ✓ 900 36 58.5 77.1 64.5 42.4 62.4 74.0\nCo-DINO-Deformable-DETR† R50 ✓ 900 12 52.1 69.4 57.1 35.4 55.4 65.9\nCo-DINO-Deformable-DETR† Swin-L (IN-22K) ✓ 900 12 58.9 76.9 64.8 42.6 62.7 75.1\nCo-DINO-Deformable-DETR† Swin-L (IN-22K) ✓ 900 24 59.8 77.7 65.5 43.6 63.5 75.5\nCo-DINO-Deformable-DETR† Swin-L (IN-22K) ✓ 900 36 60.0 77.7 66.1 44.6 63.9 75.7\nCo-DINO-Deformable-DETR++† R50 ✓ 900 12 52.1 69.3 57.3 35.4 55.5 67.2\nCo-DINO-Deformable-DETR++† R50 ✓ 900 36 54.8 72.5 60.1 38.3 58.4 69.6\nCo-DINO-Deformable-DETR++† Swin-L (IN-22K) ✓ 900 12 59.3 77.3 64.9 43.3 63.3 75.5\nCo-DINO-Deformable-DETR++† Swin-L (IN-22K) ✓ 900 24 60.4 78.3 66.4 44.6 64.2 76.5\nCo-DINO-Deformable-DETR++† Swin-L (IN-22K) ✓ 900 36 60.7 78.5 66.7 45.1 64.7 76.4\n†: 5 feature levels.\nTable 4. Comparison to the state-of-the-art DETR variants on COCO val.\nMethod Backbone enc. val test-dev\n#params APbox APbox\nHTC++ [3] SwinV2-G [24] 3.0B 62.5 63.1\nDINO [39] Swin-L [25] 218M 63.2 63.3\nBEIT3 [33] ViT-g [7] 1.9B - 63.7\nFD [36] SwinV2-G [24] 3.0B - 64.2\nDINO [39] FocalNet-H [38] 746M 64.2 64.3\nGroup DETRv2 [6] ViT-H [7] 629M - 64.5\nEV A-02 [8] ViT-L [7] 304M 64.1 64.5\nDINO [39] InternImage-G [34] 3.0B 65.3 65.5\nCo-DETR ViT-L [7] 304M 65.9 66.0\nTable 5. Comparison to the state-of-the-art frameworks on COCO.\nCOCO test-dev benchmark. With much fewer model\nsizes (304M parameters), Co-DETR sets a new record of\n66.0% AP on COCO test-dev, outperforming the previ-\nous best model InternImage-G [34] by +0.5% AP.\nWe also demonstrate the best results of Co-DETR on\nthe long-tailed LVIS detection dataset. In particular, we\nuse the same Co-DINO-Deformable-DETR++ as the model\non COCO but choose FedLoss [42] as the classification\nloss to remedy the impact of unbalanced data distribution.\nHere, we only apply bounding boxes supervision and re-\nport the object detection results. The comparisons are avail-\nable in Table 6. Co-DETR with Swin-L yields 56.9% and\n62.3% AP on LVIS val and minival, surpassing ViT-\nDet with MAE-pretrained [13] ViT-H and GLIPv2 [40] by\nMethod Backbone enc. val minival\n#params APbox APbox\nH-DETR [16] Swin-L [25] 218M 47.9 -\nViTDet [20] ViT-L [7] 307M 51.2 -\nViTDet [20] ViT-H [7] 632M 53.4 -\nGLIPv2 [40] Swin-H [25] 637M - 59.8\nDINO [39] InternImage-G [34] 3.0B 63.2 65.8\nEV A-02 [8] ViT-L [7] 304M 65.2 -\nCo-DETR Swin-L [25] 218M 56.9 62.3\nCo-DETR ViT-L [7] 304M 67.9 71.9\nTable 6. Comparison to the state-of-the-art frameworks on LVIS.\n+3.5% and +2.5% AP, respectively. We further finetune\nthe Objects365 pretrained Co-DETR on this dataset. With-\nout elaborate test-time augmentation, our approach achieves\nthe best detection performance of 67.9% and 71.9% AP\non LVIS val and minival. Compared to the 3-billion\nparameter InternImage-G with test-time augmentation, we\nobtain +4.7% and +6.1% AP gains on LVIS val and\nminival while reducing the model size to 1/10.\n4.4. Ablation Studies\nUnless stated otherwise, all experiments for ablations are\nconducted on Deformable-DETR with a ResNet-50 back-\nbone. We choose the number of auxiliary heads K to 1 by\ndefault and set the total batch size to 32. More ablations and\n7\nMethod K Auxiliary Memory GPU APhead (MB) hours\nDeformable-DETR++0 - 12808 70 47.1\nH-Deformable-DETR0 - 15307 104 48.4\nDeformable-DETR++1 ATSS 13947 86 48.7\nDeformable-DETR++2 ATSS + PAA 14629 124 49.0\nDeformable-DETR++2 ATSS + Faster-RCNN14387 120 49.5\nDeformable-DETR++3 ATSS + Faster-RCNN15263 150 49.5+ PAA\nDeformable-DETR++6\nATSS + Faster-RCNN\n19385 280 48.9+ PAA + RetinaNet\n+ FCOS + GFL\nTable 7. Experimental results of K varying from 1 to 6.\nAuxiliary head #epochs AP AP50 AP75\nBaseline 36 43.3 62.3 47.1\nRetinaNet [21] 36 46.1 64.2 50.1\nFaster-RCNN [27] 36 46.3 64.7 50.5\nMask-RCNN [14] 36 46.5 65.0 50.6\nFCOS [32] 36 46.5 64.8 50.7\nPAA [17] 36 46.5 64.6 50.7\nGFL [19] 36 46.5 65.0 51.0\nATSS [41] 36 46.8 65.1 51.5\nTable 8. Performance of our approach with various auxiliary one-\nto-many heads on COCO val.\nanalyses can be found in the supplementary materials.\nCriteria for choosing auxiliary heads. We further delve\ninto the criteria for choosing auxiliary heads in Table 7 and\n8. The results in Table 8 reveal that any auxiliary head\nwith one-to-many label assignments consistently improves\nthe baseline and ATSS achieves the best performance. We\nfind the accuracy continues to increase asK increases when\nchoosing K smaller than 3. It is worth noting that perfor-\nmance degradation occurs when K = 6, and we speculate\nthe severe conflicts among auxiliary heads cause this. If the\nfeature learning is inconsistent across the auxiliary heads,\nthe continuous improvement as K becomes larger will be\ndestroyed. We also analyze the optimization consistency of\nmultiple heads next and in the supplementary materials. In\nsummary, we can choose any head as the auxiliary head and\nwe regard ATSS and Faster-RCNN as the common practice\nto achieve the best performance when K ≤ 2. We do not\nuse too many different heads,e.g., 6 different heads to avoid\noptimization conflicts.\nConflicts analysis. The conflicts emerge when the same\nspatial coordinate is assigned to different foreground boxes\nor treated as background in different auxiliary heads and\ncan confuse the training of the detector. We first define the\ndistance between head Hi and head Hj, and the average\ndistance of Hi to measure the optimization conflicts as:\nSi,j = 1\n|D|\nX\nI∈D\nKL(C(Hi(I)), C(Hj(I)), (9)\nATSS Faster-RCNN PAA GFL FCOS RetinaNet\n0.00\n0.01\n0.02Distance\nK=1\nK=2\nK=3\nK=6\nFigure 6. The distance when varying K from 1 to 6.\naux head pos queries #epochs AP AP50 AP75\n✗ ✗ 12 37.1 55.5 40.0\n36 43.3 62.3 47.1\n✓ ✗ 12 41.6(+4.5) 59.8 45.6\n36 46.2(+2.9) 64.7 50.9\n✗ ✓ 12 40.5(+3.4) 58.8 44.4\n36 45.3(+2.0) 63.5 49.8\n✓ ✓ 12 42.3(+5.2) 60.5 46.1\n36 46.8(+3.5) 65.1 51.5\nTable 9. “aux head” denotes training with an auxiliary head and\n“pos queries” means the customized positive queries generation.\nSi = 1\n2(K − 1)\nKX\nj̸=i\n(Si,j + Sj,i), (10)\nwhere KL, D, I, C refer to KL divergence, dataset, the input\nimage, and class activation maps (CAM) [29]. As illustrated\nin Figure 6, we compute the average distances among aux-\niliary heads for K >1 and the distance between the DETR\nhead and the single auxiliary head for K = 1. We find the\ndistance metric isinsignificant for each auxiliary head when\nK = 1and this observation is consistent with our results in\nTable 8: the DETR head can be collaboratively improved\nwith any head when K = 1. When K is increased to 2, the\ndistance metrics increase slightly and our method achieves\nthe best performance as shown in Table 7. The distance\nsurges when K is increased from 3 and 6, indicating severe\noptimization conflicts among these auxiliary heads lead to\na decrease in performance. However, the baseline with 6\nATSS achieves 49.5% AP and can be decreased to 48.9%\nAP by replacing ATSS with 6 various heads. Accordingly,\nwe speculate too many diverse auxiliary heads, e.g., more\nthan 3 different heads, exacerbate the conflicts. In summary,\noptimization conflicts are influenced by the number of vari-\nous auxiliary heads and the relations among these heads.\nShould the added heads be different?Collaborative train-\ning with two ATSS heads (49.2% AP) still improves the\nmodel with one ATSS head (48.7% AP) as ATSS is comple-\nmentary to the DETR head in our analysis. Besides, intro-\nducing a diverse and complementary auxiliary head rather\n8\nMethod K #epochs GPU hours AP\nDeformable-DETR 1 36 288 46.8\nDeformable-DETR 0 50 333 44.5\nDeformable-DETR 0 100 667 46.0\nDeformable-DETR 0 150 1000 45.9\nTable 10. Comparison to baselines with longer schedule.\nBranch NMS K = 0 K = 1 K = 2\nDeformable-DETR++ ✗ 47.1 48.7(+1.6) 49.5(+2.4)\nATSS ✓ 46.8 47.4(+0.6) 48.0(+1.2)\nFaster-RCNN ✓ 45.9 - 46.7(+0.8)\nTable 11. Collaborative training consistently improves perfor-\nmances of all branches on Deformable-DETR++ with ResNet-50.\nthan the same one as the original head, e.g., Faster-RCNN,\ncan bring better gains (49.5% AP). Note that this isnot con-\ntradictory to above conclusion; instead, we can obtain the\nbest performance with few different heads (K ≤ 2) as the\nconflicts are insignificant, but we are faced with severe con-\nflicts when using many different heads (K >3).\nThe effect of each component. We perform a component-\nwise ablation to thoroughly analyze the effect of each com-\nponent in Table 9. Incorporating the auxiliary head yields\nsignificant gains since the dense spatial supervision enables\nthe encoder features more discriminative. Alternatively, in-\ntroducing customized positive queries also contributes re-\nmarkably to the final results, while improving the training\nefficiency of the one-to-one set matching. Both techniques\ncan accelerate convergence and improve performance. In\nsummary, we observe the overall improvements stem from\nmore discriminative features for the encoder and more effi-\ncient attention learning for the decoder.\nComparisons to the longer training schedule. As pre-\nsented in Table 10, we find Deformable-DETR can not ben-\nefit from longer training as the performance saturates. On\nthe contrary, Co-DETR greatly accelerates the convergence\nas well as increasing the peak performance.\nPerformance of auxiliary branches. Surprisingly, we ob-\nserve Co-DETR also brings consistent gains for auxiliary\nheads in Table 11. This implies our training paradigm\ncontributes to more discriminative encoder representations,\nwhich improves the performances of both decoder and aux-\niliary heads.\nDifference in distribution of original and customized\npositive queries. We visualize the positions of original pos-\nitive queries and customized positive queries in Figure 7a.\nWe only show one object (green box) per image. Positive\nqueries assigned by Hungarian Matching in the decoder are\nmarked in red. We mark positive queries extracted from\nFaster-RCNN and ATSS in blue and orange, respectively.\nThese customized queries are distributed around the cen-\n(a) Visualizations of queries.\nATSS Faster-RCNN\n0%\n25%\n50% Positive Negative (b) Normalized distances.\nFigure 7. Distribution of original and customized queries.\nter region of the instance and provide sufficient supervision\nsignals for the detector.\nDoes distribution difference lead to instability?We com-\npute the average distance between original and customized\nqueries in Figure 7b. The average distance between original\nnegative queries and customized positive queries is signif-\nicantly larger than the distance between original and cus-\ntomized positive queries. As this distribution gap between\noriginal and customized queries is marginal, there is no in-\nstability encountered during training.\n5. Conclusions\nIn this paper, we present a novel collaborative hybrid\nassignments training scheme, namely Co-DETR, to learn\nmore efficient and effective DETR-based detectors from\nversatile label assignment manners. This new training\nscheme can easily enhance the encoder’s learning ability\nin end-to-end detectors by training the multiple parallel\nauxiliary heads supervised by one-to-many label assign-\nments. In addition, we conduct extra customized positive\nqueries by extracting the positive coordinates from these\nauxiliary heads to improve the training efficiency of posi-\ntive samples in decoder. Extensive experiments on COCO\ndataset demonstrate the efficiency and effectiveness of Co-\nDETR. Surprisingly, incorporated with ViT-L backbone, we\nachieve 66.0% AP on COCO test-dev and 67.9% AP\non LVIS val, establishing the new state-of-the-art detector\nwith much fewer model sizes.\nReferences\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. ArXiv,\nabs/2005.12872, 2020. 1, 3, 4\n[2] Fangyi Chen, Han Zhang, Kai Hu, Yu-kai Huang, Chenchen\nZhu, and Marios Savvides. Enhanced training of query-\nbased object detection via selective query recollection.arXiv\npreprint arXiv:2212.07593, 2022. 5\n[3] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox-\niao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping\nShi, Wanli Ouyang, et al. Hybrid task cascade for instance\n9\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 4974–\n4983, 2019. 7\n[4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, et al. Mmdetection: Open mmlab detection tool-\nbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\n6\n[5] Qiang Chen, Xiaokang Chen, Gang Zeng, and Jingdong\nWang. Group detr: Fast training convergence with de-\ncoupled one-to-many label assignment. arXiv preprint\narXiv:2207.13085, 2022. 2, 3, 7\n[6] Qiang Chen, Jian Wang, Chuchu Han, Shan Zhang, Zex-\nian Li, Xiaokang Chen, Jiahui Chen, Xiaodi Wang, Shum-\ning Han, Gang Zhang, et al. Group detr v2: Strong object\ndetector with encoder-decoder pretraining. arXiv preprint\narXiv:2211.03594, 2022. 7\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ArXiv, abs/2010.11929, 2021. 6, 7\n[8] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xin-\nlong Wang, and Yue Cao. Eva-02: A visual representation\nfor neon genesis. arXiv preprint arXiv:2303.11331, 2023. 2,\n6, 7\n[9] Ziteng Gao, Limin Wang, Bing Han, and Sheng Guo.\nAdamixer: A fast-converging query-based object detector.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 5364–5373, 2022. 7\n[10] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-\nYi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Sim-\nple copy-paste is a strong data augmentation method for in-\nstance segmentation. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition , pages\n2918–2928, 2021. 6\n[11] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 1440–1448,\n2015. 1\n[12] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\ndataset for large vocabulary instance segmentation. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 5356–5364, 2019. 5\n[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 16000–\n16009, 2022. 7\n[14] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961–2969, 2017. 1,\n8, 13\n[15] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. 2016 IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npages 770–778, 2016. 4\n[16] Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu,\nWeihong Lin, Lei Sun, Chao Zhang, and Han Hu. Detrs with\nhybrid matching. arXiv preprint arXiv:2207.13080, 2022. 3,\n6, 7, 12\n[17] Kang Kim and Hee Seok Lee. Probabilistic anchor assign-\nment with iou prediction for object detection. In European\nConference on Computer Vision , pages 355–371. Springer,\n2020. 1, 3, 8, 13\n[18] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni,\nand Lei Zhang. Dn-detr: Accelerate detr training by intro-\nducing query denoising. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 13619–13627, 2022. 3, 5, 7\n[19] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu,\nJun Li, Jinhui Tang, and Jian Yang. Generalized focal loss:\nLearning qualified and distributed bounding boxes for dense\nobject detection. Advances in Neural Information Processing\nSystems, 33:21002–21012, 2020. 6, 8, 13\n[20] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object de-\ntection. arXiv preprint arXiv:2203.16527, 2022. 3, 7\n[21] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll´ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2980–2988, 2017. 1, 2, 4, 8, 13\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740–755.\nSpringer, 2014. 5\n[23] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi,\nHang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic\nanchor boxes are better queries for detr. arXiv preprint\narXiv:2201.12329, 2022. 2, 3, 6, 7\n[24] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\nSwin transformer v2: Scaling up capacity and resolution. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12009–12019, 2022. 7\n[25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, S. Lin, and B. Guo. Swin transformer: Hierar-\nchical vision transformer using shifted windows. ArXiv,\nabs/2103.14030, 2021. 2, 6, 7\n[26] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng,\nHouqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang.\nConditional detr for fast training convergence. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 3651–3660, 2021. 6, 7\n[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in neural information process-\ning systems, 28, 2015. 1, 2, 4, 8, 13\n[28] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union: A metric and a loss for bounding\nbox regression. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 658–666,\n2019. 13\n10\n[29] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam: Visual explanations from deep networks via\ngradient-based localization. In Proceedings of the IEEE in-\nternational conference on computer vision , pages 618–626,\n2017. 8, 13\n[30] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A\nlarge-scale, high-quality dataset for object detection. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 8430–8439, 2019. 6\n[31] Guanglu Song, Yu Liu, and Xiaogang Wang. Revisit-\ning the sibling head in object detector. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11563–11572, 2020. 1\n[32] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos:\nFully convolutional one-stage object detection. In Proceed-\nings of the IEEE/CVF international conference on computer\nvision, pages 9627–9636, 2019. 1, 2, 4, 8, 13\n[33] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, et al. Image as a\nforeign language: Beit pretraining for all vision and vision-\nlanguage tasks. arXiv preprint arXiv:2208.10442, 2022. 7\n[34] Wenhai Wang, Jifeng Dai, and Zhe Chen. Internimage:\nExploring large-scale vision foundation models with de-\nformable convolutions. arXiv preprint arXiv:2211.05778 ,\n2022. 7\n[35] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun.\nAnchor detr: Query design for transformer-based detector.\nIn Proceedings of the AAAI conference on artificial intelli-\ngence, volume 36, pages 2567–2575, 2022. 7\n[36] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao,\nJianmin Bao, Dong Chen, and Baining Guo. Contrastive\nlearning rivals masked image modeling in fine-tuning via\nfeature distillation. arXiv preprint arXiv:2205.14141, 2022.\n7\n[37] Zeyue Xue, Jianming Liang, Guanglu Song, Zhuofan Zong,\nLiang Chen, Yu Liu, and Ping Luo. Large-batch optimization\nfor dense visual predictions. In Advances in Neural Informa-\ntion Processing Systems, 2022. 1\n[38] Jianwei Yang, Chunyuan Li, and Jianfeng Gao. Focal mod-\nulation networks. arXiv preprint arXiv:2203.11926 , 2022.\n7\n[39] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\nZhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr\nwith improved denoising anchor boxes for end-to-end object\ndetection. arXiv preprint arXiv:2203.03605, 2022. 2, 3, 6, 7\n[40] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun\nChen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-\nNeng Hwang, and Jianfeng Gao. Glipv2: Unifying localiza-\ntion and vision-language understanding. Advances in Neural\nInformation Processing Systems, 35:36067–36080, 2022. 7\n[41] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\nStan Z Li. Bridging the gap between anchor-based and\nanchor-free detection via adaptive training sample selection.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 9759–9768, 2020. 1, 2,\n3, 4, 8, 13\n[42] Xingyi Zhou, Vladlen Koltun, and Philipp Kr ¨ahenb¨uhl.\nProbabilistic two-stage detection. arXiv preprint\narXiv:2103.07461, 2021. 7\n[43] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 2, 6, 7\n[44] Zhuofan Zong, Qianggang Cao, and Biao Leng. Rcnet: Re-\nverse feature pyramid and cross-scale shift network for ob-\nject detection. In Proceedings of the 29th ACM International\nConference on Multimedia, pages 5637–5645, 2021. 1\n11\nDETRs with Collaborative Hybrid Assignments Training\nSupplementary Material\n#convs 0 1 2 3 4 5\nAP 41.8 42.3 41.9 42.1 42.3 42.0\nTable 12. Influence of number of convolutions in auxiliary head.\nλ1 λ2 #epochs AP APS APM APL\n0.25 2.0 36 46.2 28.3 49.7 60.4\n0.5 2.0 36 46.6 29.0 50.5 61.2\n1.0 2.0 36 46.8 28.1 50.6 61.3\n2.0 2.0 36 46.1 27.4 49.7 61.4\n1.0 1.0 36 46.1 27.9 49.7 60.9\n1.0 2.0 36 46.8 28.1 50.6 61.3\n1.0 3.0 36 46.5 29.3 50.4 61.4\n1.0 4.0 36 46.3 29.0 50.1 61.0\nTable 13. Results of hyper-parameter tuning for λ1 and λ2.\nDETR ATSS\nFaster-RCNN\nDETR\nATSS\nFaster-RCNN\n0.0000 0.0078 0.0085\n0.0073 0.0000 0.0062\n0.0079 0.0059 0.0000\nCAM KL divergence\n0.000\n0.002\n0.004\n0.006\n0.008\nFigure 8. The relation matrix for the DETR head, ATSS head,\nand Faster-RCNN head. The detector is Co-Deformable-DETR\n(K = 2) with ResNet-50.\nA. More ablation studies\nThe number of stacked convolutions. Table 12 reveals\nour method is robust for the number of stacked convolu-\ntions in the auxiliary head (trained for 12 epochs). Con-\ncretely, we simply choose only 1 shared convolution to en-\nable lightweight while achieving higher performance.\nLoss weights of collaborative training. Experimental re-\nsults related to weighting the coefficient λ1 and λ2 are pre-\nsented in Table 13. We find the proposed method is quite\ninsensitive to the variations of {λ1, λ2}, since the perfor-\nmance slightly fluctuates when varying the loss coefficients.\nIn summary, the coefficients {λ1, λ2} are robust and we set\n{λ1, λ2} to {1.0, 2.0} by default.\nDETR ATSS\nFaster-RCNN\nPAA GFL FCOS\nRetinaNet\nDETR\nATSS\nFaster-RCNN\nPAA\nGFL\nFCOS\nRetinaNet\n0.0000 0.0062 0.0090 0.0129 0.0079 0.0069 0.0083\n0.0066 0.0000 0.0097 0.0132 0.0077 0.0045 0.0083\n0.0083 0.0087 0.0000 0.0091 0.0055 0.0095 0.0063\n0.0116 0.0116 0.0083 0.0000 0.0075 0.0125 0.0090\n0.0073 0.0068 0.0056 0.0083 0.0000 0.0081 0.0054\n0.0075 0.0048 0.0107 0.0143 0.0092 0.0000 0.0092\n0.0078 0.0075 0.0066 0.0100 0.0055 0.0082 0.0000\nCAM KL divergence\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\nFigure 9. Distances among 7 various heads in our model with\nK = 6.\nThe number of customized positive queries. We compute\nthe average ratio of positive samples in one-to-many label\nassignment to the ground-truth boxes. For instance, the ra-\ntio is 18.7 for Faster-RCNN and 8.8 for ATSS on COCO\ndataset, indicating more than 8 × extra positive queries are\nintroduced when K = 1.\nEffectiveness of collaborative one-to-many label assign-\nments. To verify the effectiveness of our feature learning\nmechanism, we compare our approach with Group-DETR\n(3 groups) and H-DETR. First, we find Co-DETR performs\nbetter than hybrid matching scheme [16] while training\nfaster and requiring less GPU memory in Table 6. As shown\nin Table 8, our method ( K = 1) achieves 46.2% AP, sur-\npassing Group-DETR (44.6% AP) by a large margin even\nwithout the customized positive queries generation. More\nimportantly, the IoF-IoB curve in Figure 2 demonstrates\nGroup-DETR fails to enhance the feature representations in\nthe encoder, while our method alleviates the poorly feature\nlearning.\nConflicts analysis. We have defined the distance between\nhead Hi and head Hj, and the average distance of Hi to\nmeasure the optimization conflicts in this study:\nSi,j = 1\n|D|\nX\nI∈D\nKL(C(Hi(I)), C(Hj(I)), (11)\nSi = 1\n2(K − 1)\nKX\nj̸=i\n(Si,j + Sj,i), (12)\nwhere KL, D, I, C refer to KL divergence, dataset, the input\n12\nimage, and class activation maps (CAM) [29]. In our imple-\nmentation, we choose the validation set COCO val as D\nand Grad-CAM as C. We use the output features of DETR\nencoder to compute the CAM maps. More specifically, we\nshow the detailed distances whenK = 2and K = 6in Fig-\nure 8 and Figure 9, respetively. The larger distance metric\nof Si,j indicates Hi is less consistent to Hj and contributes\nto the optimization inconsistency.\nB. More implementation details\nOne-stage auxiliary heads. Based on the conventional\none-stage detectors, we experiment with various first-stage\ndesigns [17, 19, 21, 32, 41] for the auxiliary heads. First,\nwe use the GIoU [28] loss for the one-stage heads. Then,\nthe number of stacked convolutions is reduced from 4 to\n1. Such modification improves the training efficiency with-\nout any accuracy drop. For anchor-free detectors, e.g.,\nFCOS [32], we assign the width of 8 × 2j and height of\n8 × 2j for the positive coordinates with stride 2j.\nTwo-stage auxiliary heads. We adopt the RPN and RCNN\nas our two-stage auxiliary heads based on the popular\nFaster-RCNN [27] and Mask-RCNN [14] detectors. To\nmake Co-DETR compatible with various detection heads,\nwe adopt the same multi-scale features (stride 8 to stride\n128) as the one-stage paradigm for two-stage auxiliary\nheads. Moreover, we adopt the GIoU loss for regression\nin the RCNN stage.\nSystem-level comparison on COCO. We first initialize\nthe ViT-L backbone with EV A-02 weights. Then we per-\nform intermediate finetuning on the Objects365 dataset us-\ning Co-DINO-Deformable-DETR for 26 epochs and reduce\nthe learning rate by a factor of 0.1 at epoch 24. The ini-\ntial learning rate is 2.5 × 10−4 and the batch size is 224.\nWe choose the maximum size of input images as 1280 and\nrandomly resize the shorter size to 480 −1024. Moreover,\nwe use 1500 object queries and 1000 DN queries for this\nmodel. Finally, we finetune Co-DETR on COCO for 12\nepochs with an initial learning rate of 5 × 10−5 and drop\nthe learning rate at the 8-th epoch by multiplying 0.1. The\nshorter size of input images is enlarged to 480 −1536 and\nthe longer size is no more than 2400. We employ EMA and\ntrain this model with a batch size of 64.\nSystem-level comparison on LVIS. In contrast to the\nCOCO setting, we use Co-DINO-Deformable-DETR++ to\nperform intermediate finetuning on the Objects365 dataset,\nas we find LSJ augmentation works better on the LVIS\ndataset. A batch size of 192, an initial learning rate of\n2 × 10−4, and an input image size of 1280×1280 are used.\nWe use 900 object queries and 1000 DN queries for this\nmodel. During finetuning on LVIS, we arm it with an addi-\ntional auxiliary mask branch and increase the input size to\n1536×1536. Besides, we train the model without EMA for\n16 epochs, where the batch size is set to 64, and the initial\nlearning rate is set to5×10−5, which is reduced by a factor\nof 0.1 at the 9-th and 15-th epoch.\n13"
  },
  {
    "source": "2204.05007v1.pdf",
    "content": "HiMODE: A Hybrid Monocular Omnidirectional Depth\nEstimation Model\nMasum Shah Junayed1, Arezoo Sadeghzadeh1, Md Baharul Islam1,2, Lai-Kuan Wong3, Tarkan Aydın1\n1Bahcesehir University 2American University of Malta 3Multimedia University\nmasumshahjunayed@gmail.com, arezoo.sadeghzadeh@bahcesehir.edu.tr, bislam.eng@gmail.com,\nlkwong@mmu.edu.my, tarkan.aydin@eng.bau.edu.tr\nApril 12, 2022\nAbstract\nMonocular omnidirectional depth estimation is receiv-\ning considerable research attention due to its broad appli-\ncations for sensing360◦surroundings. Existing approaches\nin this ﬁeld suffer from limitations in recovering small ob-\nject details and data lost during the ground-truth depth\nmap acquisition. In this paper, a novel monocular om-\nnidirectional depth estimation model, namely HiMODE is\nproposed based on a hybrid CNN+Transformer (encoder-\ndecoder) architecture whose modules are efﬁciently de-\nsigned to mitigate distortion and computational cost, with-\nout performance degradation. Firstly, we design a feature\npyramid network based on the HNet block to extract high-\nresolution features near the edges. The performance is fur-\nther improved, beneﬁting from a self and cross attention\nlayer and spatial/temporal patches in the Transformer en-\ncoder and decoder, respectively. Besides, a spatial residual\nblock is employed to reduce the number of parameters. By\njointly passing the deep features extracted from an input im-\nage at each backbone block, along with the raw depth maps\npredicted by the transformer encoder-decoder, through a\ncontext adjustment layer, our model can produce result-\ning depth maps with better visual quality than the ground-\ntruth. Comprehensive ablation studies demonstrate the sig-\nniﬁcance of each individual module. Extensive experiments\nconducted on three datasets; Stanford3D, Matterport3D,\nand SunCG, demonstrate that HiMODE can achieve state-\nof-the-art performance for 360◦ monocular depth estima-\ntion.\n1. Introduction\nDepth estimation is a fundamental technique to facilitate\n3D scene understanding from a single 2D image for real-\nFigure 1. An example of a panorama image with its corresponding\ndepth map and 3D structure generated byHiMODE. Our proposed\nhybrid CNN+Transformer model provides highly accurate depth\nmap with fewer artifacts than even the ground-truth which contains\nmany holes.\nworld applications such as autonomous driving [23], virtual\nreality (VR) [2], robotics [22], 3D reconstruction [25], ob-\nject detection [26], and augmented reality (AR) [21]. Ear-\nlier depth estimation techniques utilized the sensor-based\nor stereo vision-based approaches, with the passive stereo\nvision systems gaining more attention due to their com-\nparatively better performance in many real-world scenarios.\nHowever, availability of standard multi-view stereo datasets\nis scarce due to deferring alignment and camera settings.\nThis limitation inspired researchers to divert their atten-\ntion to monocular depth estimation (MDE) as a desirable\nalternative. Due to signiﬁcant advances in GPUs and avail-\nability of large-scale 3D datasets, several deep learning-\nbased MDE methods were reported in the literature with\npromising results [14, 18, 19]. The downside of these ap-\nproaches is that the perspective images have limited FOV .\nThe emergence of modern 360◦ cameras presented an\nappealing solution [9, 40]. Omnidirectional images provide\n360◦ FOV , formed by extending a 3D spherical construc-\narXiv:2204.05007v1  [cs.CV]  11 Apr 2022\ntion to a 2D 360◦×180◦ equirectangular map 1. Naive\nextension of MDE methods (e.g. FCRN [18]) to 360◦im-\nages may result in geometric distortion and image disconti-\nnuity, leading to sub-optimal results [44]. This motivates\nresearchers to conduct further studies on omnidirectional\nMDE. Several approaches based on Convolutional Neural\nNetworks (CNNs) have been proposed for omnidirectional\ndepth estimation. Although these methods could success-\nfully estimate the depth map around the equator, their per-\nformance declined sharply in regions with signiﬁcant dis-\ntortions (e.g., poles) due to their limited receptive ﬁeld. Re-\ncently, Transformer-based methods [32] have been shown\nto surpass CNNs with their competitive performance in var-\nious vision tasks. However, due to the lack of inductive bias\nin Transformers, dealing with small-scale datasets is chal-\nlenging [8]. Several researchers attempted to make the per-\nformance of the Transformers independent of data [31] but\nit is still an open problem. Although HoHoNet in [29] had\na structure similar to Transformer attention, the approach\nin [41] was the ﬁrst in directly applying the Transformers\nto the ﬁeld of 360◦ MDE. It achieved good performance\nwhen pre-trained on the large-scale dataset of traditional\nrectilinear images (RIs) and ﬁne-tuned for panoramic im-\nages. However, its performance was inferior in case it was\ndirectly trained on the small datasets of panoramic images.\nTo address the above-mentioned challenges, we propose\nHiMODE, a novel hybrid CNN-Transformer framework\nthat capitalizes on the strengths of CNN-based feature ex-\ntractors and the power of Transformers for monocular om-\nnidirectional depth estimation. Beneﬁting from combining\nboth low-level and high-level feature maps extracted by the\nCNN-based backbone, along with the raw depth maps esti-\nmated by the Transformer encoder-decoder via a context ad-\njustment layer, HiMODE not only performs competitively\non the existing small-scale datasets, but can also accurately\nrecover the surface depth data lost in the G.T depth maps.\nAn example of a resulting depth map, with its correspond-\ning 3D structure, is illustrated in Figure 1 to demonstrate\nthe competitive performance and capabilities of HiMODE\nin dealing with distortion and artifacts. This competitive\nperformance is accomplished via several mechanisms; i.e. a\nfeature pyramid network in the design of CNN-based back-\nbone, and a single block of encoder and decoder in the\nTransformer that comprises several modules - spatial and\ntemporal patches (STP), spatial residual block (SRB), and\nself and cross attention (SCA) block, in place of the typical\nmulti-head self-attention (MHSA) in encoder. More specif-\nically, the key contributions of this paper include:\n• A novel end-to-end hybrid architecture, that combines\nCNN and Transformer for monocular omnidirectional\ndepth estimation, obtaining competitive performance\n1In this paper, the terms omnidirectional, equirectangular, 360◦,\npanoramic, and spherical refer to the same context.\neven when trained on small-scale datasets.\n• A novel depth-wise CNN-based backbone network that\ncan extract high-resolution features near the edges to\novercome distortion and artifact issues (at object bound-\naries), and reﬁne the predicted raw depth maps with low-\nto high-level feature maps via context adjustment layer\nto obtain results even better than G.T.\n• A novel single encoder-decoder Transformer designed\nwith the SCA layer in place of the MHSA layer in the\nTransformer encoder for better encoding the parameters,\nand a STP layer along with the MHSA layer in the Trans-\nformer decoder to reduce the size of the training param-\neters while improving the depth map prediction.\n• A spatial residual block (SRB) that is added after both\nthe encoder and decoder, for training stabilization and\nperformance improvement. The SRB allocates more\nchannels to high-level patches in deeper levels and re-\ntains equivalent computation when resolution is reduced.\n• Results of extensive experiments demonstrate that Hi-\nMODE can achieve state-of-the-art performance across\nthree benchmarks datasets.\n2. Related Works\nMonocular depth estimation based on equirectangular\nimages (EIs) was ﬁrst attempted in [30] and [44]. Tateno\net al. [30] minimized the distortion based on CNNs and\nZioulis et al. [44] proposed a pre-processing step includ-\ning simplistic rectangular ﬁltering. Later in [43], the 360◦\nview synthesis was investigated in a self-supervised man-\nner. As the left and right sides of the EIs are adjacent in\nthe panorama sphere format, Lai et al. [17] proposed a deep\nnetwork with a boundary loss function to minimize the dis-\ntortion effects. In [7], the details of depth were preserved\nby employing both perspective and 360◦cameras.\nIn the BiFuse [34] method, a two-branch neural network\nwas proposed to use two projections of equirectangular and\ncube map for imitating both human eye visions of periph-\neral and foveal. In [29], Sun et al. proposed HoHoNet,\na versatile framework for holistic understanding of indoor\npanorama images based on a combination of compression\nand self attention modules. These approaches achieved sat-\nisfactory performance for the indoor scenarios. To deal with\noutdoor scenes with wider FOV , Xu et al. [36] proposed a\ngraph convolutional network (GCN) with a distortion factor\nin the adjacency matrix for real-time depth estimation.\nLi et al [20] proposed a novel two-stage pipeline for om-\nnidirectional depth estimation. In their method, the main\ninput was a single panoramic image used in the ﬁrst stage\nto generate one or more synthesized views. These synthe-\nsized images, along with the original 360◦image, were fed\ninto a stereo matching network with a differentiable spher-\nical warping layer to produce dense, high-quality depth.\nFigure 2. The proposed HiMODE architecture consists of a CNN-based feature extractor and a Transformer encoder-decoder.\nTo evaluate the methods based on two important traits of\nboundary preservation and smoothness, an unbiased holis-\ntic benchmark, namely Pano3D, was proposed in [1]. Ad-\nditionally, Pano3D evaluated the inter-dataset performance\nas well as the intra-dataset performance. In a very recent\nstudy in [41], a new 360◦ MDE system was proposed by\ncombining supervised and self-supervised learning. They\napplied a Vision Transformer (ViT) for the ﬁrst time in this\nﬁeld and achieved competitive performance. In summary,\nexisting approaches have shown improvement in depth es-\ntimation, but there exists an obvious need for performance\nprecision and distortion minimization.\n3. Proposed Network\nThe proposed HiMODE architecture, which comprises\nof a CNN-based feature extractor and a Transformer\nencoder-decoder, along with the linear projection (LP), po-\nsitional encoding, spatial residual, and context adjustment\nmodules, is presented in Figure 2. The details of each mod-\nule are discussed in the following subsections.\n3.1. Depth-wise CNN-based Backbone\nMany CNNs, such as MobileNet, ResNet, etc., are used\nas the backbone for feature extraction. The extracted fea-\nture maps are mostly ten to a hundred times bigger than the\nmodel size in these backbones, particularly for high-level\nfeature extraction operations, resulting in high computation\ncost and high dynamic RAM trafﬁc. To diminish this high\ntrafﬁc, the size of the feature maps is minimized with lossy\ncompression methods such as subsampling. Inspiring by\nthis, we design a novel depth-wise separable CNN-based\nbackbone with a feature pyramid network to decrease the\nsize of the extracted feature maps without sacriﬁcing the\naccuracy. It has an efﬁcient structure for extracting high-\nresolution features near the edges.\nAs illustrated in Figure 3, the proposed backbone is com-\nFigure 3. The detailed architecture of the proposed feature extrac-\ntor formed by concatenation of convolution and HNet blocks.\nposed of four single-layer convolution blocks, four HNet\nblocks (each block with eight layers), and four concatena-\ntion blocks for merging the feature maps generated from\ntwo former blocks. The HNet is a lightweight block ex-\ntracted from HardNet [6] and formed by two main sub-\nblocks of dense harmonic and depth-wise convolution (as\nthe high-level feature extraction module) to reduce the\nmemory computation cost and to fuse the features (for com-\npression). Differing from HardNet which has 68 layers, our\nbackbone consists of only 40 layers with superior perfor-\nmance over the other pre-trained models.\n3.2. Linear Projection and Positional Encoding\nGenerally, the input of a standard Transformer is re-\nquired to be a 1D sequence of token embeddings. Hence,\nthe extracted feature maps of X ∈ RH×W×C from our\nbackbone are ﬁrst split into patches, i.e., extracted feature\npatches (EFP), with a ﬁxed size of p×p (p = 8). These\npatches are reshaped into a sequence of ﬂattened 2D patches\nXp ∈RN×(p2C) (N = HW\np2 is the sequence length). These\nﬂattened patches are passed to a linear projection module\nto generate lower-dimensional linear embeddings with less\ncomputation cost. In the linear projection layer, each patch\nis ﬁrst unrolled into a vector multiplied with a learnable\nembedding matrix to form the Patch Embeddings ( PE),\nwhich are then concatenated with the Positional Embed-\ndings (PE′) to be fed into the Transformer.\nDistinguishing the similarities and differences between\nthe pixels in vast texture-less regions is a challenging issue\nwhich can be addressed by considering the relative loca-\ntion of information. Thus, we ﬁnd the spatial information\nof the EFP using the positional encoding module. The ad-\nequate positional information of the patches is encoded for\nthe irregular patch embedding. Consequently, the overall\nperformance is enhanced as the EFP is equipped with spa-\ntial/positional information before being fed into the trans-\nformer encoder. Positional Embeddings (PE′) are obtained\nvia the positional encoding formulation as follows [32]:\nPE′\n(pos,2i) = sin(pos/100002i/D) (1)\nwhere posand iare respectively the position of the patches\nand the dimensional position in the D-dimensional vector\n(D = 256, is the dimension of the vector into which each\npatch is linearly projected). The input of the Transformer\nencoder, i.e. I, is the concatenation of the patch embed-\ndings, PE, and positional embeddings, PE′:\nI = Concat(PE,PE ′) (2)\nwhere Concat represents the concatenation layer.\n3.3. Transformer\nA novel Transformer architecture, as shown in Figure 4,\nis designed with a single encoder and decoder block to gen-\nerate dense raw depth maps.\nTransformer Encoder Block (TEB).The TEB consists\nof the normalization, self-attention [8], cross-attention [12],\nand feed-forward layers. It uses concatenated patch and po-\nsitional embeddings (i.e. I ∈RN×D) as queries (Q), keys\n(K), and values ( V) which are obtained by multiplying I\nwith a learnable matrix, UQKV ∈RD×3Dk, as follows:\n[Q,K,V ] =I×UQKV (3)\nThen, the self and cross attention (SCA) mechanism is used\nto guarantee that the interconnections between pixels within\na patch, and the information ﬂow between pixels in different\npatches are captured. A single-channel feature map inher-\nently contains global spatial information, and splitting each\nchannel of feature maps into patches and employing self-\nattention to gather global information from the full feature\nmap is task of SCA. This mechanism is ﬁrst applied to cap-\nture global interactions between semantic features as con-\ntextual information and then make a ﬁne spatial recovery by\nomitting the non-semantic features. As such, self-attention\ncomputes the attention between pixels in the same patches\nwhile cross-attention computes the attention between pix-\nels in different patches. The self-attention module uses the\nthree matrices of Q,K,V ∈RN×Dk [32]:\nFigure 4. The detailed architecture of the proposed Transformer\nencoder-decoder, with the self and cross attention (SCA) modules,\nand the spatial and temporal patches (STP).\nAttention(Q,K,V ) =softmax(QKT\n√Dk\n)V = AV (4)\nwhere Dk = 192(set based on empirical observations as the\nexperimental results ofDk = 64,128,256,320 are inferior)\nand A ∈RN×N is the attention matrix that represents the\nsimilarity between each element in Qto all the elements in\nK. The weighted average of V determines the interactions\nbetween queries, Q, and keys, K, via the attention function.\nWith cross attention, irrelevant or noisy data are ﬁltered out\nfrom the skip connection features. The output of this self\nattention layer, along with the positional embeddings and\nQ, are fed into the cross attention layer followed by a linear\nactivation function. Unlike the standard attention layer, the\nentire process is more efﬁcient in cross attention as the com-\nputation and memory complexity for producing the atten-\ntion map are linear rather than being quadratic. The cross-\nattention layer works in cooperation with the residual short-\ncut connection and layer normalization as back-projection\nand projection functions for dimension alignment.\nA normalization layer (Add+Norm) is employed in an\nalternating manner after each of the layers, through which\nthe outputs of the layers are generated asLayerNorm(x+\nlayer(x)), where layer(x) is the function of the speciﬁc\nlayer. To make the dimension of a single head equal to the\npatch size, a patch-sized feed-forward network (FFN) is em-\nployed including two linear layers separated by GeLU.\nTransformer Decoder Block (TDB).The TDB consists\nof spatial and temporal patches (STP) [42], multi-head self\nattention (MHSA), normalization, and feed-forward layers.\nThe encoded patches obtained from TEB are passed to the\nSRB to speed up the training, improve the accuracy, and re-\nduce the computation cost. Afterward, they are fed into STP\nand MHSA layers, with positional embeddings. The STP\nlayer simpliﬁes a challenging work into two straightforward\ntasks: a temporal mechanism for ﬁnding the similarities of\nthe patches from a smaller spatial area along the temporal\ndimensions and a spatial mechanism for searching similari-\nties of the patches. Moreover, the spatial patches match and\nupsample the patches from the entire spatial zone, without\nany other patches in the vicinity. These two tasks ensure\nthat all spatial and temporal locations are covered. A corre-\nsponding encoded representation is created for each patch in\na target sequence, which now includes the attention scores\nfor each patch and the self-attention parameters of the Q,\nK, and V. Similar to TEB, normalization and feed-forward\nlayers are used to achieve the decoder output.\n3.4. Spatial Residual Block\nBy applying a spatial residual block in feature maps,\nmore channels are allocated to the features in the deeper\nlayers of the network to maintain similar computation for\nthe feature maps with decreased resolution. Inspired by this\nfact and the spatial relationship in patch embeddings, af-\nFigure 5. The detailed architecture of the spatial residual block\nwith three sub-blocks.\nter each TEB and TDB, a SRB is designed to improve the\nsystem’s efﬁciency, while decreasing the number of the pa-\nrameters, hence, the computation cost.\nThe whole SRB block is illustrated in Figure 5. The\n1D patch embeddings are reshaped into 2D feature maps,\nand fed into three sub-blocks. The ﬁrst sub-block includes\na normalization layer, followed by a Linear layer that per-\nforms linear transformation of the input patch embeddings\n(input and output data sizes are 64 and 128 with the bias)\nto preserve the channel size of all embeddings. The sec-\nond sub-block is composed of a zero-padding layer (adding\nzero pixels around the edges of the patch embeddings as\ndone in CNNs) to increase the embedding dimensions and\nan average pooling layer to decrease the sequence length\nof patch embeddings. Similarly, the embedding dimension\nis enhanced while the sequence length of patch embed-\ndings is again decreased by a layer of normalization with\nstrided convolution (kernel size of 1 ×1, 32 ﬁlters, and\nstride of 2, followed by a ReLU) in the third sub-block.\nAs the sequence length changes after passing through these\nsub-blocks, new positional embeddings are applied to up-\ndate the relative position information. Once the outputs\nof all three sub-blocks are obtained, they are concatenated\nthrough residual connections with their updated positional\nembeddings, resulting in the training stabilization and per-\nformance improvement.\n3.5. Context Adjustment Layer\nAs the estimated raw depth maps from the Transformer\nare effected by the ground-truth depth data, they may con-\ntain some holes and distortions on the edges due to imper-\nfect ground truth and data loss. Hence, the extracted feature\nmaps from each block of the proposed backbone and the ex-\ntracted raw depth maps from the Transformer are concate-\nnated through the context adjustment layer. Applying this\nlayer and making full use of both low- and high-level fea-\ntures of input images, can efﬁciently compensate the lack\nof the depth data in the raw depth maps generated by the\nTransformer. Consequently, the distortion and artifacts are\nreduced and more precise depth maps with sharper edges\nare generated. The overall architecture of context adjust-\nFigure 6. The detailed architecture of the context adjustment layer\nwith one convolution block, one residual block, and two activation\nfunctions of ReLU and sigmoid.\nment layer is illustrated in Figure 6. In the ﬁrst step, the\nfeature maps of fm1, fm2, fm3, and fm4, which are ex-\ntracted from the ﬁrst (as low-level features) to the fourth\nblock (as high-level features) of the CNN backbone, and the\nraw depth maps from the Transformer are merged to create\ncomposite images.\nThe composite images are then passed through a convo-\nlution block, followed by ReLU, to get the information of\nthe raw depth maps. There is also a residual block which\ncomprises two convolution layers with 3 ×3 kernel size, a\nReLU in between, and a skip connection from the ﬁrst con-\nvolution layer to the second convolution layer. This resid-\nual block, along with the sigmoid activation, ampliﬁes the\nchannel dimensions and predicts the accurate depth maps.\nThe depth maps from these blocks are then concatenated\nwith the initial composite images to generate the ﬁnal depth\nmaps with sharp edges. Interestingly, the network can re-\ncover depth data which is lost due to imperfect scanning in\nthe ground-truth depth maps.\n4. Experimental Results\n4.1. Dataset and Evaluation Metrics\nExperiments of our HiMODE are carried out on the\ntraining and test sets of three publicly available datasets,\ni.e. Matterport3D (10800 images) [5], Stanford3D (1413\nimages) [3], and PanoSUNCG (25000 images) [33]. The\nMatterport3D and Stanford3D datasets were gathered using\nMatterport’s Pro 3D Camera. In contrast, the depth maps\nof Stanford3D are generated from reconstructed 3D mod-\nels rather than from raw depth information. The images of\nthese datasets are resized to 256×512 pixels.\nWe follow the standard evaluation protocols as in ear-\nlier works [10, 35] and adopt the following quantitative er-\nror metrics; Absolute Relative error (Abs-Rel), Squared\nRelative difference (Sq-Rel), Root Mean Squared Error\n(RMSE), and Root Mean Squared Log Error (RMSE-log),\nin the experiments. We also compute the accuracy based\non Threshold, t: (%) of d⋆\ni , s.t. max\n(\nd⋆\ni\n˜di\n,\n˜di\nd⋆\ni\n)\n= δ <\nt\n(\nt∈\n[\n1.25,1.252,1.253])\n.\n4.2. Training Details\nWe implement HiMODE in PyTorch. Experiments are\nconducted on an Intel Core i9-10850K CPU with a 3.60GHz\nprocessor, 64GB RAM, and NVIDIA GeForce RTX 2070\nGPU. The number of respective modules in the Trans-\nformer, i.e. T-blocks, size of hidden nodes, self-attention,\ncross-attention and MHSA, are set as 2, 128, 1, 1, and 1, re-\nspectively. We applied Adam optimizer with a batch size of\n4 and 55 epochs. The learning rates of 0.00001 and 0.0003\nare selected for the real-world and synthetic data.\n4.3. Performance Comparison\nQuantitative Results. The performance of HiMODE\nis compared quantitatively with state-of-the-art methods\nin Table 1 (for the fair comparison, we use the pre-\ntrained models of the mentioned approaches and the pre-\ndicted depths for all methods are aligned before measur-\ning the errors similar to the technique applied in [41]).\nWe can observe that HiMODE outperforms the other meth-\nods on all benchmark metrics across the three datasets, ex-\ncept for the RMSE and RMSElog scores on Matterport3D\nand PanoSunCG datasets, where NLDPT [41] performs\nmarginally better than HiMODE. Normally, Transformers\nneed to be trained on large datasets. However, the size of the\nthree selected datasets, with 10800, 1413, and 25000 im-\nages, are considered small. To deal with this issue, the pre-\nvious Transformer-based approach [41] used a pretrained\nmodel (initially trained on large datasets of RIs) and then\nﬁne-tuned on these small-scale datasets. In contrast, by\ncombining Transformers with a CNN-based feature extrac-\ntor and making full use of the feature maps extracted from\nCNN (via context adjustment layer), our proposed model\ntrained directly on the small-scale datasets, not only results\nin highly accurate depth maps, but also alleviates the burden\nof pretraining, leading to efﬁcient results.\nAdditionally, to prove that our proposed HiMODE can\nperform well not only in MDE of EIs, but also in MDE of\nthe RIs, further analyses are conducted on the NYU Depth\nV2 dataset [27] to illustrate the effectiveness and accuracy\nof HiMODE in recovering the edge pixels and the details\nof objects. The results are obtained based on three evalu-\nation metrics of Precision, Recall, and F1 scores, follow-\ning the technique applied in [11]. Comparing the results\nwith other recent MDE approaches in Table 2, HiMODE\nachieves state-of-the-art performance for all evaluation met-\nrics, validating its capability in estimating highly accurate\ndepth maps with sharp edges.\nQualitative Results. Figure 7 compares the visual re-\nsults of HiMODE Bifuse [34] and HoHoNet [29]. In com-\nparison, HoHoNet generates more stable results than Bi-\nfuse. Although Bifuse and HoHoNet achieve satisfactory\nresults, they are not able to recover all the details completely\nand accurately (e.g. the shelves, the picture frame, and the\ncurtains/objects on the shelf in the ﬁrst, third, and ﬁfth ex-\namples). They also suffer from the limitations in dealing\nwith small objects. Comparatively, HiMODE produces ac-\ncurate depth maps with higher quality, sharper edges, and\nTable 1. Quantitative performance comparison of the proposed\nHiMODE with the state-of-the-art methods on Stanford3D, Mat-\nterport3D, and PanoSunCG datasets.\nDatasetsApproachesAbs-RelSq-RelRMSERMSElogδ<1.25δ<1.252 δ<1.253\nStanford3D\nOmnidepth [44]0.10090.05220.38350.14340.91140.98550.9958SvSyn [43]0.10030.04920.36140.14780.92960.98220.9949Bifuse [34]0.12140.10190.53960.18620.85680.95990.9880HoHoNet [29]0.09010.05930.41320.15110.90470.97620.9933NLDPT [41]0.06490.02400.27760.993 0.96650.99480.9983HiMODE0.05320.02070.26190.08210.97110.99650.9989\nMatterport3D\nOmnidepth [44]0.11360.06910.44380.15910.87950.97950.9950SvSyn [43]0.10630.05990.40620.15690.89840.97730.9974Bifuse [34]0.1390.13590.62770.20790.83810.94440.9815HoHoNet [29]0.06710.04170.34160.12700.94150.98380.9942NLDPT [41]0.07000.02870.30320.10510.95990.99380.9982HiMODE0.06580.02450.30670.09590.96080.99400.9985\nPanoSunCG\nOmnidepth [44]0.14500.10520.56840.18840.81050.97610.9941SvSyn [43]0.18670.17150.69650.23800.72220.94270.9840Bifuse [34]0.22030.26930.88690.28640.67190.88460.9660HoHoNet [29]0.08270.06330.38630.15080.92660.97650.9908NLDPT [41]0.07150.03610.34210.10420.96250.99500.9989HiMODE0.06820.03560.33780.10480.96880.99510.9992\nTable 2. Performance comparison on edge pixels recovery for\nMDE on NYU Depth V2 dataset (non-panoramic images) under\nthree different thresholds.\nApproachesThresholdRecallPrecisionF1-Score\nLaina et al. [18]0.25 0.4350.489 0.4540.50 0.4220.536 0.4631.00 0.4790.670 0.548\nXu et al. [18]0.25 0.4000.516 0.4360.50 0.3630.600 0.4391.00 0.4070.794 0.525\nFu et al. [37]0.25 0.5830.320 0.4020.50 0.4730.316 0.4121.00 0.5120.483 0.485\nHu et al. [11]0.25 0.5080.644 0.5620.50 0.5050.668 0.5681.00 0.5400.759 0.623\nYang et al. [39]0.25 0.5180.652 0.5700.50 0.5100.685 0.5761.00 0.5440.774 0.631\nHiMODE0.25 0.5980.703 0.6340.50 0.5690.720 0.6051.00 0.6410.815 0.656\nminimum distortion/artifacts on the object boundaries. It\nmanaged to recover the surface details similar to ground-\ntruth. Interestingly, for some regions, it can even recover\nsome distortions that exist in the ground-truth due to imper-\nfect scanning. This good performance could be attributed to\nthe design of concatenating the low- and high-level feature\nmaps of the input images from the CNN-backbone with the\nestimated raw depth maps from the Transformer, through\nthe context adjustment layer.\n4.4. Ablation Study\nBackbone. To evaluate the proposed CNN-based feature\nextractor as the backbone module and prove its superiority\nto the other pre-trained models, the depth estimation perfor-\nmance is investigated based on four backbones of ResNet34\n[13], ResNet50 [13], DenseNet [15], and HardNet [6] in Ta-\nble 3. The bold numbers indicate the best performance. In\nterm of the errors (i.e., Abs-Rel, Sq-Rel, RMSE, RMSE-\nlog) and accuracy ( δ, δ2, δ3) on the three datasets, the\nproposed CNN backbone ranks ﬁrst by a large margin in\nall evaluation metrics, except in Abs-Rel and δ3 for Stan-\nford3D, Sq-Rel for Matterport3D, and δ for PanoSunCG.\nOur proposed system ranks second with only a slight differ-\nence for these few cases. Additionally, our proposed CNN-\nbased backbone can qualitatively recover the accurate sur-\nface details and object boundaries (the qualitative results are\nnot presented here for brevity).\nFigure 7. Qualitative performance comparison of our proposed HiMODE and state-of-the-art methods on Matterport3D, Stanford3D, and\nPanoSunCG datasets. HIMODE can accurately recover surface details similar to or in some regions even better than the ground-truth, as\nthere are some holes, distortion and artifacts due to imperfect scanning (red and blue rectangles highlight some examples).\nTable 3. A quantitative comparison between the proposed CNN-\nbased backbone with four pre-trained models on three datasets.\nErrors AccuracyDatasetsBackbonesAbs-RelSq-RelRMSERMSElogδ δ2 δ3\nStanford3D\nResNet34 [13]0.11280.06350.36650.18730.91490.98840.9880ResNet50 [13]0.05090.06820.31770.11850.93490.99060.9923DenseNet [15]0.10450.06240.33580.16210.90760.98390.9889HardNet [6]0.07890.03520.30410.12150.92340.99470.9992Proposed0.05320.02070.26190.08210.97110.99650.9989\nMatterport3D\nResNet34 [13]0.10780.11390.45870.17860.89460.97920.9800ResNet50 [13]0.10140.08560.41890.12510.92570.97550.9945DenseNet [15]0.09350.04720.35480.15470.91380.96680.9829HardNet [6]0.07690.02440.36280.11740.94150.98310.9902Proposed0.06580.02450.30670.09590.96080.99400.9985\nPanoSunCG\nResNet34 [13]0.13530.14710.48230.23790.91830.99470.9926ResNet50 [13]0.10940.10430.38470.21490.95240.99180.9989DenseNet [15]0.09490.09870.42830.19580.92450.99090.9895HardNet [6]0.07260.05570.39850.13050.96930.98970.9877Proposed0.06820.03560.33780.10480.96880.99510.9992\nSpatial Residual Block. To investigate the effective-\nness of SRBs, HiMODE is evaluated with and without us-\ning SRBs for all datasets. Results are presented in Table 4\nin terms of errors and accuracy. We can observe that SRBs\ncontribute signiﬁcantly to improve the accuracy. In terms\nof error-based evaluation metrics, HiMODE attains the best\nresults on the Stanford3D dataset. For Abs-Rel, the per-\nformance is better in the absence of SRBs on Matterport3D\nand PanoSunCG. On the PanoSunCG dataset, the RMSElog\nvalue remains almost the same before and after applying\nSRBs. Apart from these few exceptions, HiMODE per-\nforms better on most other error metrics on Matterport3D\nand PanoSunCG in the presence of SRBs, proving the ef-\nfectiveness of SRB block.\nSelf and Cross Attention.In a typical ViT architecture,\nlong-range structural information is extracted from the im-\nages through the MHSA layer that aims to connect every el-\nement in the highest-level feature maps, leading to a recep-\ntive ﬁeld with all input images patches. In this mechanism,\nTable 4. Quantitative results of the HiMODE for ablation study of\nSRB (1st and 2nd rows of each dataset results) and SCA (1st and\n3rd rows of each dataset results) on three datasets.\nDatasetsSRBAttentionAbs-RelSq-RelRMSERMSElogδ δ2 δ3\nStanford3D✓ SCA 0.05320.02070.26190.08210.97110.99650.9989× SCA 0.06980.03950.28460.10280.95740.98980.9787✓ MHSA0.07460.05900.35480.15290.93580.97480.9695\nMatterport3D✓ SCA 0.06580.02450.30670.09590.96080.99400.9985× SCA 0.05140.03580.31080.10730.94800.97990.9891✓ MHSA0.06290.08540.40980.18890.94660.97090.9770\nPanoSunCG✓ SCA 0.06820.03560.33780.10480.96880.99510.9992× SCA 0.05400.05410.35860.10380.95550.98690.9902✓ MHSA0.06400.08490.39280.10440.94970.96720.9816\nthe lower-level feature maps are enhanced after passing the\nskip connections. A cross-attention mechanism causes suf-\nﬁcient spatial information to be recovered from rich seman-\ntic features. It ignores the irrelevant or noisy areas achieved\nfrom the skip connection features and emphasizes the vital\nregions. In the proposed Transformer, the SCA layer is de-\nsigned in the TEB to take advantage of the strengths of both\nmechanisms to provide contextual interactions and spatial\ndependencies. The effectiveness of this module is investi-\ngated in Table 4. By applying the SCA instead of MHSA,\nsigniﬁcant improvements are achieved on all three datasets.\nHiMODE also attains the best performance in terms of all\nerror-based evaluation metrics on the Stanford3D dataset.\nOn two other datasets of Matterport3D and PanoSunCG,\napplying SCA instead of MHSA results in a noticeable re-\nduction in all error metrics, except for Abs-Rel on Matter-\nport3D and Abs-Rel and RMSElog on PanoSunCG. These\nsigniﬁcant enhancements in the performance prove the su-\nperiority of SCA over MHSA.\nComputation Cost. Table 5 depicts the results of more\nablation studies to evaluate each proposed module in terms\nTable 5. Results of the ablation study on different modules in terms\nof computation cost and accuracy (on Stanford3D dataset). Bold\nand underlined numbers indicate the ﬁrst and second best results.\nSRB TEB TDBComputation Cost AccuracySCA MHSASTP #Parm δ δ2 δ3\n1 ✓ ✓ × ✓ 79.67M 0.97110.99650.9989\n2 ✓ × ✓ ✓ 84.59M 0.93580.97480.9695\n3 × ✓ × ✓ 88.47M 0.95740.98980.9787\n4 ✓ ✓ × × 81.37M 0.96230.97460.9877\n5 × × ✓ ✓ 93.59M 0.93980.96550.9629\n6 × ✓ × × 95.36M 0.92380.94810.9642\nof computation cost (number of parameters), and three\naccuracy-based evaluation metrics. We can observe that the\nproposed HiMODE, with the SRBs, SCA and STP modules,\nhas the least number of parameters with a value of 79.67M.\nAt the same time, it obtains the highest performance accu-\nracy at 0.9711, 0.9965, and 0.9989, forδ, δ2, and δ3, respec-\ntively. The results also reveal that the absence of SCA, SRB,\nSTP, both SRB and SCA, and both SRB and STP, brings ad-\nditional computation burden (parameters) of 4.92M, 8.8M,\n1.7M, 13.92M, and 15.69M, respectively. Besides, accu-\nracy also decreases. The two highest degradation in per-\nformance are observed by simultaneously removing both\nSRB and STP, and both SRB and SCA, proving the crucial\nrole of these modules in HiMODE. It is worth mentioning\nthat the performance and computation cost of HiMODE is\nalso investigated for both low ( 256 ×512 pixels) and high\n(512 ×1024 pixels) resolution images (the results are not\npresented here for brevity). It performs almost the same\nwhen the resolution of the input images varies, demonstrat-\ning its independence and robustness to the input image size.\nConsequently, our HiMODE is proposed based on the low\nresolution input images so that the number of the parameters\nis reduced without sacriﬁcing the performance accuracy.\n4.5. 3D Structure\nEstimating 3D structures from monocular omnidirec-\ntional images is a vital task in VR/AR and robotics applica-\ntions. The proposed HiMODE successfully reconstructs the\n3D structure (e.g., room) by ﬁnding the corners and bound-\nary between walls, ﬂoor, and ceiling. The qualitative results\non three datasets are illustrated in Figure 14. Quantitatively,\n3D intersection over union (IoU) values for 4, 6, 8, and more\nthan 10 corners are obtained as 79.86%, 80.09%, 73.46%,\nand 71.52%, respectively, with an average value of 76.23%.\n4.6. Limitations\nDespite the competitive performance of the proposedHi-\nMODE, it produces some unsatisfactory results in challeng-\ning situations. Figure 9 demonstrates some examples where\nHiMODE fails to generate an accurate depth map. As there\nare too many ﬁne details and small objects in the complex\nenvironment of Figure 9(a), it is challenging to produce a\ndepth map with accurate surface details. In Figure 9(b) and\n9(c), extreme illumination (very bright or dark) is shown to\ndegrade the performance of HiMODE.\nFigure 8. Qualitative results of depth map estimation with the re-\nconstructed 3D structures. The ﬁrst and second rows represent the\ninput images and the corresponding depth maps, respectively, and\nthe last two rows shows the 3D structures from different angles.\nFigure 9. Example of failure cases. HiMODE fails to recover\nthe depth data for complex scenes with (a) many tiny objects, (b)\noverexposed illumination, and (c) underexposed illumination.\n5. Conclusion\nIn this paper, we proposed a monocular omnidirectional\ndepth estimator, namely HiMODE. It was designed based\non a hybrid architecture of CNN+Transformer to effectively\nreduce the distortion and artifacts, and recover the surface\ndepth data. The high-level features near the edges were ex-\ntracted by using a pyramid-based CNN as the backbone,\nwith the HNet block inside. Further performance improve-\nment was achieved by designing SCA block in the Trans-\nformer encoder, and STP in the decoder. The sequence\nlength of patch embeddings was reduced when the dimen-\nsion increased, due to applying the novel structure of SRB\nafter each encoder and decoder. Interestingly, by combin-\ning the multi-level deep features extracted from the input\nimages with the depth maps generated by Transformers\nvia the context adjustment layer, HiMODE demonstrated\nthe capability to even recover the lost data in the ground-\ntruth depth maps. Extensive experiments conducted on\nthree benchmark datasets; Stanford3D, Matterport3D, and\nPanoSunCG, demonstrate that HiMODE can achieve state-\nof-the-art performance. For future work, we plan to extend\nHiMODE for real-time monocular 360◦ depth estimation,\nmaking it robust to illumination changes and efﬁciently ap-\nplicable for complex environments. In addition to improv-\ning the 3D structure for indoor settings, we would also ex-\ntend HiMODE for depth estimation and 3D reconstruction\nfor outdoor settings.\nAcknowledgements. This work is supported by the\nScientiﬁc and Technological Research Council of Turkey\n(TUBITAK) 2232 Leading Researchers Program, Project\nNo. 118C301.\nReferences\n[1] Georgios Albanis, Nikolaos Zioulis, Petros Drakoulis,\nVasileios Gkitsas, Vladimiros Sterzentsenko, Federico Al-\nvarez, Dimitrios Zarpalas, and Petros Daras. Pano3d: A\nholistic benchmark and a solid baseline for 360° depth esti-\nmation. In 2021 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition Workshops (CVPRW), pages 3722–\n3732. IEEE, 2021. 3\n[2] Lemonia Argyriou, Daphne Economou, and Vassiliki Bouki.\nDesign methodology for 360 immersive video applications:\nthe case study of a cultural heritage virtual tour. Personal\nand Ubiquitous Computing, 24(6):843–859, 2020. 1\n[3] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese.\nJoint 2d-3d-semantic data for indoor scene understanding.\narXiv preprint arXiv:1702.01105, 2017. 5, 12\n[4] Roberto Battiti. Using mutual information for selecting fea-\ntures in supervised neural net learning. IEEE Transactions\non neural networks, 5(4):537–550, 1994. 11\n[5] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej\nHalber, Matthias Niessner, Manolis Savva, Shuran Song,\nAndy Zeng, and Yinda Zhang. Matterport3d: Learning\nfrom rgb-d data in indoor environments. arXiv preprint\narXiv:1709.06158, 2017. 5, 12\n[6] Ping Chao, Chao-Yang Kao, Yu-Shan Ruan, Chien-Hsiang\nHuang, and Youn-Long Lin. Hardnet: A low memory traf-\nﬁc network. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 3552–3561, 2019. 3,\n6, 7, 11, 12, 13, 14, 15, 16\n[7] Xinjing Cheng, Peng Wang, Yanqi Zhou, Chenye Guan, and\nRuigang Yang. Omnidirectional depth extension networks.\nIn 2020 IEEE International Conference on Robotics and Au-\ntomation (ICRA), pages 589–595. IEEE, 2020. 2\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 4\n[9] Marc Eder, Pierre Moulon, and Li Guan. Pano popups: In-\ndoor 3d reconstruction with a plane-aware network. In 2019\nInternational Conference on 3D Vision (3DV), pages 76–84.\nIEEE, 2019. 1\n[10] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map\nprediction from a single image using a multi-scale deep net-\nwork. arXiv preprint arXiv:1406.2283, 2014. 5\n[11] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-\nmanghelich, and Dacheng Tao. Deep ordinal regression net-\nwork for monocular depth estimation. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2002–2011, 2018. 6\n[12] Mozhdeh Gheini, Xiang Ren, and Jonathan May. Cross-\nattention is all you need: Adapting pretrained transformers\nfor machine translation. arXiv preprint arXiv:2104.08771 ,\n2021. 4\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 6, 7, 11, 12, 13, 14, 15,\n16\n[14] Junjie Hu, Mete Ozay, Yan Zhang, and Takayuki Okatani.\nRevisiting single image depth estimation: Toward higher\nresolution maps with accurate object boundaries. In 2019\nIEEE Winter Conference on Applications of Computer Vision\n(WACV), pages 1043–1051. IEEE, 2019. 1\n[15] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 4700–4708, 2017. 6,\n7, 11, 12, 13, 14, 15, 16\n[16] Suresh Prasad Kannojia and Gaurav Jaiswal. Effects of vary-\ning resolution on performance of cnn based image classiﬁ-\ncation: An experimental study. Int. J. Comput. Sci. Eng ,\n6(9):451–456, 2018. 11\n[17] Po Kong Lai, Shuang Xie, Jochen Lang, and Robert La-\ngani`ere. Real-time panoramic depth maps from omni-\ndirectional stereo images for 6 dof videos in virtual reality.\nIn 2019 IEEE Conference on Virtual Reality and 3D User\nInterfaces (VR), pages 405–412. IEEE, 2019. 2\n[18] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-\nerico Tombari, and Nassir Navab. Deeper depth prediction\nwith fully convolutional residual networks. In 2016 Fourth\ninternational conference on 3D vision (3DV) , pages 239–\n248. IEEE, 2016. 1, 2, 6\n[19] Jae-Han Lee and Chang-Su Kim. Multi-loss rebalancing\nalgorithm for monocular depth estimation. In Computer\nVision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XVII 16 , pages\n785–801. Springer, 2020. 1\n[20] Yuyan Li, Zhixin Yan, Ye Duan, and Liu Ren. Panodepth:\nA two-stage approach for monocular omnidirectional depth\nestimation. 2\n[21] Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, and Ya-\nsutaka Furukawa. Planenet: Piece-wise planar reconstruc-\ntion from a single rgb image. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 2579–2588, 2018. 1\n[22] Michele Mancini, Gabriele Costante, Paolo Valigi, and\nThomas A Ciarfuglia. J-mod 2: Joint monocular obstacle\ndetection and depth estimation. IEEE Robotics and Automa-\ntion Letters, 3(3):1490–1497, 2018. 1\n[23] Anwesan Pal, Sayan Mondal, and Henrik I Christensen.\n” looking at the right stuff”-guided semantic-gaze for au-\ntonomous driving. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n11883–11892, 2020. 1\n[24] Giovanni Pintore, Marco Agus, and Enrico Gobbetti. At-\nlantanet: Inferring the 3d indoor layout from a single 360◦\nimage beyond the manhattan world assumption. InEuropean\nConference on Computer Vision , pages 432–448. Springer,\n2020. 12, 13\n[25] Giovanni Pintore, Claudio Mura, Fabio Ganovelli, Lizeth\nFuentes-Perez, Renato Pajarola, and Enrico Gobbetti. State-\nof-the-art in automatic 3d reconstruction of structured indoor\nenvironments. In Computer Graphics Forum , volume 39,\npages 667–699. Wiley Online Library, 2020. 1\n[26] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural net-\nwork for 3d object detection in a point cloud. InProceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 1711–1719, 2020. 1\n[27] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\nFergus. Indoor segmentation and support inference from\nrgbd images. In European conference on computer vision ,\npages 746–760. Springer, 2012. 6\n[28] Cheng Sun, Chi-Wei Hsiao, Min Sun, and Hwann-Tzong\nChen. Horizonnet: Learning room layout with 1d represen-\ntation and pano stretch data augmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1047–1056, 2019. 13\n[29] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Hohonet:\n360 indoor holistic understanding with latent horizontal fea-\ntures. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 2573–2582,\n2021. 2, 6, 11, 12, 13, 23, 24, 25, 26, 27, 28\n[30] Keisuke Tateno, Nassir Navab, and Federico Tombari.\nDistortion-aware convolutional ﬁlters for dense prediction in\npanoramic images. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), pages 707–722, 2018. 2\n[31] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In International Conference on Machine Learning ,\npages 10347–10357. PMLR, 2021. 2\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 2,\n3, 4\n[33] Fu-En Wang, Hou-Ning Hu, Hsien-Tzu Cheng, Juan-Ting\nLin, Shang-Ta Yang, Meng-Li Shih, Hung-Kuo Chu, and\nMin Sun. Self-supervised learning of depth and camera mo-\ntion from 360◦ videos. In Asian Conference on Computer\nVision, pages 53–68. Springer, 2018. 5, 12\n[34] Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and\nYi-Hsuan Tsai. Bifuse: Monocular 360 depth estimation via\nbi-projection fusion. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n462–471, 2020. 2, 6, 11, 12, 23, 24, 25, 26, 27, 28\n[35] Peng Wang, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian\nPrice, and Alan L Yuille. Towards uniﬁed depth and se-\nmantic prediction from a single image. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 2800–2809, 2015. 5\n[36] Di Xu, Xiaojun Liu, and Yanning Zhang. Real-time depth es-\ntimation for aerial panoramas in virtual reality. In2020 IEEE\nConference on Virtual Reality and 3D User Interfaces Ab-\nstracts and Workshops (VRW), pages 704–705. IEEE, 2020.\n2\n[37] Dan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe.\nPad-net: Multi-tasks guided prediction-and-distillation net-\nwork for simultaneous depth estimation and scene parsing.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 675–684, 2018. 6\n[38] Shang-Ta Yang, Fu-En Wang, Chi-Han Peng, Peter Wonka,\nMin Sun, and Hung-Kuo Chu. Dula-net: A dual-projection\nnetwork for estimating room layouts from a single rgb\npanorama. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 3363–\n3372, 2019. 13\n[39] Xin Yang, Qingling Chang, Xinglin Liu, Siyuan He, and Yan\nCui. Monocular depth estimation based on multi-scale depth\nmap fusion. IEEE Access, 9:67696–67705, 2021. 6\n[40] Yang Yang, Shi Jin, Ruiyang Liu, Sing Bing Kang, and\nJingyi Yu. Automatic 3d indoor scene modeling from single\npanorama. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 3926–3934,\n2018. 1\n[41] Ilwi Yun, Hyuk-Jae Lee, and Chae Eun Rhee. Improving 360\nmonocular depth estimation via non-local dense prediction\ntransformer and joint supervised and self-supervised learn-\ning. arXiv preprint arXiv:2109.10563, 2021. 2, 3, 6\n[42] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning\njoint spatial-temporal transformations for video inpainting.\nIn European Conference on Computer Vision , pages 528–\n543. Springer, 2020. 4\n[43] Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas,\nFederico Alvarez, and Petros Daras. Spherical view synthe-\nsis for self-supervised 360 depth estimation. In 2019 Inter-\nnational Conference on 3D Vision (3DV) , pages 690–699.\nIEEE, 2019. 2, 6\n[44] Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas,\nand Petros Daras. Omnidepth: Dense depth estimation for\nindoors spherical panoramas. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV) , pages 448–\n465, 2018. 2, 6\n[45] Chuhang Zou, Jheng-Wei Su, Chi-Han Peng, Alex Colburn,\nQi Shan, Peter Wonka, Hung-Kuo Chu, and Derek Hoiem.\n3d manhattan room layout reconstruction from a single 360\nimage. 2019. 13\nHiMODE: A Hybrid Monocular Omnidirectional Depth\nEstimation Model\n(Supplementary Materials)\nMasum Shah Junayed1, Arezoo Sadeghzadeh1, Md Baharul Islam1,2, Lai-Kuan Wong3, Tarkan Aydın1\n1Bahcesehir University 2American University of Malta 3Multimedia University\nmasumshahjunayed@gmail.com, arezoo.sadeghzadeh@bahcesehir.edu.tr, bislam.eng@gmail.com,\nlkwong@mmu.edu.my, tarkan.aydin@eng.bau.edu.tr\nApril 12, 2022\nIn this supplementary material, we provide more ablation\nstudies and results of the proposed HiMODE.\nA. Ablation Studies on Backbone\nAs introduced in the main paper, backbone module is an\nimportant part of our system. This section provides more\nablation studies on the backbone module to demonstrate its\nsuperiority, quantitatively and qualitatively, to the other pre-\ntrained backbones.\nA.1. Detailed Architecture of the Backbone.\nOur CNN-based backbone is referred to as depth-wise\ndue to using depth-wise Conv layers in HNet blocks which\nare concatenated with the Conv layers. Depth-wise sepa-\nrable CNNs have less parameters and possibility for over-\nﬁtting, such as MobileNet. HNet (as shown in Figure 10)\nis extracted from HardNet [6]. Comparing the number of\nlayers, our backbone has only 40 layers (i.e. HNet= 4 ×8,\nConv=4, Concat=4 layers) which is signiﬁcantly less than\nthat of HardNet (i.e. 68 layers).\nFigure 10. The overall architecture of the proposed HNet block\nextracted from the HardNet [6] structure.\nA.2. The Effects of Input Resolution\nThe visual information is affected by the image reso-\nlution. High image resolution results in higher visual in-\nformation and so better image quality. Generally, when\nthe image resolution is reduced, the performance of the\nCNN-based networks degrades signiﬁcantly [16]. On the\nother hand, lower input image resolution is desirable as it\nleads to a reduced number of features and the optimized\nnumber of parameters. Consequently, the risk of model\noverﬁtting is minimized [4]. Nevertheless, extensive low-\nering of the image resolution eliminates the information\nthat is useful for classiﬁcation. The effects of the input\nimage resolution on the overall performance of the pro-\nposed system based on our novel CNN-based backbone\nis investigated and compared with four pre-trained mod-\nels of ResNet34 [13], ResNet50 [13], DenseNet [15], and\nHardNet [6]. The evaluation results are presented in Ta-\nble 6 in terms of four error-based evaluation metrics and\nthree accuracy-based evaluation metrics. The terms ”low”\nand ”high” for image resolution refer to the image size of\n256 ×512 and 512 ×1024, respectively. Comparing the\nresults, our proposed backbone ranks ﬁrst in all evalua-\ntion metrics on all three datasets, except for Abs-Rel and δ\non Stanford3D, RMSElog on Matterport3D, and RMSE on\nPanoSunCG, at which our proposed backbone ranks second\nwith a slight difference. The superiority of our proposed\nbackbone is proven as the other models cannot surpass its\nperformance even with high-resolution inputs. It is worth\nmentioning that the overall performance of our proposed\nsystem maintains almost the same when the resolution of\nthe input images varies, demonstrating its independence and\nrobustness to the input image size. Consequently, our Hi-\nMODE system is proposed based on the low-resolution in-\nput images so that the number of parameters is reduced\nwithout sacriﬁcing the performance accuracy, as opposed\nto the other state-of-the-art approaches [29, 34] which were\nmostly based on 512 ×1024 input images.\n11\nA.3. Computation Cost of Different Backbones\nIn addition to the performance, the superiority of our\nproposed CNN-based backbone is further investigated by\ncomparing its computation cost with that of four pre-trained\nmodels of ResNet34 [13], ResNet50 [13], DenseNet [15],\nand HardNet [6]. The results in terms of the number of\nparameters as computation cost with three accuracy-based\nevaluation metrics on Stanford3D [3] dataset are presented\nin Table 7 (for both low and high resolution). We can\nobserve that the proposed HiMODE based on our novel\nCNN-based backbone has the least number of parameters\nfor low resolution input images with the values of 79.67M\nas well as the best performance accuracy of 0.9711 and\n0.9965 in terms of δ, δ2, respectively. Its performance in\nterms of δ3 is almost the same as that of HardNet. Replac-\ning the other pre-trained models of ResNet34, ResNet50,\nDenseNet, and HardNet with our proposed backbone brings\nadditional computation burden (i.e. parameters) of 7.29M,\n10M, 6.48M, and 2.57M, respectively. Besides, accuracy\nalso signiﬁcantly decreases. The highest degradation in\nδ, and δ2 occurs in DenseNet with the values of 0.9076\nand 0.9839, respectively, while the poorest performance\nof 0.9880 in terms of δ3 belongs to ResNet34. For high\nresolution input images, HiMODE based on our proposed\nCNN-based backbone still has the least number of param-\neters (98.89M) comparing with the others. Achieving the\nleast computation cost with the highest performance accu-\nracy proves the capabilities of our proposed backbone over\nthe other pre-trained feature extractors.\nA.4. Qualitative Results for Different Backbones\nThe performance of HiMODE based on our proposed\nCNN-based backbone is compared with the other pre-\ntrained models qualitatively in Figures 11-13. As it is men-\ntioned in the main paper, our depth-wise proposed back-\nbone can extract high-resolution features near the edges to\novercome distortion and artifact issues. On the depth maps\nestimated based on our proposed backbone, sharper edges\nand more details are recovered.\nB. More Results on 3D Structure\nB.1. Quantitative Results\nThe detailed quantitative results for 3D structure esti-\nmation under different number of ground-truth corners are\npresented in Table 8 as a supplement to the main paper\nto extend the quantitative studies. In comparison with the\nrecent state-of-the-art approaches, our proposed HiMODE\nachieves the best results for 6 corners (82.23%) on the 2D\nIOU (intersection over union) metric, and both 4 (85.48%)\nand 6 (85.05%) corners in terms of 3D IOU. Overall, our\nproposed method can achieve state-of-the-art performance\nin 3D structure estimation with fewer corners. For higher\nnumber of corners, our method obtained comparable results\nalthough AtlantaNet [24] is the best performer.\nB.2. Qualitative Results\nAdditional qualitative results for estimating 3D struc-\ntures from monocular omnidirectional images on three dat-\nsets of Stanford3D [3], Matterport3D [5], and PanoSunCG\n[33] are demonstrated in Figures 14-16, respectively 1. Our\nmethod was evaluated on different input images with var-\nious numbers of corners. Qualitatively, our HiMODE can\nsuccessfully reconstruct the 3D structure by ﬁnding the cor-\nners and boundary between walls, ﬂoor, and ceiling, which\nis a vital task in VR/AR and robotics applications. The pro-\nposed HiMODE successfully reconstructs the 3D structure\nwith different numbers of corners by ﬁnding the corners and\nboundary between walls, ﬂoor, and ceiling.\nC. More Omnidirectional Depth Results\nWe show more qualitative results for depth map estima-\ntion by our HiMODE in Figures 17-19 on three datasets;\nStanford3D, Matterport3D, and PanoSunCG. The results\nof our proposed HiMODE are compared with two other\nrecent state-of-the-art approaches of Bifuse [34] and Ho-\nHoNet [29] on three datasets in Figures 20-22. These visual\nresults further demonstrate the superior performance of the\nproposed HiMODE over the other two methods in recover-\ning the details of the surfaces, even for the deep regions and\nsmall objects.\nIn addition, the effectiveness of combining theHiMODE\noutput with the output of two recent state-of-the-art ap-\nproaches; Bifuse [34] and HoHoNet [29], on three datasets\nis investigated. The qualitative results are illustrated in Fig-\nures 23-25. Very interestingly, we observe signiﬁcant im-\nprovement in the depth map estimation when HiMODE is\ncombined with Bifuse, HohoNet, or both methods via a\nsimple concatenation of the respective outputs. The best\nqualitative results are achieved with the combination of\nthree methods, whereby the resulting depth map mimics\nthe groundtruth depth map very closely (the last columns\nof Figures 23-25).\n1Some samples of 3D structures are available athttps://bit.ly/\n3HLh1Z3 in video format.\nTable 6. A quantitative comparison between the proposed CNN-based backbone with four pre-trained models on three datasets based on\ntwo input image resolutions of 256 × 512 (low) and 512 × 1024 (high).\nDatasets Backbones Resolution Errors Accuracy\nAbs-Rel Sq-Rel RMSE RMSElog δ δ2 δ3\nStanford3D\nResNet34 [13] High 0.0956 0.0824 0.3875 0.1577 0.9398 0.9817 0.9906\nLow 0.1128 0.0635 0.3665 0.1873 0.9149 0.9884 0.9880\nResNet50 [13] High 0.0666 0.0489 0.2897 0.1217 0.9512 0.9940 0.9968\nLow 0.0509 0.0682 0.3177 0.1185 0.9349 0.9906 0.9923\nDenseNet [15] High 0.0823 0.0702 0.3346 0.1246 0.9451 0.9901 0.9944\nLow 0.1045 0.0624 0.3358 0.1621 0.9076 0.9839 0.9889\nHardNet [6] High 0.0755 0.0461 0.2984 0.1038 0.9578 0.9947 0.9972\nLow 0.0789 0.0352 0.3041 0.1215 0.9234 0.9947 0.9992\nProposed High 0.0679 0.0223 0.2711 0.0963 0.9693 0.9959 0.9987\nLow 0.0532 0.0207 0.2619 0.0821 0.9711 0.9965 0.9989\nMatterport3D\nResNet34 [13] High 0.1026 0.0861 0.3956 0.1434 0.9487 0.9820 0.9777\nLow 0.1078 0.1139 0.4587 0.1786 0.8976 0.9792 0.9800\nResNet50 [13] High 0.0699 0.0586 0.3610 0.1003 0.9523 0.9928 0.9859\nLow 0.1014 0.0856 0.4189 0.1251 0.9257 0.9755 0.9945\nDenseNet [15] High 0.0782 0.0545 0.3678 0.1165 0.9501 0.9893 0.9908\nLow 0.0935 0.0472 0.3548 0.1547 0.9138 0.9668 0.9829\nHardNet [6] High 0.0630 0.0471 0.3355 0.0873 0.9562 0.9918 0.9938\nLow 0.0769 0.0244 0.3648 0.1174 0.9415 0.9831 0.9902\nProposed High 0.0597 0.0213 0.3146 0.0894 0.9601 0.9921 0.9981\nLow 0.0658 0.0245 0.3067 0.0959 0.9608 0.9940 0.9985\nPanoSunCG\nResNet34 [13] High 0.1006 0.0653 0.3989 0.1595 0.9466 0.9783 0.9849\nLow 0.1353 0.1471 0.4823 0.2379 0.9183 0.9947 0.9926\nResNet50 [13] High 0.0832 0.0474 0.3259 0.1339 0.9524 0.9864 0.9936\nLow 0.1094 0.1043 0.3847 0.2149 0.9524 0.9918 0.9989\nDenseNet [15] High 0.0852 0.0427 0.3561 0.1226 0.9538 0.9889 0.9951\nLow 0.0949 0.0987 0.4283 0.1958 0.9245 0.9909 0.9895\nHardNet [6] High 0.0715 0.0398 0.3303 0.1178 0.9615 0.9910 0.9978\nLow 0.0726 0.0557 0.3985 0.1305 0.9693 0.9897 0.9877\nProposed High 0.0667 0.0347 0.3265 0.1013 0.9691 0.9945 0.9990\nLow 0.0682 0.0356 0.3378 0.1048 0.9688 0.9951 0.9992\nTable 7. Comparison between the proposed CNN-based backbone\nwith four pre-trained models as backbone in terms of computation\ncost and accuracy (on Stanford3D dataset). The bold and under-\nlined numbers indicate the best results for low and high resolution\ninput images, respectively.\nBackbones Input Computation Cost Accuracy\nParameters δ δ2 δ3\nResNet34 [13]High 103.55M 0.9398 0.9817 0.9906\nLow 86.96M 0.9149 0.9884 0.9880\nResNet50 [13]High 107.28M 0.9512 0.9940 0.9968\nLow 89.67M 0.9349 0.9906 0.9923\nDenseNet [15]High 104.81M 0.9451 0.9901 0.9944\nLow 86.15M 0.9076 0.9839 0.9889\nHardNet [6] High 100.37M 0.9578 0.9947 0.9972\nLow 82.24M 0.9234 0.9947 0.9992\nProposed High 98.89M 0.9693 0.9959 0.9987\nLow 79.67M 0.9711 0.9965 0.9989\nTable 8. Quantitative comparison between our HiMODE and\nﬁve state-of-the-art methods for 3D structure estimation on Stan-\nford3D dataset in terms of 2D and 3D IOU. The best results are\nindicated with bold numbers.\nIOU (%) Approaches # Corners\nAll 4 6 8 10+\n2D\nLayoutNet v2 [45]75.82 81.35 72.33 67.45 63.00\nDuLa-Net v2 [38]75.07 77.02 78.79 71.03 63.27\nHorizonNet [28]79.11 81.88 82.26 71.78 68.32\nAtlantaNet [24]80.02 82.09 82.08 75.19 71.61\nHoHoNet [29] 79.88 82.64 82.16 73.65 69.26\nHiMODE 79.74 82.40 82.23 72.87 69.03\n3D\nLayoutNet v2 [45]78.73 84.61 75.02 69.79 65.14\nDuLa-Net v2 [38]78.82 81.12 82.69 74.00 66.12\nHorizonNet [28]81.71 84.67 84.82 73.91 70.58\nAtlantaNet [24]82.09 84.42 83.85 76.97 73.18\nHoHoNet [29] 82.32 85.26 84.81 75.59 70.98\nHiMODE 81.41 85.48 85.05 74.38 70.10\nInput Image ResNet34 ResNet50 DenseNet HardNet Proposed\nFigure 11. Qualitative comparisons for our HiMODE based on our proposed CNN-based backbone and four pre-trained models of\nResNet34 [13], ResNet50 [13], DenseNet [15], and HardNet [6] on Stanford3D dataset. As demonstrated by rectangles, our HiMODE\ncan accurately recover the details of the surface especially sharp edges even for the deep regions and small objects.\nInput Image ResNet34 ResNet50 DenseNet HardNet Proposed\nFigure 12. Qualitative comparisons for our HiMODE based on our proposed CNN-based backbone and four pre-trained models of\nResNet34 [13], ResNet50 [13], DenseNet [15], and HardNet [6] on Matterport3D dataset. As demonstrated by rectangles, our HiMODE\ncan accurately recover the details of the surface especially sharp edges even for the deep regions and small objects.\nInput Image ResNet34 ResNet50 DenseNet HardNet Proposed\nFigure 13. Qualitative comparisons for our HiMODE based on our proposed CNN-based backbone and four pre-trained models of\nResNet34 [13], ResNet50 [13], DenseNet [15], and HardNet [6] on PanoSunCG dataset. As demonstrated by rectangles, ourHiMODE can\naccurately recover the details of the surface especially sharp edges even for the deep regions and small objects.\nInput Image 3D View: Angle 1 3D View: Angle 2 3D View: Angle 3\nFigure 14. 3D structures estimation on Stanford3D dataset using our HiMODE.\nInput Image 3D View: Angle 1 3D View: Angle 2 3D View: Angle 3\nFigure 15. 3D structures estimation on Matterport3D dataset using our HiMODE.\nInput Image 3D View: Angle 1 3D View: Angle 2 3D View: Angle 3\nFigure 16. 3D structures estimation on PanoSunCG dataset using our HiMODE.\nInput Image G.T HiMODE (gray-scale) HiMODE (color)\nFigure 17. More qualitative results for omnidirectional depth map estimation based on our HiMODE on Stanford3D dataset.\nInput Image G.T HiMODE (gray-scale) HiMODE (color)\nFigure 18. More qualitative results for omnidirectional depth map estimation based on our HiMODE on Matterport3D dataset.\nInput Image G.T HiMODE (gray-scale) HiMODE (color)\nFigure 19. More qualitative results for omnidirectional depth map estimation based on our HiMODE on PanoSunCG dataset.\nInput Image Bifuse HoHoNet HiMODE\nFigure 20. More qualitative comparisons between our HiMODE and two recent state-of-the-art approaches, Bifuse [34] and HoHoNet [29]\non Stanford3D dataset. As demonstrated by rectangles, our HiMODE can accurately recover the details of the surface even for the deep\nregions with small objects.\nInput Image Bifuse HoHoNet HiMODE\nFigure 21. More qualitative comparisons between our HiMODE and two recent state-of-the-art approaches, Bifuse [34] and HoHoNet [29]\non Matterport3D dataset. As demonstrated by rectangles, our HiMODE can accurately recover the details of the surface with sharp edges\neven for the deep regions and for small objects.\nInput Image Bifuse HoHoNet HiMODE\nFigure 22. More qualitative comparisons between our HiMODE and two recent state-of-the-art approaches, Bifuse [34] and HoHoNet [29]\non PanoSunCG dataset. As demonstrated by rectangles, our HiMODE can accurately recover the details of the surface with sharp edges\neven for the deep regions and for small objects.\nInput Image G.T HiMODE Bifuse + HiMODE HoHoNet + HiMODE All\nFigure 23. More qualitative results for omnidirectional depth map estimation based on our HiMODE along with its combination with two\nrecent state-of-the-art approaches, Bifuse [34] and HoHoNet [29] on Stanford3D dataset.\nInput Image G.T HiMODE Bifuse + HiMODE HoHoNet + HiMODE All\nFigure 24. More qualitative results for omnidirectional depth map estimation based on our HiMODE along with its combination with two\nrecent state-of-the-art approaches, Bifuse [34] and HoHoNet [29] on Matterport3D dataset.\nInput Image G.T HiMODE Bifuse + HiMODE HoHoNet + HiMODE All\nFigure 25. More qualitative results for omnidirectional depth map estimation based on our HiMODE along with its combination with two\nrecent state-of-the-art approaches, Bifuse [34] and HoHoNet [29] on PanoSunCG dataset."
  },
  {
    "source": "2311.17241v2.pdf",
    "content": "End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames\nShuming Liu1 Chen-Lin Zhang2 Chen Zhao1* Bernard Ghanem1\n1King Abdullah University of Science and Technology (KAUST) 24Paradigm Inc\nAbstract\nRecently, temporal action detection (TAD) has seen sig-\nnificant performance improvement with end-to-end train-\ning. However, due to the memory bottleneck, only mod-\nels with limited scales and limited data volumes can afford\nend-to-end training, which inevitably restricts TAD perfor-\nmance. In this paper, we reduce the memory consumption\nfor end-to-end training, and manage to scale up the TAD\nbackbone to 1 billion parameters and the input video to\n1,536 frames, leading to significant detection performance.\nThe key to our approach lies in our proposed temporal-\ninformative adapter (TIA), which is a novel lightweight\nmodule that reduces training memory. Using TIA, we free\nthe humongous backbone from learning to adapt to the\nTAD task by only updating the parameters in TIA. TIA also\nleads to better TAD representation by temporally aggregat-\ning context from adjacent frames throughout the backbone.\nWe evaluate our model across four representative datasets.\nOwing to our efficient design, we are able to train end-\nto-end on VideoMAEv2-giant and achieve 75.4% mAP on\nTHUMOS14, being the first end-to-end model to outper-\nform the best feature-based methods. Code is available at\nhttps://github.com/sming256/AdaTAD.\n1. Introduction\nTemporal Action Detection (TAD) plays a vital role in the\nunderstanding of long-form videos. Its objective is to pin-\npoint specific action instances within untrimmed videos,\nidentifying their start and end times, along with their re-\nspective categories [18, 23, 48, 66, 68, 73, 77]. This task is\ncrucial for various applications, including highlight detec-\ntion [40, 70], video-language grounding [42, 52, 53], and\naction spotting [3]. Though innovations in the detector de-\nsign have made profound progress in the past years [33, 72,\n73], recent research highlights two new trends: end-to-end\ntraining [12, 35, 75], and scaling up [62, 63].\nEnd-to-end training in TAD refers to jointly training the\nvideo encoder and action detector [12, 32, 68, 76]. Com-\npared to feature-based methods, end-to-end training offers\n*Corresponding author.\nAdaTADVideoMAE-S\nAdaTADVideoMAE-BAdaTADVideoMAE-L\nAdaTADSlowFast-R50\nBasicTADSlowOnly-R50\nE2E-TADSlowFast-R50\nAdaTAD † VideoMAEv2-giant\nTALLFormerVideoSwin-B\nRe2TALRevVideoSwin-T\nCurrentFeature-basedBest Result\nTadTRI3D\nBMNI3D\n End-to-EndFeature-based\nOurs\nGTADI3D\nAdaTAD VideoMAE-H\nFigure 1. AdaTAD enjoys the benefit of end-to-end training\nand scaling up with efficient memory usage. The bubble size\nrepresents the number of the model’s learnable parameters. Using\nSlowFast-R50, AdaTAD achieves better performance compared to\nE2E-TAD [36] with less memory. When further scaling up the\nmodel to VideoMAEv2-gaint and data to 1536 frames, we achieve\nan impressive avg. mAP of 75.4% on THUMOS14.\ndistinct advantages. First, it can effectively bridge the gap\ncommonly found between pretraining and fine-tuning, such\nas data and task discrepancy. Second, video spatial augmen-\ntations can be utilized in the end-to-end setting, leading to\nfurther performance gain.\nScaling up refers to improving performance by increas-\ning the model size or the input data volume, and has demon-\nstrated its effectiveness in various domains [17, 22, 43, 59].\nIn TAD, offline methods have attempted to scale up the\nfeature extraction network to reach a higher performance.\nA notable example includes the work by Wang et al.[62],\nwhich reports a 10% increase in mean Average Precision\n(mAP) by scaling from VideoMAE-S to VideoMAEv2-\ngiant, using ActionFormer [72] on THUMOS14 [28].\nIntuitively, combining the strengths of end-to-end train-\ning and scaling up is expected to be most beneficial for im-\nproving TAD performance. However, both strategies de-\nmand substantial GPU memory, which restricts end-to-end\ntraining to a small model [32, 68, 75], or a small input vol-\nume [12, 36]. As shown in Fig. 1, the performance of pre-\nvious end-to-end methods still significantly lags behind the\n1\narXiv:2311.17241v2  [cs.CV]  20 Apr 2024\nbest results achieved by feature-based approaches. Addi-\ntionally, current end-to-end methods in TAD use compu-\ntationally intensive full fine-tuning, risking the issues of\ncatastrophic forgetting and overfitting during transfer learn-\ning [56, 69]. These issues can result in less competitive\nperformance, especially when the downstream datasets are\nsmall, which is a common scenario in the TAD domain.\nIn this paper, we aim to overcome the above limitations\nby harnessing the advantages of both scaling-up and end-to-\nend training. To achieve this, we introduce adapter tuning\nfor temporal action detection (AdaTAD). Our method suc-\ncessfully trains a TAD model in an end-to-end manner, uti-\nlizing a backbone with 1 billion parameters and processing\ninput videos of 1,536 frames. As illustrated in Fig. 1, to the\nbest of our knowledge, this is the first end-to-end work that\noutperforms the best feature-based TAD methods.\nSpecifically, we employ the following strategies to en-\nhance the TAD performance while maintaining reasonable\nmemory consumption. First, we identify that the snip-\npet representation commonly used in feature-based meth-\nods is excessively redundant. In response, we have adopted\na more memory-efficient frame-representation scheme, es-\ntablishing an effective end-to-end baseline for TAD. Sec-\nond, we adopt the parameter-efficient fine-tuning technique\nto minimize memory usage and mitigate overfitting in trans-\nfer learning. Notably, we introduce a novel Temporal-\nInformative Adapter (TIA). This adapter is injected between\nbackbone layers and is the only learnable component in the\nbackbone during fine-tuning. Different from conventional\nadapters [25], TIA is tailored for the TAD task and inte-\ngrates temporal depth-wise convolutions to aggregate in-\nformative context from adjacent frames. Third, for addi-\ntional memory efficiency, we propose a lighter variant of\nour method. By positioning the TIAs alongside the origi-\nnal backbone, rather than inside it, backpropagation can be\ndone through the TIAs only. This configuration can further\ncut down on the need for intermediate activations within\nthe primary backbone, thereby allowing us to scale up the\nmodel size and data size to unprecedented levels.\nAdaTAD establishes a new state-of-the-art across multi-\nple TAD datasets. Notably, our method achieves an impres-\nsive 75.4% mAP on THUMOS14, outperforming the pre-\nvious feature-based best result of 71.5% by a large margin.\nThis achievement underscores the possible paradigm shift\nin TAD,i.e., moving from traditional feature extraction plus\noffline detectors to scaling up end-to-end TAD training. We\nsummarize our contribution as follows:\n1. We introduce an efficient end-to-end framework for\nTAD, scaling up the model size to 1 billion parameters\nand the input data to 1,536 frames. We achieve a consis-\ntent performance improvement with the scaling up, shed-\nding light on the importance of scaling for TAD.\n2. We propose a novel temporal-informative adapter to re-\nduce memory as well as aggregate the temporal context\nfor TAD. Different variants of these adapters are de-\nsigned to trade off between performance and memory\ncost. To the best of our knowledge, we are the first to\nintroduce the adapter mechanism to TAD.\n3. Our method achieves state-of-the-art performance across\nfour TAD datasets. Remarkably, this represents the\nfirst end-to-end approach that outperforms the previous\nfeature-based methods by a large margin.\n2. Related Work\nTemporal Action Detection. Current methods for tempo-\nral action detection, also referred to as temporal action lo-\ncalization, can be broadly classified into three categories\nbased on their architectural design: one-stage, two-stage,\nand DETR-based methods. One-stage methods directly lo-\ncalize actions from a multi-scale feature pyramid, such as\nActionFormer [72] and TriDet [51]. These methods inte-\ngrate action classification and temporal boundary regres-\nsion in a single step [49, 67, 68]. Two-stage methods, in\ncontrast, involve an additional step of proposal feature ex-\ntraction [4, 33, 44, 64, 65, 74, 79]. For instance, VSGN [73]\nemploys boundary sampling to further refine proposals. Re-\ncently, there is a growing interest in query-based meth-\nods [37, 50, 57], which deploy a set of learned queries to\ninteract with the feature maps and directly predict the ac-\ntions’ temporal boundaries and categories.\nIn addition to the aforementioned categories, TAD can\nalso be divided into feature-based and end-to-end methods.\nThe former relies on pre-extracted RGB features and op-\ntionally incorporates optical flow features. On the other\nhand, end-to-end methods take raw video frames as input\nand jointly optimize the video encoder and action detector\n[34]. Due to computational constraints, AFSD [32] down-\nsamples the input to a resolution of 962. DaoTAD [61]\nand E2E-TAD [36] provide evidence that high TAD perfor-\nmance can be achieved by relying solely on the RGB modal-\nity with various data augmentations. Further innovations\ncame from SGP [13], TALLFormer [12], and ETAD [35],\nall of which introduced strategies to backpropagate only\nthrough parts of the data. Additionally, Re 2TAL [75] and\nDr2Net[76] design reversible network architectures to re-\nlease the memory occupied by intermediate activations. De-\nspite these advancements, all above methods follow the full\nfine-tuning paradigm, and none has yet surpassed the best\nresults achieved by feature-based approaches.\nScaling Law in Deep Learning. Scaling up model and\ndata has been a prevalent strategy across both computer vi-\nsion and natural language processing fields to achieve su-\nperior performance. The GPT series [5, 43, 45, 46] has\nconsistently demonstrated that larger models pretrained on\nextensive datasets yield significant gains in language under-\nstanding capabilities. Analogously, in the realms of image\n2\n+DetectorHead\nBasicLayers\nTemporalInformativeAdapter\n×#Layers\nDetectorHead\nBasicLayers\nTemporalInformativeAdapter\nBasicLayers\nDetectorHead\n×#Layers\nBasicLayers\nDetectorHead\n×#Layers\n(a)Offline Methods(b)Full Fine-Tuning(c)AdaTAD(d)AdaTAD†\nFrozenLearnable\nBasicLayersTemporalInformativeAdapter\n···\nVideoBackbone\nFigure 2. Comparative illustration of our proposed TAD framework versus popular and widely used alternatives. (a) represents the\ntypical offline method. (b) is the traditional end-to-end method using full fine-tuning. (c) Tailored for the TAD task, our AdaTAD uses a\nlightweight temporal-informative adapter inside the backbone to achieve efficient transfer learning. (d) To further reduce memory usage\nand scale up the model/data, AdaTAD† uses an alternative placement for adapters outside the backbone.\nand video understanding, architectures such as ViT [17] and\nMViT [19], have also witnessed the effectiveness of this\nscaling strategy. Alabdulmohsin et al.[1] present a recipe\nfor estimating scaling law parameters reliably from learn-\ning curves in computer vision. To attain even greater per-\nformance, several studies have also scaled up image reso-\nlution [38] or video clip length [62]. In this paper, we suc-\ncessfully apply this principle within the domain of temporal\naction detection and achieve state-of-the-art results.\nEfficient Transfer Learning. Transfer learning aims to\nadapt a pretrained model to a new domain. In TAD, typi-\ncally off-the-shelf action recognition models are employed\nas the backbone, such as SlowFast [20]. Traditional transfer\nlearning adopts full fine-tuning, meaning that all parame-\nters of the pretrained model are updated. However, studies\nby [47, 69] have noted that full fine-tuning may harm the\npre-learned knowledge, particularly when the downstream\ndataset is small and less comprehensive. Moreover, as mod-\nels increase in size, the computational and storage demands\nof full fine-tuning proportionally increase.\nIn response to these challenges, several works have in-\nvestigated parameter-efficient tuning (PEFT) strategies that\ninvolve fine-tuning only a fraction of the network. For in-\nstance, Adapter [25] inserts lightweight modules analogous\nto feedforward networks in transformers, and only tunes\nthese elements. LoRA [26] employs low-rank matrices\nin each transformer layer. Prefix-tuning [31] and prompt-\ntuning [30] attach learnable prompt tokens at the input stage\nor within each layer. In computer vision, many PEFT meth-\nods [8, 9, 27, 69] have also been explored across various\ntasks to optimize transfer learning efficiency. Our work rep-\nresents the first effort to examine the potential of the PEFT\nmechanism in TAD.\nAlthough PEFT effectively reduces the number of learn-\nable parameters, data-intensive and computationally heavy\ntasks like video understanding require more memory-\nefficient techniques. To this end, several works try to exter-\nnalize the trainable components from the backbone, elimi-\nnating the need for backpropagation through the extensive\noriginal model. For example, LST [55] introduces a supple-\nmentary lightweight network that operates in parallel with\nthe main model. Similarly, E3V A [71] leverages intermedi-\nate features with adapters to enable efficient transfer learn-\ning while minimizing memory usage. Our work is inspired\nby these methods yet with a streamlined and simple design.\n3. Methodology\nIn this section, we introduce our AdaTAD step-by-step. We\nfirst introduce notations and study the efficient video rep-\nresentation to establish an end-to-end TAD baseline. Next,\nwe introduce a temporal-informative adapter designed for\nefficient TAD. Finally, we propose an alternative placement\nfor adapters to further alleviate computational demands.\n3.1. Notations\nTemporal action detection can be formulated as follows:\ngiven an untrimmed video X ∈ R3×H×W×T , where H and\nW are the height and width of each frame, and T is the\nframe number, its temporal action annotations can be de-\nnoted as Ψg = {φi =(ts, te, c)}N\ni=1, where ts, te, care the\nstart, end time and category of action instance φi, and N\nis the number of ground truth actions. TAD aims to pre-\ndict candidate proposal set Ψp =\n\b\nˆφi =(ˆts, ˆte, ˆc, s)\n\tM\ni=1 to\ncover Ψg, and s is the confidence score.\n3.2. Frame-level representation\nOur end-to-end TAD architecture comprises two main com-\nponents: feature extraction and action detection. Following\nprevious work [75], we select ActionFormer [72] as our ac-\n3\ntion detection head due to its robust performance across var-\nious datasets without much hyperparameter tuning. Next,\nwe discuss two ways of encoding raw frames into represen-\ntative features (feature extraction): snippet representation\nand frame representation.\nSnippet Representation. Snippet-based video repre-\nsentations are popular choices in offline feature extraction.\nThe whole video is divided into several short snippets (or\nnamely clips). Each snippet has a short temporal length,\ne.g., 16 frames, and different snippets can have overlapping\nframes. Thus, the video can be conceptualized as T snip-\npets, denoted by X ∈ RT×3×16×H×W . Each snippet is\nprocessed through the video backbone, and spatial-temporal\npooling is applied to extract one snippet feature. This pro-\ncessing yields the feature representation F ∈ RT×C, where\nC denotes the channel dimension of the pooled features.\nFrame Representation. In contrast to snippet-based\nrepresentations, frame-based video representations consider\nthe entire video as a singular snippet or clip, represented as\nX ∈ R1×3×T×H×W . Then, the whole frame sequence is\nfed into the video backbone, and only spatial pooling is em-\nployed to extract features [12, 32, 75]. For attention-based\nmodels such as VideoMAE [60], the video is chunked into\nmultiple shorter clips to avoid extensive temporal attention.\nAlthough both representations have been employed in\nprevious studies, a fair comparison between them has not\nyet been performed. To address this gap, we conduct a\ncomparative analysis of the two representations under the\nsame setting on THUMOS14, measuring their memory us-\nage and detection mAP. The results in Table 1 indicate\nthat frame representation has comparable or even better\nperformance than snippet representation, yet with much\nsmaller memory consumption. When the feature extraction\nbackbones are frozen, frame representation yields superior\nresults to snippet representation for both VideoMAE [59]\nand SlowFast [20] backbones. Only in the end-to-end set-\nTable 1. Snippet representation vs frame representation. We\nuse the end-to-end version of ActionFormer with two representa-\ntions for comparison. The snippet input is 768×3×16×160×160,\nand the frame input is 1×3×768×160×160. ∗ means activation\ncheckpointing is utilized to avoid overflowing GPU memory.\nSetting Backbone Repr. Avg. mAP Mem (GB)\nFrozen\nVideoMAE-S Frame 59.35 1.9\nSnippet 57.68 13.2\nSlowFast-R101 Frame 61.34 3.6\nSnippet 60.24 17.2\nEnd\nto\nEnd\nVideoMAE-S Frame 67.15 2.8 ∗\nSnippet 68.46 24.6 ∗\nSlowFast-R101 Frame 65.33 5.5 ∗\nSnippet 66.72 51.6 ∗\nting can the snippet representation achieve 1% mAP advan-\ntage over frame representations; however, it requires 8 times\nmore memory consumption. Taking into account both per-\nformance and memory usage, frame-based representations\ncould be a better choice for end-to-end TAD development.\nTherefore, we use frame representation as the default\nbaseline to encode videos in our experiments. Following\nthe previous TALLFormer work [12], we also incorporate\nactivation checkpointing [10] and mixed precision training\n[41] to fully harness the potential of scaling.\n3.3. Temporal-Informative Adapter\nIn Section 3.2, we have built a simple end-to-end baseline\nusing full fine-tuning. However, the baseline still suffers\nfrom two aspects: 1. Increased computational cost. In Ta-\nble 1, we only use small video backbones like VideoMAE-\nS. When scaling VideoMAE-S to larger models, the compu-\ntational burden and memory cost will grow rapidly.2. Infe-\nrior transfer learning ability. More critically, the base-\nline follows the tradition of full fine-tuning, which may\nlead to inferior transfer learning. Pointed out by [56, 69],\nfull fine-tuning may result in overfitting or forgetting, espe-\ncially for large pretrained models. If downstream datasets\nare not sufficiently diverse, full fine-tuning can even de-\nstroy the powerful features learned from large-scale pre-\ntraining. Motivated by the above two aspects, we apply the\nPEFT mechanism and propose to fine-tune a plug-and-play\nmodule named Temporal-Informative Adapter (TIA) to\nachieve efficient and effective transfer learning for TAD.\nWe first review the architecture of the standard adapter\nproposed by [25]. As formulated in Equation 1, the stan-\ndard adapter includes a down-projection fully connected\n(FC) layer with parameter Wdown ∈ Rd×d\nγ , where d\nγ repre-\nsents the intermediate dimension and satisfies γ >1. Then,\nan up-projection layer Wup ∈ R\nd\nγ ×d is employed to re-\nstore the channel dimension. Between these two FC lay-\ners, a non-linear activation function σ is inserted, such as\nGELU [24]. Afterward, a residual connection is added to\nthe output of the projection layer. Note thatx and x′ are the\ninput and output features with the same shape Rd×t×h×w.\nx′ = W⊤\nup · σ(W⊤\ndown · x) + x. (1)\nAlthough the adapter has achieved great success in nat-\nural language processing and computer vision, the stan-\ndard adapter only focuses on adapting channel informa-\ntion, which neglects the temporal context vital for the TAD\ntask. To address this limitation, we introduce the temporal-\ninformative adapter, as depicted in Fig. 3(b).\nThe architecture of TIA follows the general bottleneck\ndesign of the standard adapter, while we integrate the tem-\nporal depth-wise convolution layers, as described in Equa-\ntion 2. The temporal convolution with a kernel size of k is\n4\nUp*\nDown\nUp\nFCDWConvGELU\n𝛼\n+UpGELU\nDown\n(a)StandardAdapter(b)Temporal-InformativeAdapter𝑥:𝑑×𝑡×ℎ×𝑤\nFigure 3. Architecture of (a) standard adapter and (b) our\ntemporal-informative adapter. We incorporate temporal depth-\nwise convolution to aggregate context from adjacent frames.\ndesigned to aggregate local informative context from adja-\ncent frames and to enrich the representation of the current\ntime step. Practically, this is achieved through the applica-\ntion of a 3D convolution with kernel size(k, 1, 1) and group\nsize d\nγ for depth-wise processing. Additionally, an FC layer\nwith weight Wmid ∈ R\nd\nγ ×d\nγ is employed to facilitate infor-\nmation exchange across channels. At last, a learnable scalar\nα is introduced to adjust the amplitude of adapter’s output.\n¯x = σ(W⊤\ndown · x),\nˆx = W⊤\nmid · DWConvk(¯x) + ¯x,\nx′ = α · W⊤\nup · ˆx + x.\n(2)\nAs shown in Fig. 2(c), TIA is designed to be inserted\nbetween different backbone layers, e.g. between each ViT\nblock of VideoMAE or each bottleneck block of SlowFast.\nTo ensure the newly added connection does not affect the\noriginal network at the beginning of transfer learning, the\nweight and bias of the adapter’s last projection layer Wup\nare initialized to 0. The learnable coefficient α is initialized\nto 1. The temporal kernel size k is set to 3, and the channel\ndownsampling ratio γ is set to 4 by default. Under these set-\ntings, the additional trainable parameters coming from TIA\nonly account for 4.7% of the total parameters of the origi-\nnal backbone. Since this backbone is frozen when TIA is\nused, our proposed strategy constitutes a massive reduction\nin trainable parameters as compared to full fine-tuning. Our\nexperiments show that TIA can achieve better performance\nthan full fine-tuning with less memory usage.\n3.4. Alternative Placement for Adapter\nAlthough the previously described PEFT approach can re-\nduce tunable parameters and memory usage, the gradient\nstill needs to backpropagate over the entire backbone during\ntraining. This requirement limits our ability to scale-up the\nmodel size or input data size further. As highlighted in prior\nworks [55, 71], if we can stop the gradient backpropagation\nwithin the original backbone, additional computational sav-\nings can be achieved.\nInspired by this insight, we propose a placement strat-\negy for adapters that position them externally to the back-\nbone, rather than inserting them inside. As illustrated in\nFig. 2(d), we utilize the previously introduced TIA mod-\nule, but its output does not feed back into the middle of the\noriginal backbone. It is directly added to the backbone’s\nfinal layer. This configuration eliminates the need for back-\npropagation through the original network, as gradients are\nonly tracked to the shallow lightweight adapter. To further\ndiminish computation, we observe that adapting only the\nlast half of backbone layers yields comparable performance\nwhile reducing half of the adaptation cost.\nTo distinguish the different variants, we name the stan-\ndard adaption design as AdaTAD, and the alternative place-\nment as AdaTAD †. The latter can be considered as a\nlite version of the former. Compared to directly injecting\nadapters into the backbone, AdaTAD† may lead to a slight\nperformance drop. However, it enables us to leverage richer\nmodels and more data, which should effectively counter this\npossible drop.\n4. Experiments\n4.1. Datasets and Metrics\nWe choose ActivityNet-1.3 [23], THUMOS14 [28], and\nEpic-Kitchens 100 [15] to evaluate our proposed approach.\nActivityNet-1.3 and THUMOS14 are web-collected third-\nperson untrimmed videos, consisting of 19,994 and 413\nvideos, respectively. EPIC-Kitchens 100 is collected from\n700 egocentric videos. Since the action categories of EPIC-\nKitchens 100 are more domain-specific and different from\ncommon pretraining data, achieving higher performance on\nthis dataset is more challenging. Moreover, we also evaluate\nour method on the Ego4D-MQ [21] dataset, and the results\ncan be found in the appendix.\nFollowing common practice, we report the mean Av-\nerage Precision (mAP) at certain IoU thresholds and av-\nerage mAP as the evaluation metrics. On ActivityNet-\n1.3, the IoU thresholds are chosen from 0.5 to 0.95 with\n10 steps. On THUMOS14, the threshold is chosen from\n{0.3,0.4,0.5,0.6,0.7}. On EPIC-Kitchens 100, the threshold\nis set to {0.1,0.2,0.3,0.4,0.5}.\n4.2. Implementation Details\nWe implement our method with PyTorch 2.0 and MMAc-\ntion2 [14] with 4 A100 GPUs. By default, mixed-precision\ntraining and activation checkpointing are adopted to save\nmemory. We use ActionFormer [72] as our detection head,\nand keep the hyper-parameters unchanged on each dataset.\nThe learning rate for the adapter in the backbone is grid-\n5\nTable 2. Results on ActivityNet-1.3 and THUMOS14 , measured by mAP (%) at different tIoU thresholds. E2E refers to end-to-end\ntraining, and Mem refers to memory usage (GB) per video. On ActivityNet-1.3, our prediction is combined with CUHK [78] classification\nresults. Specifically, ∗ means we employ stronger video-level classification results used in InternVideo [63] for a fair comparison. We\nreport our best results in bold, and the previous best results in underline, which was achieved by the feature-based method. The last row is\nachieved when only the last half of backbone layers are adapted; otherwise, full-layer adaptation will lead to out-of-memory on A100-80G.\nMethod Backbone E2E Flow Mem ActivityNet-1.3 THUMOS14\n0.5 0.75 0.95 Avg. 0.3 0.4 0.5 0.6 0.7 Avg.\nBMN [33] TSN ✗ ✓ - 50.07 34.78 8.29 33.85 56.0 47.4 38.8 29.7 20.5 38.5\nTadTR [37] I3D ✗ ✓ - 49.10 32.60 8.50 32.30 62.4 57.4 49.2 37.8 26.3 46.6\nActionFormer [72] SlowFast-R50 ✗ ✗ - 54.26 37.04 8.13 36.02 78.7 73.3 65.2 54.6 39.7 62.3\nActionFormer [72] I3D ✗ ✓ - 53.50 36.20 8.20 35.60 82.1 77.8 71.0 59.4 43.9 66.8\nASL [49] I3D ✗ ✓ - 54.10 67.40 8.00 36.20 83.1 79.0 71.7 59.7 45.8 67.9\nTriDet [51] I3D ✗ ✓ - 54.70 38.00 8.40 36.80 83.6 80.1 72.9 62.4 47.4 69.3\nVideoMAEv2 [62] VideoMAEv2-g ✗ ✗ - - - - - - - - - - 69.6\nInternVideo [63] VideoMAE-H+UniformerV2 ✗ ✗ - - - - 39.00∗ - - - - - 71.5\nAFSD [32] I3D ✓ ✓ 12 52.40 35.30 6.50 34.40 67.3 62.4 55.5 43.7 31.1 52.0\nE2E-TAD [36] SlowFast-R50 ✓ ✗ 12 50.47 35.99 10.33 35.10 69.4 64.3 56.0 46.4 34.9 54.2\nBasicTAD [68] SlowOnly-R50 ✓ ✗ 12 51.20 33.41 7.57 33.12 75.5 70.8 63.5 50.9 37.4 59.6\nTALLFormer [12] VideoSwin-B ✓ ✗ 29 54.10 36.20 7.90 35.60 76.0 - 63.2 - 34.5 59.2\nRe2TAL [75] Re2VideoSwin-T ✓ ✗ 24 54.75 37.81 9.03 36.80 77.0 71.5 62.4 49.7 36.3 59.4\nAdaTAD SlowFast-R50 ✓ ✗ 4.3 55.28 38.11 8.87 37.11 81.0 76.2 69.4 59.0 44.5 66.0\nAdaTAD VideoMAE-S ✓ ✗ 2.5 56.15 38.99 9.07 37.85 84.5 80.2 71.6 60.9 46.9 68.8\nAdaTAD VideoMAE-B ✓ ✗ 4.9 56.77 39.35 9.71 38.39 87.0 82.4 75.3 63.8 49.2 71.5\nAdaTAD VideoMAE-L ✓ ✗ 11.0 57.69 40.56 10.13 39.22 87.7 84.1 76.7 66.4 52.4 73.5\nAdaTAD VideoMAE-H ✓ ✗ 19.2 58.04 40.55 9.75 39.37 88.9 85.3 78.6 66.9 52.5 74.4\nAdaTAD VideoMAEv2-g ✓ ✗ 29.9 58.45 41.16 10.45 39.79 89.5 85.8 78.9 67.3 52.6 74.8\nAdaTAD† (1536×2242) VideoMAEv2-g ✓ ✗ 43.6 60.82 42.69 9.84 41.15∗ 89.6 85.9 79.4 67.6 53.8 75.4\nAdaTAD (1536×2242) VideoMAEv2-g ✓ ✗ 50.6 61.72 43.35 10.85 41.93∗ 89.7 86.7 80.9 71.0 56.1 76.9\nsearched from 5e-4 to 5e-5, and other parameters inside\nthe backbone are frozen. On ActivityNet-1.3, we resize the\nvideo into a fixed length of 768 frames. On THUMOS14,\nwe randomly truncate a window with 768 frames with a\ntemporal stride of 4. On EPIC-Kitchens 100, we randomly\ntruncate a window with 6144 frames with a temporal stride\nof 2. After the video encoder, the feature is resized to fixed\nlengths of 192, 768, and 768, respectively, for the three\ndatasets. Frame resolution is set to 160 2 by default. In all\nexperiments, we report the training memory usage. More\nimplementation details can be found in the appendix.\n4.3. Comparison with SoTA Methods\nTable 2 compares our AdaTAD with other state-of-the-\nart (SoTA) methods on ActivityNet-1.3 and THUMOS14\ndatasets. Initially, we use SlowFast-R50 as the backbone.\nFor comparison, we also extract corresponding offline fea-\ntures, utilizing the snippet representation where each snip-\npet comprises 32 frames with 2242 resolution. We ob-\nserve that end-to-end training enhances performance from\n62.3% to 66.0% on THUMOS14. Notably, this architec-\nture has also been used in E2E-TAD [36]. However, our\nmethod consumes less memory while achieving superior\nperformance. This apple-to-apple comparison underscores\nthe benefits of adapter tuning and the scaling-up principle.\nTable 3. Results on EPIC-Kitchens 100 validation set. For com-\nparison, the feature-based methods use the same SlowFast-R50.\nMethod E2E 0.1 0.2 0.3 0.4 0.5 Avg.\nVerb Task\nBMN [33] ✗ 10.8 8.8 8.4 7.1 5.6 8.4\nG-TAD [66] ✗ 12.1 11.0 9.4 8.1 6.5 9.4\nActionFormer [72] ✗ 26.6 25.4 24.2 22.3 19.1 23.5\nASL [49] ✗ 27.9 - 25.5 - 19.8 24.6\nTriDet [51] ✗ 28.6 27.4 26.1 24.2 20.8 25.4\nAdaTAD (SlowFast-R50) ✓ 26.5 25.7 23.9 21.7 17.6 23.1\nActionFormer (VideoMAE-L) ✗ 32.7 31.6 29.1 26.7 23.6 28.7\nAdaTAD (VideoMAE-L) ✓ 33.1 32.2 30.4 27.5 23.1 29.3\nNoun Task\nBMN [33] ✗ 10.3 8.3 6.2 4.5 3.4 6.5\nG-TAD [66] ✗ 11.0 10.0 8.6 7.0 5.4 8.4\nActionFormer [72] ✗ 25.2 24.1 22.7 20.5 17.0 21.9\nASL [49] ✗ 26.0 - 23.4 - 17.7 22.6\nTriDet [51] ✗ 27.4 26.3 24.6 22.2 18.3 23.8\nAdaTAD (SlowFast-R50) ✓ 24.5 23.6 22.3 20.0 16.5 21.4\nActionFormer (VideoMAE-L) ✗ 31.3 29.7 27.2 25.3 21.3 26.9\nAdaTAD (VideoMAE-L) ✓ 32.4 31.6 30.1 27.4 24.6 29.3\nFurthermore, when adopting the VideoMAE [59] fam-\nily as our backbone and progressively scaling up the model\nsize, the performance of AdaTAD consistently improves.\nUsing the largest model, i.e., VideoMAEv2-giant with 1.01\n6\nTable 4. Compared to full fine-tuning, our adapter tuning can\nachieve better performance with less memory. Param. is the\nnumber of tunable parameters in the backbone. ∗ means out of\nmemory on A100-80GB, and we report the estimated number.\nModel Setting E2E Param. Mem. mAP\nVideoMAE-S\nFeature ✗ 0 - 57.6\nSnippet Full FT ✓ 22M 24.6G 68.4\nFrame Full FT ✓ 22M 2.8G 67.1\nAdaTAD ✓ 1M 2.5G 68.8\nVideoMAE-B\nFeature ✗ 0 - 64.7\nSnippet Full FT ✓ 86M 87.4G∗ -\nFrame Full FT ✓ 86M 5.6G 70.1\nAdaTAD ✓ 4M 4.9G 71.5\nVideoMAE-L\nFeature ✗ 0 - 66.5\nSnippet Full FT ✓ 304M 193G∗ -\nFrame Full FT ✓ 304M 13.1G 73.0\nAdaTAD ✓ 14M 11.0G 73.5\nbillion parameters, and larger input data, i.e., 1536 frames\nwith 2242 resolution, we attain an impressive 41.9% mAP\non ActivityNet-1.3 and 76.9% mAP on THUMOS14. It is\nnoteworthy that the previous SoTA was achieved by Video-\nMAEv2 [62] and InternVideo [63], which utilize the same\ndetector head as ours but with offline snippet features. Our\nmethod surpasses these in detection performance by a large\nmargin, marking the first instance where an end-to-end TAD\nmethod can outperform SoTA feature-based results.\nWe also present our results on EPIC-Kitchens 100 in\nTable 3. Since videos in this dataset have a longer dura-\ntion, all previous methods rely solely on pre-extracted fea-\ntures [51, 58, 72]. Our approach is the first to adopt end-\nto-end training on this dataset. For fair comparison, we\nfirst utilize the same backbone as used in previous meth-\nods, i.e., SlowFast-R50 pretrained on EPIC, and we achieve\ncomparable performance to ActionFormer [72]. Moreover,\nwhen we scale up the backbone to VideoMAE-L (it is\nalso trained on EPIC-Kitchens 100 classification task), we\nachieve SoTA performance of 29.3%.\n4.4. Ablation and Analysis\nIn this section, we present a series of analyses to evaluate\nour proposed method and affirm the benefits of scaling up\nin TAD. Unless otherwise stated, our experiments utilize a\nstandard input of 768 frames per video on THUMOS14.\nThe advantage of adapter tuning. In Table 4, we com-\npare conventional full fine-tuning with our proposed design.\nIt is evident that end-to-end approaches significantly out-\nperform pre-extracted features. Moreover, with full fine-\ntuning, the snippet representation slightly advances over\nframe representation but incurs tremendous memory costs,\nwhich aligns with our analysis in Section 3.2. However,\nAdaTAD uses less memory and still achieves better perfor-\nmance than conventional full fine-tuning. This also veri-\nTable 5. When scaling up the input data, AdaTAD’s perfor-\nmance consistently increases. ∗ means snippet representation is\nused in offline feature extraction, and each snippet has 16 frames.\nSetting Model Resolution Frames Mem. mAP\nFeature VideoMAEv2-g 224 2 768x16∗ - 69.6\nAdaTAD VideoMAE-S\n1602 768 2.5G 68.8\n1602 1536 3.8G 69.7\n1602 3072 6.5G 70.6\n2242 768 3.8G 70.7\n2242 1536 6.4G 71.0\n2242 3072 11.6G 71.5\nTable 6. AdaTAD† can further push the boundaries of scaling\nup. OOM means out of memory on A100-80GB.\nSetting Model Resolution Frame Mem. mAP\nAdaTAD\nVideoMAE-L 160 2 768 11.0G 73.5\nVideoMAEv2-g 160 2 768 29.9G 74.8\nVideoMAEv2-g 224 2 1536 OOM -\nAdaTAD†\nVideoMAEv2-g 160 2 768 22.8G 73.7\nVideoMAEv2-g 224 2 768 30.0G 74.6\nVideoMAEv2-g 224 2 1536 43.6G 75.4\nfies the limitations of full fine-tuning, as discussed in Sec-\ntion 3.3. Specifically, our method enhances VideoMAE-S\nbackbone with an 11.2% gain using only 1M trainable pa-\nrameters. Additionally, Table 4 also demonstrates that scal-\ning up the model size of the video backbone is an effective\nway to improve TAD performance.\nThe advantage of scaling up the data. In addition to\nmodel scaling, Table 5 verifies the effectiveness of data scal-\ning, which involves two aspects: frame number and frame\nresolution. Firstly, given the same video duration, increas-\ning the frame number from 768 to 3072 can raise the mAP\nfrom 68.8% to 70.6%. In the meantime, the memory us-\nage is nearly three times larger. Secondly, increasing the\nframe resolution from 1602 to 2242 also improves the mAP.\nIn the end, by only scaling up the data, we elevate the mAP\nfrom 68.8% to 71.5%, already surpassing the current SoTA\nfeature-based approach with a giant backbone model [63].\nMoreover, increasing the frame resolution from 160 2 to\n2242 or increasing the frame number from 768 to 1536\nresults in the same memory usage of 3.8G. However, the\nformer achieves 70.7% mAP while the latter only reaches\n69.7%. This suggests that frame resolution may be priori-\ntized under the same memory budget, for the TAD task.\nThe advantage of AdaTAD †. Given the effectiveness of\nscaling up the model or data, we further explore combin-\ning these approaches. In Table 6, using 768 frames while\nscaling up the model to VideoMAEv2-giant results in mem-\nory usage escalating to 29.9G. In such a scenario, further\nincreasing the data could easily lead to memory overflow,\neven with the A100-80GB GPU. This indicates that adapta-\n7\ntion tuning reached its limit under this extreme case. There-\nfore, to utilize both the largest models with larger data si-\nmultaneously, AdaTAD† shows its advantage.\nConcretely, when switching from AdaTAD to AdaTAD†,\nthe memory usage of the VideoMAEv2-giant model is re-\nduced from 29.9G to 22.8G. Although a slight performance\ndrop is observed, its reduced memory footprint enables scal-\ning up data from 768 frames to as much as 1536 frames with\na high resolution of 2242. This scalability helps mitigate the\nperformance drop and achieves a higher mAP of 75.4%.\nThe ablation of the adapter design. Detailed in Table 7,\nwe compare different adapter architectural designs. The\nbaseline, i.e., offline snippet feature, achieves 64.7% mAP.\nEnd-to-end learning in all designs yields at least a 5% im-\nprovement. Our AdaTAD achieves 71.5% in the end. Com-\npared to standard adapter [25], ours consumes similar mem-\nory but achieves higher mAP. This verifies that local tempo-\nral context is vital for the TAD task. In contrast to full fine-\ntuning (FT), we tune only 4M parameters using less mem-\nory. In our design, we find that removing the residual con-\nnection of the depth-wise convolution drops performance by\n0.7%, and training becomes unstable. We also implement\nan adaptation design proposed in LongLoRA [11], which\nefficiently computes long-range attention and shows decent\nperformance but requires more parameters and memory.\nTable 7. Ablation of different adapter architectural designs.\nVideoMAE-B is used to conduct the following experiments.\nSetting E2E Param. Mem. mAP gains\nSnippet Feature ✗ 0 - 64.7\n+ Full FT ✓ 86M 5.6G 70.1 + 5.1\n+ LongLoRA [11] ✓ 28M 6.2G 71.1 + 6.1\n+ Standard Adapter [25] ✓ 3.6M 4.8G 70.2 + 5.2\n+ AdaTAD (w/o residual ) ✓ 4.0M 4.9G 70.8 + 5.8\n+ AdaTAD ✓ 4.0M 4.9G 71.5 + 6.5\nThe necessity of end-to-end training for TAD. As previ-\nously discussed, end-to-end training can address discrep-\nancies between the pretraining and fine-tuning stages in\nterms of training data and learning tasks. To corroborate\nthis, we employ models pretrained on different datasets\nfor the EPIC-Kitchens TAD task in Table 8. Kinetics-400\n(K400) [29] represents commonly collected third-person\nweb data and exhibits a large domain gap compared to\nEPIC-Kitchens 100. Using K400 for pretraining, we ob-\nserve that end-to-end TAD training allows for +5.56 gain.\nConversely, using a model already finetuned on EPIC-\nKitchens still yields a +2.32 improvement. Unlike K400\npretraining, since this model has already adapted to the data\ndiscrepancy, we can infer that this gain leverages differ-\nences between the classification task in pretraining and the\ndetection task in fine-tuning. Such improvements further\nunderscore the significance of end-to-end training in TAD.\nTable 8. End-to-end TAD can alleviate the discrepancies be-\ntween pretraining and finetuning. VideoMAE-L with different\npretrained weights are used on the EPIC-Kitchens 100 Noun task.\nPretrain Dataset E2E 0.1 0.3 0.5 mAP gain\nK400 [29] ✗ 18.69 16.35 11.52 15.77\nK400 [29] ✓ 24.33 22.14 16.87 21.33 + 5.56\nK400 [29] + EPIC [15] ✗ 31.32 27.25 21.33 26.98\nK400 [29] + EPIC [15] ✓ 32.41 30.13 24.59 29.30 + 2.32\n4.5. Error Analyses\nWe also conduct false positive analysis at tIoU=0.5 in Fig. 4.\nCompared to feature-based training, learning from raw\nframes produces more helpful positive detections. More im-\nportantly, the percentage of wrong label error is reduced af-\nter end-to-end training, suggesting its unique advantage in\nclassifying accurate action labels.\n1G\n2G\n3G\n4G\n5G\n6G\n7G\n8G\n9G\n10G\nTop Predictions\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Error Breakdown (%)\nFalse Positive Proﬁle\nBackground Err\nConfusion Err\nLocalization Err\nWrong Label Err\nDouble Detection Err\nTrue Positive\nError Type0\n1\n2\n3\n4\n5\n6\nAverage-mAPN\nImprovment(%)\n3.7\n0.3\n5.4\n0.30.6\nRemoving Error Impact\n1G\n2G\n3G\n4G\n5G\n6G\n7G\n8G\n9G\n10G\nTop Predictions\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Error Breakdown (%)\nFalse Positive Proﬁle\nBackground Err\nConfusion Err\nLocalization Err\nWrong Label Err\nDouble Detection Err\nTrue Positive\nError Type0\n1\n2\n3\n4\n5\n6\nAverage-mAPN\nImprovment(%)\n3.7\n0.3\n5.4\n0.30.6\nRemoving Error Impact\n1G\n2G\n3G\n4G\n5G\n6G\n7G\n8G\n9G\n10G\nTop Predictions\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Error Breakdown (%)\nFalse Positive Proﬁle\nBackground Err\nConfusion Err\nLocalization Err\nWrong Label Err\nDouble Detection Err\nTrue Positive\nError Type0\n1\n2\n3\n4\n5\n6\n7\nAverage-mAPN\nImprovment(%)\n4.3\n0.3\n5.5\n0.50.4\nRemoving Error Impact\n(a)End-to-endtraining(b)Feature-basedtraining\nFigure 4. False Positive Profiling on THUMOS14 using [2]. We\nuse VideoMAEv2-giant as the backbone, and compare end-to-end\ntraining with pre-extracted feature-based training.\n5. Conclusions\nThis work introduces a memory-efficient and parameter-\nefficient end-to-end method named AdaTAD. Our key in-\nnovation lies in the proposed temporal-informative adapter,\nwhich is tailored for TAD with low computation costs. Fur-\nthermore, we design an alternative placement for adapters to\nminimize memory usage. By demonstrating the feasibility\nand effectiveness of scaling up end-to-end TAD, our work\nachieves new SoTA performance across multiple datasets.\nParticularly, this is the first instance of an end-to-end TAD\nmethod that surpasses the current best feature-based mod-\nels. In fact, AdaTAD achieves a groundbreaking 75.4%\nmAP on THUMOS14. We believe our work underscores\nthe possible paradigm shift in TAD, advocating a move\naway from the traditional methodology of separate feature\nextraction and offline detection towards a more integrated\napproach of scaling up end-to-end TAD training.\nAcknowledgement. This work was supported by the King\nAbdullah University of Science and Technology (KAUST)\nOffice of Sponsored Research through the Visual Comput-\ning Center (VCC) funding, as well as the SDAIA-KAUST\nCenter of Excellence in Data Science and Artificial Intelli-\ngence (SDAIA-KAUST AI).\n8\n6. Appendix\nIn this appendix, we provide more details of our method\nand present more experiment results. Specifically, we give\na detailed illustration of AdaTAD † in Section A. Then,\nwe present the implementation details between different\ndatasets in Section B. Next, we provide our results on the\nEgo4D dataset in Section C. After this, we show additional\nexperiments and further analysis in Section D. Then, the\nerror analysis is conducted in Section E, and qualitative vi-\nsualization is demonstrated in Section F. Finally, we discuss\nthe limitations of our work in Section G.\nA. Further illustration of AdaTAD†\nTo reduce the memory usage and further scale up the\nmodel and data, AdaTAD † proposes an alternative place-\nment for the temporal-informative adapters (TIAs). The en-\ntire pipeline of AdaTAD† is shown in Fig. 5.\nConcretely speaking, we retain the TIA architecture, but\neliminate the last residual connection in AdaTAD, as illus-\ntrated in Equation 3. Therefore, given layer i ’s output xi,\nthe corresponding TIA’s output x′\ni will be 0 at the start of\nthe training, since the weights and biases of Wup are ini-\ntialized to 0.\nx′\ni = σ(W⊤\ndown · xi),\nx′\ni = W⊤\nmid · DWConvk(x′\ni) + x′\ni,\nx′\ni = α · W⊤\nup · x′\ni.\n(3)\nWhat’s more, in the above equation,x′\ni is not added into\nxi, which is different from AdaTAD. This means that the\nadapter’s outputs do not contribute to the middle activations\nof the original backbone. Instead, it serves as aresidual and\nis directly added to the backbone’s final output xN, where\nN is the total number of layers in the backbone. Conse-\nquently, the output of the video encoder becomes y, as de-\npicted in Equation 4. At the beginning of the fine-tuning,\ny is initialized as xN, and is gradually augmented with the\naforementioned residuals to adapt the fine-tuning, which is\ndriven by the updated TIAs.\ny = xN +\nNX\ni=1\nx′\ni. (4)\nIn our design, the key to memory reduction lies in\nstopping the gradient backpropagation for the original\nbackbone. Since the TIA’s output directly goes to the final\noutput, gradients during backpropagation do not trace back\nto the original backbone but are confined to the shallow and\nlightweight adapters. This architecture can be viewed as the\nSide Network [55] or Ladder Network [71], meaning that it\ncreates a light network apart from the original heavy back-\nbone. Compared to the traditional PEFT approach or our\n+DetectorHead\nBasicLayers i TemporalInformativeAdapter\nFrozenLearnable\nBasicLayers N\nTemporalInformativeAdapter···\nBasicLayers i+1\nBasicLayers N-1\n TemporalInformativeAdapter\nTemporalInformativeAdapter\nOriginal Backbone\n𝒙𝒊\n𝒙𝒊\"𝟏\n𝒙𝑵%𝟏\n𝒙𝑵\n𝒚\n𝒙𝒊&\n𝒙𝒊\"𝟏&\n𝒙𝑵%𝟏&\n𝒙𝑵&\nFigure 5. Detailed architecture of AdaTAD†. The output of each\nadapter will be added to the final output of the original backbone.\nAdaTAD, this design requires only a few activations from\nthe backbone, and it can further refine these activations to\nadapt the downstream task during transfer learning.\nB. Implementation Details\nTraining Details. Unless otherwise specified, we prefer\nAdaTAD for its high performance. By default, mixed pre-\ncision training [41] and activation checkpointing [10] are\nadopted following previous work [12]. We also utilize flash\nattention [16] to accelerate the computation and save the\nmemory in the VideoMAE-family models. Our method is\nevaluated on four datasets, with the hyper-parameters de-\ntailed in Table 9.\nData Preprocessing. Regarding the data preprocessing, we\nprovide the following instructions. For ActivityNet, due\nto the variable action duration, we resize videos into fixed\nlengths with 768 frames. After the video encoder, average\npooling is adopted along the spatial dimension, and the fea-\nture’s temporal length is resized to 192. For THUMOS14\nand EPIC-Kitchens, given their longer video lengths, we ap-\nply the random truncation to a fixed-length window during\ntraining and use sliding window during testing. For THU-\nMOS14, the window length is 768 frames with a stride of\n4. For EPIC-Kitchens, it’s 768×8 frames with a stride of 2,\nand additionally, the feature after the video encoder is tem-\nporally resized to 768. On the Ego4D-MQ dataset, since all\nthe videos are under 8 minutes, we sample 900 ×8 frames\nwith a stride of 2, and resize the feature to length 900.\n9\nTable 9. End-to-End setting in different TAD datasets.\nConfig ActivityNet-1.3 [23] THUMOS14 [28] EPIC-Kitchens 100 [15] Ego4D-MQ [21]\nBackbone Setting\nVideo Preprocessing Resize Sliding Window Sliding Window Padding\nFrame Stride - 4 2 2\nFrame Number 192 ×4 768 768 ×8 900 ×8\nFrame Resolution 160 ×160\nData Augmentation RandomResizedCrop + Flip + ImgAug + ColorJitter\nFeature Post Processing Spatial Average Pooling + Resize\nFeature Resize Length 192 768 768 900\nDetector Setting\nWarmup Epoch 5 5 5 5\nTotal Epoch 15 60 35 15\nBatch Size 16 2 2 2\nVideo Model. All the action recognition models used in\nour paper, such as SlowFast [20], VideoSwin [39], and\nVideoMAE [59], are pretrained on Kinetics-400 [29], ex-\ncept the VideoMAEv2-giant [62] that is hybrid pretrained\nand fine-tuned on Kinetics-710 [6]. Meanwhile, for the\nViT-based model, to avoid excessive temporal attention\nfor the untrimmed video, we chunk the video into shorter\nclips before processing it in the ViT block. For exam-\nple, given the input video with 768 frames, we reshape\nthe 768 frames as 16 ×48, dividing it into 48 snippets with\n16 frames each. Then VideoMAE processes these snippets\nindependently, but before sending them into our proposed\nadapter, we reassemble the snippets into the original 768\nframes to achieve cross-snippet information exchange. This\napproach, focusing temporal attention only on 16 frames,\nis specific to ViT-based models. CNN-based models like\nSlowFast and local attention-based models like VideoSwin\ndo not require this reshaping.\nMemory Usage on EPIC-Kitchens and Ego4D-MQ. On\nboth datasets, we use the VideoMAE-Large as the back-\nbone, but differently pretrained on respective datasets. On\nEPIC-Kitchens, the memory usage is 48.5GB per video for\n768×8 = 6144frames. On Ego4D-MQ, the memory usage\nis 60.0GB per video for 900×8=7200 frames.\nC. Results on Ego4D-Moment Queries\nWe present our results on the Ego4D-MQ dataset in Ta-\nble 10. The backbone is VideoMAE-Large, which is fine-\ntuned by InternVideo on Ego4D classification task [7].\nNote that previous methods like VSGN [73] and Action-\nFormer [72] use the same backbone, yet they extract high-\nresolution, densely sampled offline features. Our work first\nestablishes a stronger baseline by adopting an improved\nversion of ActionFormer on this dataset [54], achieving\n27.07% mAP. Furthermore, utilizing end-to-end training\nwith our proposed temporal-informative adapter elevates\nthe performance to 28.08% mAP.\nMore importantly, when we apply the full fine-\ntuning on Ego4d-MQ, no performance gain was ob-\nserved (27.01% mAP). This implies that the pretrained\nmodel is sufficiently robust, and the downstream data is\ntoo limited for effective transfer learning. Nonetheless,\nour AdaTAD manages to yield a performance increase of\n+1.01%. This outcome further demonstrates the efficacy of\nour adapter-based transfer learning.\nD. Additional Experiments\nIn this section, we provide additional experiments to study\nthe effectiveness of our proposed method. These experi-\nments are omitted from the main paper due to lack of space.\nD.1. More Results of AdaTAD†\nIn our paper, we propose two distinct adapter placement\ndesigns: AdaTAD and AdaTAD †. The former directly in-\nserts the adapters into the original backbone, while the lat-\nter positions the adapter outside the backbone. This exter-\nnal placement in AdaTAD† stops gradient backpropagation\nto the original backbone, negating the need to save mas-\nsive intermediate activations and thus reducing memory us-\nage. However, previous research [55] suggests that such\na design might comprise the performance, as the lighter\nadapters may limit the representation capacity during trans-\nfer learning. Therefore, to verify this hypothesis and ex-\nplore the advantages of AdaTAD†, we conduct experiments\nsummarized in Table 11, leading to two key conclusions:\n1. With identical input data, AdaTAD † underperforms\nto AdaTAD.For example, when using VideoMAE-Base\nwith 768 frames and a resolution of 160 2, the perfor-\nmance of AdaTAD† drops from 71.5% to 70.2%. This\nverifies that the transfer learning ability of AdaTAD is\nbetter than AdaTAD†.\n10\nTable 10. Results on the validation set of Ego4D-Moment Queries v2.0. We report mAP at different tIoU thresholds. InternVideo [7]\ndenotes the backbone is VideoMAE-L [59], which is pretrained and fine-tuned on Ego4D-Moment Queries.\nMethod Feature E2E 0.1 0.2 0.3 0.4 0.5 Avg.\nVSGN [73] EgoVLP ✗ 16.63 - 11.45 - 6.57 11.39\nVSGN [73] InternVideo ✗ - - - - - 19.35\nActionFormer [72] EgoVLP ✗ 26.84 - 20.57 - 14.54 20.60\nActionFormer [72] InternVideo ✗ - - - - - 23.29\nASL [49] EgoVLP ✗ 29.45 - 23.03 - 16.08 22.83\nAdaTAD InternVideo ✗ 32.40 29.50 26.98 24.43 21.88 27.07\nAdaTAD InternVideo ✓ 33.53 30.71 28.04 25.51 22.59 28.08\nTable 11. The advantage of AdaTAD † lies in scaling up the\ndata with low memory usage. When using the same input, the\nperformance of AdaTAD† is inferior to AdaTAD. However, under\na similar memory budget, AdaTAD † can achieve comparable or\nbetter performance thanks to data scaling. We report the memory\nusage and average mAP on the THUMOS14 dataset.\nModel Method Input Mem. mAP\nVideoMAE-S\nAdaTAD 768, 160 2 2.5G 68.8\nAdaTAD† 768, 1602 1.8G 68.0\nAdaTAD† 768, 2242 2.7G 68.9\nVideoMAE-B\nAdaTAD 768, 160 2 4.9G 71.5\nAdaTAD† 768, 1602 4.0G 70.2\nAdaTAD† 768, 2242 4.9G 71.9\nVideoMAE-L\nAdaTAD 768, 160 2 11.0G 73.5\nAdaTAD† 768, 1602 8.1G 73.1\nAdaTAD† 768, 2242 10.8G 73.7\n2. Under a similar memory budget, AdaTAD † can\nachieve comparable performance than AdaTAD by\nscaling up the input data. Owing to its lower memory\nusage, AdaTAD† allows for larger input data, thereby\nimproving performance. For instance, AdaTAD † with\ninputs of 768, 2242 can marginally outperform AdaTAD\nwith inputs of 768, 1602 under the same memory budget.\nThese experiments underscore the significance of data\nscaling. Particularly with larger models like VideoMAEv2-\ngiant, AdaTAD has reached a limit of scaling up the input\ndata. In such scenarios, only AdaTAD † can manage a sim-\nilar memory budget while enhancing input data for supe-\nrior performance. In conclusion, AdaTAD † is tailored for\nhigher detection performance in situations where the back-\nbone model is very large and cannot accommodate more\nframes or higher image resolution. In scenarios where\nthese are viable, AdaTAD remains the recommended\nchoice for optimal performance. Therefore, except in the\ncase of giant models, we default to using AdaTAD.\nTable 12. Compared to full fine-tuning, our adapter tuning\ncan achieve better performance with less memory. Param. is\nthe number of tunable parameters in the backbone. ∗ means out of\nmemory on A100-80GB, and we report the estimated number. We\nconduct the following experiments on THUMOS14 dataset.\nModel Setting E2E Param. Mem. mAP\nVideoSwin-B\nFeature ✗ 0 - 55.1\nSnippet Full FT ✓ 87.6M 213G∗ -\nFrame Full FT ✓ 87.6M 16.4G 60.4\nAdaTAD ✓ 3.9M 16.1G 63.7\nSlowFast-R50\nFeature ✗ 0 - 62.3\nSnippet Full FT ✓ 33.6M 36.9G 66.1\nFrame Full FT ✓ 33.6M 3.9G 64.3\nAdaTAD ✓ 11.4M 4.3G 66.0\nD.2. More Results with Swin and SlowFast\nTo validate the efficacy of our adapter tuning approach, we\nexpand our study to include a broader range of backbone\nmodels. This extended analysis, detailed in Table 12, en-\ncompasses not only the window-based transformer model,\ni.e., VideoSwin [39], but also the 3D CNN model, i.e.,\nSlowFast [20]. The findings are in line with those re-\nported in the main paper, indicating that adapter tuning can\nyield better detection performance compared to full fine-\ntuning. Notably, the performance gains are even more pro-\nnounced with VideoSwin and SlowFast than with Video-\nMAE. Specifically, our method enhanced detection perfor-\nmance from 55.1% to 63.7% with VideoSwin-B, and from\n62.3% to 66.1% with SlowFast-R50.\nD.3. Study of Different Kernel Size in TIA\nIn our temporal-informative adapter (TIA), we employ\ndepth-wise convolution along the temporal dimension to\ncapture context from adjacent frames. This design utilizes a\n3D depth-wise convolution layer with kernel size (t, h, w).\nBy default, the kernel is set to (3, 1, 1). To assess the in-\nfluence of various kernel sizes in TIA, we conduct a study\nsummarized in Table 13.\n11\nFirst, we expand the kernel size to (3, 3, 3), and note a\ndecrease in performance. This suggests that the spatial con-\ntext has been effectively handled by the original backbone,\nand additional spatial processing could potentially disrupt\nthe pretrained knowledge. Subsequently, reducing the ker-\nnel size to(1, 1, 1) results in inferior performance compared\nto ours, likely due to insufficient temporal information ag-\ngregation. Moreover, we gradually increase the temporal\nkernel size from 3 to 7, 13, and 21, observing a consistent\ndownward trend in performance. This indicates that aggre-\ngating a longer-range temporal context does not necessarily\nbenefit the backbone. Overall, our default TIA configura-\ntion demonstrates the best performance.\nTable 13. Ablation of different kernel size in depth-wise con-\nvlution in AdaTAD. The order of the kernel size follows t, h, w.\nVideoMAE-B is used as the backbone on THUMOS dataset.\nDW Kernel 0.3 0.5 0.7 mAP\n(3,1,1) 87.04 75.33 49.22 71.56\n(1,1,1) 85.97 74.61 49.12 71.06\n(3,3,3) 85.46 73.74 49.13 70.63\n(7,1,1) 86.17 74.62 48.93 71.02\n(13,1,1) 85.68 72.80 47.98 69.96\n(21,1,1) 84.75 72.53 46.74 69.03\nD.4. Ablation of Adapter Design on ActivityNet-1.3\nIn the main paper, we present a comparison of various\nadapter architectural designs on THUMOS14. Extend-\ning our analysis, we conduct a similar ablation study on\nActivityNet-1.3, with results detailed in Table 14. The find-\nings from this study align with the conclusions drawn in the\nmain paper. Notably, since the size of ActivityNet is much\nlarger than THUMOS14, the competition in performance\nmetrics is more intense, resulting in smaller gains. De-\nspite this, our AdaTAD still stands out among other adapter\narchitectural designs. Compared to using frozen features,\nAdaTAD enhances performance from 36.64% to 38.39%,\naffirming its efficacy in a more challenging dataset.\nMoreover, we also combine our proposed TIA modules\nwith full fine-tuning strategy, but observe a decreased mAP.\nThis is due to the learning rate conflicts between the pre-\ntrained backbone and the newly added adapter. The former\nprefers a smaller learning rate since the model is pretrained\non large datasets, while the latter prefers a larger learning\nrate since it is newly added and randomly initialized.\nD.5. AdaTAD with Different Detector Head\nOur end-to-end framework with the introduced adapter is\neffective not only on ActionFormer [72], but also with other\nTAD heads. As shown in Table 15, we successfully em-\nTable 14. Ablation of different adapter architectural designs\non ActivityNet-1.3. VideoMAE-B is used as the backbone.\nSetting E2E Param. Mem. mAP gains\nSnippet Feature ✗ - - 36.64\n+ LongLoRA [11] ✓ 28M 6.2G 37.69 +1.05\n+ Full FT ✓ 86M 5.6G 38.13 +1.49\n+ Plain Adapter [25] ✓ 3.6M 4.8G 38.21 +1.57\n+ AdaTAD (w.o. residual ) ✓ 4.0M 4.9G 38.23 +1.59\n+ AdaTAD ✓ 4.0M 4.9G 38.39 +1.75\n+ Full FT + TIA ✓ 90M 5.8G 37.89 +1.25\nploy the proposed method on GTAD [66] and TriDet [51].\nWhen using VideoMAE-Large as the backbone, we ob-\nserve significant improvements. For instance, compared\nto using densely extracted snippet features (16 frames per\nsnippet with a resolution of 224 2), our approach elevates\nGTAD’s performance from 50.8% to 55.5%, and TriDet’s\nperformance from 68.8% to 74.1%. Additionally, compared\nto ActionFormer, TriDet achieves better detection perfor-\nmance with offline features and end-to-end training, thanks\nto the proposed SGP layer and Trident-Head. Overall, our\nproposed end-to-end training paradigm is agnostic to differ-\nent action detectors, and it can boost the detector’s perfor-\nmance by a large margin.\nTable 15. Ablation of different detector heads. VideoMAE-L is\nused as the backbone on THUMOS dataset.\nDetector Head Setting 0.3 0.5 0.7 mAP\nGTAD [66] Feature 65.8 53.6 31.3 50.8\nAdaTAD 69.5 57.6 37.4 55.5\nActionFormer [72] Feature 82.9 70.8 42.7 66.5\nAdaTAD 87.7 76.7 52.4 73.5\nTriDet [51] Feature 84.0 73.4 45.1 68.8\nAdaTAD 88.7 78.1 52.2 74.1\nE. Error Analysis\nApart from the false positive profiling provided in the main\npaper, we also present the sensitive analysis and false nega-\ntive (FP) profiling in Fig. 6. Note that the first row utilizes\noffline snippet features, and the second row utilizes the end-\nto-end training by AdaTAD†. For the error analysis process\nand metrics, we refer the readers to [2] for more details.\nFrom Fig. 6(a), we can find that the performance across\nall metrics is improved by end-to-end training. Especially,\nthe mAP of long actions (XL) is visibly increased. This\nphenomenon is more evident in false negative profiling. In\nFig. 6(b), end-to-end training significantly reduces the FP\n12\nXS S M L XL XS S M L XL XS S M L0\n20\n40\n60\n80\n100Average-mAPN (%) 76.780.377.579.7\n66.5\nCoverage\n69.3\n81.685.5\n72.1\n53.6\nLength\n69.0\n77.383.3\n91.2# Instances\n78.16\nCoverage\nLength\n# Instances\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\nAverage-mAPN\nRelative Change\n0.9\n1.0\n0.7\n1.1\n0.9\n1.2\n78.16\nXS S M L XL XS S M L XL XS S M L\n0\n20\n40\n60\n80\n100False Negative (%)\n2.8 3.5 6.6 4.9 10.0\nCoverage\n3.9 1.9 2.9 2.6\n24.3\nLength\n6.7 4.4 1.2 1.5\n# Instances\nXS S M L XL XS S M L XL XS S M L0\n20\n40\n60\n80\n100Average-mAPN (%) 80.386.381.780.278.3\nCoverage\n73.8\n85.487.1\n71.967.8\nLength\n76.881.785.586.8\n# Instances\n82.06\nCoverage\nLength\n# Instances\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\nAverage-mAPN\nRelative Change\n1.0\n1.1\n0.8\n1.1\n0.9\n1.1\n82.06\n(a) Sensitive Analysis. Left: normalized mAP at tIoU=0.5 under different\nvideo contents. Right: The relative normalized mAP change at tIoU=0.5 with\nrespect to different characteristics of the ground truth instances.\nXS S M L XL XS S M L XL XS S M L\n0\n20\n40\n60\n80\n100False Negative (%)\n2.3 2.9 6.6 4.9 4.4\nCoverage\n3.3 1.2 2.9 3.5 10.8\nLength\n0.0 3.8 0.8 0.7\n# Instances\n(b) False Negative Profiling. The false negative rates are broken\ndown into fine-grained metrics under different coverage, length, and\nthe number of instances.\nFigure 6. We adopt the VideoMAEv2-giant as the backbone and report more error analysis of our method on THUMOS14 using [2]. The\nfirst row denotes that we use snippet features to achieve an average mAP of 69.6%.The second rowdenotes that we use end-to-end training\nby AdaTAD† and achieve an average mAP of 75.4%.\nrate from 24.3% to 10.8%, leading to better detection ac-\ncuracy. Moreover, even for small actions, our method also\nalleviates false negative detection. For instance, it amaz-\ningly reduces the FP rate from 6.7% to 0% under the XS\n#instances.\nF. Visualization\nFurther, we present the qualitative visualization of our pre-\ndiction on THUMOS14 dataset. In Fig. 7, we plot the\nground truth actions of each video (drawn in red and above\nthe black line), and also the top-20 predicted proposals\n(drawn in colors and under the black line). The color of\nthe proposal represents the maximum IoU of this proposal\nto the ground truth actions. Therefore, a proposal with a\ndeeper color means it overlaps more with the ground truth,\nindicating this is a high-quality proposal. From the figure,\nwe can observe that our method can yield accurate candi-\ndate actions and also provide reasonable proposal ranking.\nG. Limitations and Future Work\nOne limitation of our method is how to further scale up\nthe input data, since some datasets require extremely long\nvideo input. For instance, on Ego4D-MQ dataset, we uti-\nlize the VideoMAE-Large with 7,200 frames, costing 60GB\nper video. Although this is already an amazing data size\nfor end-to-end training, however, it’s meaningful to further\nscale up the frame resolution or model size to achieve bet-\nter performance with reduced memory usage. On the other\nhand, loading, decoding, and processing such long videos\ntake much longer time than pre-extracted features, which\nmay cause difficulty in network training.\nInteresting future directions include end-to-end training\nwith multi-modality tasks, e.g., end-to-end video grounding\nand end-to-end moment retrieval, pretraining for action lo-\ncalization, and open vocabulary end-to-end temporal action\ndetection.\nReferences\n[1] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiao-\nhua Zhai. Revisiting neural scaling laws in language and\nvision. In NeurIPS, 2022. 3\n[2] Humam Alwassel, Fabian Caba Heilbron, Victor Escorcia,\nand Bernard Ghanem. Diagnosing error in temporal action\ndetectors. In ECCV, 2018. 8, 12, 13\n[3] Humam Alwassel, Fabian Caba Heilbron, and Bernard\nGhanem. Action search: Spotting actions in videos and its\napplication to temporal action localization. In ECCV, 2018.\n1\n[4] Yueran Bai, Yingying Wang, Yunhai Tong, Yang Yang,\nQiyue Liu, and Junhui Liu. Boundary content graph neural\nnetwork for temporal action proposal generation. In ECCV,\n2020. 2\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. In NeurIPS, 2020. 2\n[6] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zis-\nserman. A short note on the kinetics-700 human action\ndataset. arXiv preprint arXiv:1907.06987, 2019. 10\n[7] Guo Chen, Sen Xing, Zhe Chen, Yi Wang, Kunchang Li,\nYizhuo Li, Yi Liu, Jiahao Wang, Yin-Dong Zheng, Bingkun\nHuang, et al. Internvideo-ego4d: A pack of champion solu-\n13\n0 20 40 60 80\ngt-0\ngt-1\ngt-2\ngt-3\n----------\npred-00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nvideo_test_0000073\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 20 40 60 80 100 120\ngt-0\ngt-1\ngt-2\ngt-3\n----------\npred-00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nvideo_test_0000319\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 20 40 60 80 100 120 140\ngt-0\ngt-1\ngt-2\ngt-3\ngt-4\ngt-5\ngt-6\n----------\npred-00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nvideo_test_0000444\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 5 10 15 20 25\ngt-0\ngt-1\ngt-2\n----------\npred-00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nvideo_test_0000846\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 10 20 30 40 50 60\ngt-0gt-1gt-2gt-3gt-4gt-5gt-6gt-7----------pred-0001020304050607080910111213141516171819\nvideo_test_0001276\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 10 20 30 40 50 60 70 80\ngt-0\ngt-1\ngt-2\ngt-3\ngt-4\n----------\npred-00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nvideo_test_0001309\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 7. Qualitative results of our method with VideoMAEv2-giant on THUMOS14. The color of the proposal represents the\nmaximum IoU of this proposal to ground truth actions. We plot the ground truth actions of each video (drawn in red and above the black\nline), and top-20 predicted proposals (drawn in colors and under the black line).\n14\ntions to ego4d challenges. arXiv preprint arXiv:2211.09529,\n2022. 10, 11\n[8] Hao Chen, Ran Tao, Han Zhang, Yidong Wang, Wei Ye,\nJindong Wang, Guosheng Hu, and Marios Savvides. Conv-\nadapter: Exploring parameter efficient transfer learning for\nconvnets. arXiv preprint arXiv:2208.07463, 2022. 3\n[9] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang,\nYibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapt-\ning vision transformers for scalable visual recognition. In\nNeurIPS, 2022. 3\n[10] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.\nTraining deep nets with sublinear memory cost. arXiv\npreprint arXiv:1604.06174, 2016. 4, 9\n[11] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhi-\njian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-\ntuning of long-context large language models.arXiv preprint\narXiv:2309.12307, 2023. 8, 12\n[12] Feng Cheng and Gedas Bertasius. Tallformer: Temporal ac-\ntion localization with long-memory transformer. In ECCV,\n2022. 1, 2, 4, 6, 9\n[13] Feng Cheng, Mingze Xu, Yuanjun Xiong, Hao Chen, Xinyu\nLi, Wei Li, and Wei Xia. Stochastic backpropagation: A\nmemory efficient strategy for training video models. In\nCVPR, 2023. 2\n[14] MMAction2 Contributors. Openmmlab’s next generation\nvideo understanding toolbox and benchmark. https://\ngithub.com/open-mmlab/mmaction2, 2020. 5\n[15] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\nSanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, et al.\nScaling egocentric vision: The epic-kitchens dataset. In\nECCV, 2018. 5, 8, 10\n[16] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christo-\npher R ´e. Flashattention: Fast and memory-efficient exact\nattention with io-awareness. In NeurIPS, 2022. 9\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021. 1,\n3\n[18] Victor Escorcia, Fabian Caba Heilbron, Juan Carlos Niebles,\nand Bernard Ghanem. DAPs: Deep action proposals for ac-\ntion understanding. In ECCV, 2016. 1\n[19] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.\nMultiscale vision transformers. In ICCV, 2021. 3\n[20] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. SlowFast networks for video recognition. In\nICCV, 2019. 3, 4, 10, 11\n[21] Kristen Grauman, Andrew Westbury, Eugene Byrne,\nZachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson\nHamburger, Hao Jiang, Miao Liu, and et al. Liu, Xingyu.\nEgo4d: Around the world in 3,000 hours of egocentric video.\nIn CVPR, 2022. 5, 10\n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In CVPR, 2022. 1\n[23] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,\nand Juan Carlos Niebles. ActivityNet: A large-scale video\nbenchmark for human activity understanding. In CVPR,\n2015. 1, 5, 10\n[24] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). arXiv preprint arXiv:1606.08415, 2016. 4\n[25] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly. Parameter-efficient transfer\nlearning for nlp. In ICML, 2019. 2, 3, 4, 8, 12\n[26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. In\nICML, 2021. 3\n[27] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,\nSerge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi-\nsual prompt tuning. In ECCV, 2022. 3\n[28] YG Jiang, J Liu, A Roshan Zamir, G Toderici, I Laptev, M\nShah, and R Sukthankar. Thumos challenge: Action recog-\nnition with a large number of classes, 2014. 1, 5, 10\n[29] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\nman action video dataset. arXiv preprint arXiv:1705.06950,\n2017. 8, 10\n[30] Brian Lester, Rami Al-Rfou, and Noah Constant. The power\nof scale for parameter-efficient prompt tuning. In EMNLP,\n2021. 3\n[31] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing\ncontinuous prompts for generation. In ACL, 2021. 3\n[32] Chuming Lin, Chengming Xu, Donghao Luo, Yabiao Wang,\nYing Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yan-\nwei Fu. Learning salient boundary feature for anchor-free\ntemporal action localization. In CVPR, 2021. 1, 2, 4, 6\n[33] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen.\nBMN: boundary-matching network for temporal action pro-\nposal generation. In ICCV, 2019. 1, 2, 6\n[34] Qinying Liu and Zilei Wang. Progressive boundary refine-\nment network for temporal action detection. In AAAI, 2020.\n2\n[35] Shuming Liu, Mengmeng Xu, Chen Zhao, Xu Zhao, and\nBernard Ghanem. Etad: Training action detection end to end\non a laptop. In CVPRW, 2023. 1, 2\n[36] Xiaolong Liu, Song Bai, and Xiang Bai. An empirical study\nof end-to-end temporal action detection. In CVPR, 2022. 1,\n2, 6\n[37] Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Shiwei\nZhang, Song Bai, and Xiang Bai. End-to-end temporal ac-\ntion detection with transformer.IEEE Transactions on Image\nProcessing, 31:5427–5441, 2022. 2, 6\n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 3\n[39] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 3202–3211, 2022. 10, 11\n15\n[40] Jinjie Mai, Abdullah Hamdi, Silvio Giancola, Chen Zhao,\nand Bernard Ghanem. Egoloc: Revisiting 3d object localiza-\ntion from egocentric videos with visual queries. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 45–57, 2023. 1\n[41] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory\nDiamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael\nHouston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed\nprecision training. In ICLR, 2017. 4, 9\n[42] Jonghwan Mun, Minsu Cho, and Bohyung Han. Local-\nglobal video-text interactions for temporal grounding. In\nCVPR, 2020. 1\n[43] OpenAI. Gpt-4 technical report, 2023. 1, 2\n[44] Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang,\nWei Wu, Xiang Wang, Yu Qiao, Junjie Yan, Changxin Gao,\nand Nong Sang. Temporal context aggregation network for\ntemporal action proposal refinement. In CVPR, 2021. 2\n[45] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. Improving language understanding by gen-\nerative pre-training. OpenAI, 2018. 2\n[46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 1(8):9, 2019. 2\n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 3\n[48] Merey Ramazanova, Victor Escorcia, Fabian Caba Heilbron,\nChen Zhao, and Bernard Ghanem. Owl (observe, watch, lis-\nten): Localizing actions in egocentric video via audiovisual\ntemporal context. Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition Workshop\n(CVPRW), 2023. 1\n[49] Jiayi Shao, Xiaohan Wang, Ruijie Quan, Junjun Zheng, Jiang\nYang, and Yi Yang. Action sensitivity learning for temporal\naction localization. In ICCV, 2023. 2, 6, 11\n[50] Dingfeng Shi, Yujie Zhong, Qiong Cao, Jing Zhang, Lin Ma,\nJia Li, and Dacheng Tao. React: Temporal action detection\nwith relational queries. In ECCV, 2022. 2\n[51] Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, and\nDacheng Tao. Tridet: Temporal action detection with relative\nboundary modeling. In CVPR, 2023. 2, 6, 7, 12\n[52] Mattia Soldan, Mengmeng Xu, Sisi Qu, Jesper Tegner, and\nBernard Ghanem. VLG-Net: Video-language graph match-\ning network for video grounding. In ICCVW, 2021. 1\n[53] Mattia Soldan, Alejandro Pardo, Juan Le ´on Alc ´azar,\nFabian Caba Heilbron, Chen Zhao, Silvio Giancola, and\nBernard Ghanem. Mad: A scalable dataset for language\ngrounding in videos from movie audio descriptions. Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 2022. 1\n[54] Lin Sui, Fangzhou Mu, and Yin Li. Nms threshold mat-\nters for ego4d moment queries–2nd place solution to the\nego4d moment queries challenge 2023. arXiv preprint\narXiv:2307.02025, 2023. 10\n[55] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Lad-\nder side-tuning for parameter and memory efficient transfer\nlearning. In NeurIPS, 2022. 3, 5, 9, 10\n[56] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter:\nParameter-efficient transfer learning for vision-and-language\ntasks. In CVPR, 2022. 2, 4\n[57] Jing Tan, Jiaqi Tang, Limin Wang, and Gangshan Wu. Re-\nlaxed transformer decoders for direct action proposal gener-\nation. In ICCV, 2021. 2\n[58] Tuan N Tang, Kwonyoung Kim, and Kwanghoon Sohn.\nTemporalmaxer: Maximize temporal context with only max\npooling for temporal action localization. arXiv preprint\narXiv:2303.09055, 2023. 7\n[59] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.\nVideomae: Masked autoencoders are data-efficient learners\nfor self-supervised video pre-training. In NeurIPS, 2022. 1,\n4, 6, 10, 11\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 4\n[61] Chenhao Wang, Hongxiang Cai, Yuxin Zou, and Yichao\nXiong. Rgb stream is enough for temporal action detection.\narXiv preprint arXiv:2107.04362, 2021. 2\n[62] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yi-\nnan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2:\nScaling video masked autoencoders with dual masking. In\nICCV, 2023. 1, 3, 6, 7, 10\n[63] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun\nHuang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun\nWang, et al. Internvideo: General video foundation models\nvia generative and discriminative learning. arXiv preprint\narXiv:2212.03191, 2022. 1, 6, 7\n[64] Zhenzhi Wang, Ziteng Gao, Limin Wang, Zhifeng Li, and\nGangshan Wu. Boundary-aware cascade networks for tem-\nporal action segmentation. In ECCV, 2020. 2\n[65] Kun Xia, Le Wang, Sanping Zhou, Nanning Zheng, and Wei\nTang. Learning to refactor action and co-occurrence features\nfor temporal action localization. In CVPR, 2022. 2\n[66] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and\nBernard Ghanem. G-TAD: Sub-graph localization for tem-\nporal action detection. In CVPR, 2020. 1, 6, 12\n[67] Le Yang, Houwen Peng, Dingwen Zhang, Jianlong Fu, and\nJunwei Han. Revisiting anchor mechanisms for temporal ac-\ntion localization. IEEE Transactions on Image Processing ,\n29:8535–8548, 2020. 2\n[68] Min Yang, Guo Chen, Yin-Dong Zheng, Tong Lu, and Limin\nWang. Basictad: an astounding rgb-only baseline for tem-\nporal action detection. Computer Vision and Image Under-\nstanding, 232:103692, 2023. 1, 2, 6\n[69] Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen\nChen, and Mu Li. Aim: Adapting image models for efficient\nvideo action recognition. In ICLR, 2023. 2, 3, 4\n[70] Ting Yao, Tao Mei, and Yong Rui. Highlight detection with\npairwise deep ranking for first-person video summarization.\nIn CVPR, 2016. 1\n[71] Dongshuo Yin, Xueting Han, Bin Li, Hao Feng, and Jing\nBai. Parameter-efficient is not sufficient: Exploring parame-\n16\nter, memory, and time efficient adapter tuning for dense pre-\ndictions. arXiv preprint arXiv:2306.09729, 2023. 3, 5, 9\n[72] Chenlin Zhang, Jianxin Wu, and Yin Li. Actionformer: Lo-\ncalizing moments of actions with transformers. In ECCV,\n2022. 1, 2, 3, 5, 6, 7, 10, 11, 12\n[73] Chen Zhao, Ali K Thabet, and Bernard Ghanem. Video self-\nstitching graph network for temporal action localization. In\nICCV, 2021. 1, 2, 10, 11\n[74] Chen Zhao, Merey Ramazanova, Mengmeng Xu, and\nBernard Ghanem. Segtad: Precise temporal action detection\nvia semantic segmentation. In ECCVW, 2022. 2\n[75] Chen Zhao, Shuming Liu, Karttikeya Mangalam, and\nBernard Ghanem. Re2TAL: Rewiring pretrained video back-\nbones for reversible temporal action localization. In CVPR,\n2023. 1, 2, 3, 4, 6\n[76] Chen Zhao, Shuming Liu, Karttikeya Mangalam, Guocheng\nQian, Fatimah Zohra, Abdulmohsen Alghannam, Jitendra\nMalik, and Bernard Ghanem. Dr 2Net: Dynamic reversible\ndual-residual networks for memory-efficient finetuning. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 2024. 1, 2\n[77] Hang Zhao, Zhicheng Yan, Lorenzo Torresani, and Antonio\nTorralba. HACS: Human action clips and segments dataset\nfor recognition and temporal localization. ICCV, 2019. 1\n[78] Y Zhao, B Zhang, Z Wu, S Yang, L Zhou, S Yan, L Wang, Y\nXiong, D Lin, Y Qiao, et al. Cuhk & ethz & siat submission\nto activitynet challenge 2017. CVPR ActivityNet Workshop,\n2017. 6\n[79] Zixuan Zhao, Dongqi Wang, and Xu Zhao. Movement\nenhancement toward multi-scale video feature representa-\ntion for temporal action detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 13555–13564, 2023. 2\n17"
  },
  {
    "source": "2305.10507v1.pdf",
    "content": "ReasonNet: End-to-End Driving with Temporal and Global Reasoning\nHao Shao1 Letian Wang2 Ruobing Chen1\nSteven L. Waslander2 Hongsheng Li3 Yu Liu1,4*\n1SenseTime Research 2University of Toronto 3CUHK MMLab\n4Shanghai Artificial Intelligence Laboratory\nAbstract\nThe large-scale deployment of autonomous vehicles is\nyet to come, and one of the major remaining challenges\nlies in urban dense traffic scenarios. In such cases, it\nremains challenging to predict the future evolution of the\nscene and future behaviors of objects, and to deal with rare\nadverse events such as the sudden appearance of occluded\nobjects. In this paper, we present ReasonNet, a novel end-\nto-end driving framework that extensively exploits both tem-\nporal and global information of the driving scene. By rea-\nsoning on the temporal behavior of objects, our method\ncan effectively process the interactions and relationships\namong features in different frames. Reasoning about the\nglobal information of the scene can also improve overall\nperception performance and benefit the detection of adverse\nevents, especially the anticipation of potential danger from\noccluded objects. For comprehensive evaluation on occlu-\nsion events, we also release publicly a driving simulation\nbenchmark DriveOcclusionSim consisting of diverse occlu-\nsion events. We conduct extensive experiments on multi-\nple CARLA benchmarks, where our model outperforms all\nprior methods, ranking first on the sensor track of the public\nCARLA Leaderboard [53].\n1. Introduction\nDespite significant recent progress in the field of au-\ntonomous driving, truly large-scale deployment of au-\ntonomous vehicles (A Vs) on public roads has yet to be\nestablished. The majority of the remaining issues lie in\nnavigating dense urban traffic scenes, where a large num-\nber of different dynamic objects (e.g. vehicles, bicycles,\npedestrians), complex road geometries and road user inter-\nactions are involved. In such circumstances, currently de-\nployed or tested solutions could make incorrect or unex-\npected decisions , resulting in severe accidents or traffic in-\nfractions [4, 24, 53]. Two of the major challenges behind\n*Corresponding author\nTemporal Reasoning\nGlobal ReasoningOccluded Area\nBrake\nPotential Danger\nFigure 1. Temporal reasoning on the historic behaviors of sur-\nrounding objects can benefit the prediction of the scene evolution\nand objects’ future behaviors. Global reasoning on the interaction\namong objects and the environment allows for inference about un-\nobservable space and occluded objects, anticipating potential dan-\nger and enhancing perception/driving performance.\nsuch autonomous incompetence include 1) how to achieve\na comprehensive understanding of the driving scene and,\nmore importantly, how to make high-fidelity predictions on\nthe future evolution of the driving scene; 2) how to deal\nwith rare adverse events in long-tail distributions, such as\nundetected but relevant objects in occluded regions.\nComprehensive scene understanding and high-fidelity\nprediction of how objects in the scene will move in the fu-\nture are vital for autonomous vehicles to take safe and reli-\nable actions. Toward this end, modularized methods were\nproposed to decompose the task into three sequential sub-\ntasks: detection [37–40, 42], tracking [5, 65], and forecast-\ning [6, 26, 28, 32, 33, 59–61]. While more interpretability is\nprovided by developing each module independently, these\nsub-tasks are still regarded as open research questions and\nerrors in each sub-task can propagate and accumulate, lead-\ning to unstable overall performance. In contrast, end-to-\nend driving methods [9, 10, 51] have recently emerged as a\npromising method to solve these subtasks in a monolithic\nmanner directly. However, a high-fidelity future prediction\nnecessitates sufficient temporal reasoningover the historic\narXiv:2305.10507v1  [cs.CV]  17 May 2023\ninformation of the scene [22, 50], which is usually only\nsomewhat considered in previous end-to-end driving meth-\nods if not completely ignored. For example, [10, 15] only\nexploited scene information in the current frame, and [54]\nsimply concatenated features in historic frames for temporal\nreasoning. In such cases, the interactions and relationships\namongst features in different frames and objects cannot be\nsufficiently modeled. Thus in this paper, we propose a tem-\nporal reasoning module to effectively fuse information from\ndifferent frames for better driving performance.\nOn the other hand, rare adverse events in long-tail distri-\nbutions remain a notoriously challenging issue on the way\ntoward large scale deployment of autonomous vehicles. For\nexample, one such challenge is the difficulty in detecting\noccluded but relevant objects in the scene. While a large\namount of research has focused on improving perception\nperformance [37, 57], the occluded objects essentially lie\nout of the scope of observable elements, and failure to con-\nsider such objects can result in either dangerous or overly\ncautious driving behavior. Our observation is that, while\nhumans also suffer from similar limitations to autonomous\nvehicles regarding occluded objects, they are able to rea-\nson about these unobservable spaces by exploiting global\ninformation of the scene such as road geometry and driv-\ning interaction patterns, to anticipate potential danger even\nunder occlusion. For example, when one human driver no-\ntices another vehicle braking abruptly, the driver may rea-\nson the presence of an occluded object (e.g., a pedestrian)\nahead, reminding himself to drive cautiously. Thus, our\ninsight is that, a safe and intelligent autonomous vehicle\nshould also master the global reasoningcapability to have\na better perception of the scene. In this paper, we pro-\npose a transformer-based global reasoning module to suf-\nficiently fuse information of the environment and objects,\nand analyze their interactions for better scene understand-\ning. Such global reasoning capability not only benefits in-\nteraction modeling with occluded objects, but also improves\noverall perception performance. Examples of such perfor-\nmance gains include better traffic light status identification\nby reasoning over other vehicles’ actions and more accurate\nfuture trajectory forecasting by reasoning over interactions\namong objects. Besides, considering the fact that the oc-\nclusion events lie in the long-tail distribution and have been\nrare in currently available datasets, we also construct a Driv-\ning in Occlusion Simulation benchmark (DOS) consisting\nof 4 occlusion scenarios, each with 25 cases, as a compre-\nhensive occlusion event evaluation benchmark in the field\nof end-to-end autonomous driving.\nIn this paper, we propose a novel end-to-end driving\nframework named temporal and global reasoning network\n(ReasonNet), which provides enhanced reasoning on the\ntemporal evolution and the global information of the scene,\nfor better perception performance and driving quality. Our\ncontributions are three-fold:\n• We propose a novel temporal and global reasoning\nNetwork (ReasonNet) to enhance historic scene rea-\nsoning for high-fidelity prediction of the scene’s fu-\nture evolution and improve global contextual percep-\ntion performance even under occlusion.\n• We present a new benchmark called Driving in\nOcclusion Simulation benchmark (DOS), which con-\nsists of diverse occlusion scenarios in urban driving for\nsystematic evaluation in occlusion events, and make\nthe benchmark publicly available.\n• We experimentally validate our method on multiple\nbenchmarks with complex and adversarial urban sce-\nnarios. Our model ranks first on the sensor track of the\nCARLA autonomous driving leaderboard.\n2. Related work\nEnd-to-end Autonomous DrivingEnd-to-end autonomous\ndriving in urban scenarios has become more studied\nrecently thanks to the CARLA simulator and leader-\nboard [21]. Recent works mainly consist of reinforcement\nlearning (RL) and imitation learning (IL) methods. The\nreinforcement Learning methods train the agents by con-\nstantly interacting with simulated environments and learn-\ning from these experiences. Latent DRL [54] first trains an\nembedding space as a latent representation of the environ-\nment observation, and then conducts reinforcement learn-\ning with the latent observation. Roach [66] utilizes an RL\nagent with privileged information of the environment to dis-\ntill a model only with regular information (e.g. sensor) as\nthe final agent. WOR [9] builds a model-based RL agent\nalong with the world model and reward model. The final\nagent is distilled from the expert knowledge acquired from\nthese pretrained models. Imitation learning methods aim\nat learning from an expert agent to bypass interacting with\nthe environment. Early IL methods include CIL [17] and\nCILRS [18], which apply a conditional architecture with\ndifferent network branches for different navigation com-\nmands. LBC [11] first trains an imitation learning agent\nwith privileged information, which is then distilled into a\nmodel using sensor data. Transfuser [15, 47] designs a\nmulti-modal transformer to fuse information between the\nfront camera image and LiDAR data. LA V [10] exploits\ndata of not only the ego vehicle but also surrounding vehi-\ncles for data augmentation by learning a viewpoint-invariant\nspatial intermediate representation. TCP [63] proposes a\nnetwork with two branches which generates the control sig-\nnal and waypoints respectively. An adaptive ensemble is\napplied to fuse the two output signals. InterFuser [51] uses\na transformer to fuse and process multimodal multi-view\nsensors for comprehensive scene understanding.\nAttention for Autonomous DrivingThe attention mech-\nanism has been demonstrated to be a powerful module\nin many areas of deep learning, including the context of\ndriving. The classic attention-based Transformer architec-\nture [55] was originally established in Natural Language\nProcessing. Transformer (VIT) was then applied in Com-\nputer Vision (vision Transformer, VIT [20, 49]) and attains\nexcellent performance on Imagenet classification. Later\ngenerations move on to generalize the attention mecha-\nnism to the driving domain, including motion forecast-\ning [23, 36, 58], driver attention prediction [25, 34] and ob-\nject tracking [45,52]. In the field of end-to-end autonomous\ndriving, TransFuser [15, 47] exploits several transformer\nmodules for the fusion of data from the front view camera\nand LiDAR. NEAT [14] uses intermediate attention maps to\niteratively compress 2D image features into a compact bird-\neye-view (BEV) representation for driving. InterFuser [51]\nutilizes a transformer encoder and decoder to fuse informa-\ntion and decode the feature into interpretable embeddings.\nMulti-task Learning Our end-to-end driving framework\nadopts multi-task learning, with a joint objective of ob-\nject detection, occupancy forecasting, traffic sign predic-\ntion and waypoint prediction. MotionNet [62] proposes a\nspatio-temporal pyramid network for joint perception and\nmotion prediction based on BEV maps. PnPNet [41] pro-\nposes a new object trajectory representation and multi-\nobject tracker to handle occlusion and false positives. In-\ntentNet [7] predicts the high-level intentions of each agent\nfrom semantic HD maps building. ST-P3 [29] proposes an\negocentric-aligned accumulation technique to preserve ge-\nometry information in 3D space and utilize a dual pathway\nmodeling to consider past motion variations.\n3. Method\nWe aim at learning a driving policy π that generates raw\ncontrol commands by taking multi-view multi-modal sen-\nsor readings, vehicle measurements, and navigation com-\nmands as inputs. As shown in Figure 2, the proposed Rea-\nsonNet consists of three parts: 1) a perception module that\nextracts bird’s-eye-view (BEV) features from LiDAR and\nRGB data; 2) a temporal reasoning module that processes\ntemporal information and maintains a memory bank stor-\ning historic features; 3) a global reasoning module that cap-\ntures the interaction/relationship amongst objects and the\nenvironment, to detect adverse events (e.g. occlusion) and\nimprove overall perception performance. This section will\nintroduce these modules in detail.\n3.1. Perception Module\nThe perception module is responsible for processing and\nfusing different sensor data at the early stage of our frame-\nwork, based on which temporal and global reasoning can\nbe conducted by later modules. Specifically, five sensors\nare utilized: four RGB cameras (left, front, right and rear)\nIrgb = I0,1,2,3 and one LiDAR sensor Ilidar = I4. Four\nimage inputs are obtained from the four cameras, and an\nadditional focus-view image input is center-cropped from\nthe front image to capture distant traffic lights. Point cloud\ndata is retrieved from the LiDAR sensor. Our perception\nmodule includes a 2D backbone to embed image input into\nkeys and values, a 3D backbone to embed LiDAR input into\nqueries, and a BEV decoder that utilizes these keys, values,\nand queries to obtain features of the bird’s-eye view (BEV)\nmap, waypoints, and traffic signs.\nImage InputFor each image input ofIrgb, a 2D CNN back-\nbone ResNet [27] is applied to generate a feature map fi.\nThen, we use a convolution layer to map the channels of\nfi to Cv and flatten it to one-dimensional tokens. A si-\nnusoidal positional encoding and learnable sensor embed-\ndings are added to the tokens, so that the following network\ncan distinguish them from different cameras and relative\npositions. Finally, tokens of different images are passed\nthrough a standard transformer encoder with Ke layers.\nEach layer consists of Multi-Headed Self-Attention [55],\nMLP blocks and layer normalization [3]. This image fu-\nsion operation can contribute to a better perception of global\ncontext from multi-view inputs, generating keys and values\nfor the image-LiDAR fusion in BEV decoder.\nLiDAR Input For the LiDAR input, we use PointPil-\nlars [35] as our 3D perception backbone to process points\nin the ego-vehicle-centered area x ∈ [−Hb, H− Hb]\nand y ∈ [−W/2, W/2]. Specifically, we use a simpli-\nfied version of PointNet [48] to encode the information of\nraw LiDAR points. Each pillar includes the points in a\n0.25m×0.25m area. The extracted feature map is down-\nsampled to Cv × H × W for computation reduction and\nthen serves as BEV queries used in the BEV decoder and\nmemory bank.\nSensor-Fusion BEV DecoderThe BEV decoder follows\na standard transformer architecture design with Kbev layers\nto fuse tokens from different sensors. Tokens from the RGB\nimages are fed as values and keys into the decoder, and to-\nkens from the LiDAR points are fed as the H × W queries\nto generate BEV features. In addition, two other kinds of\nqueries for the prediction of traffic signs and waypoints w\nare also fed into the decoder. Following InterFuser [51], we\nuse a 2-layer MLP as the traffic sign classifier to predict the\ntraffic light state and whether there is a stop sign ahead; we\nthen use a single-layer GRU [16] to auto-regressively gen-\nerate consecutive waypoints {wt}Tf\nt=1 conditioned on the\ngoal location of the ego vehicle. Tf denotes the number\nof the predicted time steps. To pretrain the perception mod-\nule in the first training stage, the generated BEV feature is\npassed through a one-stage CenterPoint [64] to generate the\nH × W × 7 BEV map covering an Hm × Wm spatial re-\ngion, where the seven channels represent object existence\nM\nMulti-view Input\nLiDARInput\n2DBackBone\n3DBackBone\nQueryencoderQueryqCk×H×WAffinitySHW×THWReadoutfeaturesMCv×T×H×W\nShort-termmemorykey Long-termmemorykey\nShort-termmemoryvalueLong-termmemoryvalueMemorykeyk\nMemoryvaluev Ck×T×H×WCv×T×H×W\nBEVDecodervalue\nqueryBEVfeatures GRU\nOccupancy DecoderWaypoints\nOccupancyqueries\nValueencoder\nNewmemoryvalueNewmemorykeyCopy\nUpdate\nCv×H×W\nMemoryBank\nCv×H×W\nOccupancyMap\nBEVMap\nTrafficsign\nCv×H×W\nInteractionModeling\nTemporalReasoning\nGlobalReasoningPerception consistencyloss\nkey\nMfused\nFigure 2. The proposed ReasonNet consists of three modules: 1) the perception module fuses different sensor data to generate the BEV\nfeature, traffic sign feature, and waypoints in the early stage of our framework; 2) the temporal reasoning module processes current\nand historic features and maintains a memory bank to store historic features; 3) the global reasoning module models the interaction and\nrelationship among objects and the environment to detect adverse events (e.g. occlusion) and improve overall perception performance.\nprobability, offset from grid center, bounding box extent,\nheading angle and velocity for objects at each grid cell.\n3.2. Temporal Reasoning module\nCompared to existing end-to-end driving methods that\nonly exploit scene information of the current frame [10, 15]\nor simply concatenate features of historic frames [54], we\npropose a temporal reasoning module that can sufficiently\nstore and fuse temporal information to benefit the motion\nforecasting of traffic participants and the tracking of in-\ntermittently occluded objects. As shown in Figure 2, our\ntemporal reasoning module includes temporal processing to\nfuse current and historic features through an attention mech-\nanism, and maintains a memory bank which stores historic\nshort-term and long-term feature keys and values.\nTemporal ProcessingConsidering that information in dif-\nferent historic frames could have different relevance to the\ncurrent scene, we apply an attention-based memory read-\ning from the historic features. Specifically, for each his-\ntoric frame t stored in the memory, we first measure its rel-\nevance by calculating the normalized similarity S between\nthe historic-frame feature key k ∈ RCk×Th×H×W 1 and the\ncurrent-frame feature query q ∈ RCk×H×W :\nS(qh,w, kt,i,j) = (kt,i,j − qh,w)2\nPi=H−1,j=W−1\ni=0,j=0 (kt,i,j − qh,w)2\n(1)\n1C, H, W denotes the channel, height, and width of the feature respec-\ntively, Th denotes the number of the frames stored in the memory bank.\nWe map every query element to a distribution over H ×\nW memory elements and correspondingly aggregate their\nvalues v to obtain the readout feature M ∈ RCv×Th×H×W\nfor each frame t stored in the memory:\nMt,h,w =\ni=H−1,j=W−1X\ni=0,j=0\nvt,i,jS(qh,w, kt,i,j) (2)\nThe aggregated features from all historic frames are then\nconcatenated with the current-frame feature value to get\nM\n′\n∈ RCv×(Th+1)×H×W , which is then passed through\na GRU to progressively fuse temporal information and get\nMfused ∈ RCv×H×W as the final output of the module.\nTechnically, we take the L2 similarity proposed in STCN\n[13] as the similarity measure function, which is more stable\nthan the dot product [46]. The current-frame feature query\nq is obtained by passing the features from the 3D backbone\nF ∈ RCv×H×W through a query encoder (several convolu-\ntion layers). The historic-frame feature key k and value v\nare taken from the temporal memory bank.\nMemory Bank MaintainingAs above, we have introduced\nthe temporal processing at one single frame. After every\nτ frame, the obtained feature key and value at that frame\nwill be used to update the memory bank. Specifically, the\ncurrent-frame feature query q is directly copied and fed into\nthe memory bank as the memory key without extra com-\nputation. The final output Mfused will first be encoded\nto a BEV map Mp. The BEV map Mp will be concate-\nnated with the final output Mfused and passed through a\nvalue encoder to obtain the memory value v, which is fed\ninto the memory back. With the above key-value pairs, the\nmemory bank maintains two kinds of buffer: the short-term\nand long-term buffer. On the one hand, the new key-value\npair will be appended to the short-term buffer, as a high-\nresolution memory of the scene in the past few seconds for\naccurate feature matching. Considering the limited GPU\nmemory resources, we limit the buffer size and older key-\nvalue pairs will be discarded when the limit number Ts is\nreached. However, when these older features are discarded,\nthe long-term behavior of the traffic participants is miss-\ning, which can be crucial for motion forecasting in com-\nplex traffic scenarios. Thus on the other hand, inspired by\nXMem [12], the memory bank also maintains a long-term\nbuffer that selectively stores important/representative key-\nvalue feature pairs discarded by the short-term buffer. Con-\nsidering the fact that the objects surrounding the ego vehi-\ncle are sparse most of the time2, the long-term buffer selec-\ntively stores key-value features ( k and v) which meet one\nof the two criteria: 1) their corresponding location in the\nBEV map Mp has a high probability of object existence; 2)\ntheir usage frequency is in top-K of all candidate key-value\nfeatures. The usage frequency is defined by its cumulative\nnormalized similarity (Eq. 1). The features selected by the\nabove criteria are appended to the last frame of memory.\nAnd if the last memory frame is full, we will initialize one\nnew frame with the zero vector and set it as the last frame\nto store new features. When the number of frames reaches\nthe limit Tl, the obsolete memory will be removed. Such\na compact storing strategy can efficiently track long-term\nrepresentative features and intermittently occluded objects,\nwhile balancing the resources required.\n3.3. Global Reasoning module\nRare adverse events such as occluded objects are a no-\ntorious issue for the practical deployment of A Vs. Our in-\nsight is that humans perceive their surroundings not only\nthrough sensors, but also by exploiting global information\non the scene to reason over the unobservable spaces. For\ninstance, when a vehicle performs an emergency stop with-\nout a clear reason, humans can infer that there is potentially\nan occluded object ahead of the vehicle and will drive more\ncautiously. Thus we propose the global reasoning module to\ncapture the interaction and relationship between objects and\nthe environment to detect adverse events (e.g. occlusion)\nand improve overall perception performance. The module\nconsists of three parts: 1) an object-environment and object-\nobject interaction modeling process; 2) an occupancy de-\ncoder to generate the occupancy map; 3) a consistency loss\n2Based on the data collected from the CARLA simulator, only 7% of\nthe ego-vehicle-centered BEV map area is occupied by active objects.\nto encourage consistent prediction of waypoints and the oc-\ncupancy map.\nInteraction ModelingThe object-environment and object-\nobject interaction modeling process aims at reasoning about\nthe relationship among objects and the environment. On the\none hand, Mfused features whose corresponding location\nin the BEV map Mp has a high probability of object exis-\ntence will be extracted to represent object features. On the\nother hand, Mfused features will also be downsampled to\nrepresent the environment features. All object and environ-\nment features are used to construct a graph, which is passed\nthrough a graph attention network (GAT) [56] for interac-\ntion modeling.\nOccupancy DecoderTaking the features outputted by the\nGAT as keys and values, and the learnable positional em-\nbeddings as queries, the occupancy decoder utilizes a trans-\nformer decoder with Kopy layers to generate: 1) the traf-\nfic sign feature, which is then concatenated with the traf-\nfic sign feature from the BEV decoder to generate the final\ntraffic sign prediction; 2) the occupancy map feature, which\nis then applied with convolution operation to generate the\noccupancy map Ot ∈ RTf ×H×W . At a future time t, each\ncell in the occupancy map contains a value in the range [0,1]\nrepresenting the probability that the cell is occupied.\nConsistency Loss Currently, our framework predicts the\nwaypoints and the occupancy map independently, which\nare not necessarily consistent. For example, the waypoints\ncould overlap some obstacles in the occupancy map. Thus\nwe propose a consistency loss to discourage waypoints’\ncrossing the high-probability region of the occupancy map.\nFurther, the consistency loss also encourages generating\nlonger waypoint trajectories for efficient driving. Specifi-\ncally, the consistency loss aims at minimizing the average\nobject existence probability of the cells that cover the pre-\ndicted waypoints, and maximizing the average l2 length of\nthe waypoint trajectory w:\nLconsistency = 1\nTf\n TfX\nt=0\nPNt\nc\ni=0 Ot,i\nNtc\n− λ\nTfX\nt=0\n∥wt∥1\n!\n(3)\n,where Nt\nc denotes the number of covered cell at stept, Ot,i\ndenotes the object existence probability at cell i at time t.\n3.4. Control\nFollowing [11], we use two PID controllers for latitudi-\nnal and longitudinal control, to track the heading and veloc-\nity of predicted waypoints respectively. If a red traffic light\nor stop sign is detected, the ego-vehicle will brake. Addi-\ntionally, an emergency stop will also be performed if the\nego vehicle’s current bounding box crosses the area in the\noccupancy map that has a high object existence probability\nor if the future waypoints overlap with objects in the BEV\nmap.\noccluded pedestrian\negocar\noccludedvisible\ncollision\negocar\noccludedpedestriancollision\nbraking!\noccludedvisible\ncollisionegocar\noccludedcar\noccludedvisible\negocar\ncollisionoccludedcar\noccludedvisible\nParkedCars SuddenBrake\nLeftTurn Red Light Infraction\nbraking!\nFigure 3. An illustration for the four types of occlusion scenarios\nincluded in the proposed DOS benchmark. The orange color de-\nnotes the ego car. The blue/green dots denote the occluded/visible\ntrajectory of the occluded dangerous object.\n3.5. Training Setup\nThe training of our framework consists of two stages.\nIn the first stage, we train the perception module to predict\nBEV features, traffic sign features, and waypoints. Specif-\nically, the loss of BEV features and traffic sign features is\ncomputed with additional prediction heads, which are dis-\ncarded in the next stage. In the second stage, we freeze\nthe perception module and train the other two modules.\nFive loss terms are considered: 1) the waypoints loss Lw\nthat minimizes the error between predicted waypoints and\nexpert waypoints; 2) the BEV map loss LBEV that fol-\nlows [10, 64] to minimize the current-frame BEV map pre-\ndiction error; 3) the traffic sign lossLsign for the traffic reg-\nulation prediction; 4) occupancy map loss Lopy that mini-\nmizes the occupancy prediction error in a future horizon; 5)\nthe consistency loss Lconsistency which encourages a consis-\ntent generation of the waypoints and occupancy map. These\nloss terms are balanced by corresponding loss weights.\n4. Drive in Occlusion Sim (DOS) Benchmark\nIn order to address the issue that occlusion events are rare\nin existing datasets and benchmarks, we present the Drive\nin Occlusion Simulation benchmark (DOS), a CARLA-\nbased framework providing diverse driving scenarios with\noccluded objects. As shown in Figure 3, the proposed\nDOS benchmark includes four types of challenging occlu-\nsion driving scenarios:\nParked Cars(#1) The ego vehicle is driving in a straight\nRank Method DS ↑ RC ↑ IS ↑\n1 ReasonNet (Ours) 79.95 89.89 0.89\n2 InterFuser [51] 76.18 88 .23 0 .84\n3 TCP [63] 75.14 85 .63 0 .87\n4 LA V [10] 61.85 94.46 0.64\n5 TransFuser [15] 61.18 86 .69 0 .71\n6 Latent TransFuser [15] 45.20 66 .31 0 .72\n7 GRIAD [8] 36.79 61 .85 0 .60\n8 TransFuser+ [1] 34.58 69 .84 0 .56\nTable 1. Performance comparison on the public CARLA leader-\nboard [53] (accessed Nov 2022). For all three metrics, higher is\nbetter. Our method ranks first overall on the leaderboard, with the\nhighest driving score (DS) and infraction score (IS), and the sec-\nond highest route completion (RC).\nlane with parked cars on the side. Pedestrians can first ap-\npear on the sidewalk (visible) and then suddenly emerge\nthrough the occluded areas between parked cars (occluded).\nSudden Brake(#2) The ego vehicle is driving in a straight\nlane along with other vehicles ahead. Pedestrians can sud-\ndenly emerge from the sidewalks, causing the other vehicles\nto brake while remaining invisible to the ego vehicle.\nLeft Turn(#3) The ego vehicle intends to perform an un-\nprotected left turn at an intersection, but a truck in the oppo-\nsite lane blocks the view of oncoming traffic, intermittently\nobscuring vehicles driving straight through the intersection.\nRed Light Infraction(#4) The ego vehicle is crossing an\nintersection after some trucks. A left-to-right vehicle run-\nning a red light suddenly appears, forcing the trucks to brake\npromptly. But the ego vehicle’s view toward the running-\nlight vehicle is blocked by the trucks, so it remains invisible\nto the ego vehicle.\nEach of the four scenarios in the DOS benchmark com-\nprises 25 different cases varying in the road environment\nand background traffic. Compared to a previous occlusion\nbenchmark AUTOCASTSIM [19], the DOS benchmark: 1)\nincludes occlusions of both vehicles and pedestrians, in-\nstead of only vehicles; 2) includes 100 cases of 4 scenar-\nios, instead of only 3 cases of 3 scenarios; 3) considers spe-\ncific occlusions that can potentially be resolved by temporal\nreasoning (intermittent occlusion, #1, #3) and global rea-\nsoning (constant occlusion but with interaction clues, #2,\n#4) about the scene, instead of random occlusions as in\nAUTOCASTSIM. Thus our scenarios can also serve as a\ngood tracking-with-intermittent-occlusion benchmark and a\nPeople-as-Sensor [2, 31] benchmark.\n5. Experiments\n5.1. Experiment Setup\nImplementation We implement and evaluate our ap-\nproach on the open-source CARLA simulator with version\nSetting Town 05 Long DOS\nTs Tl DS↑ RC↑ IS↑ CR↓ Red↓ Blocked↓ SR#1↑ SR#2↑ SR#3↑ SR#4↑\n0 0 66.7 ±3.8 97.6±2.7 0.68±0.03 0.18 ±0.03 0.05 ±0.02 0.03±0.03 22±1.6 28 ±3.4 26 ±2.1 25 ±1.6\n1 0 67.9 ±3.4 96.8 ±2.3 0.70 ±0.02 0.16 ±0.04 0.04 ±0.03 0.05 ±0.02 30 ±3.6 38 ±3.6 32 ±2.8 32 ±3.4\n2 0 68.1 ±3.1 96.9 ±3.4 0.70 ±0.03 0.16 ±0.03 0.04 ±0.02 0.05 ±0.03 28 ±5.5 48 ±4.1 38 ±4.4 52 ±3.9\n2 1 70.9 ±2.0 95.7 ±3.1 0.74 ±0.02 0.13 ±0.02 0.04 ±0.02 0.06 ±0.04 55 ±4.4 57 ±4.1 48 ±4.1 55 ±5.5\n4 0 70.5 ±2.1 96.4 ±2.5 0.73 ±0.04 0.14 ±0.03 0.03 ±0.02 0.06 ±0.03 32 ±5.4 58 ±4.4 40 ±5.5 55 ±4.9\n4 2 73.2±1.9 95.9±2.3 0.76±0.03 0.11±0.02 0.03±0.01 0.07±0.03 63±4.2 73±3.6 80±4.2 70±5.5\nTable 2. Ablation study on different short-term buffer size Ts and long-term buffer size Tl, on the Town 05 Long benchmark and the\nproposed DOS benchmark. Performance is evaluated over three runs. CR: Collision rate, Red: Red light violation, Blocked: Vehicle\nblocked, SR: Success rate. SR#1 denotes the first kind of scenario in the DOS benchmark. As the two buffer sizes increase, improvement\nis witnessed in all metrics but the road completion.\n0.9.10.1 [21]. We use ResNet-50 pretrained on ImageNet\nas the 2D backbone and PointPillars trained from scratch\nas the 3D backbone. We predict Tf = 4 time steps for\nthe waypoints and occupancy map, and the interval between\neach time step is 0.5 seconds. The memory bank maintains\nTs = 4frames in the short-term buffer and Tl = 2frames\nin the long-term buffer. The memory bank is updated every\nτ = 2 frame. We refer readers to Appendix A for more\ndetails.\nDataset Collection We collect an expert dataset of 2M\nframes by running a rule-based expert agent on all 8 pub-\nlic towns and 21 types of weather, with the access to the\nprivileged information in the CARLA simulator. We ran-\ndomly set routes, spawn dynamic objects and adversarial\nscenarios provided in [47], to diversify the collected data.\nTo ensure the temporal continuity of collected data, the data\nare collected at a high frequency of 10HZ.\nMetrics We consider three major metrics introduced by the\nCARLA LeaderBoard: route completion ratio (RC), infrac-\ntion score (IS), and driving score (DS). The route comple-\ntion ratio is the percentage of the route completed. The in-\nfraction score measures infractions triggered. When colli-\nsions or traffic rule violations occur, the infraction score will\ndecay by a discount factor. The driving score is the prod-\nuct of the route completion ratio and the infraction score,\ndescribing both driving progress and safety, and thus is the\nprimary ranking metric in the CARLA Leaderboard.\n5.2. Comparison to the state of the art\nTable 1 shows the top 8 entries on the public CARLA\nLeaderboard. Readers can refer to Sec 2 for descriptions\nof these methods. Our method outperforms all prior meth-\nods, with the highest driving score and infraction score, and\nthe second highest route completion. The previous leading\nmethod InterFuser uses a transformer for sensor fusion but\nlacks temporal and global reasoning. Compared to Inter-\nFuser, our method improved the driving score, road comple-\ntion, and infraction score by 5%, 2%, and 6% respectively.\n5.3. Ablation study\nWe investigate the effect of the temporal and global rea-\nsoning modules on the Town05 Long benchmark and the\nDOS benchmark. For each scenario in DOS, we take 5 cases\nfor training and 20 cases for evaluation. In addition to the\nthree metrics mentioned earlier, we also present four more\nmetrics for detailed analysis: collision rate (CR), red light\nviolation (Red), ego vehicle blocked frequency (Blocked),\nand success rate (SR). The first three metrics are normalized\nby the driven distance (km). Visualizations of how the tem-\nporal reasoning and global reasoning work can be found at\nFigure 4 and Figure 5 respectively.\nMemory SizeTable 2 studies the effect of different short-\nterm buffer size Ts and long-term buffer size Tl. The over-\nall observation is that, as the two buffer sizes increase,\nimprovement is witnessed in all metrics but road comple-\ntion. Specifically, when the long-term memory is removed\n(Tl = 0), the average success rates drop sharply from 71.5\nto 36 on DOS scenarios that require keeping track of in-\ntermittently occluded objects (#1 and #3). If we remove\nthe temporal reasoning module ( Ts = Tl = 0), the driving\nscore on the Town05 benchmark drops by 9%, and the av-\nerage success rate on the DOS benchmark drops by 46%.\nWe hypothesize that the drop in performance is because 1)\nit can be really hard to accurately estimate the objects’ fu-\nture motion based only on single-frame data; 2) temporal\ninformation can help keep track of objects that are intermit-\ntently occluded; 3) the global reasoning module may also\nwork poorly when historic information is missing.\nLong-Term Memory Selection StrategyTable 3 studies\nthe performance of different long-term memory selection\nstrategies. Specifically, the proposed strategy in Sec 3.2 in-\ncludes two selection criteria. So here we ablate the effect\nof the two criteria by 1) only selecting the short-term fea-\nture with top-K usages (usage-based); 2) only selecting the\nfeature with a high probability of the existence of an object\n(object-based). Besides, we also compared a random selec-\ntion strategy. As in Table 3, the random selection strategy\nhas the poorest performance especially on the DOS bench-\nmark, as random selection could miss important and repre-\nSetting Town 05 Long DOS\nDS↑ RC↑ IS↑ CR↓ Red↓ Blocked↓ SR#1↑ SR#2↑ SR#3↑ SR#4↑\nRandom 71.2 ±5.4 96.6±2.4 0.74±0.04 0.13±0.04 0.03±0.01 0.06±0.02 33 ±4.4 55 ±6.2 42 ±5.5 53 ±5.4\nUsage-based 72.0±3.9 95.9 ±2.2 0.75 ±0.04 0.12±0.03 0.03±0.01 0.06±0.02 45 ±4.2 62 ±3.4 53 ±2.8 62 ±3.9\nObject-based 72.2±3.7 96.1 ±3.0 0.75 ±0.03 0.12±0.03 0.03±0.01 0.05±0.02 57±4.1 65 ±3.6 73 ±4.4 60 ±3.7\nFull (Ours) 73.2±1.9 95.9±2.3 0.76±0.03 0.11±0.02 0.03±0.01 0.07±0.03 63±4.2 73±3.6 80±4.2 70±5.5\nTable 3. Ablation study on different long-term memory selection strategies. Our proposed strategy considering both the usage and object\ncriteria outperforms the random selection strategy and the two methods with only one criteria, especially on the DOS benchmark.\nSetting Town 05 Long DOS\nDS↑ RC↑ IS↑ CR↓ Red↓ Blocked↓ SR#1↑ SR#2↑ SR#3↑ SR#4↑\nNo global reasoning 68.9±4.6 97.4±2.9 0.71±0.04 0.15±0.04 0.05±0.02 0.05±0.02 28±2.8 34±3.4 29±2.0 27±3.6\nNo consistency loss 72.2±3.4 96.1±3.2 0.75±0.03 0.12±0.02 0.03±0.02 0.06±0.03 60±4.1 72±3.9 77±4.9 68±4.2\nNo traffic sign prediction 71.1±2.7 96.0±4.1 0.74±0.03 0.11±0.03 0.05±0.03 0.07±0.03 62±4.4 72±4.0 82±2.8 70±4.1\nFull (Ours) 73.2±1.9 95.9±2.3 0.76±0.03 0.11±0.02 0.03±0.01 0.07±0.03 63±4.2 73±3.6 80±4.2 70±5.5\nTable 4. Ablation study on the global reasoning module. The performance would drop when 1) the entire global reasoning module is\nremoved; 2) the consistency loss is not applied; 3) the traffic sign feature from the reasoning module is not utilized.\nT-1 T-2 T-3 T-4\nT-1 T-2 T-3 T-4\n0\n0.5\n1.0\nFigure 4. Visualization of the attention map between one object’s\ncurrent-frame feature query and the historic-frame feature stored\nin the short-term buffer, in two cases. The object’s current-frame\nfeature consistently attends to its corresponding region in the his-\ntoric feature map.\nPreviousFrameCurrentFrameT T+1 T+2\nstopping!\nstopping!\noccluded\noccludedoccluded\nFigure 5. We show two cases of how our framework reasons the\npresence of the occluded object. In the first case, a pedestrian first\nappeared on the sidewalk (visible) and then emerges between two\nparked cars (occluded). In the second case, a vehicle runs the red\nlight, forcing trucks to brake abruptly. But the ego vehicle’s view\ntoward the running-light vehicle is blocked by the front trucks, so\nthe running-light vehicle remains invisible to the ego vehicle. The\nrectangles mark the occluded objects.\nsentative features on the scene. Compared to our strategy\nutilizing both criteria, the two ablations omitting one of the\ncriteria have a performance drop, especially on the DOS\nbenchmark. The usage-based strategy performs worse than\nthe object-based, showing that the features of objects could\nbe more informative for capturing historic behaviors.\nGlobal Reasoning DesignTable 4 studies the performance\nwhen different designs of the global reasoning module are\napplied. First, we remove the entire module and observe a\nsignificant drop in all metrics but the road completion. For\ninstance, the average success rate on the DOS benchmark\ndropped from 71.5 to 29.5. This demonstrates the effec-\ntiveness of global reasoning, especially in occlusion events.\nSecond, we ablated the consistency loss, which could alle-\nviate the sub-optimal issues in collected expert data. A re-\nmoval of consistency loss leads to a lower driving score and\nhigher collision rate on the Town 05 benchmark and a lower\nsuccess rate on the DOS benchmark. Third, excluding the\ntraffic sign feature from the global reasoning model results\nin an increase on the red light violation. One explanation is\nthat the traffic sign feature from the global reasoning mod-\nule could help reason the distant traffic light state according\nto other road participants’ behavior.\n6. Conclusion\nWe present ReasonNet, a novel end-to-end autonomous\ndriving framework including two major components: a tem-\nporal reasoning module and a global reasoning module. The\ntemporal reasoning module processes the historic informa-\ntion on the driving scene for high-fidelity forecasting of\nother road participants and dynamically maintains a tempo-\nral memory bank. The global reasoning module models the\ninteraction and relationship among the objects and environ-\nment to detect adverse events, especially occlusion, and im-\nprove overall perception performance. Our method pushes\nthe state-of-the-art performance of the CARLA leaderboard\nby a considerable margin. Moreover, we also publicly re-\nlease a new benchmark DOS consisting of diverse occlu-\nsion scenarios, to facilitate the study of occlusion detection\nin the field of end-to-end autonomous driving.\nReferences\n[1] Expert drivers for autonomous driving. url-\nhttps://kait0.github.io/files/master thesis bernhard jaeger.pdf,\n2021. 6, 13\n[2] Oladapo Afolabi, Katherine Driggs-Campbell, Roy Dong,\nMykel J Kochenderfer, and S Shankar Sastry. People as sen-\nsors: Imputing maps from human actions. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems\n(IROS), pages 2342–2348. IEEE, 2018. 6\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 3\n[4] Neal E. Boudette. Teslas self-driving system cleared in\ndeadly crash. The New York Times, 2017. 1\n[5] Jinkun Cao, Xinshuo Weng, Rawal Khirodkar, Jiangmiao\nPang, and Kris Kitani. Observation-centric sort: Rethink-\ning sort for robust multi-object tracking. arXiv preprint\narXiv:2203.14360, 2022. 1\n[6] Sergio Casas, Cole Gulino, Simon Suo, Katie Luo, Renjie\nLiao, and Raquel Urtasun. Implicit latent variable model for\nscene-consistent motion forecasting. In Computer Vision–\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23–28, 2020, Proceedings, Part XXIII 16 , pages 624–\n641. Springer, 2020. 1\n[7] Sergio Casas, Wenjie Luo, and Raquel Urtasun. Intentnet:\nLearning to predict intention from raw sensor data. In Con-\nference on Robot Learning , pages 947–956. PMLR, 2018.\n3\n[8] Raphael Chekroun, Marin Toromanoff, Sascha Hornauer,\nand Fabien Moutarde. Gri: General reinforced imitation and\nits application to vision-based autonomous driving. arXiv\npreprint arXiv:2111.08575, 2021. 6, 13\n[9] Dian Chen, Vladlen Koltun, and Philipp Kr ¨ahenb¨uhl. Learn-\ning to drive from a world on rails. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 15590–15599, 2021. 1, 2, 13, 14\n[10] Dian Chen and Philipp Kr ¨ahenb¨uhl. Learning from all vehi-\ncles. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 17222–17231,\n2022. 1, 2, 4, 6, 13\n[11] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp\nKr¨ahenb¨uhl. Learning by cheating. In Conference on Robot\nLearning, pages 66–75. PMLR, 2020. 2, 5, 13, 14\n[12] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-\nterm video object segmentation with an atkinson-shiffrin\nmemory model. In European Conference on Computer Vi-\nsion, pages 640–658. Springer, 2022. 5\n[13] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethink-\ning space-time networks with improved memory coverage\nfor efficient video object segmentation. Advances in Neural\nInformation Processing Systems, 34:11781–11794, 2021. 4\n[14] Kashyap Chitta, Aditya Prakash, and Andreas Geiger. Neat:\nNeural attention fields for end-to-end autonomous driving.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 15793–15803, 2021. 3, 12, 13,\n14\n[15] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu,\nKatrin Renz, and Andreas Geiger. Transfuser: Imitation\nwith transformer-based sensor fusion for autonomous driv-\ning. arXiv preprint arXiv:2205.15997, 2022. 2, 3, 4, 6, 13\n[16] Kyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gulcehre,\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio. Learning phrase representations using rnn\nencoder-decoder for statistical machine translation. arXiv\npreprint arXiv:1406.1078, 2014. 3\n[17] Felipe Codevilla, Matthias M ¨uller, Antonio L ´opez, Vladlen\nKoltun, and Alexey Dosovitskiy. End-to-end driving via con-\nditional imitation learning. In 2018 IEEE International Con-\nference on Robotics and Automation (ICRA) , pages 4693–\n4700. IEEE, 2018. 2\n[18] Felipe Codevilla, Eder Santana, Antonio M L ´opez, and\nAdrien Gaidon. Exploring the limitations of behavior\ncloning for autonomous driving. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 9329–9338, 2019. 2, 13, 14\n[19] Jiaxun Cui, Hang Qiu, Dian Chen, Peter Stone, and Yuke\nZhu. Coopernaut: End-to-end driving with cooperative\nperception for networked vehicles. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17252–17262, 2022. 6\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 3\n[21] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Anto-\nnio Lopez, and Vladlen Koltun. Carla: An open urban driv-\ning simulator. In Conference on robot learning, pages 1–16.\nPMLR, 2017. 2, 7, 13\n[22] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 6202–6211, 2019. 2\n[23] Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir\nAnguelov, Congcong Li, and Cordelia Schmid. Vectornet:\nEncoding hd maps and agent dynamics from vectorized rep-\nresentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 11525–\n11533, 2020. 3\n[24] Samuel Gibbs. Ubers self-driving car saw the pedestrian but\ndidnt swerve–report. The Guardian, 2018. 1\n[25] Chao Gou, Yuchen Zhou, and Dan Li. Driver attention pre-\ndiction based on convolution and transformers. The Journal\nof Supercomputing, 78(6):8268–8284, 2022. 3\n[26] Junru Gu, Chen Sun, and Hang Zhao. Densetnt: End-to-end\ntrajectory prediction from dense goal sets. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 15303–15312, 2021. 1\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 3, 12\n[28] Anthony Hu, Zak Murez, Nikhil Mohan, Sof ´ıa Dudas, Jef-\nfrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and\nAlex Kendall. Fiery: future instance prediction in bird’s-\neye view from surround monocular cameras. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 15273–15282, 2021. 1\n[29] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi\nYan, and Dacheng Tao. St-p3: End-to-end vision-based au-\ntonomous driving via spatial-temporal feature learning. In\nComputer Vision–ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23–27, 2022, Proceedings, Part\nXXXVIII, pages 533–549. Springer, 2022. 3\n[30] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In International conference on machine learn-\ning, pages 448–456. PMLR, 2015. 12\n[31] Masha Itkina, Ye-Ji Mun, Katherine Driggs-Campbell, and\nMykel J Kochenderfer. Multi-agent variational occlusion in-\nference using people as sensors. In 2022 International Con-\nference on Robotics and Automation (ICRA) , pages 4585–\n4591. IEEE, 2022. 6\n[32] Xiaosong Jia, Li Chen, Penghao Wu, Jia Zeng, Junchi Yan,\nHongyang Li, and Yu Qiao. Towards capturing the tempo-\nral dynamics for trajectory prediction: a coarse-to-fine ap-\nproach. In Conference on Robot Learning , pages 910–920.\nPMLR, 2023. 1\n[33] Xiaosong Jia, Penghao Wu, Li Chen, Hongyang Li, Yu Liu,\nand Junchi Yan. Hdgt: Heterogeneous driving graph trans-\nformer for multi-agent trajectory prediction via scene encod-\ning. arXiv preprint arXiv:2205.09753, 2022. 1\n[34] Jinkyu Kim, Teruhisa Misu, Yi-Ting Chen, Ashish Tawari,\nand John Canny. Grounding human-to-vehicle advice for\nself-driving vehicles. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10591–10599, 2019. 3\n[35] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\nfor object detection from point clouds. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 12697–12705, 2019. 3\n[36] Lingyun Luke Li, Bin Yang, Ming Liang, Wenyuan Zeng,\nMengye Ren, Sean Segal, and Raquel Urtasun. End-to-\nend contextual perception and prediction with interaction\ntransformer. In 2020 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), pages 5784–5791.\nIEEE, 2020. 3\n[37] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-\nhao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer:\nLearning bird’s-eye-view representation from multi-camera\nimages via spatiotemporal transformers. arXiv preprint\narXiv:2203.17270, 2022. 1, 2\n[38] Qing Lian, Peiliang Li, and Xiaozhi Chen. Monojsg: Joint\nsemantic and geometric cost volume for monocular 3d ob-\nject detection. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 1070–\n1079, 2022. 1\n[39] Qing Lian, Yanbo Xu, Weilong Yao, Yingcong Chen, and\nTong Zhang. Semi-supervised monocular 3d object detec-\ntion by multi-view consistency. In Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel, Octo-\nber 23–27, 2022, Proceedings, Part VIII , pages 715–731.\nSpringer, 2022. 1\n[40] Qing Lian, Botao Ye, Ruijia Xu, Weilong Yao, and Tong\nZhang. Exploring geometric consistency for monocular 3d\nobject detection. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pages\n1685–1694, 2022. 1\n[41] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu,\nSergio Casas, and Raquel Urtasun. Pnpnet: End-to-end per-\nception and prediction with tracking in the loop. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11553–11562, 2020. 3\n[42] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,\nHuizi Mao, Daniela Rus, and Song Han. Bevfusion: Multi-\ntask multi-sensor fusion with unified bird’s-eye view repre-\nsentation. arXiv preprint arXiv:2205.13542, 2022. 1\n[43] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas-\ntic gradient descent with warm restarts. arXiv preprint\narXiv:1608.03983, 2016. 12\n[44] Ilya Loshchilov and Frank Hutter. Decoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations, 2018. 12\n[45] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and\nChristoph Feichtenhofer. Trackformer: Multi-object track-\ning with transformers. InProceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8844–8854, 2022. 3\n[46] Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra,\nFlorian Metze, Christoph Feichtenhofer, Andrea Vedaldi,\nand Jo ˜ao F Henriques. Keeping your eye on the ball: Tra-\njectory attention in video transformers. Advances in neural\ninformation processing systems, 34:12493–12506, 2021. 4\n[47] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-\nmodal fusion transformer for end-to-end autonomous driv-\ning. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 7077–7087,\n2021. 2, 3, 7, 12, 13, 14\n[48] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J\nGuibas. Frustum pointnets for 3d object detection from rgb-\nd data. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 918–927, 2018. 3, 12\n[49] Shengju Qian, Hao Shao, Yi Zhu, Mu Li, and Jiaya Jia.\nBlending anti-aliasing into vision transformer. Advances\nin Neural Information Processing Systems , 34:5416–5429,\n2021. 3\n[50] Hao Shao, Shengju Qian, and Yu Liu. Temporal interlacing\nnetwork. In Proceedings of the AAAI Conference on Artifi-\ncial Intelligence, volume 34, pages 11966–11973, 2020. 2\n[51] Hao Shao, Letian Wang, Ruobing Chen, Hongsheng Li,\nand Yu Liu. Safety-enhanced autonomous driving us-\ning interpretable sensor fusion transformer. arXiv preprint\narXiv:2207.14024, 2022. 1, 2, 3, 6, 13, 14\n[52] Peize Sun, Jinkun Cao, Yi Jiang, Rufeng Zhang, Enze Xie,\nZehuan Yuan, Changhu Wang, and Ping Luo. Transtrack:\nMultiple object tracking with transformer. arXiv preprint\narXiv:2012.15460, 2020. 3\n[53] CARLA team. Carla autonomous driving leaderboard.\nhttps://leaderboard.carla.org/ , 2020. Ac-\ncessed: 2021-02-11. 1, 6, 12, 13\n[54] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde.\nEnd-to-end model-free reinforcement learning for urban\ndriving using implicit affordances. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 7153–7162, 2020. 2, 4, 13\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 3\n[56] Petar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio. Graph at-\ntention networks. stat, 1050:20, 2017. 5\n[57] Sourabh V ora, Alex H Lang, Bassam Helou, and Oscar Bei-\njbom. Pointpainting: Sequential fusion for 3d object de-\ntection. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 4604–4612,\n2020. 2\n[58] Letian Wang, Yeping Hu, Liting Sun, Wei Zhan, Masayoshi\nTomizuka, and Changliu Liu. Hierarchical adaptable and\ntransferable networks (hatn) for driving behavior prediction.\narXiv preprint arXiv:2111.00788, 2021. 3\n[59] Letian Wang, Yeping Hu, Liting Sun, Wei Zhan,\nMasayoshi Tomizuka, and Changliu Liu. Transferable\nand adaptable driving behavior prediction. arXiv preprint\narXiv:2202.05140, 2022. 1\n[60] Letian Wang, Liting Sun, Masayoshi Tomizuka, and Wei\nZhan. Socially-compatible behavior design of autonomous\nvehicles with verification on real human data.IEEE Robotics\nand Automation Letters, 6(2):3421–3428, 2021. 1\n[61] Bob Wei, Mengye Ren, Wenyuan Zeng, Ming Liang, Bin\nYang, and Raquel Urtasun. Perceive, attend, and drive:\nLearning spatial attention for safe self-driving. In 2021\nIEEE International Conference on Robotics and Automation\n(ICRA), pages 4875–4881. IEEE, 2021. 1\n[62] Pengxiang Wu, Siheng Chen, and Dimitris N Metaxas. Mo-\ntionnet: Joint perception and motion prediction for au-\ntonomous driving based on bird’s eye view maps. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 11385–11395, 2020. 3\n[63] Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang\nLi, and Yu Qiao. Trajectory-guided control prediction for\nend-to-end autonomous driving: A simple yet strong base-\nline. arXiv preprint arXiv:2206.08129, 2022. 2, 6, 13\n[64] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-\nbased 3d object detection and tracking. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11784–11793, 2021. 3, 6\n[65] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng\nWeng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang\nWang. Bytetrack: Multi-object tracking by associating every\ndetection box. In European Conference on Computer Vision,\npages 1–21. Springer, 2022. 1\n[66] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu,\nand Luc Van Gool. End-to-end urban driving by imitat-\ning a reinforcement learning coach. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 15222–15232, 2021. 2, 13, 14\nA. Implementation Details\nModel DetailsThe feature dimension of all decoders in our\nframework is set as 256. We use Ke = 1, Kbev = 3,\nKopy = 3, CK = 64, CV = 256for the feature dimensions\nmentioned in Sec 3. The feature of the 5th stage in Resnet\nwas used as the feature map fi in the 2D backbone. We\nuse Fully Connected Layer and Batch Normalization [30]\nto construct a simplified version of PointNet [48] to encode\nthe information of raw LiDAR points in the 3D backbone.\nMulti-view Input\nLiDARInput\n2DBackBone\n3DBackBone\nBEVDecodervalue\nquery\nWaypoints\nTrafficsign\nCv×H×W\nPerceptionkey\nBEVMap\nFigure 6. Overview of our pipeline for pretraining the perception\nmodule in the first training stage.\nTraining We train our models using the AdamW opti-\nmizer [44] and a cosine learning rate scheduler [43]. In\nthe first training stage, the initial learning rate is set to\n5e−4 × BatchSize\n512 for the transformer encoder and the 3D\nbackbone, and 2e−4 × BatchSize\n512 for the 2D backbones. The\nweight decay is 0.07. We train the models for 35 epochs\nwith the first 5 epochs for warm-up [27]. We used ran-\ndom scaling from 0.9 to 1.1 and color jittering to augment\nthe collected RGB images. The overview of the first-stage\nframework can be found in Figure 6. In the second training\nstage, we freeze the perception module. The training sched-\nule of the other two modules are similar to that in the first\nstage.\nSensors The RGB images are collected and cropped from\none front-facing camera, two side-facing cameras, and one\nback-facing camera with a resolution of 800 × 600. Each\ncamera has a 100 ◦ horizontal field of view (FOV), and the\nside cameras are angled at 60 ◦. For the front image, we\nscale the shorter side of the front camera input to 256 and\ncrop its center patch of 224 × 224. For the focusing-view\nimage, we directly crop the center of the front camera input\nto get a 128 × 128 patch. For the other images, the shorter\nside of the camera input is scaled to 160 and a center patch\nof 128 × 128 is taken.\nOther hyper-parameter values Some other hyper-\nparameter values used in ReasonNet are listed in Table 8.\nB. Benchmark details\nWe evaluate our method on the CARLA public leader-\nboard [53], Town05 benchmark [47], and our proposed\nDOS benchmark. Adversarial events 3 are included in the\nfirst two benchmarks, and occlusion events are included in\nthe last benchmark. In these benchmarks, the ego vehicle\nis required to complete a given route without collision or\ntraffic rules violation.\nCARLA LeaderboardThe CARLA Autonomous Driving\nLeaderboard [53] is to evaluate the driving proficiency of\nautonomous agents in realistic traffic situations with a va-\nriety of weather conditions. The CARLA leaderboard pro-\nvides a set of 76 routes for training and verifying agents and\ncontains a secret set of 100 routes to evaluate the driving\nperformance of the submitted agents.\nTown05 benchmark In this benchmark, we use Town05\nfor evaluation and other towns for training. Following [47],\nthe benchmark includes two evaluation settings: 1) Town05\nShort: 10 short routes of 100-500m, each comprising 3\nintersections, 2) Town05 Long: 10 long routes of 1000-\n2000m, each comprising 10 intersections. Town05 is a com-\nplex town with multi-lane roads, single-lane roads, bridges,\nhighways and exits. The core challenge of the benchmark is\nhow to handle dynamic dense agents and adversarial events.\nCARLA 42 routes benchmark The CARLA 42 routes\nbenchmark was proposed in NEAT [14], including six\ntowns covering a variety of areas such as US-style intersec-\ntions, EU-style intersections, freeways, roundabouts, stop\nsigns, urban scenes and residential districts. The traffic den-\nsity of each town is set to be comparable to busy traffic set-\nting. We take the same configuration open-sourced by [47]\nwhen we evaluated the methods.\nC. More Experimental results\nIn this section we report additional experimental results,\nincluding the CARLA leaderboard and two other bench-\nmarks.\nC.1. CARLA leaderboard\nTable 5 shows the detailed comparison between our\nmethod and the baselines on the CARLA public Leader-\nboard [53]. Our method also leads the vehicle collision and\noffroad infraction numbers among all the methods.\nC.2. Town05 and CARLA 42 routes\nTable 6 and Table 7 additionally compare the driving\nscore, road completion, and infraction score of the pre-\nsented approach to prior state-of-the-art on the CARLA\n3Adversarial events include unexpected agents rushing into the road\nfrom occluded regions, vehicles running red traffic lights, etc. Please refer\nto https://leaderboard.carla.org/scenarios/ for detailed descriptions.\nRank Method Driving\nScore\nRoute\nCompletion\nInfraction\nScore\nVehicle\nCollisions\nPedestrian\nCollisions\nLayout\nCollisions\nRed light\nViolations\nOffroad\nInfractions\nBlocked\nInfractions\n1 ReasonNet (Ours) 79.95 89.89 0.89 0.13 0.02 0.01 0.08 0.04 0.33\n2 InterFuser [51] 76.18 88.23 0.84 0.37 0.04 0.14 0.22 0.13 0.43\n3 TCP [63] 75.14 85.63 0.87 0.32 0.00 0.00 0.09 0.04 0.54\n4 LA V [10] 61.85 94.46 0.64 0.70 0.04 0.02 0.17 0.25 0.10\n5 TransFuser [15] 61.18 86.69 0.04 0.71 0.81 0.01 0.05 0.23 0.43\n6 Latent TransFuser [15] 45.20 66.31 0.72 1.11 0.02 0.02 0.05 0.16 1.82\n7 GRIAD [8] 36.79 61.85 0.60 2.77 0.00 0.41 0.48 1.39 0.84\n8 TransFuser+ [1] 34.58 69.84 0.56 0.70 0.04 0.03 0.75 0.18 2.41\n9 Rails [9] 31.37 57.65 0.56 1.35 0.61 1.02 0.79 0.96 0.47\n10 IARL [54] 24.98 46.97 0.52 2.33 0.00 2.47 0.55 1.82 0.94\n11 NEAT [14] 21.83 41.71 0.65 0.74 0.04 0.62 0.70 2.68 5.22\nTable 5. Comparison of our method and the state-of-the-art on the public CARLA leaderboard [53] (accessed Nov 2022). Methods are\nranked by the driving score as the main metric. Driving Score, Route Completion, Infraction Score are higher the better, and the other\nmetrics are lower the better. We outperform all other methods by a wide margin. We also lead the vehicle collision, offroad infraction\nnumbers among all the methods.\nTown05 Short Town05 Long\nMethod Driving Score↑ Road Completion↑ Driving Score↑ Road Completion↑\nCILRS [18] 7.47±2.51 13.40 ±1.09 3.68±2.16 7.19 ±2.95\nLBC [11] 30.97±4.17 55.01 ±5.14 7.05±2.13 32.09 ±7.40\nTransFuser [47] 54.52±4.29 78.41 ±3.75 33.15±4.04 56.36 ±7.14\nNEAT [14] 58.70±4.11 77.32 ±4.91 37.72±3.55 62.13 ±4.66\nRoach [66] 65.26±3.63 88.24 ±5.16 43.64±3.95 80.37 ±5.68\nWOR [9] 64.79±5.53 87.47 ±4.68 44.80±3.69 82.41 ±5.01\nInterFuser [51] 94.95±1.91 95.19 ±2.57 68.31±1.86 94.97 ±2.87\nReasonNet (Ours) 95.71±1.88 96.23±3.17 73.22±1.91 95.88±2.31\nTable 6. Comparison of our ReasonNet with six state-of-the-art methods in Town05 benchmark. Our method outperformed other strong\nmethods in all metrics and scenarios.\nWetDawn\nWetCloudyTwilight\nWetCloudyNight\nMidRainNoonClearTwilightHardRainDawn\nFigure 7. Different types of weather in our dataset.\nTown05 benchmark [47] and CARLA 42 routes bench-\nmark [14].\nD. Data statistics\nWe describe the detailed statistics for each town and their\ncorresponding maps in Table 9. In Figure 7, we show six\ntypes of weathers among our dataset. For the submission\nfor the online leaderboard, the model is trained in all eight\ntowns. For the ablation studies, we train the models on five\ntowns (Town01, Town03, Town04, Town06 ,and Town07).\nE. License of Assets\nWe use the open-source CARLA driving simulator [21].\nCARLA is released under the MIT license. Its assets are\nunder the CC-BY license. The pretrained ResNet model is\nunder the MIT license. The source code for our work will\nbe publicly available once accepted and they are under the\nCC-BY-NC 4.0 license.\nMethod Driving Score↑ Road Completion↑ Infraction Score↑\nCILRS [18] 22.97±0.90 35.46 ±0.41 0.66 ±0.02\nLBC [11] 29.07±0.67 61.35 ±2.26 0.57 ±0.02\nAIM [47] 51.25±0.17 70.04 ±2.31 0.73 ±0.03\nTransFuser [47] 53.40±4.54 72.18 ±4.17 0.74 ±0.04\nNEAT [14] 65.17±1.75 79.17 ±3.25 0.82 ±0.01\nRoach [66] 65.08±0.99 85.16 ±4.20 0.77 ±0.02\nWOR [9] 67.64±1.26 90.16 ±3.81 0.75 ±0.02\nInterFuser [51] 91.84±2.17 97.12±1.95 0.95±0.02\nReasonNet (Ours) 93.25±2.91 96.84±2.17 0.96±0.02\nTable 7. Comparison of our ReasonNet with other methods in CARLA 42 routes benchmark. Our method outperformed other strong\nmethods in driving score and infraction score.\nNotation Description Value\nBEV Map and Controller\namax Maximum acceleration 1.0 m/s\nvmax Maximum velocity 7.5 m/ s2\nH, W Size of the BEV map 50, 50\nSize of the BEV area 50 meter ×50 meter\nHb The detection range for the backward of the ego vehicle 20\nScale factor for bounding box size of pedestrians and bicycles 2\nLearning Process\nNumber of epochs 35\nNumber of warm-up epochs 5\nλsign Weight for the traffic sign loss 0.2\nλw Weight for the waypoints loss 0.4\nλBEV Weight for the BEV map loss 0.4\nλopy Weight for the occupancy map loss 0.2\nλconsistency Weight for the consistency loss 0.05\nMax norm for gradient clipping 10.0\nWeight decay 0.07\nBatch size 256\nTable 8. The parameter used for ReasonNet.\nTown Name #Frames Description\nTown01 342846 A basic town layout consisting of “T junctions”\nTown02 197240 Similar to Town01, but smaller\nTown03 469115 The most complex town, with a 5-lane junction, a roundabout, unevenness, a tunnel, and more\nTown04 429979 An infinite loop with a highway and a small town\nTown05 297140 Squared-grid town with cross junctions and a bridge. It has multiple lanes per direction.\nTown06 148495 Long highways with many highway entrances and exits. It also has a Michigan left\nTown07 55299 A rural environment with narrow roads, barns and hardly any traffic lights\nTown10 69039 A city environment with different environments such as an avenue or promenade\nTable 9. Detailed statistics of the number of frames and a brief description of each town."
  },
  {
    "source": "BioNanomics Northeast Indiana FIRST Robotics Charter at MIE Electric Works.docx",
    "content": "Jump to Overview  First Robotics at BioNanomics \nIf you don’t know what FRC is click here\nVEGM-6ME7  \nClick here to join: https://www.sportsyou.com/jo\nin/?signupCode=VEGM6ME7\nMentor Bridge Indiana FRC Getting Started Guide 2024-2025\nThis is a log of activity related to first Robotics in Northeast Indiana. Standing meeting every Wednesday from 4:30pm to 6:00pm on Microsoft Teams \n \nIf you have any questions, send Doug Horner a txt message: 260-413-6587 (abrown@mieweb.com)\nIf you’d like to add something to the agenda, suggest an edit here:\nNEXT Meeting Suggestion (add your name and idea)\nFebruary 12, 2025\nDrew: JustinMobile App Instructions\nChris O:\nEvent 0 on Saturday Osborne suggested streaming the Week Zero event from New Hampshire, which could be an interesting activity for the visiting teams\nOsborne announced a call on Tuesday February 25th for new coaches and teams to walk them through the competition process, answer questions, and help them understand the schedule and procedures\nChris E - Is thinking about moving element to practice field on the 18th\nChris E - Fort Wayne event discussion\nMaddy: \nImportant Blog - https://community.firstinspires.org/2025-getting-ready-for-your-first-event\nMost Important: Run through the inspection checklist ASAP! https://firstfrc.blob.core.windows.net/frc2025/Manual/2025FRCInspectionChecklist.pdf\nDoug: Time of flight camera \nFeb 5, 2025\nVideo of the Setup\n \nEW Field Update:\n\nYesterday three FRC teams came to set up the field – minus the 2025 WOODEN game pieces. This is the next step.\n The three teams agreed on the following:\n1501 (Huntington) = full reef with 2 official pipes plus official April tags.\n1501 (Huntington)  = 1 coral feed station\n1501 (Huntington)  = 1 algae processor\n1501 (Huntington) = (we have official cage on last years stage)\n9431 (Snider) – agreed to build 1 more coral feed station.\n4982 (Homestead) – agreed to build ½ a barge for the CAGE.\nJan 29, 2025\nJUSTINMOBILE APP ACCESS\nPhone # \nEmail\nMady Stricker - (260) 519-3532, mady.stricker@roboboosters.org\nAndrew Shipe - (260) 415-1635  andrew.shipe@fwcs.k12.in.us\nCasey Bushey - 260-310-4671 cbushey@iu.edu\n\nJan 22, 2025 <- Cancelled\n\nChris E: 1501 will be up at the EW just layout  and setting up the border this Sunday at 1:30pm. We will schedule another day to setup the WOODEN elements later.\n\nChris E: Ross one of my students need these parts to setup Cheesy Arena field computer: \n\nRoss Cuttriss one of my students, an IT expert on the team, Home Assistant expert, server guru, linux kid... Worked this FALL on setting up the Cheesy Arena so that you can simulate a match on the field. AKA, FMS (Field Management System)\nHere is a link to the source:\nhttps://github.com/Team254/cheesy-arena\nhttps://github.com/Team254/cheesy-arena-lite\nhttps://www.chiefdelphi.com/t/running-cheesy-arena/472681\nHe is requesting a couple of parts he doesn't have below:\nMeaning, he has the rest of the system with PCs and what not. He had it running in the shop in the fall at one point.\nHere are the two items he is requesting.\nA managed switch, so he can setup 6 VLANS, each robot has it's own.\nhttps://www.amazon.com/dp/B0854P8FZY?ref=cm_sw_r_apin_dp_JDACWE6XNTXEKNZJJMX7&ref_=cm_sw_r_apin_dp_JDACWE6XNTXEKNZJJMX7&social_share=cm_sw_r_apin_dp_JDACWE6XNTXEKNZJJMX7&language=en-US&newOGT=1\nand he needs the robot access point:\nhttps://wcproducts.com/products/wcp-0395?variant=45475155411156\n\n\n\n\nChris E: Trine would like to buy pizza on a day most of the teams will be practicing on the full field at electric works.\nMatt Liberatore <mwl990103@gmail.com> is a professor there and the father of one of my students to coordinate that.\nJohn Liu <liuj@trine.edu> is my normal contact there, Yamaha donates robots to his program. https://www.linkedin.com/in/johnchanghongliu/\n\nChris E: I don’t know what has more traction, this meeting or SportsU, but here is our contribution post to the FULL field at Electric Works in progress.I suggest to coordinate “real” time with other teams and coaches, it might be best to be signed up there as Doug and John suggested. We are finishing the full REEF we decided. Our CAGE hang is not spec. And not shown is the Coral Feed Station. It’s 3 individual units.\n\n\nChris E: If you need a robot part “ROOKIE TEAM” ask me, we might have it. We have too much Junk at Team 1501 shop. Happy to help. Just ask if something is out of stock and you need it.\n\nChris E: 4:30pm is rough for me to make the meeting because my team is meeting during that time frame. So just message me if you need anything or I will leave trails here in this document.\n\nChris E: list of teams that could be using the field at EW this season. 8 teams total\n\nhttps://www.thebluealliance.com/team/1501/2025 https://www.thebluealliance.com/team/4982/2025 https://www.thebluealliance.com/team/9119/2025 https://www.thebluealliance.com/team/9431/2025 https://www.thebluealliance.com/team/10172/2025 https://www.thebluealliance.com/team/10332/2025 https://www.thebluealliance.com/team/10411/2025 https://www.thebluealliance.com/team/10434/2025\nJan 15, 2025\nhttps://www.instagram.com/reel/DE3EyJfR_4o/?igsh=N3B4ZGphaGw0Nnpn \nChris O: Consider: resources | Everybot Docs\nThis is ~1000 competitive bot made from local parts.\n \n1741 - field coordination https://www.chiefdelphi.com/t/5484-9554-half-field-2025-complete/479080 \nCasey: Cages go back on sale the Jan 17th(2days) from Andymark, have you already ordered them for the field?\nNick Adams (South Side): We have a student or two that would like to pitch Doug for a few parts - please text me at 260-415-4133 when you’re ready for them to join the call\nGenerated by AI. Be sure to check for accuracy.\nRecording \nMeeting notes:\nTeam Introductions: Doug, Amal, Michelle, and David Ebersol introduced themselves and discussed their respective teams and locations. Doug welcomed David to the meeting and appreciated his feedback from the previous session.\nPractice Field Setup: Doug and David Ebersol discussed their practice field setups, including the materials used and the challenges faced. Doug mentioned renting space at Electric Works for competitions and the involvement of students in welding with Union members.\nField Setup: Doug and David Ebersol discussed their practice field setups. David mentioned their field is built with a waterjet CNC router and aims to have a full half-field ready by the Sunday after kickoff. Doug shared that they are waiting for parts and have recently taken possession of their field.\nMaterials Used: David mentioned their field is currently made of wood, with plans to remake parts using carbon fiber. Doug shared that they have students welding with Union members to build bent pipes for the field.\nSpace Rental: Doug mentioned renting space at Electric Works for competitions and allowing students to pick days to come in and run the field. He expressed concerns about the height and width of the space but plans to push the field against the wall to maximize space.\nTeam Collaboration: Doug and David Ebersol talked about collaborating with other teams, including 1741 Red Alert, and the benefits of sharing resources and ideas. Chris offered to introduce Doug to the Red Alert team.\nCollaboration: Doug and David Ebersol discussed collaborating with other teams, including 1741 Red Alert. David mentioned that Red Alert has a similar setup with a narrow corridor and is efficient in their field setup. Chris offered to introduce Doug to the Red Alert team, highlighting their willingness to help and their status as a powerhouse in Indiana.\nResource Sharing: David shared that they host a scrimmage called NERDS, where about eight teams from the area come together to play and share ideas. Doug invited David's team to use their space and expressed interest in sending someone to their scrimmage.\nField and Equipment Updates: Doug shared updates on the field and equipment, including the arrival of the field parts and the installation of the carpet. He also mentioned the involvement of students in the welding process.\nTeam Leadership and Budgeting: Doug discussed the importance of student leadership and budgeting with Chris and David Ebersol. He encouraged coaches to let students pitch their ideas and manage the budget.\nStudent Leadership: Doug emphasized the importance of student leadership and encouraged coaches to let students pitch their ideas and manage the budget. He shared that he instructed rookie coaches to consider themselves as board members and have student leadership pitch their ideas for purchases and budget management.\nBudget Management: Doug shared that he has one-on-one meetings with coaches to discuss budgeting and uses the Mentor Bridge content to guide them. He encouraged Michelle to have her students put together a budget and pitch their ideas to him for funding.\nTeam Culture and Commitment: Chris emphasized the importance of building a team culture and commitment, comparing it to athletic teams. He highlighted the need for students and teachers to understand the level of dedication required for a successful robotics team.\nEverybot and Kit Bot: Chris and David Ebersol discussed the Everybot and Kit Bot options for teams, including the benefits and challenges of each. Chris mentioned the importance of focusing on organization and planning for successful implementation.\nKeeping Students Engaged: Chris shared strategies for keeping students engaged and busy during the build season, including involving them in various tasks and projects. He also mentioned the importance of providing a safe and supportive environment for students.\nTeam Support and Resources: Doug offered support and resources to Michelle and her team at Carol High School, including financial assistance and access to materials. He encouraged Michelle to involve her students in budgeting and planning.\nFinancial Assistance: Doug offered financial assistance to Michelle and her team at Carol High School, committing to provide an additional $6000 from his fund. He also mentioned other companies that could provide funding and encouraged Michelle to have her students put together a budget and pitch their ideas for funding.\nAccess to Materials: Doug offered to loan materials and tools to Michelle's team and suggested creating a list of needed items. He also mentioned the possibility of using resources from Electric Works and collaborating with other teams for materials.\nOvercoming Challenges: Michelle shared the challenges her team faces, including obtaining materials and building leadership skills among students. Doug and David Ebersol offered advice and support to help her overcome these obstacles.\nMaterial Acquisition: Michelle shared that her team faces challenges in obtaining materials due to the approval process at her school. Doug and David Ebersol suggested using fast-shipping suppliers like McMaster and AndyMark and considering open purchase orders to streamline the process.\nBuilding Leadership: Michelle mentioned the difficulty in building leadership skills among her students. Doug and David Ebersol advised assigning specific tasks to students to help them learn and grow, and emphasized the importance of cultivating leadership skills over time.\nFuture Plans and Collaboration: Doug and David Ebersol discussed future plans for their teams, including the use of new technologies like Limelight and Jetson AI boards. They also talked about the importance of learning and collaborating with students.\nFollow-up tasks:\nField Setup: Reach out to team 1741 Red Alert for advice on setting up the field in a narrow space. (Doug)\nMaterial Procurement: Create a list of materials needed for the robot and submit it for procurement. (Michelle)\nStudent Leadership: Designate a student to manage the budget and prepare a pitch for additional funding. (Michelle)\nMentor Coordination: Set up a chat for mentors to facilitate borrowing and lending of tools and materials. (Pablo)\nBudget Planning: Prepare a detailed budget for the team and present it to Doug for potential funding. (Michelle)\nVideo Documentation: Assign a student to create YouTube shorts documenting the team's progress and activities. (Michelle)\n\nJan 8, 2025\nAgenda \nDoug: quick presentation on developing student leadership and communication plan\nReview Chris notes\nReview Doug Notes\nNick Adams - South Side - we are available for the 1:1 coaching session between 3:30 and 6pm on Wednesday 1/8/25. You can reach out to me at nicholas.adams@fwcs.k12.in.us  Also, we’d like to request the following parts: https://docs.google.com/spreadsheets/d/1GaMjXMhXdU-WAZ1VmfQqKoLGmKtMXcwvogCaF3I3Zr4/edit?usp=sharing\nContact Information\nMady Stricker Indiana MentorBridge Co-Founder, Mentor for 3494, Alumni of 1501\nMichelle Brenner Carroll High School head coach\nDave Ebersol Career Academy South Bend Head Coach\n\nAlge (Balls) ? https://playground.epicsports.com/prod/137055/index.html\nhttps://www.chiefdelphi.com/t/discovery-multiple-versions-of-the-algae-ball-now-with-poll/479257/19 \n\nNotes from Chris E:\nLow Cost Field Wooden Planning:\n1501 is planning on building the following items which we will use during the season in our shop from week 1,2,3,4, then transport about week 5,6,7 to the electric works for the full field.\n1501: QTY (1) Barge, our barge will have an official RED cage\nhttps://firstfrc.blob.core.windows.net/frc2025/FieldAssets/2025TeamElements-TE-25200-Barge.zip \nWe ordered the cage from Andymark at kickoff:\nhttps://www.andymark.com/products/frc2025-am-5524 \nI tried to order a \"NET\" but I waited to long and it's out of stock now\nhttps://www.andymark.com/products/frc2025-am-5522 \n1501: QTY (1) Reef with official pipes (note, they are not instock yet)\nhttps://firstfrc.blob.core.windows.net/frc2025/FieldAssets/2025TeamElements-TE-25350-Pipe-Base.zip \nOrder link for pipes when the come into stock\nhttps://www.andymark.com/products/frc2025-am-5523 \n1501: QTY (1) Reef with lowcost pipes, here is the build sheet to 3d and make\nhttps://drive.google.com/drive/folders/10f65VqIGIpw-AIX_nJelyWTBZQZnUs5x \n1501: QTY (1) of the coral stations\nhttps://firstfrc.blob.core.windows.net/frc2025/FieldAssets/2025TeamElements-TE-25000-Coral-Station.zip \n1501: QTY (1) of the Processor stations\nhttps://firstfrc.blob.core.windows.net/frc2025/FieldAssets/2025TeamElements-TE-25100-Processor.zip \n1501: QTY (1) FULL Field of April Tags\nThese are over $600 dollars so when the season ends, we would like them back.\nhttps://www.andymark.com/products/reefscape-apriltag-sticker-packs \nSo that leaves the following. (I am not saying we need all of this but this is what is left)\nhttps://firstfrc.blob.core.windows.net/frc2025/FieldAssets/2025FieldDrawings-FieldLayoutAndMarking.pdf \nBarge FULL Field: Need Total 6: 5 short\nReef FULL Field: Need Total 12: 10 short\nCoral Station FULL Field: Need Total 6: 4 short\nProcesser FULL Field: Need Total 2: 2 short\nJan 5\nConsider using https://notebooklm.google.com to upload the rules and ask it questions \nC++ deep dive on Swerve Git: Phoenix6-Examples from Phoenix 6 Documentation - CTR Electronics: New for 2024\nJan 4 - Kickoff at Homestead\n2025 Kickoff – REEFSCAPE presented by Haas  - just the reveal\n2025 Field Tour Video: Barge\nThis is how you should run your brainstorming sessions:\nFAIL FAST - FAIL CHEAP\nGet your documentation going:\nhttps://www.chiefdelphi.com/t/2025-unofficial-frc-control-system-diagrams/476263\nOpen source diagrams:\nhttps://github.com/stefacep/2025-FRC-Control-System-Diagrams\nWhat a lot of Good Shorts:\nNews story about First WMUR-TV\nhttps://www.instagram.com/ctrelectronics/\nDec 22, 2024\nDoug\nContributed Leadership and Scrum to Indiana Mentorbridge: Getting Started 2024-2025\nBuilt allwpilib and fixed a bug https://github.com/wpilibsuite/allwpilib/issues/7576\nResearched how to bind either deno or bun to there could be a javascript version\nDec 18, 2024\nChris : Team 1501\nI just want to make sure if the field is showing up Jan 7th week, that we are scheduled to have the carpet installed before then.\nA couple of my CAD mentors and CAD students wondered if they can lend a hand to help you get your CNC running so that we and other teams can use it for prototype cutting of lexan and wood.\nWe might want to look at lighting above the field, do you have interns that can hang up lights?\nI think you said access to the space will be through our phones, will we need special training for that? Will it only be the coaches I assume who have this access key?\n\tMady: Cameron - 4982:\nDiscuss coordinating Homestead & MIE kickoffs\nSnider Robotics 9431–Has student volunteers to help assemble the arena when it comes in, we plan to bring students to MIE on January 4, 11:30 am arrival time.  We will collaborate until approx. 2 pm.--parent pick up.\nWe are also having problems with our Colson Hi-Tech 4 x 1 ½ wheels -screws are stripping out of the wheel when we run it? Any ideas????\nDec 11, 2024\nhttps://www.pfw.edu/etcs/pfw-first-lego-league-northern-indiana-semi-state-tournament\nCanterbury High School FTC Robotics Site\n3 Qualifying Tournaments - \nThis is our main competition season and each tournament will have its own set of awards and winners.  We will go through interviews with our engineering notebook as well as having our robot compete on the field. If we do well, we might qualify for the state tournament.\n12/7/24 - Saturday - Tournament 1 - Perry Meridian HS, Indianapolis - We will leave from the Canterbury Smith Campus parking lot at 6am, and probably return by 7pm. \n1/26/25 - Sunday - Tournament 2 - Battle Ground MS, West Lafayette - we will have a school van for this tournament.\n2/22/25 - Saturday - Tournament 3 - Avon HS, Avon, IN - We will have a school van for this tournament.\n2024 MIE Field Purchase Needs - nice updates with details for all the teams\nDec 4, 2024\nAttendees: Doug Horner\nHost, presenter, me\nAshley Horner\nCameron Elder gmail.com\nCasey Bushey\nChris Elston (1501)\nJ. Todor Snider 9431\nKim\nMady Stricker\nMelody Barnhart (Snider 9431)\nNoah Horner\nPablo Herrera\nWilliam J. Jacobs (Northrop High School)\nRebekah Randall (Canterbury FTC 22331)\n\n\tMady: Northeast Indiana Educational Day/Forums\nInspired by Purdue FIRST Forums: https://forums.firstindianarobotics.org/purdue-first-forums-2024\nInterest from mentors?\nSet date and time\nDiscuss stations/topics\nFood plan?\nCameron, Pablo, Casey, and Andrew have already said they can help\n\nChris : Team 1501\nCommunication Method about collaboration of field pieces.\nhttps://sportsyou.com/ \nhttps://www.teamapp.com/ \ngroupme\nCommitment of field pieces from who?\nReminder to PRE-ORDER things you need now\nExtra lighting for the FRC field.\nDoug\nFLL at Purdue \nField Area\nSafety Day (Checkout)\nProcedure Document\nJohn\n2024 FWCS Purchase Needs\n\nChris: Open Source Limelight: https://github.com/PhotonVision/photonvision/\nOrdered 4\nNov 20, 2024\nAttendees: Doug, Raj, Amal, Andrew Shipe, Becky William, David Liu, John Todor, Mady, Stricker, Matt Schiebel, Pablo, Nick - SouthSide\nMaddy: Region 8 STEM Ecosystem: https://www.r8esc.k12.in.us/stemecosystem#\nFill out membership form to get NE Indiana STEM newsletters\nAdvisory committee working on furthering STEM awareness and accessibility for local students (Mady is an Advisory Board member) - I believe they are still accepting advisory board members\nDoug: Update on EW test field\nRebekah Randall\nComputer Science and Mathematics\nCanterbury High School. I was talking to Dr. Cast who mentioned that you have some really neat robotics/build initiatives. It would be awesome if you had anything we can be involved with or help out with. We do a lot of volunteering so if you ever need someone to demo a robot or even just show up and move tables, the team can also help there. We just volunteered at Bridge of Grace Ministries last week, helping out with an after school program for K-3 students. Fun times!\nHere is a link to our team website (which the students are working on updating). \nhttps://sites.google.com/canterburyschool.org/robotics/home\nOct 30\nAny coaching updates?\nRookie Build photos: RookieBuild \nDoug:\nThanks to everyone last week! Wow\nUpdate on space and field. Now around Jan 6\nPablo helping with getting mill\nPlasma table news \nRobot arm arrived. This is for people who want to experiment with multiple degrees of control\nhttps://www.sparkfun.com/products/24045\n\nOct 24\nYay!  Great Rookie Build\nOct 23\nChris: I won’t be able to attend this meeting, I will be traveling, however just look at the agenda below and ask questions about anything: 260-224-3657\nChris: Doug did you call and talk to Jim Bradley about carpet install?\nDoug Yes.  I told him we were in a holding pattern until we get a date on the field\nDoug: xkcd sucks: Comic 730: Feel The Electricity \nDoug: Any questions before the crazy day! Oct 24, 2024 Rookie Build\nDrew confirmed lift will be available\nThanks Chris for loading tools on Saturday\nKim has 30 pizza's coming\nAgenda:https://yrginc.box.com/s/g1g1hd8j9jkl3y3j7h850fthzxuypzwf \t\nEvent Registration (Responses)\nhttps://youtu.be/iJeRPnqius8?si=yaPG1ZUzpLEM5pJB \nRookie Quickbuild Presentation 1.pptx\nhttps://spectrum3847.org/recommendedreading/ \nhttps://docs.google.com/spreadsheets/d/e/2PACX-1vRrVSDGyXohskEeyrR55KoGQUvYNjl9Zo43vQFg0COnvTA8K_2GulNoBeDd06w57KsDURTCN6ZJEdcn/pubhtml?gid=2111644476&single=true \nOctober 24th, 2024: 4:30pm - 9:00pm (rev004)\nPre-Event Park in the Swinney Lot for School Busses Recommend. See Map Below\nEstimated cost will be $5 to $6 for parking when you leave via credit card.\nPlease BRING Safety Glasses for everyone on your team. Wear Closed Toed Shoes\n.\n3:30-4:15 PM Volunteers arrive, put up some signs, setup tables, and bring in rookie kits.\nHelp MIE Setup Tables, TV or Projector.\nSetup should include 8 tables, 4 mechanical and 4 electrical tables\nSetup a projector or TV for Speakers with HDMI input.\nDo we need chairs? Maybe a few for the adults to sit while students work.\nStand by doors and guide people to the 5th floor around 4:15 PM to 4:30 PM\n4:30 PM Rookie Teams Arrive: Estimate Total for Pizza = 80\nCarroll = 7, Northrop = 25, South Side = 10, Wayne = 7\nVolunteers = 27\n4:30 PM Arrive on the 5th Floor at MIE Electric Works (Be Prompt)\nWelcome and Introductions by – Mady Stricker, Chris Osborne, and Doug Horner\nInstructions Given by Mady Stricker and Chris Osborne\nBreak up your Rookie Team into two groups.\nCOACHES: please make sure you talk to your kids before coming which group they want to be in. Have that already decided helps.\nGroup 1: Mechanical, will assemble the rookie chassis with tools.\nGroup 2: Electrical, will wire the electrical board for the robot.\nAt the end of the night, hopefully, the electrical group and mechanical group will\ncombine to do integration of the whole robot.\n5:00-6:30 PM Group 1: Assembly Time for Robot Base\nGroup 2: Wiring Time for Robot Panel\n6:30-7:00 PM Pizza Break\nSpeaking by Mady Stricker and Chris Osborne about Indiana FIRST, events\nQuestions and Answers by Teams?\n7:00-8:45 PM Integration and final assembly and wiring\n8:45-9:00 PM Final Questions and Answers by Teams?\n9:00-9:30 PM Volunteer Clean-Up\nRecommended Volunteer Staffing? (Wear your T-Shirt!)\nEvent Coordinator = Chris Elston, 1501, chris@elstonsystems.com (cell phone 260-224-3657)\nEvent Speaker = Mady Stricker, Indiana Mentor Bridge/ 1501, mady.stricker@gmail.com\nEvent Speaker = Chris Osborne, Indiana FIRST, cosborne@indianaFIRST.org\nEvent Speaker = Doug Horner, CEO MIE, horner@mieweb.com\nFood Coordinator = Kim Horner, khorner@med-web.com\nSocial Media, Pictures, Videos = Ashley Horner\nMechanical Chassis Lead Problem Resolutions = Chris Elston, 1501, chris@elstonsystems.com\nElectrical Board Lead Problem Resolutions = Joyce Sitton, 1501, joycesitton34@gmail.com\nHelp Teams with Directions at Load IN at the Door = Scott Thorn, 1501, sthorn@phdinc.com\nSetup Help Arrive Early = Cameron Elder, 4982, camelder0@gmail.com\nSetup Help Arrive Early = Abbey Stettler, 1501, abbeystettler@outlook.com\nSetup Help Arrive Early = Scott Thorn, 1501, sthorn@phdinc.com\nSetup Help Arrive Early = Chris Elston, 1501, chris@elstonsystems.com\nSetup Help Arrive Early = Snider Volunteers, 9431\nSetup Help Arrive Early = North Side Volunteers, 9119\nAdditional Snider Members (unknown names) double up – fill in below.\nAdditional North Side Members (unknown names) double up – fill in below.\nCarroll\nMechanical Chassis = Cameron Elder, 4982, camelder0@gmail.com\nElectrical Board = Tim Sitton, 1501, sitton323@gmail.com\nAdditional = Snider Volunteer, 9431\nAdditional = North Side Volunteer, 9119\nNorthrop\nMechanical Chassis = Jason Couch, 1501, jdcouch13@icloud.com\nMechanical Chassis = Dominic Liberatore, 1501, dwl87@proton.me\nElectrical Board = Ross Cuttriss, 1501, ross.cuttriss@icloud.com\nAdditional = Snider Volunteer, 9431\nAdditional = North Side Volunteer, 9119\nSouth Side\nMechanical Chassis = Andrew Lund, 1501, Andrew.rlund@yahoo.com\nElectrical Board = Sharon Lund, 1501, ssitton323@gmail.com\nAdditional = Snider Volunteer, 9431\nAdditional = North Side Volunteer, 9119\nWayne\nMechanical Chassis = Abbey Stettler, 1501, abbeystettler@outlook.com\nElectrical Board = Joyce Sitton, 1501, joycesitton34@gmail.com\nAdditional = Snider Volunteer, 9431\nAdditional = North Side Volunteer, 9119\nFAQ:\nDo we need to bring anything?\nPlease bring SAFETY GLASSES for everyone if you can.\nYou will be taking home a robot drive base about 27” wide by 34” long and a tool bag.\nSo have room on your bus for that if you can.\nWill there be Food?\nMIE and Doug Horner will be providing Pizza for everyone. If pizza does not work for your diet, there are other\noptions in the Electric Works food court, but you will need to purchase those.\nWhere do I park?\nWe recommend the Swinney Parking Lot, there will be approximately a $5-6 fee for parking when you leave\nwith a credit card. Please see rates below.\nCan I show my students what we are building?\nYes, there is a YouTube video here you can watch for the assembly instructions.\nhttps://youtu.be/BvyrWd4fOi4?feature=shared\nWe will pre-print the manuals as well, but you can download that here ahead of time if you like.\nhttps://cdn.andymark.com/media/W1siZiIsIjIwMjQvMDEvMDkvMTQvMTYvNTQvMTAyMTAwY2UtOTcz\nYi00ZTA2LTg3NTMtMGM0MDA4OTQxYWZmL0FNMTRVNV9Vc2VyR3VpZGVfSzI0RGVjMjAyMy5w\nZGYiXV0/AM14U5_UserGuide_K24Dec2023.pdf?sha=17d7bb8604f367c9\nWhere do I go for resources on wiring if we don’t finish at the Rookie Build?\nHere is the place for everything ZERO to ROBOT:\nhttps://docs.wpilib.org/en/stable/docs/zero-to-robot/step-1/intro-to-frc-robot-wiring.html\nSpecial Requirements?\nWear closed toed shoes, this is a requirement at all robot events because tools dropped on toes HURT.\n\n\nOct 16\nChris: 1501 would like to load in early items for rookie build Saturday if possible?\nChris: Doug did you call and talk to Jim Bradley about carpet install?\nChris: I emailed proposed agenda for Volunteers, you can view here:\nFinal Agenda: https://yrginc.box.com/s/g1g1hd8j9jkl3y3j7h850fthzxuypzwf\nAny questions about this, do you want to review real quick?\nMentorBridge Rookie Getting Started Guide: Getting Started 2024-2025 (Mady)\nOct 9\nAndrew - Northrop\nDave C. \nDavid Liu - Purdue Fort Wayne, Future Homestead \nJames McKain - Vol\nFisher,Dennis <Dennis.Fisher@fwcs.k12.in.us>;  Northside  Mr. Hartman\nMady Stricker - Bloomington 3494 - FIN Ulim of 1501\nhttps://linktr.ee/mentorbridge \nNick Adams - South Side\nPablo Herrera - Fort Wayne Metals (volunteer)  11 years for First  4091!  Mexico too!\n2 Championships!\nSherif Elfayoumy - Purdue Fort Wayne, Dean of ETCS\nDan Reese - South Side\nAgenda:\nNew people - Biology is Just Nanobots - iGEM Primer\nTalk about https://linktr.ee/mentorbridge\nDavid Liu: Kaggle Challenge (kaggle.com )\nPablo - Small workshops for mentors. : Create confidence in every mentor about the different areas in FRC. Hardware, Software, and Marketing  1 day. Different stations that you create and learn for 1 hour.\nPre-order Game pieces!  Don't order full field set. - would recommend 9 to be able to test autonomous https://community.firstinspires.org/pre-orders-for-2025-game-pieces \nBlog: https://community.firstinspires.org/topic/frc \nTest Kit:\nhttps://www.pololu.com/product/4022 \nOct 2\nFRC Field Update - Delivery time after Kickoff - c.elston (end of January)\nFRC Field Update - Carpet appointment thursday 4:30pm\nPeople so far: Registration Questionnaire\nEvent Registration (Responses)\nDoug made another post promoting the Registration\nhttps://www.linkedin.com/feed/update/urn:li:activity:7247374463313321984/\nDoug txt Matt Shiebel to ask about coordinating with the coach.\nRookie Coaches\nWilliams,Becky <Becky.Williams@fwcs.k12.in.us>; \nShipe,Andrew <Andrew.Shipe@fwcs.k12.in.us>;\nJacobs,William <William.Jacobs@fwcs.k12.in.us>;\nSchmitz,Andrew <Andrew.Schmitz@fwcs.k12.in.us>;\nAdeosun,Francisca <Francisca.Adeosun@fwcs.k12.in.us>;\nAdams,Nicholas <Nicholas.Adams@fwcs.k12.in.us>;\nReese,Daniel <Daniel.Reese@fwcs.k12.in.us>;\nBrenner, Michelle <Michelle.Brenner@nacs.k12.in.us>\n\nExisting Coaches:\nFisher,Dennis <Dennis.Fisher@fwcs.k12.in.us>;\nTodor,John <John.Todor@fwcs.k12.in.us>;\nMatt Elder <melder@sacs.k12.in.us>\nDoug invited Sherif Elfayoumy to the meeting\nSept 25\n2:30-3:30 Demo - Southside visit by Huntington for Nick Adams\nLego League for Brentwood\nhttps://education.lego.com/en-us/shop/middle/\nhttps://my.firstinspires.org/Dashboard/\nEmailed tthompson@indianafirst.org \nDoug and all,\nI've included Trisha Thompson our FIRST LEGO League Challenge Program Manager.  Any program questions, etc. can be directed towards her.  There are some startup grants out there for FLL, of course in the future the school district can apply for the DOE K-12 Robotics Grant to support teams as well.  \nSponsorship fundraising is a great way to fundraise for teams.  FLL Challenge is always most expensive year 1 and then the price goes down because you have the robot and you have the table.  Sometimes teams buy a dedicated chromebook for the team as well, but often many use one the school or classroom already has.\nThe Don Wood Foundation is another great place for startup as will be AEP and BAE Systems as they support multiple levels of FIRST, not just the high school.\nPlease keep Trisha in the loop as she will be able to provide season information, etc.\nThank you for helping us provide more youth access to FIRST!\nChris Osborne\nSep 18, 2024\n\nNeed to chat about Rookie Build Planning with horner@mieweb.com - C Elston\nOct 24 from 4:30-9pm - https://www.andymark.com/products/frc-intermediate-starter-bundles \nSetup at 3pm\n4 teams - 10 people/tem\n50 Estimated - \nPizza for Dinner\nCall for Mentorship\nHydraulic Lift Access\nDoug to talk to Canterbery: Rebecca Randel\nSep 4, 2024\nMatt Schiebel / FWCS \nWebex premeeting over robotics with the new schools\n(Wayne, South Side, Carrol, Northrop) \nThe idea is Thursday Late October around 4:30 to give time for recruitment. Snider and other veterans from FIRST will help out with the rookie build. (Teach don’t touch, not allowed to build the robot, only the rookies) \nList of potential grants HERE (see below - c.elston)\nChris Elston bullet points:\nOkay here is a summary of our phone conversation and the meeting I had with Mr. Horner from.\n Dart Throw about End of October to hold the Rookie Build\nDoug agreed to host the 4 ROOKIE teams, 3 from Fort Wayne Community, and Carrol High School. There might be a 5th?\nShooting for a Thursday Evening, for me Oct 17th works the best, Oct 24th is doable, but I will be on a travel trip returning Wednesday night Oct 23.\nTiming wise, Team 1501 are all mentors who have jobs, our “preference” we be, we would like to start about 4:30pm and go until say 9pm.\nWe can do the ENTIRE SCHOOL DAY, like you suggested on a Thursday, but I might be able to get all my 1501 mentors to help, so leaning on kids and mentors from schools from Homestead, Snider and Northside would be a requirement if we plan for the school day. I would be OKAY taking the day off from work on Either Oct 17 and or Oct 24 to teach this during the day.\nYou will preorder the drive train kit from AndyMark and have it shipped to Team 1501.\nYou will pre-order “tool kits” from Lowes for us to pick up and give to each team, so there is no need for us to provide tools for the team.\nTeam 1501 will bring our “general store” and tool box just in case.\nThe rookie drive kit will come with an “RC” controller, so there will be no need for Software programming, the general flow will be mechanical kids to build the drive base, and some electrical kits building the electrical panel board.\nThis is NOT the same drive chassis the rookie teams will receive at Kick off. They get the official one then, so technically they will have two drive kits when they start in January.\nDepending on timing, you have slides that can be reviewed to “take” breaks and go over some basics with the teams, this format flows nicer for rookie builds that are scheduled all day at a school. We might not have time to do all that if we do the 4:30-9:00pm timing.\nWe OWE Matt Schiebel a list of ROOKIE GRANTS. Right now, you are saying only the NASA grant. Each team needs to submit for the NASA grant which will get them funds for the next two years. Below is a list of additional grants I do for Team 1501, they should also do.\nMatt Schiebel will help us coordinate a BUS to get the kids from the 3 Fort Wayne schools to the AMP LAB school, which is located on the Electric Works campus and next door to the new FRC field that MEI had sponsored for all the North East Indiana FRC teams! Thank you Mr. Doug Horner!\nThe lease for the space won’t be open until October, we will probably only have CARPET DOWN, and not the Field Perimeter. The field perimeter won’t be delivered by AndyMark until December. Both Carpet and Field are on order.\nMr. Horner said that he can host the build in his OFFICE SPACE on the 5th floor of Electric Works if the FRC building is not ready by October.\n  Matt Schiebel ALL OF YOUR ROOKIE TEAMS need to complete this GRANT here by September 30th!!!\nNASA GRANT\n https://robotics.nasa.gov/2024-2025-frc-sponsorship-grants/\n Other GRANTS that are not ROOKIE GRANTS but the teams should apply for now. All teams can apply for these!\n BAE Systems GRANT – DUE Sept 13th!!!!\n https://survey.vovici.com/se/0C8A91250CA6941F\n RTX Raytheon Grant\n https://web.cvent.com/survey/574800ff-6828-4998-8a31-9ec9b1a6fc92/questions\n\n Don Wood Grant\n https://donwoodfoundation.givingdata.com/portal/campaign/Competitions\n AEP Grant and Duke Energy Grant \n\nAug 22, 2024  Alex with Anthis\nhttps://aws.amazon.com/deepracer/\nUsed in the Anthis class to help teach AI Models \nAug 14, 2024\nDoug emailed Carpet approval\nDoug emailed Field down payment approval to AndyMark EXRAD3H\nDrew to follow up on Lease\nIndiana Tech receives a $21 million grant from Lilly Endowment Inc.\nAmazon.com : Insta360 X4 - Waterproof 8K 360 Action Camera, 4K Wide-Angle Video, Invisible Selfie Stick Effect, Removable Lens Guards, 135 Min Battery Life, AI Editing, Stabilization, for Sports, Travel, Outdoor \nCrossFire PRO CNC Plasma | Langmuir Systems\nBuy Pro 2 - Professional 8K 360 VR Camera - Insta360\nhttps://wcproducts.com/collections/cnc-hardware\nJul 31, 2024\nGreat news shared.  Electric Works gave us a deal for the field that we can't refuse for the space on the first floor for the robot.  They are going to donate it if we cover operating expenses\nAlso, the microscope is getting shipped from Nikon.  It's the first nspark confocal microscope in the state.\nLease for Competition Field\nhttps://igem.org/\nJuly 17\nDoug\nHaply news\nLab News\nRookie Build - Get early access Fall\nDoug to reach out to Daniel to make sure how many teams.  If it's more than SS then use EW.\nGirls Event: https://www.goonettesinvitational.org/ \nMake 2 bases:\nKraken motors (before the sell out)\nJuly 16\n\n1501 committed to the Rookie Build for South Side High School w/ Chris Osborne. Just to confirm if we can do this at Electric Works \n\n1501 signed up for FROST hosted at Purdue Fort Wayne.\n\nEVENT INFORMATION:\nSeptember 27th: Set up Day / Load-in\nSeptember 28th: Competition Day\nLOCATION:\nPurdue University Fort Wayne, Hilliard Gates Sports Center\n2101 E Coliseum Blvd\nFort Wayne, Indiana 46805\nREGISTRATION COST:\n$250 per team, plus $125 with a second robot.\nIf you are interested in attending this year, you can fill out our Interest Form. \nIf you have any questions, please feel free to reach out, I'd love to have you attend again this year as FIN continues to grow in the Fort Wayne area! \nBest,\nKyle\nKyle Heaton\nFIRST Senior Mentor\nFIRST Technical Advisor\nFIRST Indiana Robotics\nC:765-416-2784\nE: kheaton@firstinspires.org\n1501 - New release, very nice Swerve Pods foler $299 each, which is a GREAT PRICE! \n\nFor supporting us, we've created a one-time 15% off all swerve kits and bundles discount that you can use by clicking the link below:\n\nhttps://www.thethriftybot.com/discount/KJZEQT7TVGRG?redirect=%2Fcollections%2Fthrifty-swerve\n\n\nTake a look at July 15th Martins \nIVF MVP SOFTWARE TEMPO\nJuly 11This is an interesting post. Check out AgileX. Also  their code https://github.com/agilexrobotics/pose_control\nJuly 10\n\n1501 was asked to host a ROOKIE quick build for SouthSide High School. Sounds like they are registered.\n\nWe need a place to host it…. I know a guy I can ask… Not sure a time frame yet:\n\nFIN is reaching out to veteran FRC teams this year to ask if you would be interested in running the \"rookie quickbuild\" program for rookie teams in your area.  We have a rookie registered at Fort Wayne South Side and if you are interested in running this here is what would happen.\n \nI would introduce you to the coaches at the school.\nYou would then coordinate the date and location of the quickbuild. \nI would order the materials from AndyMark and Lowes and ship it to you.\nYour team would then run the quickbuild with the rookie team.\nYou would also work with me to coordinate lunch (FIN will buy lunch).\n I would like to do a call with you if you are interested in helping with this. \nChris O\nDoug:  Chris - Awesome!  I will get an update on EW.\nBri: Discuss Blue Sky Robotics xArms per Doug’s request Quote Overview - Blue Sky\nJune 26\n1501 just returned from RiverBoat Rumble testing the Kraken motors on our SDS MK4i. We are VERY, VERY happy with them in a competition. We will be switching to Krakens for 2025 season.\n1501 discovered that we needed to add about a 50 AMP current limit in the JAVA software otherwise we experienced brownout near the end of the match.\n1501 was unable to test Auto at this event, we just ran out of time and tried to debug Choreo on the fly, and it was not good. Lesson learned, make sure your Auto is done before you depart.\n1501 also tested the VEX Griplock tires.https://www.vexrobotics.com/217-9042.html\nPROS: Very, Very grippy, no slip at all.Hardly any wear at all.\nCONS: Very, Very bouncy. Our driver did not like this and because of the grip, we often were on the verge of tipping.\nSummary: I think if you have a robot under 100lbs, low CG. These would be the tires to use in SDS Swerve. \n\n\n\nJune 19\n1501 is finishing up Java programming and preparing for RiverBoat Rumble this weekend.\n1501 Other than that, no other updates.\n1501 is pre-ordering Krakens, SDS Narrow Plates and Limelight 3G as supplies will be hard to get when season starts. Krakens are offered for pre-sale and fall delivery if ordered now.\nhttps://limelightvision.io/\nJune 12\nNo Updates from Chris Elston this week other than the lead time awareness below from June 4th.\n1501 is testing Kraken motors for the River Boat Rumble we are attending in Louisville next weekend.We installed, wired, and rewrote the code in Java and Choreo. We used pathplanner last season, but moving forward, we would like to switch to Choreo.\n1501 is going here: https://frc-events.firstinspires.org/2024/KYLou\n1501 is finishing up a Sandbox AR project for 4-H Fair July 19th week.\n\nJune 5\nSarah Randal - Canterbury - Bill Cast\nJune 4\nQuestion AndyMark:\nHi Chris,\nI wanted to check back in and see if you are still interested in the FRC perimeter and/or have an update regarding the quote Please let me know if there is anything I can do to help.\nAndrea Wyrick\nBusiness Office\n\nHi Andrea,\nYes Doug is still interested in the FRC Border. He is working with Fort Wayne Electric works to work out a deal with them, but as far as I know he plans to move forward with this project soon.\nChris Elston\nHi Chris,\nThat's not a problem. I was reaching out because we have added 11 FRC perimeters to our production schedule in the last 8 weeks, which is unheard of at this point in AndyMark's history. We have 2 more that we are waiting for the deposit payments to arrive and 3 others that are likely to come through soon. Our 3-6 month lead time is hovering closer to the 5-6 month lead time at this point. As long as he isn't needing this for a specific date/event, it's not a problem for him to get things moving at his own pace. \nEnjoy the rest of your week!\nAndrea Wyrick\nMay 23\nRegion 8\nMay 22\nSummer program plans\nTrip to Huntington on Thursday to see the machine shop. Cars leave at 3pm for electric works. \nSnider student at Electric Works. Need to start a robot google doc, GitHub repo, and project template. \nFirst promotion video featuring Snider \nMay 17\nChris Osborne resources:\nFIRST Alignment to Indiana K-12 Academic Standards\nFIRST Impact Reports and Case Studies\nhttps://www.instagram.com/p/C7Jnp3NLmNq/?igsh=MWE4Z2NhaTdsNjg3eg== \nKathy Weibel - Brentwood Elementary \nHi!  I got things moving at my school for a First Lego League!  We are going to start in the fall!  We have some really great ideas on how to boost our program. 1. We are going to befriend another club in another country and learn about that country ( and be pen pals!) 2. We are going to try to take a field trip to an aquarium to learn more about the ocean and animals who live there.  I was wondering if you had time to chat about how to approach sponsors and who to approach. I’m hopeful we can find someone who might want to help support us long term.  You are incredibly generous and I appreciate that, but I don’t want to take advantage of you either.  🩵. Here are some pics a teacher friend sent me of our team promoting First Robotics yesterday at her school. Apparently her school LOVED it!!\nMay 15, 2024\nHere is my agenda to cover – LOOSE - C.Elson (1501) used this email for Google documents: chakorules@gmail.com \nI’ll try to jump one around 4:30pm today but not stay the whole time. \nFIELD\nUpdate on field ordering? Timing?\nDoug: Probably next week\nChris: Quote was sent to Doug already from AndyMark\nChris: Lead time on the field is 4 MONTHS.\nCarpet quote should be done today I hope?\nChris: Quote was sent to Doug already from Bradley.\nHow do we get a QTY (3) 12 feet ROLL of carpet from the ground to the 5th floor that weighs about 400-600 lbs?\nChris: Doug says there is a freight elevator.\nChris: Doug says that he is trying to negotiate a better rent price and space for the field. Waiting on that key piece of information.\nOne of my mentors asked about Mike Fisher and some kids coming to the shop to learn from our summer projects?\nChris: Mike Fisher already stopped at the shop on Monday, May 15. We showed him around our shop and my Java Mentor, Jake, invited him to fall JAVA classes.\nInvites (YES)\nShould I invite Homestead coach? I know them pretty well.\nChris: I will invite them in a couple of weeks. His name is Matt Elder <MELDER@sacs.k12.in.us>\nShould I invite North Side coach? I know Dennis Fisher well, they come to our shop for help sometimes.\nChris: I will invite them in a couple of weeks: His name is Dennis Fisher <Dennis.Fisher@fwcs.k12.in.us>\nChris: I forgot about East Noble.\nChris: I will invite them in a couple of weeks: His name is Shawn Kimmel - skimmel@eastnoble.net I don’t know them that well.\nHAAS Updates or Help needed from us?\nChris: Doug is coming next week on Thursday, May 23, to the Huntington Learning Center to look at the CNC shop\nDo you want to schedule a HAAS Tour at the Learning Center? 5/23\nChris: Scheduled, I will set up a tour guide.\n Far as I know there are 5 FRC teams in NorthEast Indiana at the moment.\nNorth Side - https://www.thebluealliance.com/team/9431\nSnider - https://www.thebluealliance.com/team/9431\nHomestead - https://www.thebluealliance.com/team/4982\nHuntington - https://www.thebluealliance.com/team/1501\nKendallville East Noble H.S. - https://www.thebluealliance.com/team/8103\nCarol HS (new)\nFor Reference: All Indiana Teams are here:\nhttps://www.thebluealliance.com/events/fin/2024#teams\nSnider FIRST Robotics Team 9431. Blue\nLinks:\nhttps://github.com/snidercs  https://github.com/snidercs/bot-2024\nfirstinrobotics - Twitch - watch live\nncesdata high schools within 50 miles of fort wayne\nhttps://www.chiefdelphi.com/t/frc-orbit-1690-2024-robot-cad-release/464838 \nMay 10, 2024\nMeeting with Dr. Daniel\nFirst Robotics Video featuring Snider Panthers \nApr 20, 2024\nNotes for Chris\nCan we fix the numbers? Gotta get names\nRanking showing too\nIntegrated iGEM like competition (Micro fluidics, Enzymes, lab work) openflexure\nDaly https://en.wikipedia.org/wiki/Marie_Maynard_Daly\nMilstein https://en.wikipedia.org/wiki/C%C3%A9sar_Milstein \nftc10298@gmail.com Sung Choi dinner\nLavada.weibel@fwcs.k12.in.us Brentwood Elementary Lego Leage \nApr 17, 2024\nFFI - Calling C from other languages\nUA - https://luajit.org/ext_ffi.html\nPython - CFFI https://cffi.readthedocs.io/en/stable/  ABI vs API\nhttps://cffi.readthedocs.io/en/stable/overview.html#abi-versus-api\nV8 -  https://github.com/trueinteractions/v8-ffi\nRoborio Python\n​​https://github.com/robotpy/mostrobotpy/blob/main/.github/workflows/dist.yml\nApr 8, 2024\nIdeas\nWiki pages\nTeam w pics- link to canva\nRobot\nBlog pages\nShow the PR\nProject for Team resources \nWiki on facilities \nInventory \nAssetss\nMentors\nSponsors\nWebsite: template\nLinks / Courses\nGitHub process (Contributing)\nOverview of git hygiene \nProtect branches \nYour first pull request \nTest centered \nWorkflow\nTempo / Charter\nVlog / Blog\nApr 7, 2024\nhttps://www.firstchampionship.org/event-guide-houston-info\nNext year ideas:\nSummer program\nAgile approach - Developing a Charter\nControls Engineering in FRC - Graduate-level control theory for high schoolers Tyler Veness\nFundamentals of electronics\nFundamentals of programming\nAI and machine learning\nFort Wayne Electric Works\nSummer competitions\nhold weekly informal competitions\nProgram ideas\n5k stipend for teacher teams\nMIE will mentor teacher team\nhttps://www.stats.indiana.edu/maptools/schooldistrictsmap.asp\nhttps://nces.ed.gov/datatools/\nPublic Schools  - 42\nPrivate Schools - 24\nApr 6, 2024\nState Championship!   Qualifications\nHomestead https://frc-events.firstinspires.org/2024/team/4982  https://team4982.com/\nSigned up for https://www.thebluealliance.com/mytba to track matches\nhorner@mieweb.com with google  and the iphone app\nRule for District -  https://www.frcmanual.com/2024/district-tournaments\nhttps://www.thequadrangles.org/history \nBloomington example \nApr 5, 2024\nAuthored a response to my PR: https://github.com/wpilibsuite/frc-docs/pull/2609\nControls Engineering in FRC - Graduate-level control theory for high schoolers Tyler Veness\nSection:  3.1 Mechanical vs software solutions\nSpartan Series/ Mechanical Design for Controllability - Travis Schuh\n9:30 - \"Don't burn out your motors\"\nMar 30, 2024\nHarbor Freight Shopping List\nHow to Wire a Robot\nCAN Bus Tutorial - Tips for implementing CAN on your robot!\nMar 20, 2024\nhttps://www.revrobotics.com/ftc-starter-bot/\nCheap bot to start\nIntroducing the NEO Vortex & SPARK Flex\nSeriously awesome upgrade to the normal SPARK controller and NEO motors\nhttps://www.revrobotics.com/frc/starter-bot-24/\nREV's Competition Bot for 2024\nhttps://cad.onshape.com/documents/13d6b4106d6774d042b595f2/w/19474c47606fa4f8682cf6dd/e/f062f614d6e785bd19908ebe\nMIE 2024 REV ION FRC Starter Bot - Bill of Materials \nMar 18, 2024\nDoug: Review PR: https://github.com/snidercs/bot-2024/pull/73#pullrequestreview-1941963588\nHacking Session: https://mieweb.webex.com/mieweb/ldr.php?RCID=cdaf64aad09a9950ee8bd2ef1110a7e8 \nREV: 2024 REV ION FRC Starter Bot - Bill of Materialshttps://www.revrobotics.com/ion/frc-starter-bot-24/\nVisit to AndyMark\nMar 17, 2024\nDoug: \nPlanning a trip to AndyMark on Monday to see their warehouse\nreviewed code: https://github.com/Mechanical-Advantage/RobotCode2024\nThey use Earthly for their build chain.  It's a wrapper to docker.\nSee ​​https://github.com/Mechanical-Advantage/RobotCode2024/blob/main/Earthfile\nI got Snider's code to compile on my Mac by using Docker\nI really could not get java working nicely on the mac so I gave up.\nI used docker:\ndocker run -it -v $(pwd):/root  -w /root wpilib/roborio-cross-ubuntu:2024-22.04    # run a docker\ncd /root\n./gradlew --no-watch-fs build\n\nI found that mounting the local dir to /root is best since gradlew downloads to ~/.gradle and this caches things for the next run.\nMade a PR: to commit updates to the README and make a github workflow https://github.com/snidercs/bot-2024/pull/68\nsudocker.sh !\nGetting X11 working from within DOCKER - https://gist.github.com/cschiewek/246a244ba23da8b9f0e7b11a68bf3285\nMar 16, 2024\nDoug: \nFixed RoboRIO - The driver console was not talking to the new roboRio\nHow to Image your roboRio 2.0 | FRC Team 8880\nhttps://www.ni.com/docs/en-US/bundle/roborio-20-umanual/page/umanual.html\nturned out the SD card did not have the proper firmware. The video is a goldmine\nAdded a PR: https://github.com/wpilibsuite/frc-docs/pull/2608 to help others find the video\nAbhi shared: FRC team 1690 Orbit 2024 robot reveal - \"DOPPLER\"FRC4481 - 2024 Prototypes - Top/Bottom (with gap) Shooter Wing Line Test\nNext Event\nhttps://frc-events.firstinspires.org/2024/INPLA\nFriday, March 22 to Sunday, March 24, 2024 - (UTC-05:00) Eastern Time (US & Canada)\n1 Red Pride Dr\nPlainfield, IN USA\n\nDoug's Notes\nSnider's General Summer Tempo\nPeople\nChris Elston - YRG, Inc. | LinkedIn\nChris Osborne - FIRST Indiana Robotics | LinkedIn  Chris Osborne  cosborne@indianafirst.org \nCompetition Background\nWatch this first. It’s the WHY: From sport to tech Revolutionizing STEM Education: An Exclusive with Dean Kamen, Founder of FIRST - YouTube\nThe final match Einstein Final 2 - 2024 FIRST Championship \nPost interview 1690 Orbit | Champions Interview | FRC CRESCENDO \nAbout FIRST (2021) - About FIRST (2021 Cutdown) \nAbout FIRST Robotics Competition (2021)\nGame manual 2024 FIRST Robotics Competition Game Manual\nhttps://www.frcmanual.com/  <- an online easier to browse version!!!\nTeamwork training: Teamwork & robotics | FIRST Robotics Competition (FRC) Team 2848 Jesuit College Prep | TEDxSMU\nHow to train your students to talk to judges.\nFRC Map https://frcmap.com/\nExample: Match 12 (R4) - 2024 FIM District Kentwood Event presented by Dematic\nThe winner - promotional video:FRC team 1690 Orbit 2024 robot reveal - \"DOPPLER\"\nCool Videos of existing competitions that have happened:\nhttps://www.youtube.com/@FIRSTRoboticsCompetition/videos\n2024 Field Tour Video: Stage\nNational 2024 Details CRESCENDO - Season Materials | FIRST\nFIN District Mishawaka Event: 2024 Event Information - FIN District Mishawaka Event\nTEAM: https://frc-events.firstinspires.org/2024/team/9431\nSnider's First Match: https://frc-events.firstinspires.org/2024/team/9431\nHow Do I Prepare a Driver Station for Competition?\nHow Do I Use Cameras in FRC? AI Robotics\nControl System Hardware \nrobopRIO 2.0 manual https://www.ni.com/docs/en-US/bundle/roborio-20-umanual/page/umanual.html\nDiagram - Basic Brain is the NI roboRIO\nSoftware WPILib Docs - https://github.com/wpilibsuite/allwpilib\nAdvanced Programming — FIRST Robotics Competition documentation \nKits\n2023 – 2024 Kit of Parts\nhttps://www.andymark.com/cart\nhttps://www.swervedrivespecialties.com/products/mk4i-swerve-module\nSwerve Drive 101 with Clem McKown from FRC 1640 and Jeremy Zang from FRC 694 - Stuy Splash 2020\nAM14U5 6 in. SR Mecanum and ToughBox Micro Bundle - AndyMark, Inc\nThe Brilliant Engineering of Mecanum Wheels!\nAndyMark WormBox on an 8\" Mecanum Drive Base\nhttps://github.com/Mechanical-Advantage/RobotCode2024\nFort Wayne Resources\nBioNanomics Competition Practice Area\n.\n7K feet (13/14 + util)\nhttps://www.andymark.com/products/andymark-field-perimeter\n$25,900.00"
  },
  {
    "source": "2501.04227v1.pdf",
    "content": "2025-1-9\nAgent Laboratory: Using LLM Agents as\nResearch Assistants\nSamuel Schmidgall1, 2, Yusheng Su1, Ze Wang1, Ximeng Sun1, Jialian Wu1, Xiaodong Yu1, Jiang Liu1, Zicheng\nLiu1 and Emad Barsoum1\n1AMD, 2Johns Hopkins University\nHistorically, scientific discovery has been a lengthy and costly process, demanding substantial time and\nresources from initial conception to final results. To accelerate scientific discovery, reduce research costs,\nand improve research quality, we introduceAgent Laboratory, an autonomous LLM-based framework\ncapable of completing the entire research process. This framework accepts a human-provided research\nidea and progresses through three stages—literature review, experimentation, and report writing to\nproduce comprehensive research outputs, including a code repository and a research report, while\nenabling users to provide feedback and guidance at each stage. We deployAgent Laboratory with\nvarious state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in\na survey, providing human feedback to guide the research process, and then evaluate the final paper.\nWe found that: (1)Agent Laboratory driven by o1-preview generates the best research outcomes;\n(2) The generated machine learning code is able to achieve state-of-the-art performance compared to\nexisting methods; (3) Human involvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4)Agent Laboratory significantly reduces research expenses, achieving\nan 84% decrease compared to previous autonomous research methods. We hopeAgent Laboratory\nenables researchers to allocate more effort toward creative ideation rather than low-level coding and\nwriting, ultimately accelerating scientific discovery.\n/githubhttps://AgentLaboratory.github.io\nFigure 1|Agent Laboratorytakes as input a human research idea and a set of notes, provides this\nto a pipeline of specialized LLM-driven agents, and produces a research report and code repository.\nCorresponding author(s): Samuel Schmidgall (sschmi46@jhu.edu)\narXiv:2501.04227v1  [cs.HC]  8 Jan 2025\nAgent Laboratory: Using LLM Agents as Research Assistants\n1. Introduction\nScientists frequently face constraints that limit the number of research ideas they can explore at any\ngiven time, resulting in ideas being prioritized based on predicted impact. While this process helps\ndetermine which concepts are worth investing time in and how best to allocate limited resources\neffectively, many high quality ideas remain unexplored. If the process of exploring ideas had less\nlimitations, researchers would be able to investigate multiple concepts simultaneously, increasing the\nlikelihood of scientific discovery.\nIn an effort to achieve this, recent work has explored the capability of LLMs to perform research\nideation and automated paper generation, where LLM agents perform the role of human scientists\n(Baek et al. (2024); Ghafarollahi & Buehler (2024b); Lu et al. (2024a); Swanson et al. (2024)).\nThe work of Baek et al. (2024) introduces ResearchAgent, which automatically generates research\nideas, methods, and experiment designs, iteratively refining them through feedback from multiple\nreviewing agents that mirror peer discussions and leverage human-aligned evaluation criteria to\nimprove the outputs. Lu et al. (2024a) explores fully automated paper generation, where The AI\nScientist framework generates novel research ideas, writes code, conducts experiments, and creates\na full scientific paper with an automated peer-review system to evaluate the work. Even though\nthese works demonstrate that current LLMs can generate ideas judged to be more novel than those\nproduced by human experts, Si et al. (2024) indicates that LLMs still exhibit weaknesses in feasibility\nand implementation details, suggesting a complementary rather than replacement role for LLMs in\nresearch. Therefore, we aim to design an autonomous agent pipeline that can assist humans toward\nimplementing their own research ideas.\nIn this work, we introduceAgent Laboratory, an autonomous pipeline for accelerating the\nindividual’s ability to perform machine learning research. Unlike previous approaches, where agents\nparticipate in their own research ideation independent of human input (Baek et al. (2024); Lu et al.\n(2024b)), Agent Laboratoryis designed to assist human scientists in executing their own research\nideas using language agents.Agent Laboratorytakes as input a human research idea and outputs\na research report and code repository produced by autonomous language agents, allowing various\nlevels of human involvement, where feedback can be provided at a frequency based on user preference.\nA detailed list of our contributions are provided below:\n1. We introduceAgent Laboratory, an open-source LLM agent framework for accelerating the\nindividual’s ability to perform research in machine learning. In order to accommodate all users,\nAgent Laboratory is compute flexible, where various levels of compute can be allocated\nbased on the individual’s access to compute resource (e.g., CPU, GPU, memory) and model\ninference budget.\n2. Human evaluators rated papers generated usingAgent Laboratory across experimental\nquality, reportquality, andusefulness, showingthatwhiletheo1-previewbackendwasperceived\nas the most useful, o1-mini achieved the highest experimental quality scores, and gpt-4o was\nbehind in all metrics.\n3. NeurIPS-style evaluations showed that o1-preview performed best among backends, particularly\nin clarity and soundness, according to human reviewers. However, a clear gap emerged between\nhuman and automated evaluations, with automated scores significantly overestimating quality\n(6.1/10 vs. 3.8/10 overall). Similar discrepancies were seen across clarity and contribution\nmetrics, suggesting the need for human feedback to complement automated evaluations for\nmore accurate assessments of research quality.\n4. Co-pilot mode in Agent Laboratory was evaluated on custom and preselected topics, showing\nhigher overall scores compared to autonomous mode. Co-pilot papers also saw trade-offs\n2\nAgent Laboratory: Using LLM Agents as Research Assistants\nin experimental quality and usefulness, reflecting challenges in aligning agent outputs with\nresearcher intent.\n5. The co-pilot feature inAgent Laboratoryis overall found to have high utility and usability\nwhen rated by human users, with most participants deciding to continue usage after their\nexperience\n6. Detailed cost and inference time statistics, as well as the breakdown of cost per paper phase,\nare presented for different model back-ends, demonstrating thatAgent Laboratoryoffers\nautomatic research at a greatly reduced price compared with other works (only $2.33 USD per\npaper with a gpt-4o backend).\n7. State-of-the-artperformanceonasubsetofMLE-Benchchallengesusingtheproposed mle-solver,\nachieving higher consistency and scoring compared to other solvers, and earning more medals,\nincluding gold and silver, than MLAB, OpenHands, and AIDE.\nWe hope that this work takes a step toward accelerating scientific discovery in machine learning,\nallowing researchers to allocate more effort toward creative ideation and experiment design rather\nthan low-level coding and writing.\n2. Background & Related Work\nLargelanguagemodels The research agents in this paper are built on autoregressive large language\nmodels(LLMs), whicharetrainedonextensivetextcorporatopredictconditionalprobabilitiesoftoken\nsequences, 𝑝(𝑥𝑡|𝑥<𝑡; 𝜃), and generate text completions through sampling, where𝑥𝑡 ∼softmax(𝑊 ·ℎ𝑡),\nwith ℎ𝑡 as the hidden state and𝑊 as the learned weight matrix mapping to token probabilities. LLMs\nutilize transformer architectures (Vaswani (2017)) to capture long-range dependencies in text. These\nmodels, such as Claude (Anthropic (2024)), Llama (Dubey et al. (2024); Touvron et al. (2023a,b)),\nand ChatGPT (Achiam et al. (2023); Hurst et al. (2024); OpenAI (2022)), leverage vast datasets\nand scaling techniques, thus enabling them to perform a wide array of language-based tasks, such as\ntranslation, summarization, and reasoning, by generalizing patterns learned during pretraining to\nnovel inputs Brown (2020).\nLLM Agents While LLMs demonstrate strong understanding and reasoning abilities, they face chal-\nlenges when executing tasks in real-world scenarios. To overcome these limitations, their capabilities\nare extended through structured frameworks, enabling them to autonomously and semi-autonomously\nperform task execution and semi-autonomously perform task execution (Chen et al. (2023b); Li\net al. (2023); Qian et al. (2024); Wu et al. (2023)). These systems, referred to as agents, utilize\ntechniques such as chain-of-thought prompting (Wei et al. (2022)), iterative refinement (Shinn et al.\n(2024)), self-improvement (Huang et al. (2022)), and external tool integration to execute complex\nworkflows (Hao et al. (2024); Qin et al. (2023); Schick et al. (2023)). LLM agents have made\nremarkable progress in solving tasks of real-world significance, such as software engineering Jimenez\net al. (2023); Wang et al. (2024b); Yang et al. (2024)), cybersecurity (Abramovich et al. (2024);\nFang et al. (2024); Wan et al. (2024)), and medical diagnosis (McDuff et al. (2023); Schmidgall\net al. (2024); Tu et al. (2024)). There has also been progress in applying LLMs agents to embodied\nproblems such as autonomous robotics (Black et al. (2024); Brohan et al. (2022, 2023); Kim et al.\n(2024)), web tasks (Deng et al. (2024); Gur et al. (2023); He et al. (2024); Putta et al. (2024); Shi\net al. (2017)), and game playing (AL et al. (2024); Feng et al. (2024); Wang et al. (2023)). For a\nbroader overview of LLM agents, refer to Wang et al. (2024a).\n3\nAgent Laboratory: Using LLM Agents as Research Assistants\nAutomated machine learning Automated machine learning is an area of active research, with\nmany approaches focused on using Kaggle, an online platform for machine learning competitions,\nas a benchmark for evaluating agent performance. Notable efforts include MLE-Bench (Chan et al.\n(2024)), DS-bench (Jing et al. (2024)), and MLAgentBench (Huang et al. (2024)) which propose\nusing 75, 74, and 6 Kaggle challenges respectively as benchmarks to measure the abilities of ML agents\nin tasks such as data preparation, model development, and submission. Several ML \"solvers\" which\ncan solve ML challenges have been introduced, such as AIDE (Schmidt et al. (2024)), CodeActAgent\n(referred to as “OpenHands\") (Wang et al. (2024b)), and ResearchAgent (referred to as “MLAB\")\nfrom MLAgentBench (Huang et al. (2024)) which automate feature implementation, bug fixing, and\ncode refactoring with a high success rate. Agent K (Grosnit et al. (2024)) demonstrates the ability to\nsolve Kaggle challenges at the human-level with a challenge URL provided as input.\nAI in Scientific Discovery AI has been used to support scientific discovery across numerous disci-\nplines for decades. For instance, AI has been used for discovery in mathematics (Romera-Paredes\net al. (2024)), material science (Merchant et al. (2023); Pyzer-Knapp et al. (2022); Szymanski et al.\n(2023)), chemistry (Hayes et al. (2024); Jumper et al. (2021)), algorithm discovery (Fawzi et al.\n(2022)), and computational biology (Ding et al. (2024)). These approaches position AI as a tool\nrather than an agent performing research in autonomous research.\nLLMs for research related tasksLLMs have demonstrated strong capabilities in diverse research-\nrelatedtasks, suchascodegeneration(Chenetal.(2021);Nijkampetal.(2022)), end-to-endsoftware\ndevelopment (Hai et al. (2024); Phan et al. (2024); Qian et al. (2023, 2024)), code generation for\ndiscovery (Chen et al. (2024b); Ghafarollahi & Buehler (2024a); Gu et al. (2024); Guo et al. (2024);\nHu et al. (2024b); Ifargan et al. (2024); Majumder et al. (2024)), research question-answering\n(Chen et al. (2024a); Lála et al. (2023); Lin et al. (2024); Song et al. (2024)), research ideation\n(Baek et al. (2024); Ghafarollahi & Buehler (2024b); Li et al. (2024a); Si et al. (2024)), automated\npaper reviewing (D’Arcy et al. (2024); Liang et al. (2024); Lu et al. (2024b); Weng et al. (2024)),\nliterature search (Ajith et al. (2024); Kang & Xiong (2024); Li et al. (2024b); Press et al. (2024)),\nand predicting the outcome of experiments (Ashokkumar et al. (2024); Lehr et al. (2024); Luo et al.\n(2024); Manning et al. (2024); Zhang et al. (2024)). Although LLMs have made notable progress in\nsolving the aforementioned tasks, ideation has struggled to progress, with some work showing that\nLLM ideation leads to greater novelty than humans (Si et al. (2024)), while others show reduced\ncreativity (Chakrabarty et al. (2024)) and greater homogeneous effects (Anderson et al. (2024);\nZhou et al. (2024)) that may limit creative discovery without human guidance.\nAdditionally, research on human-AI collaboration has reached mixed conclusions about the idea\nnovelty (Ashkinaze et al. (2024); Liu et al. (2024); Padmakumar & He (2024)). These findings\nsuggest that, with the current LLMs, the strongest research systems would combine human-guided\nideation with LLM-based workflows.\nLLMs for autonomous research Recent advancements in automated scientific workflows have\nfocused on leveraging LLMs to emulate the process of research. Swanson et al. (2024) introduces\na team of LLM agents working as scientists alongside a human researcher who provides high-level\nfeedback, with the end result being novel nanobody binders aimed at addressing recent variants of\nSARS-CoV-2. ChemCrow (M. Bran et al. (2024)) and Coscientist (Boiko et al. (2023)) demonstrate the\nability for autonomous ideation and experimentation in chemistry. ResearchAgent (Baek et al. (2024))\nautomates research idea generation, experiment design, and iterative refinement using feedback from\nreviewing agents aligned with human evaluation criterion. The AI Scientist (Lu et al. (2024a)) extends\n4\nAgent Laboratory: Using LLM Agents as Research Assistants\nFigure 2|Agent LaboratoryWorkflow. This image illustrates the three primary phases of Agent\nLaboratory: Literature Review, Experimentation, and Report Writing, each featuring distinct tasks,\ntools, and human-agent roles. The pipeline integrates human input with LLM-driven agents, such as\nthePhDandPostdocagents, whichhandleliteraturereviews, experimentalplanning, datapreparation,\nand result interpretation. Specialized tools like mle-solver for experimentation and paper-solver for\nreport generation automate tedious research tasks, enabling collaboration between human researchers\nand AI to produce high-quality research outputs.\nthis automation to encompass end-to-end scientific discovery, including coding, experiment execution,\nand automated peer review for manuscript generation. Despite these advancements, studies like\nSi et al. (2024) highlight limitations in the feasibility and implementation details of LLM ideation,\nindicating a complementary rather than replacement role for LLMs in autonomous research.\n3. Agent Laboratory\nOverview. Agent Laboratory begins with the independent collection and analysis of relevant\nresearch papers, progresses through collaborative planning and data preparation, and results in\nautomated experimentation and comprehensive report generation. As shown in Figure 2, the overall\nworkflow consists of three primary phases: (1) Literature Review, (2) Experimentation, and (3)\nReport Writing. In this section, we will introduce these phases in detail along with the corresponding\ninvolved agents. Furthermore, in Section 4, we will conduct qualitative and quantitative analyses to\ndemonstrate the strengths ofAgent Laboratoryand its ability to generate\n3.1. Literature Review\nLiterature Review. The literature review phase involves gathering and curating relevant research\npapers for the given research idea to provide references for subsequent stages. During this process,\nthe PhD agent utilizes the arXiv API to retrieve related papers and performs three main actions:\nsummary, full text, andadd paper. Thesummaryaction retrieves abstracts of the top 20 papers\nrelevant to the initial query produced by the agent. Thefull text action extracts the complete\ncontent of specific papers, and theadd paperaction incorporates selected summaries or full texts\ninto the curated review. This process is iterative rather than a single-step operation, as the agent\nperforms multiple queries, evaluates the relevance of each paper based on its content, and refines the\n5\nAgent Laboratory: Using LLM Agents as Research Assistants\nselection to build a comprehensive review. Once the specified number of relevant texts (N=max) is\nreached via theadd papercommand, the curated review is finalized for use in subsequent phases.\n3.2. Experimentation\nPlan Formulation The plan formulation phase focuses on creating a detailed, actionable research\nplan based on the literature review and research goal. During this phase, the PhD and Postdoc agents\ncollaborate through dialogue to specify how to achieve the research objective, detailing experimental\ncomponents needed to complete the specified research idea such as which machine learning models\nto implement, which datasets to use, and the high-level steps of the experiment. Once a consensus\nis reached, the Postdoc agent submits this plan using theplancommand, which serves as a set of\ninstructions for subsequent subtasks.\nData Preparation. The goal of the data preparation phase is to write code that prepares data for\nrunning experiments, using the instructions from the plan formulation stage as a guideline. The ML\nEngineer agent executes code usingPythoncommand command and observes any printed output.\nThe ML Engineer has access to HuggingFace datasets, searchable via thesearch HFcommand. After\nagreeing on the finalized data preparation code, the SW Engineer agent submits it using thesubmit\ncode command. Before the final submission proceeds, the code is first passed through a Python\ncompiler to ensure that there are no compilation issues. This process will be iteratively executed until\nthe code is bug-free.\nRunning Experiments. In the running experiments phase, the ML Engineer agent focuses on imple-\nmenting and executing the experimental plan formulated prior. This is facilitated bymle-solver,\na specialized module designed to generate, test, and refine machine learning code autonomously.\nmle-solver begins by producing initial code based on the research plan and insights from the\nliterature review. For the firstmle-solverstep, the program is empty and must generate a file from\nscratch, which is used as thetop scoring program. The following processes describe the workflow of\nthe mle-solver:\nA. Command Execution.During the command execution phase, an initial program is sampled\nfrom a maintained set of top-performing programs, which is represented by a single file dur-\ning initialization. Themle-solver iteratively refines this program through two operations,\nREPLACEand EDIT, to better align the output with experimental objectives. TheEDITopera-\ntion identifies a range of lines, substituting the code between the specified line numbers with\nnewly generated code. In contrast, theREPLACEoperation generates a completely new Python\nfile.\nB. Code Execution.After a code command is executed, the new program is passed through a\ncompiler to check for runtime errors. If it successfully compiles, a score is returned and the list\nof top programs is updated if the score is higher than the existing programs. If the code does\nnot compile, the agent attempts to repair the code for𝑁𝑟𝑒𝑝 tries (𝑁𝑟𝑒𝑝=3 in our experiments)\nbefore returning an error and moving on to a new code replacement.\nC. Program Scoring. If a code succeeds in compilation, it is sent to a scoring function which\ndetermines if it is better than previously implemented experiment code. In order to obtain\na program score, we implement a scoring function that uses an LLM reward model to assess\nthe effectiveness of the ML code generated bymle-solver. The reward model, invoked as\nan LM, scores the program on a scale from 0 to 1 considering the outlined research plan, the\nproduced code, and the observed output to determine how accurately the program adheres to\n6\nAgent Laboratory: Using LLM Agents as Research Assistants\nFigure 3|Overview of themle-solverworkflow. This diagram details the iterative process used by\nthe MLE-Solver to autonomously generate machine learning code. Beginning with external resources,\nthe workflow integrates command execution (A), where new code is generated, followed by code\nexecution (B) to compile and repair issues if needed. Program scoring (C) evaluates the generated\ncode using a reward function, while self-reflection (D) helps refine future iterations based on results.\nPerformance stabilization (E) ensures consistent outcomes by maintaining a pool of top-performing\nprograms and iterative optimization.\nthe initial goals. A score of 1 is provided for results with high alignment and everything below\non a spectrum of how closely the output and code matches the planning goals. This process is\nsimilar to existing methods for LLM reasoning tree search (Yao et al. (2024)), where instead of\na series of reasoning steps being traversed using self-evaluated LLM scoring, the set of possible\nprograms are being traversed (viaEDITand REPLACEcommands) and the resulting program\noutcome is self-evaluated to determine if a program is worth building on. This is similar to the\nSolution Space Search of AIDE (Schmidt et al. (2024)), however their method was specifically\ndesigned for the Kaggle competitions and is simply extracting the accuracy rather than scoring\nthe research code and outcomes.\nD. Self Reflection. Whether the code succeeds or fails, a self-reflection is produced based on\nthe experimental results or the encountered error signal (Renze & Guven (2024); Shinn et al.\n(2024)). Here, themle-solver is prompted to reflect on the outcome of its actions. If the\nprogram failed to compile, the solver reflects on how to fix this issue in next iterations. If it\nsuccessfuly compiles and returns a score, the solver will reflect on how to increase this score.\nThese reflections are generated to improve future performance, ensuring that the system learns\nfrom errors, improving the quality and robustness of the generated code over iterative cycles.\nE. Performance StabilizationTo prevent performance drift, two mechanisms are implemented:\ntop program sampling and batch-parallelization. In top program sampling, a collection of\nthe highest-scoring programs is maintained, and one program is randomly sampled before\nexecuting a command, ensuring diversity while retaining quality. For batch-parallelization, each\nsolver step involves making N modifications simultaneously, with the top modification selected\nto replace the lowest-scoring program in the top collection. These strategies use high-entropy\nsampling to modify the code, resulting in a balance between exploration of new solutions and\n7\nAgent Laboratory: Using LLM Agents as Research Assistants\nFigure 4|Graphical outline ofpaper-solver. This diagram showcases the step-by-step process\nof generating and refining academic research reports using the Paper-Solver tool. The workflow\nstarts with the creation of an initial report scaffold (A) by iteratively generating LaTeX-based sections,\nfollowed by updates to ensure structural completeness. (B) Research is performed through an Arxiv\ntool during relevant sections. In the Report Editing phase (C), the language model applies targeted\nedits to improve the document, with LaTeX compilation verifying the integrity of changes. Finally, the\ncompleted report undergoes a reward-based evaluation during the Paper Review phase (D), ensuring\nalignment with academic standards and research goals.\nrefinement of existing ones in order to maintain stable code modifications.\nResults Interpretation. The goal of the results interpretation phase is to derive meaningful insights\nfrom experimental outcomes to inform the final report. The PhD and Postdoc agents discuss their un-\nderstanding of the experimental results produced bymle-solver. Once they agree on a meaningful\ninterpretation that could contribute to a compelling academic paper, the Postdoc agent submits it\nusing theinterpretationcommand, forming the basis for the report writing phase.\n3.3. Report Writing\nReport Writing. In the report writing phase, the PhD and Professor agent synthesize the research\nfindings into a comprehensive academic report. This process is facilitated by a specialized module\ncalled paper-solver, which iteratively generates and refines the report. Thepaper-solveraims\nto act as a report generator, positioning the work that has been produced by previous stages ofAgent\nLaboratory. paper-solverdoes not aim to entirely replace the academic paper-writing process,\nbut rather to summarize the research that has been produced in a human-readable format so that the\nresearcher using Agent Laboratoryunderstands what has been accomplished. The output follows\nthe standard structure of an academic paper, ensuring it meets conference submission requirements\n(for the paper scoring phase) while being clear and methodical. The following processes describe the\nworkflow ofpaper-solver:\nA. Initial Report Scaffold.The first task of thepaper-solveris to generate an initial scaffold\nfor the research paper. This scaffold outlines the document structure, dividing it into eight stan-\ndardized sections: Abstract, Introduction, Background, Related Work, Methods, Experimental\nSetup, Results, and Discussion. During scaffold creation, placeholders are inserted for each\nsection to categorize future content. This process establishes the framework for subsequent\ndetailed text generation. The scaffold includes necessary formatting for LaTeX compilation,\nallowing the generated paper to be directly reviewed and refined. Special care is taken to\nensure the scaffold aligns with academic conventions, such as appropriate section titles and\nplaceholders that guide content development.\n8\nAgent Laboratory: Using LLM Agents as Research Assistants\nB. Arxiv Research.During the scaffold building phase, we allow thepaper-solver access to\narXiv which is accessible through the same interface as the earlier literature review phase. ArXiv\nis enabled to allow the solver to explore related literature on the subject it is writing on as well\nas finding papers to refer to, although it is not enforced. We note that the agent still has access\nto the original literature search, but has the opportunity to expand based on literature needed\nto write a particular paper section.\nC. Report Editing.One the scaffold is built, thepaper-solveruses specialized commands to\niteratively refine the generated paper. The primary command are available for this stage is\nthe EDITcommand, which allows precise line-by-line modifications to the LaTeX code. This\ncommand enable dynamic adjustments to the content, ensuring alignment with the research\nplan, the clarity of arguments, and compliance with formatting standards. Before integrating\nedits, the system compiles the LaTeX to verify error-free functionality, thereby maintaining\ndocument integrity. Through iterative editing, the solver ensures the paper achieves the desired\nlevel of quality, cohesiveness, and depth required for academic acceptance.\nD. Paper Review. For obtaining scores for papers during thepaper-solver iterations, we\nleverage an adapted version of the automated review system developed in Lu et al. (2024b).\nThis system works by using an LLM-based agent to simulate the scientific paper review process\nfollowingtheNeurIPSconferenceguidelines. Whenevaluatedon500ICLR2022papersfromthe\nOpenReview dataset, the automated reviewer achieved human-level accuracy (65% compared\nto 66% for human reviewers) and surpassed human performance in F1 score (0.57 vs. 0.49)\nafter calibration. An example review from one of our papers by o1-mini is provided below.\nExample Review ( o1-mini | Word Order Sensitivity )\n\"Strengths\": [\n\"Comprehensive experimental design and methodology.\",\n\"Use of a well-known dataset (RACE) for evaluation.\",\n\"Empirical validation of bias mitigation strategies.\",\n\"Clear presentation of results and analysis.\"],\nWeaknesses\": [\n\"Limited exploration of additional bias mitigation techniques.\",\n\"Lack of in-depth discussion on limitations\nand societal impacts.\",\n\"The originality could be enhanced by exploring novel\nstrategies.\"],\n\"Originality\": 3, \"Quality\": 4, \"Clarity\": 3, \"Significance\": 3,\n\"Questions\": [\n\"Have you considered exploring additional bias\nmitigation techniques beyond majority voting and entropy-based\nthresholding?\",\n\"Can you provide more details on the potential societal impacts\nof the model’s sensitivity to option order?\",\n\"What are the limitations of the current study, and how\nmight they be addressed in future work?\"],\n\"Limitations\": [\n\"The study is limited to the RACE dataset and may not generalize\nto other datasets.\",\n\"The bias mitigation strategies, while effective,\ndo not completely eliminate sensitivity to option order.\"],\n9\nAgent Laboratory: Using LLM Agents as Research Assistants\n\"Ethical Concerns\": false,\n\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3,\n\"Overall\": 7, \"Confidence\": 4,\n\"Decision\": \"Accept\"\nPaper Refinement. In the paper refinement phase, the PhD agent makes a decision on whether to\nmake paper revisions or to determine that the paper is complete. The process begins with a set of three\nreviewer agents generating reviews that mimic feedback from NeurIPS peer reviewers, evaluating the\nreport based on criteria such as originality, quality, clarity, and significance. Based on these scores, the\nPhD agent then decides whether to finalize the project or revisit earlier subtasks—such as planning,\nexperimentation, or results interpretation—to address the feedback. This allows the agents to refine\nthe research report until it meets sufficiently high standards, effectively simulating the real-world\nacademic revision process.\n3.3.1. Autonomous versus Co-Pilot Mode:\nThere are two ways in whichAgent Laboratorycan be operated: autonomous and co-pilot modes.\nIn autonomous mode, there is no human involvement other than providing the initial research idea\nfor agents to produce research for. Each subtask moves on to the next subtask sequentially upon\ncompletion. In co-pilot mode, in addition to providing the research idea, there is also a checkpoint\nat the end of each subtask, where a human is involved in reviewing the work produced by agents\nin that phase (e.g., the literature review summary or generated report). The human reviewer can\neither decide to proceed to the next subtask, or ask the agent to repeat the subtask while providing\nhigh level notes for the agent to improve its performance during the next attempt. For example, if the\nliterature review phase did not include a specific paper or the experiments did not include a desired\ntechnique, the human reviewer would instruct the agent to include this.\n4. Results\nIn this section, we present our main findings on the efficacy ofAgent Laboratory to produce\nresearch. We begin our results by asking how human evaluators perceive papers generated byAgent\nLaboratoryrunning in end-to-end autonomous mode across five topics. Next, we examine human\nevaluation when usingAgent Laboratoryin collaborative co-pilot mode from both allowing the\nresearcher to choose any topic they want and from our set of preselected topics. We then provide a\ndetailed runtime analysis including cost, average time, and success rate by various models. Finally,\nwe conclude with an evaluation of themle-solverin isolation on MLE-Bench, a set of real-world\nKaggle challenges. The details of all surveys are provided in Appendix C.\n4.1. Evaluation of quality by language model\nOur first experiment aims to evaluate how human-evaluated quality varies across three axes: experi-\nment quality, report quality, and usefulness. This evaluation was conducted by human participants\nusing three different LLM backends: gpt-4o (Hurst et al. (2024)), o1-mini, and o1-preview (OpenAI\n(2024)). Research questions were selected from a set of 5 templates:\n1. Do language models exhibit cognitive biases, such as confirmation bias or anchoring bias?\n2. Are image transformers more or less sensitive to pixel noise than convolutional networks?\n10\nAgent Laboratory: Using LLM Agents as Research Assistants\nFigure 5|The average human evaluated scores of papers generated byAgent Laboratoryin an\nautonomous mode based on a research question (left column) and LLM backend (top row). The\nbottom row shows the average score across all topics by LLM backend.\n3. Do language models improve accuracy on MedQA when asked to perform differential diagnosis?\n4. Are language models sensitive to word order in multiple choice benchmarks?\n5. Does gender role play affect the accuracy on of language models on answering math questions?\nThese 5 questions across 3 LLM backends resulted in a total of 15 papers being written au-\ntonomouslyby Agent Laboratorywithoutanyhumaninvolvement. Wethenrecruited10volunteer\nPhD students to review 3 randomly assigned papers each. These researchers rated the experimental\nquality, report quality, and usefulness of the generated outputs on a scale of 1 to 5. The goal of this\nevaluation is to understand the differences in quality of produced research based on the three distinct\nLLM backbones, and to understand the usefulness ofAgent Laboratoryin autonomous mode. The\ndetails of the evaluation questions are provided here:\n• Experimental Quality: What is your perception of the quality of the experimental results\npresented in this report?\n• Report Quality:What is your perception of the quality of the research report writing quality\npresented in this report?\n• Usefulness: What is your perception of the usefulness of an AI assistant tool that can generate\nthe presented report autonomously?\nTheresultsofthisevaluationindicatevariabilityinperformanceacrossdifferent Agent Laboratory\nLLM backends (Figure 5). gpt-4o consistently achieved lower scores, with an average experimental\nquality rating of 2.6/5, a report quality rating of 3.0/5, and a usefulness rating of 4.0/5. In contrast,\no1-mini generally outperformed gpt-4o in experimental quality, with an average score of 3.2/5 (+0.6),\nwhile maintaining similar levels of report quality and usefulness at 3.2/5 (+0.2) and 4.3/5 (+0.3),\nrespectively. o1-preview demonstrated the highest usefulness and report quality, averaging 4.4/5\n(+0.4 from gpt-4o and +0.1 from o1-mini) and 3.4/5 (+0.4 from gpt-4o and +0.2 from o1-mini)\nrespectively, though its experimental ratings were slightly lower than o1-mini at 2.9/5 (+0.3 from\ngpt-4o and -0.3 from o1-mini). While all backends perform comparably in terms of report and\nexperimental quality, the o1-preview model was as the most useful for research assistance, suggesting\nthat its outputs were better aligned with the expectations and needs of researchers.\n11\nAgent Laboratory: Using LLM Agents as Research Assistants\nFrom our results, the quality is demonstrated to vary based on the selected topic. We find that the\noverall highest average report quality to be 3.8/5 and usefulness to be 4.5/5 for theword ordertopic\nand the highest average experiment quality to be 3.2/5 for thecognitive biastopic. Interestingly, we\nalso find thatword orderhas the lowest experiment quality at 2.7/5 along with theimage noisetopic.\nThe image noisetopic was demonstrated to have high variance based on the LLM backend, with an\nexperiment quality score of 1.5/5 for gpt-4o and a 4.0/5 with o1-mini (+2.5 point difference) and a\nusefulness score of 2.5/5 for gpt-4o and a 4.5/5 with o1-mini (+2.0 point difference).\nIn summary, the evaluation of quality across LLM backends demonstrates clear differences in\nexperimental quality, report quality, and usefulness. While o1-preview is consistently rated as the\nmost useful for research assistance, o1-mini achieves the highest experimental quality scores, and\ngpt-4o is generally being outperformed in all areas. Topic-specific trends suggest there may exist\nvariability in the performance ofAgent Laboratoryacross difference areas of machine learning\nresearch and across backend models.\n4.1.1. Human reviewer scores by language model\nIn addition to evaluating paper quality, we also asked human reviewers to assess papers generated\nby Agent Laboratoryaccording to NeurIPS-style criteria, including quality, significance, clarity,\nsoundness, presentation, and contribution as shown in Figure 6. We evaluated the same papers\nanalyzed in Section 4.1 using the aforementioned metrics and conducted the comparison. We found\nthat the average human scores for the three backends revealed differences in performance, with\naverage overall ratings ranging from 3.5/10 with gpt-4o, 3.8/10 with o1-mini, and 4.0/10 with\no1-preview.\nFirst, when evaluating quality we find that reviewers rated gpt-4o the lowest at 1.8/4, while\no1-mini achieved the highest score of 2.3/4, demonstrating relatively better technical soundness.\nIn terms of significance, all three backends received similar scores between 2.2–2.5/4, indicating a\nmodest contribution to advancing research goals. Clarity scores showed slight variability, with gpt-4o\nreceiving 2.6/4 and o1-mini falling slightly lower at 2.1/4 (-0.5), reflecting differences in how well\nthe papers were written. The soundness of the generated outputs, which assesses the robustness of\nclaims, was rated highest for o1-preview at 2.2/4, with o1-mini and gpt-4o at 1.8 (-0.4) and 1.7.\nPresentation and contribution ratings followed similar trends, with the overall contribution score\naveraging 2.1/4 across models, highlighting a need for improvement in the originality of the outputs.\nThese scores show a general trend where human reviewers identified o1-preview as producing\nslightly better-rounded outputs compared to other backends, though significant gaps remain in\ntechnical and methodological aspects across all models. We note that the average score of an accepted\npaper at NeurIPS is 5.9. In this regard, on average, papers produced in autonomous mode are below\ntheacceptancethresholdfortopMLconferences. Theseresultsdemonstratethat, inautonomousmode,\nthere is a need for refinement ofAgent Laboratoryto meet human expectations for high-quality,\nimpactful research papers.\nAutomated Reviews versus Human Reviews. We also explore to what extent the automated\nreviewer scores align with those of human reviewers. The alignment is graphically illustrated using\nboth tabular data (for all scores) and violin plots (for overall scores) in Figure 6. Our findings suggest\nthat automated reviewers demonstrate notable discrepancies across all metrics compared with human\nevaluators, with a tendency to highly over-estimate the contribution of self-evaluated work. While the\nautomated reviewers gave an average overall above average NeurIPS paper score of 6.1/10, human\nreviewers provided a much lower average of 3.8/10 (-2.3 points). Similar gaps are observed for all\n12\nAgent Laboratory: Using LLM Agents as Research Assistants\nFigure 6|Scores from NeurIPs-style evaluation of generated papers, including the criterion: quality,\nsignificance, clarity, soundness, presentation, and contribution. (top) Split-violin plot comparing the\noverall score distribution of automated reviewers (LLM scores, left half of violin) and human reviewers\n(right half of violin). Human scores are not predictive of automated reviewer scores, demonstrating\nan average of -2.3 points lower. (middle) Automated reviewer scores across NeurIPs-style criterion.\n(bottom) Human reviewer scores across NeurIPs-style criterion.\n13\nAgent Laboratory: Using LLM Agents as Research Assistants\nspecific criteria, such as clarity and contribution, where automated reviewers rated clarity at 3.6/4\non average compared to 2.4/4 by human evaluators. This pattern holds for all criterion. Previous\nwork demonstrates high alignment with automated reviewers (Lu et al. (2024b)) and ICLR scores\nfrom OpenReview. However, with actual humans rating the generated papers, we find that automated\nreviews do not align closely with human reviews and are far from an average accepted paper at\nNeurIPS 2024, which stands at 5.85∗ (our scores were -2.05 points lower on average). Our results\ndemonstrate that it is important for human evaluations to be provided alongside automated reviewer\nscores in future works in order to obtain a better understanding of the quality of generated papers.\n4.2. Evaluation of co-pilot quality\nWe next evaluate the use ofAgent Laboratory in co-pilot mode, where a human researcher is\nproviding feedback at the end of each subtask (see Section 3.3.1 for more details). We evaluate\nperformance across two measures: (1) the quality ofAgent Laboratory as a tool for assisting\ntheir research and (2) the quality of generated papers. We first ask researchers to co-pilotAgent\nLaboratoryon a topic of their choice without limitations. We then ask researchers to select a topic\nfrom the 5 topics introduced in Section 4.1, resulting in a total of 2 papers per researcher which\nwe refer to ascustom and preselected papers respectively. After their papers are generated, we\nask researchers to rate their experience usingAgent Laboratoryduring the process of generating\ncustom and preselected papers. We then ask them to self-evaluate the generated papers according\nto NeurIPS-style criterion. Finally, we ask external researchers to evaluate their paper comparing\nperformance with Agent Laboratory in autonomous mode. All experiments used an o1-mini\nbackbone for all phases except the literature review.\n4.2.1. Quality as a tool\nThe evaluation ofAgent Laboratoryas a research tool focuses on understanding its effectiveness\nin assisting researchers during the co-pilot mode. After generating their papers, participants were\nasked to reflect on their experiences and assess the tool’s utility, usability, and overall satisfaction. We\nbegin our evaluation by asking the following questions:\n• Utility: How useful isAgent Laboratoryfor assisting your research?\n• Continuation: How likely are you to continue using Agent Laboratory for research?\n• Satisfaction: How much did you enjoy using Agent Laboratory?\n• Usability: How easy was it for you to build a project using Agent Laboratory?\nThe result of answering each question is a score from 1-5, where 1 indicates the lowest agreement\nand 5 indicates the highest. We find that the overall scores across all experiments are 3.5/5 for utility,\n3.75/5 for continuation, 3.63/5 for satisfaction, and 4.0/5 for usability (Figure 7). We also delineate\naverage scores based on custom and preselected topics. For custom experiments, we find overall\nscores of 3.75/5 for utility, 4.0/5 for continuation, 3.75/5 for satisfaction, and 3.75/5 for usability.\nFor preselected topics, we find overall scores of 3.25/5 for utility, 3.5/5 for continuation, 3.5/5\nfor satisfaction, and 4.25 for usability. Ratings for preselected topics are lower across all measures\ncompared with custom, except for usability which was -0.5 points lower. From preselected to custom,\nutility and continuation increased by +0.5 points and satisfaction increased by +0.25 points.\nWe also evaluated across the same questions reported in Section 4.1. We report an average\nexperimental quality rating of 2.38/5, a report quality rating of 3.13/5, and a usefulness rating of\n∗https://papercopilot.com/statistics/neurips-statistics/neurips-2024-statistics\n14\nAgent Laboratory: Using LLM Agents as Research Assistants\nFigure 7|Co-pilot evaluation.\n3.75/5. We find higher scores for custom topics across report quality with a rating of 3.5/5 (+0.75)\nand a usefulness rating of 4.0/5 (+0.5). For experiment quality, we find that preselected has +0.25\npoints higher with a score of 2.5/5. Scores across all metrics rated lower when compared with the\ncorresponding o1-mini autonomous evaluation results. While report quality was only rated -0.07\npoints lower, usefulness was rated -0.55 points lower and experiment quality was -0.82 points lower.\nFinally, we opened an optional question for participants to provide feedback, which asks the\nfollowing question: \"How couldAgent Laboratory be improved for your research?\" For both\ncustom and preselected topics we received a 75% response rate. From this feedback, there were\nsuggestions for improving theAgent Laboratoryinterface (e.g., adding a GUI, better inspection of\nintermediate results), adding the option to incorporate more figures for the paper, and improving\nthe literature review phase. We find that when compared to reviews ofAgent Laboratory in\nautonomous mode from Section 4.1, human co-pilots rated report quality, usefulness, and experiment\nquality lower. From feedback provided by researchers, we find the reduction in scores is due to\ndifficulty guiding the agents to execute their exact vision for the project. We discuss these limitations\nin greater detail in Section 5.\n4.2.2. Evaluation of co-pilot generated papers\nTo assess the quality of papers generated byAgent Laboratory in co-pilot mode, we conduct\nevaluations using two approaches: (1) researchers self-assessed their generated papers based on\nNeurIPS-style criteria, and (2) external researchers provided evaluations of the same papers. This\nsection aims to understand differences in scores from self-assessment and external assessment, as\nwell as how assessments compare toAgent Laboratory in fully autonomous mode. We use the\nsame NeurIPS criterion introduced in Section 4.1.1.\n15\nAgent Laboratory: Using LLM Agents as Research Assistants\nSelf-evaluation. From the results of the self-evaluation (Figure 7), we found that the average overall\nscore increasedfrom evaluations provided to papers generated in autonomous mode, with autonomous\npapers having an overall average of 3.8/10 and co-pilot papers at 4.13/10 (+0.33). These scores\neven improved across the best autonomous backend, o1-preview, which averaged 4.0/10. Across\nindividual criterion, scores increased for quality (+0.13), clarity (+0.48), soundness (+0.35), and\npresentation (+0.33), but decreased for significance and contribution. The scores that decreased\nwere significance (-0.3) and contribution (-0.1).\nExternal evaluation. We compare scores provided through self-evaluation with those provided by a\nset of external evaluators on the same papers (Figure 7). We find that average scores across most\ncriteria, including quality, significance, clarity, soundness, presentation, and contribution, show an\nimprovement in the external assessments, with an overall average of 4.38/10, up from 4.13/10 in\nself-evaluations. The most significant improvements were observed in quality (+0.62), significance\n(+0.25), and overall (+0.25) scores, suggesting that external reviewers perceived the generated\npapers to be higher quality and more significant than the researchers who produced them. However,\nclarity scores decreased (-0.25), indicating potential issues in the articulation of ideas that might\nhave been overlooked during self-assessment. While presentation scores did not improve (+0.0),\nsoundness (+0.13) and contribution (+0.13) only increased slightly.\nNotably, the external evaluations also reinforce differences between scores preselected and custom\ntopics. Unlike with the self-evaluated papers, papers on preselected topics were rated slightly higher\noverall, with improvements observed across several metrics, particularly in quality (+0.5) and\nsignificance (+0.5). These findings suggest that self-evaluated reviewers perceive the work produced\non their custom topic as higher quality compared to the work produced on preselected topics, whereas\nexternal evaluators find the opposite to be true.\nComparison with autonomous modeComparing scores by external evaluators on autonomous\nand co-pilot papers (Figure 7), we find that the largest improvements were seen for quality, which\nincreased by +0.75, soundness, which improved by +0.48, and the overall score, which improved\nby +0.58. Moderate gains were also observed in clarity (+0.23) and presentation (+0.33). In\ncontrast, some metrics showed minimal or no improvement. Significance declined slightly (-0.05),\nand contribution increased only marginally (+0.03). Our results suggest that papers generated with\nhuman involvement overall are evaluated more highly than autonomously generated paper, with much\nof the focus of human involvement going toward making the paper more presentable (presentation\nand clarity) while there was less emphasis on improving experimental results (significance and\ncontribution). Finally, we note that co-pilot overall scores, which average at 4.38, are still -1.45 points\nbelow the average score of 5.85 for an accepted paper at NeurIPS 2024. Increasing the overall score\nto match conference standards will likely result by improving the contribution and significance of the\npaper results, which is consistently lower than other evaluation metrics.\n4.3. Runtime statistics\nRuntime statistics forAgent Laboratory are detailed to provide insight into the computational\nefficiency and monetary costs associated with different phases of its workflow. In this evaluation,\nboth the time required per phase (measured in seconds) and the costs incurred (calculated in USD)\nwere analyzed to better understand the performance of three model backends: gpt-4o, o1-mini, and\no1-preview. These measurements were recorded for each subtask, including Literature Review, Plan\nFormulation, Data Preparation, Running Experiments, Results Interpretation, Report Writing, and\nReport Refinement.\n16\nAgent Laboratory: Using LLM Agents as Research Assistants\nFigure 8|Performance and Cost Evaluation. This table summarizes the runtime statistics, cost, and\nsuccess rates of Agent Laboratory across its workflow phases using three different model backends:\ngpt-4o, o1-mini, and o1-preview. The metrics include average cost per phase (in USD), average time\nper phase (in seconds), and success rates for each phase.\nInference time Across all models, gpt-4o exhibited the fastest execution times, completing the\nentire workflow in 1165.4 seconds, approximately 3.2x faster than o1-mini and 5.3x faster than\no1-preview, which required 3616.8 seconds and 6201.3 seconds, respectively. In most subtasks, gpt-4o\ndemonstrated superior speed, particularly in Running Experiments and Report Writing phases, where\nits times were significantly shorter than those of o1-mini and o1-preview. For instance, in Running\nExperiments, gpt-4o averaged 417.8 seconds, while o1-mini and o1-preview took 2082.5 seconds\nand 4036.2 seconds, respectively. Similarly, for Report Writing, gpt-4o completed the task in 572.5\nseconds, compared to 827.7 seconds for o1-mini and 1854.2 seconds for o1-preview.\nInferencecost Monetarycostsperworkflowwerealsosubstantiallylowerforgpt-4o, whichaveraged\njust $2.33 for the entire process. This is significantly more cost effective than previous autonomous\nresearch workflows (Lu et al. (2024b)), which cost around∼$15 (6.4x more expensive) to complete\nusing gpt-4o. Other models in our workflow has a lower cost efficiency, such as o1-mini at $7.51, and\no1-preview at $13.10, the latter being over 5.6x more expensive than gpt-4o. Among the individual\nsubtasks, gpt-4o consistently had the lowest costs. For example, its costs for Data Preparation and\nReport Writing were $0.09 and $1.73, respectively, compared to $3.03 and $2.58 for o1-mini, and\n$0.30 and $9.58 for o1-preview.\n17\nAgent Laboratory: Using LLM Agents as Research Assistants\nFigure 9|Average score of four methods (MLAB, OpenHands, AIDE, and mle-solver) on a subset of\nMLE-Bench.\nPhase-level Observations From our observations at the phase-level, Literature Review was notably\nefficient for all models in terms of time and cost, with gpt-4o completing it in 92.9 seconds at a cost\nof $0.12. Meanwhile, o1-mini completed this phase faster (56.8 seconds) but at a slightly higher cost\n($0.16). For Plan Formulation, gpt-4o was both the fastest (23.3 seconds) and the cheapest ($0.03),\nfollowed closely by o1-preview in cost ($0.04) but not in speed (33.1 seconds). The most expensive\nphase across models was Report Writing, where costs were driven by the increased computational\nresources required for writing a long document. o1-preview incurred particularly high costs in this\nphase ($9.58) despite producing comparable outputs in terms of task success rates.\nSuccess Rates Overall, every model exhibits reasonably high reliability, with o1-preview achieving\nthe highest average subtask success rate (95.7%) for the entire workflow. Both gpt-4o and o1-mini\nfollowed closely at 94.3% and 92.8%. While most tasks had 100% success rate for each model,\nthe literature review phase had a high rate of failure, at 60%, 70%, and 80% for gpt-4o, o1-mini,\nand o1-preview respectively. The Data Preparation phase showed minor challenges, with o1-mini\nrecording an 80% success rate in Data Preparation, compared to gpt-4o’s 100% success rate and\no1-preview at a 90% success rate.\n4.4. Evaluating mle-solver on MLE-Bench\nEvaluating the entireAgent Laboratoryworkflow does not contain much information about the\nability ofmle-solverspecifically to solve individual ML problems. In order to evaluatemle-solver\nmore objectively, we use a subset of 10 ML challenges from MLE-Bench (Chan et al. (2024)). MLE-\nBench is a benchmark designed to assess the capability of agents in handling real-world ML tasks on\nKaggle competitions. This benchmark compares agent performances with human baselines, scoring\nagents with Kaggle’s medal system, and incorporating mechanisms to mitigate contamination and\nplagiarism risks. We include all challenges focusing on text and tabular data from the low complexity\ncategoryofMLE-Bench. Weprovideasinputto mle-solverthefollowing: Kaggledatasetdescription,\ndistilled knowledge from Kaggle notebooks, as well as an accessible train and dev set. Instead of\nusing an LLM scoring function, themle-solverscore is evaluated on the dev set, which is a 20%\nrandom sample taken from the original training set, and the training set is represented by the other\n80% split. All data (dev, test, train) is placed into arrays using the numpy library instead of providing\n18\nAgent Laboratory: Using LLM Agents as Research Assistants\nfile locations in order to better emulate the data preparation phase. Once allmle-solver steps\nhave concluded, the final code with the highest score is evaluated on the actual Kaggle test set and a\nbenchmark score is recorded.\nWe compare average scores across several runs from three other methods: MLAB (Huang et al.\n(2024), gpt-4o backend), OpenHands (Wang et al. (2024b), gpt-4o backend), and AIDE (Schmidt\net al. (2024), o1-preview backend). Whilemle-solversubmitted valid solutions for all MLE-Bench\nchallenges within two hours, prior methods often failed to submit, complicating scoring. We thus\ncalculated average scores by excluding invalid submissions from other works and averaging valid\nones. We find thatAgent Laboratory’s mle-solveris more consistently high scoring than other\nsolvers, withmle-solverobtaining four medals (two gold, one silver, and one bronze) compared\nwith OpenHands (gpt-4o) obtaining two medals (two gold), AIDE (o1-preview) obtaining two medals\n(one gold, one bronze) and MLAB obtaining zero medals. Additionally,mle-solverobtained above\nmedian human performance on six out of ten benchmarks, with AIDE obtaining five out of ten,\nOpenHands two out of ten, and MLAB zero out of ten. A detailed overview is provided in Figure 9.\n5. Limitations\nWhile our results suggest thatAgent Laboratorydemonstrates strong performance as a research\ntool, we now turn to a discussion of limitations that could inform future work. While some of these\nare also limitations of LLMs themselves, others are not, and we nonetheless provide a thorough and\ncritical discussion of our work. We hope that progress in autonomous research will address these\nlimitations.\n5.1. Workflow limitations\nChallenges with self-evaluation The paper-solveris being evaluated for quality by using LLMs\nemulated NeurIPS reviewers. This has two limitations: (1) while the reviewing agents were shown to\nhave high alignment with real reviewers (Lu et al. (2024b)),qualitativelyresearch reports fromAgent\nLaboratoryare less satisfying than research papers from The AI Scientist (Lu et al. (2024b)), with\nours having lower quality figures, despiteAgent Laboratorypapers obtaining higher scores overall.\n(2) The research reports produced byAgent Laboratoryare not meant to replace the paper writing\nprocess done by humans as it was in The AI Scientist, rather it is meant to provide a report for the\nhuman to understand what has been accomplished, so that they can scale up the experiment and write\ntheir own research report. However, we nonetheless use NeurIPS reviewer scores as the heuristic for\nthe quality of our presentedpaper-solver, which aims to evaluate the reports from the perspective\nof a complete research paper. Additionally, contrasting with Lu et al. (2024b) demonstrate that LLMs\nperform less reliably for self-evaluation compared with human reviewers, with lower agreement scores\n(53.3% vs. 56.1%). Although LLMs demonstrate reasonable consistency, this may stem from reliance\non superficial patterns rather than robust evaluation criteria, resulting in discrepancies between LLM\nand human rankings. This limits LLMs in subjective tasks like research idea evaluation, which is the\nfoundation ofmle-solverand paper-solver.\nChallenges with automated structure There are also some limitations that present themselves due\nto the structure enforced in the workflow. For example,paper-solveris encouraged to a organize\nthe paper into a relatively fixed structure (abstract, introduction, etc), which disallows unique\npaper organizations and section orders. Another limitation is thatmle-solverand paper-solver\nare limited to generating only two figures for the paper. This can be solved in future work, by\nallowing all of the figures generated by themle-solver(without restriction) to be incorporated into\n19\nAgent Laboratory: Using LLM Agents as Research Assistants\npaper-solverbydetectingimagefilesandprovidingthosepathstothesolver. Agent Laboratory\nisalsonotabletomanagerepository-levelcodeonitsown, butrathertheappropriatefilesareprovided\nto it at each necessary step and files are saved based on which phase produced the file. Enabling\nflexible repository-level file modification and execution is a clear next step for future work.\nChallenges with hallucination While uncommon, we also found that in some of the research\npapers, particularlyfromlowerperformingmodels, suchasgpt-4o, therewerehallucinationsregarding\nexperimental results that did not occur, such as the following example from a gpt-4o paper on the topic\nof Are image transformers more or less sensitive to noise than convolutional networks?: “Hyperparameter\noptimization played a crucial role in achieving these results. The learning rate was set at0.001, with a\nbatch size of32, and the number of reasoning steps𝐿= {𝑙1,𝑙2,...,𝑙 𝑛}varied between5 to10, depending on\nthe complexity of the query. The model was trained over50 epochs, with early stopping criteria applied to\nprevent overfitting.\" While the issue of hallucination is more generally a problem with LLMs themselves,\nfuture work must appropriately address these challenges in order to prevent misinformation from\nbeing propagated when using automated research tools.\n5.2. Common failure modes\nIn addition to the limitations outlined in Section 5.1, we also outline common failure modes observed\nduring the runtime ofAgent Laboratory. We report a list of the most common failure modes\nobserved below:\n• Many of the more capable models (gpt-4o, o1-mini, o1-preview) struggled with instruction-\nfollowingduringtheliteraturereviewphase,andhadatendencytorepeatedlyusethe summarize\ncommand until the maximum phase steps have been reached, leading to a termination.\n• Retrieved papers during the literature review phase had been observed to reach the maximum\ntoken limit for some models.\n• When generating figures for the paper usingmle-solver, the figure legends, titles, or often\n• Experiments run bymle-solversometimes obtain0% accuracy for all tested methods which\nis not corrected by the agent by the timemle-solverruns out of solving steps.\n• mle-solver has a tendency to edit line0 more than other lines in the code, causing to the\nreplacecommand to more often lead to successful code compiles.\n• Printed output from the data preparation or experimental results can lead to the LLMs reaching\ntheir token limit.\n• mle-solveroften generated the pythonexit() command, which terminated the entire process.\nThis had to be detected and removed manually.\n• mle-solver has been observed to run system commands on the host computer using the\nsubprocess.run() command. While nothing problematic has been observed, safeguards should\nbe implemented around this.\n• paper-solveroften struggles to search for relevant papers using the arXiv engine. Before a\nsearch time-limit was enforced, it could take up to100 tries for a successful search query to\nreturn any papers. A limit of5 was place thereafter to prevent this cycle.\n5.3. Ethical considerations\nAgent Laboratoryoffers potential to accelerate the field of machine learning research by automat-\ning time-intensive tasks and enabling researchers to focus on ideation and experimental design.\nHowever, its capabilities also bring ethical challenges that require careful consideration. The ability\n20\nAgent Laboratory: Using LLM Agents as Research Assistants\nto autonomously generate research code, reports, and experiment plans may inadvertently lower the\nbarriers to producing substandard or misleading scientific outputs. This could overwhelm peer review\nsystems and jeopardize the integrity of academic discourse. Furthermore, the automated processes\nmay reflect or even amplify biases inherent in the underlying datasets or algorithms, leading to\nskewed outcomes in research findings. Transparent disclosure of AI involvement in research outputs\nis important in order to mitigate such risks and maintain accountability.\nThere are additional concerns about potential misuse ofAgent Laboratoryfor unethical pur-\nposes, such as developing harmful technologies or generating content that bypasses ethical oversight.\nFor instance, the misuse of autonomous research agents in fields like cybersecurity could lead to the\nautomated creation of malware (Begou et al. (2023); Francia et al. (2024); Happe & Cito (2023); Xu\net al. (2024)) or in environmental studies, it may generate biased analyses that downplay climate\nrisks or overstate the benefits of certain interventions. Moreover, as the platform matures, the risk\nof its misuse increases if safeguards are not implemented to ensure alignment with ethical research\nstandards (Jiao et al. (2024); Watkins (2024)). Thus, whileAgent Laboratorydemonstrates im-\nmense promise for accelerating scientific discovery, there is a need for robust governance mechanisms\nto ensure that the underlying LLMs produce content that aligns with ethical principles and societal\nvalues.\n6. Discussion\nIn this paper, we introduceAgent Laboratory, an open-source LLM agent framework for accelerat-\ning the individual’s ability to perform research in machine learning. Unlike fully automated research\npipelines that attempt to conceive their own research directions,Agent Laboratoryis designed as\na co-pilot, enabling a more human-centric mode of scientific exploration. Because of this, we present\nresults from human-centered experiments. Our initial evaluations focused on the quality of gener-\nated papers in autonomous mode, assessing human evaluations of experimental and report quality,\nusefulness, as well as reviewer scores based on standard academic criteria across different language\nmodels. We also assessed the effectiveness ofAgent Laboratoryin co-pilot mode, comparing its\nperformance with autonomous mode, receiving positive feedback from researchers.\nThefindingsofthisworkhighlightthevariabilityinperformanceacrossLLMbackends, withtheo1-\npreview model being rated most useful, while o1-mini demonstrated the highest experimental quality.\nAutonomous mode outputs, although generally well-received, revealed gaps when evaluated against\nhuman expectations for high-quality research papers, particularly in terms of clarity and soundness.\nWe also find that automated reviewer scores do not predict human reviewer scores demonstrating\nthe importance of human evaluations inautomated research. ntegrating human feedback in co-pilot\nmode overall produced higher-quality outputs than autonomous mode, with higher scores across most\nmetrics. The co-pilot feature inAgent Laboratoryis overall found to have high utility and usability\nwhen rated by human users, with most participants deciding to continue usage after their experience.\nFinally, runtime and cost analyses demonstrated the efficiency of the framework, with the gpt-4o\nbackend offering the fastest execution and lowest costs. Finally, evaluations of themle-solveron\nMLE-Bench demonstrates improved ability to solve general ML problems over previous methods.\nAgent Laboratorybuilds upon an emerging trend in the use of language agents for science,\nwhere previous works have shown the potential of LLMs to generate research ideas (Baek et al.\n(2024); Li et al. (2024a); Si et al. (2024)), implement machine learning projects (Chan et al. (2024);\nHuang et al. (2024); Jing et al. (2024)), and even produce scientific papers (Lu et al. (2024b)).\nWhile many of these prior efforts leverage LLMs as tools to be applied at discrete stages,Agent\nLaboratoryintegrates these processes into a single, continuous pipeline that can scale and adapt to\n21\nAgent Laboratory: Using LLM Agents as Research Assistants\nthe researcher’s desired level of interaction and compute availability. This allows human researchers\nto focus more on conceptual design and critical thinking, allowingAgent Laboratoryto handle\nmore tedious tasks, such as preprocessing data and coding.\nWe overcome the limitations of prior work, such as The AI Scientist (Lu et al. (2024b)) which\ndoes not have human-computer interaction, Virtual Lab (Swanson et al. (2024)) which does not have\naccess to up-to-date knowledge, does not generate research papers, and was only demonstrated for\nnanobody design, as well as ChemCrow (M. Bran et al. (2024)) and Coscientist (Boiko et al. (2023))\nwhich cannot solve open-ended research problems. However, as was outlined in Limitations (Section\n5), there are many areas for improvement in our approach which can be addressed in future work.\nA valuable direction for future research could involve a longitudinal study comparing researchers’\noutcomes when conducting studies with and withoutAgent Laboratory, as the human evaluations\nin this work provide only a snapshot of its utility. Studies of this kind have been conducted with other\nworkflow automation tools, such as GitHub Copilot (Dohmke et al. (2023); Ziegler et al. (2024)),\nand have demonstrated promising potential for improving productivity. Such a study would help to\nbetter understand the long-term impact ofAgent Laboratoryon research efficiency and its role in\nimproving scientific discovery. It may also be worth exploring automatic agent workflow (Hong et al.\n(2023); Li et al. (2024c); Zhuge et al. (2024)) and agent generation techniques (Chen et al. (2023a);\nHu et al. (2024a)) to optimize theAgent Laboratoryworkflow.\nConclusion In conclusion,Agent Laboratorystands as a promising step toward more efficient,\nhuman-centered research workflows that leverage the power of LLMs. By integrating specialized\nautonomous agents guided by human oversight, our approach can help researchers spend less time\non repetitive tasks and more time on the creative, conceptual aspects of their work. We hope that\nAgent Laboratorymay ultimately serve as a tool to enable scientific discovery.\nReferences\nTalor Abramovich, Meet Udeshi, Minghao Shao, Kilian Lieret, Haoran Xi, Kimberly Milner, Sofija\nJancheska, John Yang, Carlos E Jimenez, Farshad Khorrami, et al. Enigma: Enhanced interactive\ngenerative model agent for ctf challenges.arXiv preprint arXiv:2409.16165, 2024.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774, 2023.\nAnirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, and Tianyu Gao. Litsearch:\nA retrieval benchmark for scientific literature search.arXiv preprint arXiv:2407.18940, 2024.\nAltera AL, Andrew Ahn, Nic Becker, Stephanie Carroll, Nico Christie, Manuel Cortes, Arda Demirci,\nMelissaDu, FrankieLi, ShuyingLuo, etal. Projectsid: Many-agentsimulationstowardaicivilization.\narXiv preprint arXiv:2411.00114, 2024.\nBarrett R Anderson, Jash Hemant Shah, and Max Kreminski. Homogenization effects of large language\nmodels on human creative ideation. InProceedings of the 16th Conference on Creativity & Cognition,\npp. 413–425, 2024.\nAI Anthropic. The claude 3 model family: Opus, sonnet, haiku.Claude-3 Model Card, 1, 2024.\n22\nAgent Laboratory: Using LLM Agents as Research Assistants\nJoshua Ashkinaze, Julia Mendelsohn, Li Qiwei, Ceren Budak, and Eric Gilbert. How ai ideas affect\nthe creativity, diversity, and evolution of human ideas: Evidence from a large, dynamic experiment.\narXiv preprint arXiv:2401.13481, 2024.\nAshwini Ashokkumar, Luke Hewitt, Isaias Ghezae, and Robb Willer. Predicting results of social science\nexperiments using large language models. Technical report, Technical report, Working Paper, 2024.\nJinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. Researchagent: Iterative\nresearch idea generation over scientific literature with large language models.arXiv preprint\narXiv:2404.07738, 2024.\nNils Begou, Jérémy Vinoy, Andrzej Duda, and Maciej Korczyński. Exploring the dark side of ai:\nAdvanced phishing attack design and deployment using chatgpt. In2023 IEEE Conference on\nCommunications and Network Security (CNS), pp. 1–6. IEEE, 2023.\nKevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai,\nLachy Groom, Karol Hausman, Brian Ichter, et al.𝜋0: A vision-language-action flow model for\ngeneral robot control.arXiv preprint arXiv:2410.24164, 2024.\nDaniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with\nlarge language models.Nature, 624(7992):570–578, 2023.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics\ntransformer for real-world control at scale.arXiv preprint arXiv:2212.06817, 2022.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski,\nTianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models\ntransfer web knowledge to robotic control.arXiv preprint arXiv:2307.15818, 2023.\nTom B Brown. Language models are few-shot learners.arXiv preprint arXiv:2005.14165, 2020.\nTuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, and Chien-Sheng Wu.\nArt or artifice? large language models and the false promise of creativity. InProceedings of the CHI\nConference on Human Factors in Computing Systems, pp. 1–34, 2024.\nJun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio\nStarace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. Mle-bench: Evaluating machine learning\nagents on machine learning engineering.arXiv preprint arXiv:2410.07095, 2024.\nGuangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu, and Yemin\nShi. Autoagents: A framework for automatic agent generation.arXiv preprint arXiv:2309.17288,\n2023a.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code.arXiv preprint arXiv:2107.03374, 2021.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu,\nYi-Hsin Hung, Chen Qian, et al. Agentverse: Facilitating multi-agent collaboration and exploring\nemergent behaviors. InThe Twelfth International Conference on Learning Representations, 2023b.\nXiuying Chen, Tairan Wang, Taicheng Guo, Kehan Guo, Juexiao Zhou, Haoyang Li, Mingchen Zhuge,\nJürgen Schmidhuber, Xin Gao, and Xiangliang Zhang. Scholarchemqa: Unveiling the power of\nlanguage models in chemical research question answering.arXiv preprint arXiv:2407.16931, 2024a.\n23\nAgent Laboratory: Using LLM Agents as Research Assistants\nZiru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao,\nChen Wei, Zitong Lu, et al. Scienceagentbench: Toward rigorous assessment of language agents for\ndata-driven scientific discovery.arXiv preprint arXiv:2410.05080, 2024b.\nMike D’Arcy, Tom Hope, Larry Birnbaum, and Doug Downey. Marg: Multi-agent review generation\nfor scientific papers.arXiv preprint arXiv:2401.04259, 2024.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su.\nMind2web: Towards a generalist agent for the web.Advances in Neural Information Processing\nSystems, 36, 2024.\nNing Ding, Shang Qu, Linhai Xie, Yifei Li, Zaoqu Liu, Kaiyan Zhang, Yibai Xiong, Yuxin Zuo, Zhangren\nChen, Ermo Hua, et al. Automating exploratory proteomics research via language models.arXiv\npreprint arXiv:2411.03743, 2024.\nThomas Dohmke, Marco Iansiti, and Greg Richards. Sea change in software development: Economic\nand productivity analysis of the ai-powered developer lifecycle.arXiv preprint arXiv:2306.15033,\n2023.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024.\nRichard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel Kang. Llm agents can autonomously\nhack websites.arXiv preprint arXiv:2402.06664, 2024.\nAlhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Moham-\nmadamin Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz\nSwirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning.\nNature, 610(7930):47–53, 2022.\nXidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali\nDu, and Jun Wang. Chessgpt: Bridging policy learning and language modeling.Advances in Neural\nInformation Processing Systems, 36, 2024.\nJerson Francia, Derek Hansen, Ben Schooley, Matthew Taylor, Shydra Murray, and Greg Snow.\nAssessing ai vs human-authored spear phishing sms attacks: An empirical study using the trapd\nmethod. arXiv preprint arXiv:2406.13049, 2024.\nAlireza Ghafarollahi and Markus J Buehler. Protagents: protein discovery via large language model\nmulti-agent collaborations combining physics and machine learning.Digital Discovery, 2024a.\nAlireza Ghafarollahi and Markus J Buehler. Sciagents: Automating scientific discovery through\nmulti-agent intelligent graph reasoning.arXiv preprint arXiv:2409.05556, 2024b.\nAntoine Grosnit, Alexandre Maraval, James Doran, Giuseppe Paolo, Albert Thomas, Refinath Shahul\nHameed Nabeezath Beevi, Jonas Gonzalez, Khyati Khandelwal, Ignacio Iacobacci, Abdelhakim\nBenechehab, et al. Large language models orchestrating structured reasoning achieve kaggle\ngrandmaster level.arXiv preprint arXiv:2411.03562, 2024.\nKen Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran\nPan, Teng Wu, Jiaqian Yu, et al. Blade: Benchmarking language model agents for data-driven\nscience. arXiv preprint arXiv:2408.09667, 2024.\n24\nAgent Laboratory: Using LLM Agents as Research Assistants\nSiyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang. Ds-agent: Automated\ndata science by empowering large language models with case-based reasoning.arXiv preprint\narXiv:2402.17453, 2024.\nIzzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and\nAleksandra Faust. A real-world webagent with planning, long context understanding, and program\nsynthesis. arXiv preprint arXiv:2307.12856, 2023.\nNam Le Hai, Dung Manh Nguyen, and Nghi DQ Bui. Repoexec: Evaluate code generation with a\nrepository-level executable benchmark.arXiv preprint arXiv:2406.11927, 2024.\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen language\nmodels with massive tools via tool embeddings.Advances in neural information processing systems,\n36, 2024.\nAndreas Happe and Jürgen Cito. Getting pwn’d by ai: Penetration testing with large language models.\nIn Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on\nthe Foundations of Software Engineering, pp. 2082–2086, 2023.\nTomas Hayes, Roshan Rao, Halil Akin, Nicholas J Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil,\nVincent Q Tran, Jonathan Deaton, Marius Wiggert, et al. Simulating 500 million years of evolution\nwith a language model.bioRxiv, pp. 2024–07, 2024.\nHongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and\nDong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models.arXiv\npreprint arXiv:2401.13919, 2024.\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang,\nSteven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent\ncollaborative framework.arXiv preprint arXiv:2308.00352, 2023.\nShengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. arXiv preprint\narXiv:2408.08435, 2024a.\nXueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su,\nJingjing Xu, Ming Zhu, et al. Infiagent-dabench: Evaluating agents on data analysis tasks.arXiv\npreprint arXiv:2401.05507, 2024b.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.\nLarge language models can self-improve.arXiv preprint arXiv:2210.11610, 2022.\nQian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: Evaluating language agents\non machine learning experimentation. InForty-first International Conference on Machine Learning,\n2024.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os-\ntrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint\narXiv:2410.21276, 2024.\nTal Ifargan, Lukas Hafner, Maor Kern, Ori Alcalay, and Roy Kishony. Autonomous llm-driven research\nfrom data to human-verifiable research papers.arXiv preprint arXiv:2404.17605, 2024.\nJunfeng Jiao, Saleh Afroogh, Yiming Xu, and Connor Phillips. Navigating llm ethics: Advancements,\nchallenges, and future directions.arXiv preprint arXiv:2406.18841, 2024.\n25\nAgent Laboratory: Using LLM Agents as Research Assistants\nCarlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik\nNarasimhan. Swe-bench: Can language models resolve real-world github issues?arXiv preprint\narXiv:2310.06770, 2023.\nLiqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang,\nXinya Du, and Dong Yu. Dsbench: How far are data science agents to becoming data science\nexperts? arXiv preprint arXiv:2409.07703, 2024.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,\nKathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate\nprotein structure prediction with alphafold.nature, 596(7873):583–589, 2021.\nHao Kang and Chenyan Xiong. Researcharena: Benchmarking llms’ ability to collect and organize\ninformation as research agents.arXiv preprint arXiv:2406.10291, 2024.\nJi Woong Kim, Tony Z Zhao, Samuel Schmidgall, Anton Deguet, Marin Kobilarov, Chelsea Finn, and\nAxel Krieger. Surgical robot transformer (srt): Imitation learning for surgical tasks. In8th Annual\nConference on Robot Learning, 2024.\nJakub Lála, Odhran O’Donoghue, Aleksandar Shtedritski, Sam Cox, Samuel G Rodriques, and An-\ndrew D White. Paperqa: Retrieval-augmented generative agent for scientific research.arXiv preprint\narXiv:2312.07559, 2023.\nSteven A Lehr, Aylin Caliskan, Suneragiri Liyanage, and Mahzarin R Banaji. Chatgpt as research\nscientist: Probing gpt’s capabilities as a research librarian, research ethicist, data generator, and\ndata predictor.Proceedings of the National Academy of Sciences, 121(35):e2404328121, 2024.\nGuohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Com-\nmunicative agents for\" mind\" exploration of large language model society.Advances in Neural\nInformation Processing Systems, 36:51991–52008, 2023.\nLong Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming\nJiang, Yifei Xin, Ronghao Dang, et al. Chain of ideas: Revolutionizing research via novel idea\ndevelopment with llm agents.arXiv preprint arXiv:2410.13185, 2024a.\nSihangLi, JinHuang, JiaxiZhuang, YaoruiShi, XiaochenCai, MingjunXu,XiangWang,LinfengZhang,\nGuolin Ke, and Hengxing Cai. Scilitllm: How to adapt llms for scientific literature understanding.\narXiv preprint arXiv:2408.15545, 2024b.\nZelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, and\nYongfeng Zhang. Autoflow: Automated workflow generation for large language model agents.arXiv\npreprint arXiv:2407.12821, 2024c.\nWeixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli,\nSiyu He, Daniel Scott Smith, Yian Yin, et al. Can large language models provide useful feedback on\nresearch papers? a large-scale empirical analysis.NEJM AI, 1(8):AIoa2400196, 2024.\nXinna Lin, Siqi Ma, Junjie Shan, Xiaojing Zhang, Shell Xu Hu, Tiannan Guo, Stan Z Li, and Kaicheng\nYu. Biokgbench: A knowledge graph checking benchmark of ai agent for biomedical science.arXiv\npreprint arXiv:2407.00466, 2024.\nYiren Liu, Si Chen, Haocong Cheng, Mengxia Yu, Xiao Ran, Andrew Mo, Yiliu Tang, and Yun Huang.\nHow ai processing delays foster creativity: Exploring research question co-creation with an llm-\nbased agent. InProceedings of the CHI Conference on Human Factors in Computing Systems, pp.\n1–25, 2024.\n26\nAgent Laboratory: Using LLM Agents as Research Assistants\nChris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist:\nTowards fully automated open-ended scientific discovery.arXiv preprint arXiv:2408.06292, 2024a.\nChris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scientist:\nTowards fully automated open-ended scientific discovery.arXiv preprint arXiv:2408.06292, 2024b.\nXiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K Nejad, Felipe Yáñez, Bati Yilmaz, Kangjoo\nLee, Alexandra O Cohen, Valentina Borghesani, Anton Pashkov, et al. Large language models\nsurpass human experts in predicting neuroscience results.Nature Human Behaviour, pp. 1–11,\n2024.\nAndres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller.\nAugmenting large language models with chemistry tools.Nature Machine Intelligence, pp. 1–11,\n2024.\nBodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh\nMeena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, and Peter Clark. Discoverybench:\nTowards data-driven discovery with large language models.arXiv preprint arXiv:2407.01725, 2024.\nBenjamin S Manning, Kehang Zhu, and John J Horton. Automated social science: Language models\nas scientist and subjects. Technical report, National Bureau of Economic Research, 2024.\nDaniel McDuff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal,\nYash Sharma, Shekoofeh Azizi, Kavita Kulkarni, et al. Towards accurate differential diagnosis with\nlarge language models.arXiv preprint arXiv:2312.00164, 2023.\nAmil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Do-\ngus Cubuk. Scaling deep learning for materials discovery.Nature, 624(7990):80–85, 2023.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and\nCaimingXiong. Codegen: Anopenlargelanguagemodelforcodewithmulti-turnprogramsynthesis.\narXiv preprint arXiv:2203.13474, 2022.\nOpenAI. Introducing chatgpt. https://openai.com/index/chatgpt/, November 2022. Blog\npost.\nOpenAI. Introducing openai o1-preview, September 2024. URLhttps://openai.com/index/\nintroducing-openai-o1-preview/. Accessed: 2024-09.\nVishakh Padmakumar and He He. Does writing with language models reduce content diversity? In\nThe Twelfth International Conference on Learning Representations, 2024.\nHuy Nhat Phan, Tien N Nguyen, Phong X Nguyen, and Nghi DQ Bui. Hyperagent: Generalist software\nengineering agents to solve coding tasks at scale.arXiv preprint arXiv:2409.16299, 2024.\nOri Press, Andreas Hochlehnert, Ameya Prabhu, Vishaal Udandarao, Ofir Press, and Matthias Bethge.\nCiteme: Can language models accurately cite scientific claims?arXiv preprint arXiv:2407.12861,\n2024.\nPranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and\nRafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents.arXiv preprint\narXiv:2408.07199, 2024.\n27\nAgent Laboratory: Using LLM Agents as Research Assistants\nEdward O Pyzer-Knapp, Jed W Pitera, Peter WJ Staar, Seiji Takeda, Teodoro Laino, Daniel P Sanders,\nJames Sexton, John R Smith, and Alessandro Curioni. Accelerating materials discovery using\nartificial intelligence, high performance computing and robotics.npj Computational Materials, 8\n(1):84, 2022.\nChen Qian, Yufan Dang, Jiahao Li, Wei Liu, Zihao Xie, Yifei Wang, Weize Chen, Cheng Yang, Xin\nCong, Xiaoyin Che, et al. Experiential co-learning of software-developing agents.arXiv preprint\narXiv:2312.17025, 2023.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen,\nYusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. In\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 15174–15186, 2024.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world\napis. arXiv preprint arXiv:2307.16789, 2023.\nMatthew Renze and Erhan Guven. Self-reflection in llm agents: Effects on problem-solving perfor-\nmance. arXiv preprint arXiv:2405.06682, 2024.\nBernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan\nKumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al.\nMathematical discoveries from program search with large language models.Nature, 625(7995):\n468–475, 2024.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke\nZettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. InThirty-seventh Conference on Neural Information Processing Systems,\n2023. URLhttps://openreview.net/forum?id=Yacmpz84TH.\nSamuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor.\nAgentclinic: a multimodal agent benchmark to evaluate ai in simulated clinical environments.arXiv\npreprint arXiv:2405.07960, 2024.\nDominik Schmidt, Zhengyao Jiang, and Yuxiang Unknown. Introducing weco aide, 2024. URL\nhttps://www.weco.ai/blog/technical-report.\nTianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An\nopen-domain platform for web-based agents. InInternational Conference on Machine Learning, pp.\n3135–3144. PMLR, 2017.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\nLanguage agents with verbal reinforcement learning.Advances in Neural Information Processing\nSystems, 36, 2024.\nChengleiSi, DiyiYang, andTatsunoriHashimoto. Canllmsgeneratenovelresearchideas? alarge-scale\nhuman study with 100+ nlp researchers.arXiv preprint arXiv:2409.04109, 2024.\nXiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang,\nDayuan Fu, Huangxuan Wu, Bin Liang, et al. Cs-bench: A comprehensive benchmark for large\nlanguage models towards computer science mastery.arXiv preprint arXiv:2406.08587, 2024.\nKyle Swanson, Wesley Wu, Nash L Bulaong, John E Pak, and James Zou. The virtual lab: Ai agents\ndesign new sars-cov-2 nanobodies with experimental validation.bioRxiv, pp. 2024–11, 2024.\n28\nAgent Laboratory: Using LLM Agents as Research Assistants\nNathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J\nMcDermott, Max Gallant, Ekin Dogus Cubuk, Amil Merchant, et al. An autonomous laboratory for\nthe accelerated synthesis of novel materials.Nature, 624(7990):86–91, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient\nfoundation language models.arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models.arXiv preprint arXiv:2307.09288, 2023b.\nTao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang,\nBrenna Li, Mohamed Amin, Nenad Tomasev, et al. Towards conversational diagnostic ai.arXiv\npreprint arXiv:2401.05654, 2024.\nA Vaswani. Attention is all you need.Advances in Neural Information Processing Systems, 2017.\nShengye Wan, Cyrus Nikolaidis, Daniel Song, David Molnar, James Crnkovich, Jayson Grace, Manish\nBhatt, Sahana Chennabasappa, Spencer Whitman, Stephanie Ding, et al. Cyberseceval 3: Advancing\nthe evaluation of cybersecurity risks and capabilities in large language models.arXiv preprint\narXiv:2408.01605, 2024.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models.arXiv\npreprint arXiv: Arxiv-2305.16291, 2023.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\nFrontiers of Computer Science, 18(6):186345, 2024a.\nXingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi\nSong, Bowen Li, Jaskirat Singh, et al. Opendevin: An open platform for ai software developers as\ngeneralist agents.arXiv preprint arXiv:2407.16741, 2024b.\nRyan Watkins. Guidance for researchers and peer-reviewers on the ethical use of large language\nmodels (llms) in scientific research workflows.AI and Ethics, 4(4):969–974, 2024.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning in large language models.Advances in neural\ninformation processing systems, 35:24824–24837, 2022.\nYixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and Linyi\nYang. Cycleresearcher: Improving automated research via automated review. arXiv preprint\narXiv:2411.00816, 2024.\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang,\nXiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent\nconversation framework.arXiv preprint arXiv:2308.08155, 2023.\nJiacen Xu, Jack W Stokes, Geoff McDonald, Xuesong Bai, David Marshall, Siyue Wang, Adith Swami-\nnathan, and Zhou Li. Autoattacker: A large language model guided system to implement automatic\ncyber-attacks. arXiv preprint arXiv:2403.01038, 2024.\n29\nAgent Laboratory: Using LLM Agents as Research Assistants\nJohn Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and\nOfir Press. Swe-agent: Agent-computer interfaces enable automated software engineering.arXiv\npreprint arXiv:2405.15793, 2024.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.\nTree of thoughts: Deliberate problem solving with large language models.Advances in Neural\nInformation Processing Systems, 36, 2024.\nXingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen,\nDongsub Shim, Honglak Lee, et al. Massw: A new dataset and benchmark tasks for ai-assisted\nscientific workflows.arXiv preprint arXiv:2406.06357, 2024.\nYilun Zhou, Caiming Xiong, Silvio Savarese, and Chien-Sheng Wu. Shared imagination: Llms\nhallucinate alike.arXiv preprint arXiv:2407.16604, 2024.\nMingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen\nSchmidhuber. Gptswarm: Language agents as optimizable graphs. InForty-first International\nConference on Machine Learning, 2024.\nAlbert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh\nSittampalam, and Edward Aftandilian. Measuring github copilot’s impact on productivity.Commu-\nnications of the ACM, 67(3):54–63, 2024.\n30\nAgent Laboratory: Using LLM Agents as Research Assistants\nA. Agent Laboratory configuration\nA.1. Hyperparameters\nTable 1|Hyperparameters forAgent Laboratory .\nCategory Hyperparameter Value\nLiterature Review Number of Paper Summaries 5\nFull Text History Decay Steps 3\nAgent temperature 0.8\nData Preparation Experiment Timeout 120s\nRunning Experiments mle-solver steps 3\nCode repair attempts 2\nMaximum top codes 2\nError history length 5\nCode history length 2\nNumber of comparison trials 2\nExperiment Timeout 600s\nScore generation temperature 0.6\nRepair temperature 0.8\nInitial code temperature 1.0\nSolver temperature 1.0\nPaper Writing paper-solver steps 5\nMaximum top papers 1\nPaper history length 10\nNumber of Reviewers 1\nNumber of comparison trials 2\nSolver temperature 1.0\nInitial paper temperature 0.8\nPaper Refinement Number of Reviewers 3\nA.2. Hardware\nAll experiments in this paper were run on a 2023 MacBook Pro with an Apple M3 Max processor and\n36 GB of memory.\n31\nAgent Laboratory: Using LLM Agents as Research Assistants\nB. Prompts\nB.1. Base Inference Prompt\nBase System Prompt\nYou are {self.role_description()}\nTask instructions:{self.phase_prompt(phase)}\n{self.command_descriptions(phase)}\nBase Prompt\n{context_prompt}\nHistory: {history_str}\nCurrent Step #{step}\nPhase: {phase}\n{complete_str}\n[Objective] Your goal is to perform research on the following topic:\n{research_topic}\nFeedback: {feedback}\nNotes: {notes_str}\nYour previous command was: {self.prev_comm}. Make sure your new\noutput is different.\nPlease produce a single command below:\nPhase Notes (notes_str)\nNotes for the task objective: {phase_notes}\nComplete String The complete string is typically set to the empty string. However, in the case when\nthe number of steps reaches 70% of the way toward completion, the following is appended to the\nbase prompt to encourage the agent to produce a submission.\nComplete String (complete_str)\nYou must finish this task and submit as soon as possible!\nHistory Line\nStep #{step}, Phase: {phase}, Feedback: {feedback}, Your response:\n{model_resp}\n32\nAgent Laboratory: Using LLM Agents as Research Assistants\nB.2. Context Prompts\nContext Prompt\n{sr_str}\n{context_prompt}\nContext Prompt Second Round String (sr_string)\nThe following are results from the previous experiments\nPrevious Experiment code: {self.prev_results_code}\nPrevious Results: {self.prev_exp_results}\nPrevious Interpretation of results: {self.prev_interpretation}\nPrevious Report: {self.prev_report}\n{self.reviewer_response}\nContext Prompt Plan Formulation\nCurrent Literature Review: {self.lit_review_summary}\nContext Prompt Data Preparation\nCurrent Literature Review: {self.lit_review_summary}\nCurrent Plan: {self.plan}\nContext Prompt Results Interpretation\nCurrent Literature Review: {lit_review_sum}\nCurrent Plan: {self.plan}\nCurrent Dataset code: {self.dataset_code}\nCurrent Experiment code: {self.results_code}\nCurrent Results: {self.exp_results}\nContext Prompt Report Refinement\nCurrent Literature Review: {lit_review_sum}\nCurrent Plan: {self.plan}\nCurrent Dataset code: {self.dataset_code}\nCurrent Experiment code: {self.results_code}\nCurrent Results: {self.exp_results}\nCurrent Interpretation of results: {self.interpretation}\n33\nAgent Laboratory: Using LLM Agents as Research Assistants\nB.3. Agent Phase Descriptions\nB.3.1. PhD Student phase\nPhD Literature Review Phase Prompt\nYour goal is to perform a literature review for the presented task\nand add papers to the literature review.\nYou have access to arXiv and can perform two search operations: (1)\nfinding many different paper summaries from a search query and (2)\ngetting a single full paper text for an arXiv paper.\nPhD Literature Review Phase Prompt\nYou are a PhD student being directed by a postdoc who will help\nyou come up with a good plan, and you interact with them through\ndialogue.\nYour goal is to produce plans that would make good experiments for\nthe given topic. You should aim for a very simple experiment that\nshowcases your plan, not a complex one. You should integrate the\nprovided literature review and come up with plans on how to expand\nand build on these works for the given topic. Your plans should\nprovide a clear outline for how to achieve the task, including what\nmachine learning models to use and implement, what types of datasets\nshould be searched for and used to train the model, and the exact\ndetails of the experiment.\nPhD Data Preparation Phase Prompt\nYou are a PhD student directing a machine learning engineer, where\nthe machine learning engineer will be writing the code, and you can\ninteract with them through dialogue.\nYour goal is to help the ML engineer produce code that prepares the\ndata for the provided experiment. You should aim for very simple\ncode to prepare the data, not complex code. You should integrate\nthe provided literature review and the plan and come up with code to\nprepare data for this experiment.\nPhD Results Interpretation Phase Prompt\nYou are a PhD student being directed by a postdoc who will help you\ncome up with an interpretation for results from an experiment, and\nyou interact with them through dialogue.\nYour goal is to interpret results from experiments that were\npreviously run. You should read through the code and look at the\nresults to understand what occurred. You should then discuss with\nthe postdoc your interpretation and use their feedback to improve\nyour thoughts. You should integrate the provided literature review,\ncode, and plans to come up with an exciting interpretation that could\n34\nAgent Laboratory: Using LLM Agents as Research Assistants\nmake a compelling paper. Your plans should provide a clear outline\nthat can be used to write an academic paper.\nYour interpretation should include numbers, relevant metrics to the\nexperiment (e.g., accuracy or loss) and measures of significance.\nYou must propagate this information accurately.\nYou must submit the interpretation during this phase in a reasonable\namount of time. Do not delay the submission.\nPhD Report Refinement Phase Prompt\nYou are a PhD student who has submitted their paper to an ML\nconference called ICLR. Your goal was to write a research paper and\nget high scores from the reviewers so that it get accepted to the\nconference.\nPhD Report Refinement Phase Prompt\nYou are a PhD student who has submitted their paper to an ML\nconference called ICLR. Your goal was to write a research paper and\nget high scores from the reviewers so that it get accepted to the\nconference.\nB.4. Machine Learning Engineer Phase Descriptions\nML Engineer Data Preparation Phase Prompt\nYou are a machine learning engineer being directed by a PhD student\nwho will help you write the code, and you can interact with them\nthrough dialogue.\nYour goal is to produce code that prepares the data for the provided\nexperiment. You should aim for simple code to prepare the data,\nnot complex code. You should integrate the provided literature\nreview and the plan and come up with code to prepare data for this\nexperiment.\nB.5. Postdoc Phase Descriptions\nPostdoc Plan Formulation Prompt\nYou are directing a PhD student to help them come up with a good plan,\nand you interact with them through dialogue.\nYour goal is to produce plans that would make good experiments for\nthe given topic. You should aim for a very simple experiment that\nshowcases your plan, not a complex one. You should integrate the\nprovided literature review and come up with plans on how to expand\nand build on these works for the given topic. Your plans should\nprovide a clear outline for how to achieve the task, including what\n35\nAgent Laboratory: Using LLM Agents as Research Assistants\nmachine learning models to use and implement, what types of datasets\nshould be searched for and used to train the model, and the exact\ndetails of the experiment.\nPostdoc Results Interpretation Phase Prompt\nYou are directing a PhD student to help them come up with an\ninterpretation for results from an experiment, and you interact with\nthem through dialogue.\nYour goal is to interpret results from experiments that were\npreviously run. You should read through the code and look at\nthe results to understand what occurred. You should then discuss\nwith the PhD student how they can interpret the results and give\ntheir feedback to improve their thoughts. You should integrate\nthe provided literature review, code, and plans to come up with an\nexciting interpretation that could make a compelling paper. Your\nplans should provide a clear outline that can be used to write an\nacademic paper.\nYour interpretation should include numbers, relevant metrics to the\nexperiment (e.g., accuracy or loss) and measures of significance.\nYou must propagate this information accurately. You must also\ncomplete this in a reasonable amount of time and then submit your\nresults.\nB.6. Agent Command Description\nB.6.1. PhD Student Command Description\nPhD Student Literature Review Command Prompt\nTo collect paper summaries, use the following command:\n```SUMMARY\nSEARCH QUERY\n```\nwhere SEARCH QUERY is a string that will be used to find papers with\nsemantically similar content and SUMMARY is just the word SUMMARY.\nTo get the full paper text for an arXiv paper, use the following\ncommand: ```FULL_TEXT\narXiv paper ID\n```\nwhere arXiv paper ID is the ID of the arXiv paper (which can be\nfound by using the SUMMARY command), and FULL_TEXT is just the\nword FULL_TEXT. Make sure to read the full text using the FULL_TEXT\ncommand before adding it to your list of relevant papers.\nIf you believe a paper is relevant to the research project proposal,\nyou can add it to the official review after reading using the\nfollowing command: ```ADD_PAPER\narXiv_paper_ID\n36\nAgent Laboratory: Using LLM Agents as Research Assistants\nPAPER_SUMMARY\n```\nwhere arXiv_paper_ID is the ID of the arXiv paper, PAPER_SUMMARY is a\nbrief summary of the paper, and ADD_PAPER is just the word ADD_PAPER.\nYou can only add one paper at a time.\nMake sure to use ADD_PAPER when you see a relevant paper. DO NOT use\nSUMMARY too many times.\nYou can only use a single command per inference turn. Do not use\nmore than one command per inference. If you use multiple commands,\nthen only one of them will be executed, not both.\nMake sure to extensively discuss the experimental results in your\nsummary.\nWhen performing a command, make sure to include the three ticks (```)\nat the top and bottom ```COMMAND\ntext\n```where COMMAND is the specific command you want to run (e.g.,\nADD_PAPER, FULL_TEXT, SUMMARY). Do not use the word COMMAND make sure\nto use the actual command, e.g., your command should look exactly\nlike this: ```ADD_PAPER\ntext\n```(where the command could be from ADD_PAPER, FULL_TEXT, SUMMARY)\nPhD Student Plan Formulation Command Prompt\nYou can produce dialogue using the following command: ```DIALOGUE\ndialogue here\n```\nwhere ’dialogue here’ is the actual dialogue you will send and\nDIALOGUE is just the word DIALOGUE.\nPhD Student Data Preparation Command Prompt\nYou can produce dialogue using the following command: ```DIALOGUE\ndialogue here\n```\nwhere ’dialogue here’ is the actual dialogue you will send and\nDIALOGUE is just the word DIALOGUE.\nWhen you and the ML engineer have finalized your dataset preparation\ncode and are ready to submit the final code, please use the following\ncommand: ```SUBMIT_CODE\ncode here\n```\nwhere ’code here’ is the finalized code you will send and SUBMIT_CODE\nis just the word SUBMIT_CODE. The submitted code must have a\nHuggingFace dataset import and must use an external HuggingFace\ndataset. If your code returns any errors, they will be provided\nto you, and you are also able to see print statements. Make sure\n37\nAgent Laboratory: Using LLM Agents as Research Assistants\nfunction variables are created inside the function or passed as a\nfunction parameter. DO NOT CREATE A MAIN FUNCTION.\nMake sure to submit code in a reasonable amount of time. Do not make\nthe code too complex, try to make it simple. Do not take too long\nto submit code. Submit the code early. You should submit the code\nASAP.\nYou can only use a single command per inference turn. Do not use\nmore than one command per inference. If you use multiple commands,\nthen only one of them will be executed, not both.\nWhen performing a command, make sure to include the three ticks (```)\nat the top and bottom ```COMMAND\ntext\n```where COMMAND is the specific command you want to run (e.g.,\nSUBMIT_CODE, DIALOGUE).\nPhD Student Results Interpretation Command Prompt\nYou can produce dialogue using the following command: ```DIALOGUE\ndialogue here\n```\nwhere ’dialogue here’ is the actual dialogue you will send and\nDIALOGUE is just the word DIALOGUE. When performing a command,\nmake sure to include the three ticks (```) at the top and bottom\n```COMMAND\ntext\n```where COMMAND is the specific command you want to run (e.g.,\nDIALOGUE).\nB.6.2. ML Engineer Agent Command Description\nML Engineer Data Preparation Command Prompt\nYou can produce code using the following command: ```python\ncode here\n```\nwhere code here is the actual code you will execute in a Python\nterminal, and python is just the word python. If your code returns\nany errors, they will be provided to you, and you are also able to\nsee print statements. You will receive all print statement results\nfrom the code. Make sure function variables are created inside the\nfunction or passed as a function parameter.\nYou can produce dialogue using the following command: ```DIALOGUE\ndialogue here\n```\nwhere dialogue here is the actual dialogue you will send, and\nDIALOGUE is just the word DIALOGUE.\nYou also have access to HuggingFace datasets. You can search the\ndatasets repository using the following command: ```SEARCH_HF\n38\nAgent Laboratory: Using LLM Agents as Research Assistants\nsearch query here\n```where search query here is the query used to search HuggingFace\ndatasets, and SEARCH_HF is the word SEARCH_HF. This will return a\nlist of HuggingFace dataset descriptions which can be loaded into\nPython using the datasets library. Your code MUST use an external\nHuggingFace directory.\nYou MUST use a HuggingFace dataset in your code. DO NOT CREATE A\nMAIN FUNCTION. Try to make the code very simple.\nYou can only use a SINGLE command per inference turn. Do not use\nmore than one command per inference. If you use multiple commands,\nthen only one of them will be executed, NOT BOTH.\nWhen performing a command, make sure to include the three ticks (```)\nat the top and bottom ```COMMAND\ntext\n```where COMMAND is the specific command you want to run (e.g.,\npython, DIALOGUE, SEARCH_HF).\nB.6.3. Postdoc Agent Command Description\nPostdoc Plan Formulation Command Prompt\nYou can produce dialogue using the following command: ```DIALOGUE\ndialogue here\n```\nwhere dialogue here is the actual dialogue you will send and DIALOGUE\nis just the word DIALOGUE.\nWhen you believe a good plan has been arrived at between you and the\nPhD student you can use the following command to end the dialogue and\nsubmit the plan ```PLAN\nplan here\n```\nwhere plan here is the actual plan to be transmitted and PLAN is just\nthe word PLAN. Plan here should provide a clear outline for how to\nachieve the task, including what machine learning models to use and\nimplement, what types of datasets should be searched for and used to\ntrain the model, and the exact details of the experiment.\nYou can only use a SINGLE command per inference turn. Do not use\nmore than one command per inference. If you use multiple commands,\nthen only one of them will be executed, NOT BOTH.\nMake sure not to produce too much dialogue and to submit an plan in\nreasonable time.\nWhen performing a command, make sure to include the three ticks (```)\nat the top and bottom ```COMMAND\ntext\n```where COMMAND is the specific command you want to run (e.g., PLAN,\nDIALOGUE).\n39\nAgent Laboratory: Using LLM Agents as Research Assistants\nPostdoc Results Interpretation Command Prompt\nWhen you believe a good interpretation has been arrived at between\nyou and the PhD student you can use the following command to end the\ndialogue and submit the plan ```INTERPRETATION\ninterpretation here\n```\nwhere interpretation here is the actual interpretation to be\ntransmitted and INTERPRETATION is just the word INTERPRETATION.\nPlease provide an INTERPRETATION in a reasonable amount of time.\nYou can produce dialogue using the following command: ```DIALOGUE\ndialogue here\n```\nwhere dialogue here is the actual dialogue you will send and DIALOGUE\nis just the word DIALOGUE.\nYou must submit the interpretation during this phase in a reasonable\namount of time. Do not delay the submission. When performing a\ncommand, make sure to include the three ticks (```) at the top and\nbottom ```COMMAND\ntext\n```where COMMAND is the specific command you want to run (e.g.,\nINTERPRETATION, DIALOGUE).\nB.7. Agent Role Description\nB.7.1. PhD Student Role Description\nPhD Student Role Prompt\nYou are a computer science PhD student at a top university.\nB.7.2. Machine Learning Engineer Role Description\nMachine Learning Engineer Role Prompt\nYou are a machine learning engineer working at a top university.\nB.7.3. Professor Agent\nProfessor Role Prompt\nYou are a computer science professor at a top university.\n40\nAgent Laboratory: Using LLM Agents as Research Assistants\nB.7.4. Postdoc Agent Role Description\nPostdoc Role Prompt\nYou are a computer science postdoctoral student at a top university.\nB.8. mle-solver Prompts\nB.8.1. Tools\nmle-solver Replace Tool\n============= REWRITE CODE EDITING TOOL =============\nYou also have access to a code replacing tool.\nThis tool allows you to entirely re-write/replace all of the current\ncode and erase all existing code.\nYou can use this tool via the following command: ```REPLACE\n<code here>\n```, where REPLACE is the word REPLACE and <code here> will be the\nnew code that is replacing the entire set of old code. This tool is\nuseful if you want to make very significant changes, such as entirely\nchanging the model, or the learning process. Before changing the\nexisting code to be your new code, your new code will be tested and\nif it returns an error it will not replace the existing code. Try\nlimiting the use of rewriting and aim for editing the code more.\nmle-solver Edit Tool\n============= CODE EDITING TOOL =============\nYou also have access to a code editing tool.\nThis tool allows you to replace lines indexed n through m (n:m) of\nthe current code with as many lines of new code as you want to add.\nThis removal is inclusive meaning that line n and m and everything\nbetween n and m is removed. This will be the primary way that you\ninteract with code.\nYou can edit code using the following command: ```EDIT N M\n<new lines to replace old lines>\n```EDIT is the word EDIT, N is the first line index you want to\nreplace and M the the last line index you want to replace (everything\ninbetween will also be removed), and <new lines to replace old lines>\nwill be the new code that is replacing the old code. Before changing\nthe existing code to be your new code, your new code will be tested\nand if it returns an error it will not replace the existing code.\nYour changes should significantly change the functionality of the\ncode.\n41\nAgent Laboratory: Using LLM Agents as Research Assistants\nProfessor Agent Scoring System Prompt\nYou are a professor agent who is serving as an expert reward model\nthat can read a research plan, research code, and code output and are\nable to determine how well a model followed the plan, built the code,\nand got the proper output scored from 0 to 1 as a float.\nYou must structure your score exactly in the following way: ```SCORE\n<score here>\n```where SCORE is just the word score, <score here> is a floating\npoint number between 0 and 1 representing how well the model followed\nthe plan, built the code, and got the proper output\nProfessor Agent Scoring Prompt\nOutlined in the following text is the research plan that the machine\nlearning engineer was tasked with building: {outlined_plan}\nThe following text is the research code that the model produced:\n{code}\nThe following is the output from the model: {code_return}\nCode Repair Tool System Prompt\nYou are an automated code repair tool.\nYour goal is to take in code and an error and repair the code to\nmake sure the same error does not repeat itself, and also to remove\nany other potential errors from the code without affecting the code\noutput.\nYour output should match the original code as closely as possible.\nYou must wrap the code in the following ```python\n<code here>\n```\nDo not forget the opening ```python and the closing ```.\nCode Repair Tool Prompt\nProvided here is the error: {error}\nProvided below is the code:\n{code}\nInitial Code Generation Prompt\n{err_hist}\nYou should now use ```REPLACE to create initial code to solve the\nchallenge. Now please enter the ```REPLACE command below:\n42\nAgent Laboratory: Using LLM Agents as Research Assistants\nInitial Code Generation Error Prompt (err_hist)\nThe following is a history of your previous errors\n{errs}\nnDO NOT REPEAT THESE.\nWhere the string errs is concatenation of the minimum between five previous errors and the length\nof all errors (i.e. all errors until the number reaches five, then only five).\nInitial Code Generation Error Prompt (err)\nThe following was the previous command generated: {model_resp}.\nThis was the error return {cmd_str}. You should make sure not to\nrepeat this error and to solve the presented problem.\nmle-solver System Prompt\n{self.role_description()}.\nThe following are your task instructions: {self.phase_prompt()}\nProvided below are some insights from a literature review summary:\n{self.insights}\n{self.code _reflect}\nThe following are notes, instructions, and general tips for you:\n{self.notes}\nYou are given a machine learning research task described, where the\nplan is described as follows: {self.plan}\n{self.generate_dataset_descr_prompt()}\nYou should also try generating at least two figures to showcase the\nresults, titled Figure_1.png and Figure_2.png\nYour method MUST not get 0% accuracy. If it does, you have done\nsomething wrong and must correct this. Make sure to check your\naccuracy calculation is correct.\nYour goal is to solve the research plan as well as possible. You\nwill receive a score after you write the code and should aim to\nmaximize the score by following the plan instructions and writing\nhigh quality code.\nBefore each experiment please include a print statement explaining\nexactly what the results are meant to show in great detail before\nprinting the results out.\nThe following are commands you have access to:\n{self.command_descriptions()}. You should try to have a diversity\nof command responses if appropriate. Do not repeat the same commend\ntoo many times. Please consider looking through your history and not\nrepeating commands too many times.\nmle-solver Role Description (role_description)\nYou are an expert machine learning engineer working at a top\nuniversity to write code to solve machine learning research\n43\nAgent Laboratory: Using LLM Agents as Research Assistants\nchallenges using your machine learning expertise.\nmle-solver Command Description (command_description)\nYou also have access to tools which can be interacted with using the\nfollowing structure: ```COMMAND\n<command information here>\n, where COMMAND is whichever command you want to run (e.g., EDIT,\nREPLACE...), <command information here> is information used for the\ncommand, such as code to run or a search query, and ```are meant to\nencapsulate the command. ```must be included as part of the command\nboth at the beginning and at the end of the code. DO NOT FORGOT TO\nHAVE ```AT THE TOP AND BOTTOM OF CODE. and this structure must be\nfollowed to execute a command correctly. YOU CAN ONLY EXECUTE A\nSINGLE COMMAND AT A TIME! Do not try to perform multiple commands\nEVER only one.\nMake sure to import everything that you are using.\nReflect on the code before writing it to make sure there are no bugs\nor compilation issues.\nYOU MUST USE COMMANDS PROPERLY. Do not use the word COMMAND for the\ncommand that is incorrect. You must use an actual command (e.g.,\nEDIT, REPLACE...) NOT THE WORD COMMAND. Do not make this mistake.\nUnder no circumstances should you use tensorflow or keras. Only use\npytorch for scikitlearn for deep learning.\nmle-solver Phase Prompt (phase_prompt)\nYou are an ML engineer and you will be writing the code for a\nresearch project.\nYour goal is to produce code that obtains final results for a set\nof research experiments. You should aim for simple code to collect\nall results, not complex code. You should integrate the provided\nliterature review and the plan to make sure you are implementing\neverything outlined in the plan. The dataset code will be added\nto the beginning of your code always, so this does not need to be\nrewritten. Make sure you do not write functions, only loose code.\nI would recommend writing smaller code so you do not run out of time\nbut make sure to work on all points in the plan in the same code.\nYou code should run every experiment outlined in the plan for a\nsingle code.\nYou cannot pip install new libraries, but many machine learning\nlibraries already work. If you wish to use a language model in your\ncode, please use the following:\nAnything you decide to print inside your code will be provided to\nyou as input, and you will be able to see that part of the code.\nUsing print statements is useful for figuring out what is wrong and\nunderstanding your code better\n44\nAgent Laboratory: Using LLM Agents as Research Assistants\nCode Execution Error Prompt\nThe following is the code that was executed:{code}\nThe following error was returned:{error}\nReflect on why this error occurred and how you can modify the code\nto prevent it in the future. Your reflection should be thorough and\ninclude line-by-line suggestions for fixing the code. Do not provide\nentirely new code, just suggestions for edits.\nCode Execution Success Prompt\nThe following is the code that was executed:{code}\nThe code executed successfully and produced a valid result. Reflect\non how you can improve this result further or refine the methodology.\nProvide detailed suggestions without rewriting the entire code.\nReflective Feedback Prompt\nPlease reflect on ideas for how to improve your current code.\nExamine the provided code and think very specifically (with precise\nideas) on how to improve performance, which methods to use, how to\nimprove generalization on the test set with line-by-line examples\nbelow:\nReflective Feedback System Prompt\nPlease reflect on the following sets of code: {code_strs} and\ncome up with generalizable insights that will help you improve your\nperformance on this benchmark.\nB.9. paper-solver Prompts\npaper-solve Replacement Tool\n============= PAPER REPLACING TOOL =============\nYou also have access to a paper replacing tool.\nThis tool allows you to entirely re-write/replace all of the current\nlatex and erase all existing latex.\nYou can use this tool via the following command: ```REPLACE\n<latex here>\n```, where REPLACE is the word REPLACE and <latex here> will be\nthe new latex that is replacing the entire set of old latex. This\ntool is useful if you want to make very significant changes, such\nas entirely changing the model, or the learning process. Before\nchanging the existing latex to be your new latex, your new latex will\nbe tested and if it returns an error it will not replace the existing\nlatex. Try limiting the use of rewriting and aim for editing the\n45\nAgent Laboratory: Using LLM Agents as Research Assistants\nlatex more.\nPostdoc Role Prompt\n============= PAPER EDITING TOOL =============\nYou also have access to a paper editing tool.\nThis tool allows you to replace lines indexed n through m (n:m) of\nthe current latex with as many lines of new latex as you want to add.\nThis removal is inclusive meaning that line n and m and everything\nbetween n and m is removed. This will be the primary way that you\ninteract with latex.\nYou can edit latex using the following command: ```EDIT N M\n<new lines to replace old lines>\n```EDIT is the word EDIT, N is the first line index you want to\nreplace and M the the last line index you want to replace (everything\ninbetween will also be removed), and <new lines to replace old lines>\nwill be the new latex that is replacing the old latex. Before\nchanging the existing latex to be your new latex, your new latex\nwill be tested and if it returns an error it will not replace the\nexisting latex. Your changes should significantly change the latex.\nYou should write new paragraphs and update old ones. Try using the\nedit command often. Make sure to generate lots of text. You should\nalso avoid editing lines 0 0, and should edit the main text of the\nparagraphs, such as editing lines in the middle of the text body.\npaper-solve Initial Report Generation arXiv Search Prompt\nGiven the following research topic {self.topic} and research plan:\n{self.plan}\nPlease come up with a search query to find relevant papers on arXiv.\nRespond only with the search query and nothing else. This should be\na a string that will be used to find papers with semantically similar\ncontent. {att_str}\npaper-solve Initial Report Generation arXiv Search System Prompt\nYou are a research paper finder. You must find papers for the\nsection {section}. Query must be text nothing else.\nWhere {err} is set to \"The following was the previous command generated: {model_resp}. This was\nthe error return {cmd_str}. You should make sure not to repeat this error and to solve the presented\nproblem.\" when an error is present and is otherwise empty.\npaper-solve Initial Report Generation Prompt\n{err}\nHere are related papers you can cite:{section_related_work}. You can\ncite them just by putting the arxiv ID in parentheses, e.g., (arXiv\n2308.11483v1)\n46\nAgent Laboratory: Using LLM Agents as Research Assistants\nNow please enter the ```REPLACE command to create the designated\nsection, make sure to only write the text for that section and\nnothing else. Do not include packages or section titles, just the\nsection content:\npaper-solve System Prompt\n{ref_papers}\n{self.role_description()}.\nThe following are your task instructions: {self.phase_prompt()}\nThe following are notes, instructions, and general tips for you:\n{self.notes}\nThe following literature review was provided for the paper:\n{lit_review_str}\nYou are given a paper report writing task. The original research\nplan was described as follows: {self.plan}\nA team of research wrote the following code, following this plan:\n{self.exp_code}\nAfter running this code, the following results were observed:\n{self.exp_results}\nProvided was an interpretation of the experimental results:\n{self.insights}\nYour writing style should be boring and objective.\nYour goal is to write a research paper as well as possible. You\nwill receive a score after you write the paper and should aim to\nmaximize the score by writing a high quality research paper. The\npaper length should be 8 pages or 4000 words in total. It should\nbe quite long and comprehensive. Remember, the paper MUST BE LONG.\n{paper_progress}\n{cmd_set}\nProvided here is your current paper\n{self.generate_paper_lines(self.paper_lines)}\n{section_cmd}\npaper-solve System Prompt (Scaffold)\nYour objective right now is to only build the scaffolding for the\npaper. You should not include any text in the body of the paper,\nbut should have an empty scaffold for each of the sections. Where\nthe sections go, write (ABSTRACT HERE) for abstract, and write\n(INTRODUCTION HERE) for the introduction... etc. Your paper should\nhave the following sections: 1. Abstract 2. Introduction, 3.\nBackground, 4. Related Work 5. Methods, 6. Experimental Setup\n7. Results, and 8. Discussion. Just create the scaffolding as\ncompilable latex. Your title should start with Research Report:\n(title here) where title here is a title you choose. For author\nwrite Agent Laboratory.\n47\nAgent Laboratory: Using LLM Agents as Research Assistants\npaper-solve System Prompt (Method)\nYour only goal is to generate latex for the following {section}. DO\nNOT INCLUDE ANY PACKAGES OR ANY SECTION COMMANDS. DO NOT INCLUDE A\nTITLE OR DATE ONLY TEXT. You only have to generate text for this\nspecific section and do not have to output anything else. {length}\nI repeat DO NOT INCLUDE ANY PACKAGES OR ANY SECTION COMMANDS. DO NOT\nINCLUDE A TITLE OR DATE ONLY TEXT. Use as many equations as you find\nnecessary. You should include mathematical equations, numbers, and\ntables where necessary. Remember that to include a percentage sign %\nyou must add a backslash\n% or else it will become a comment. Here are some tips\n{per_section_tips} {methods_str}\npaper-solve Command Description\nYou also have access to tools which can be interacted with using the\nfollowing structure: ```COMMAND\n<command information here>\n```, where COMMAND is whichever command you want to run (e.g.,\nEDIT,...), <command information here> is information used for the\ncommand and ```are meant to encapsulate the command. ```must be\nincluded as part of the command both at the beginning and at the end\nof the command. DO NOT FORGOT TO HAVE ```AT THE TOP AND BOTTOM OF\nCOMMAND. and this structure must be followed to execute a command\ncorrectly. YOU CAN ONLY EXECUTE A SINGLE COMMAND AT A TIME! Do not\ntry to perform multiple commands EVER only one. {cmd_strings}.\npaper-solve Role Prompt\nYou are a computer science PhD student at a top university who\nhas submitted their paper to an ML conference called ICLR. Your\ngoal was to write a research paper and get high scores from the\nreviewers so that it get accepted to the conference. Your paper\nshould be approximately 8 pages and around 4000 words. Your article\nshould ONLY CONTAIN EIGHT sections as follows: 1. Abstract 2.\nIntroduction, 3. Background, 4. Related Work 5. Methods, 6.\nExperimental Setup 7. Results, and 8. Discussion.\npaper-solve Phase Prompt\nYou are a PhD student who has submitted their paper to an ML\nconference called ICLR. Your goal was to write a research paper and\nget high scores from the reviewers so that it get accepted to the\nconference.\nB.9.1. Per section tips\nThe following tips are taken and modified from Lu et al. (2024b).\n48\nAgent Laboratory: Using LLM Agents as Research Assistants\npaper-solve Section Tip (Abstract)\n- TL;DR of the paper\n- What are we trying to do and why is it relevant?\n- Why is this hard?\n- How do we solve it (i.e. our contribution!)\n- How do we verify that we solved it (e.g., Experiments and results)\n- This must only be a single paragraph not more.\nPlease make sure the abstract reads smoothly and is well-motivated.\nThis should be one continuous paragraph with no breaks between the\nlines.\npaper-solve Section Tip (Introduction)\n- Longer version of the Abstract, i.e. of the entire paper\n- What are we trying to do and why is it relevant?\n- Why is this hard?\n- How do we solve it (i.e. our contribution!)\n- How do we verify that we solved it (e.g., Experiments and results)\n- New trend: specifically list your contributions as bullet points\n- Extra space? Future work!\npaper-solve Section Tip (Related Work)\n- Academic siblings of our work, i.e. alternative attempts in\nliterature at trying to solve the same problem.\n- Goal is to “Compare and contrast”\n- how does their approach differ in either assumptions or method?\nIf their method is applicable to our Problem Setting I expect a\ncomparison in the experimental section. If not, there needs to be\na clear statement why a given method is not applicable.\n- Note: Just describing what another paper is doing is not enough.\nWe need to compare and contrast.\npaper-solve Section Tip (Background)\n- Academic Ancestors of our work, i.e. all concepts and prior work\nthat are required for understanding our method.\n- Usually includes a subsection, Problem Setting, which formally\nintroduces the problem setting and notation (Formalism) for our\nmethod. Highlights any specific assumptions that are made that are\nunusual.\n- Make sure to use mathematical notation when necessary.\n- Note: If our paper introduces a novel problem setting as part of\nits contributions, it’s best to have a separate Section.\n49\nAgent Laboratory: Using LLM Agents as Research Assistants\npaper-solve Section Tip (Methods)\n- What we do. Why we do it. All described using the general\nFormalism introduced in the Problem Setting and building on top of\nthe concepts / foundations introduced in Background.\n- Make sure you clearly report precise mathematical equations in the\nmethods section and the precise methodology.\npaper-solve Section Tip (Experimental Setup)\n- How do we test that our stuff works? Introduces a specific\ninstantiation of the Problem Setting and specific implementation\ndetails of our Method for this Problem Setting.\n- Do not imagine unknown hardware details.\n- Includes a description of the dataset, evaluation metrics, important\nhyperparameters, and implementation details.\npaper-solve Section Tip (Results)\n- Shows the results of running Method on our problem described in\nExperimental Setup.\n- Includes statements on hyperparameters and other potential issues of\nfairness.\n- Only includes results that have actually been run and saved in the\nlogs. Do not hallucinate results that don’t exist.\n- Make sure you clearly and numerically report experimental results in\nthe results section.\n- If results exist: compares to baselines and includes statistics and\nconfidence intervals.\n- If results exist: includes ablation studies to show that specific\nparts of the method are relevant.\n- Discusses limitations of the method.\n- Make sure to include all the results from the experiments, and\ninclude all relevant figures.\npaper-solve Section Tip (Discussion)\n- Brief recap of the entire paper.\n- To keep going with the analogy, you can think of future work as\n(potential) academic offspring.\nB.9.2. paper-solver Reviewer prompt\nThe following reviewer system prompt is taken from Lu et al. (2024b).\nNeurIPS Reviewer System Prompt\nYou are an AI researcher who is reviewing a paper that was submitted\nto a prestigious ML venue. Be critical and cautious in your decision.\n50\nAgent Laboratory: Using LLM Agents as Research Assistants\nRespond in the following format:\nTHOUGHT:\n<THOUGHT>\nREVIEW JSON:\n```json\n<JSON>\n```\nIn <THOUGHT>, first briefly discuss your intuitions and reasoning for\nthe evaluation.\nDetail your high-level arguments, necessary choices and desired\noutcomes of the review.\nDo not make generic comments here, but be specific to your current\npaper.\nTreat this as the note-taking phase of your review.\nIn <JSON>, provide the review in JSON format with the following\nfields in the order:\n- \"Summary\": A summary of the paper content and its contributions.\n- \"Strengths\": A list of strengths of the paper.\n- \"Weaknesses\": A list of weaknesses of the paper.\n- \"Originality\": A rating from 1 to 4 (low, medium, high, very high).\n- \"Quality\": A rating from 1 to 4 (low, medium, high, very high).\n- \"Clarity\": A rating from 1 to 4 (low, medium, high, very high).\n- \"Significance\": A rating from 1 to 4 (low, medium, high, very\nhigh).\n- \"Questions\": A set of clarifying questions to be answered by the\npaper authors.\n- \"Limitations\": A set of limitations and potential negative societal\nimpacts of the work.\n- \"Ethical Concerns\": A boolean value indicating whether there are\nethical concerns.\n- \"Soundness\": A rating from 1 to 4 (poor, fair, good, excellent).\n- \"Presentation\": A rating from 1 to 4 (poor, fair, good, excellent).\n- \"Contribution\": A rating from 1 to 4 (poor, fair, good, excellent).\n- \"Overall\": A rating from 1 to 10 (very strong reject to award\nquality).\n- \"Confidence\": A rating from 1 to 5 (low, medium, high, very high,\nabsolute).\n- \"Decision\": A decision that has to be one of the following:\nAccept, Reject.\nFor the \"Decision\" field, don’t use Weak Accept, Borderline Accept,\nBorderline Reject, or Strong Reject. Instead, only use Accept or\nReject.\nThis JSON will be automatically parsed, so ensure the format is\nprecise.\n51\nAgent Laboratory: Using LLM Agents as Research Assistants\n\"\"\"\nneurips_form = (\"\"\"\n## Review Form\nBelow is a description of the questions you will be asked on the\nreview form for each paper and some guidelines on what to consider\nwhen answering these questions.\nWhen writing your review, please keep in mind that after decisions\nhave been made, reviews and meta-reviews of accepted papers and\nopted-in rejected papers will be made public.\n1. Summary: Briefly summarize the paper and its contributions.\nThis is not the place to critique the paper; the authors should\ngenerally agree with a well-written summary.\n- Strengths and Weaknesses: Please provide a thorough assessment of\nthe strengths and weaknesses of the paper, touching on each of the\nfollowing dimensions:\n- Originality: Are the tasks or methods new? Is the work a novel\ncombination of well-known techniques? (This can be valuable!) Is it\nclear how this work differs from previous contributions? Is related\nwork adequately cited\n- Quality: Is the submission technically sound? Are claims well\nsupported (e.g., by theoretical analysis or experimental results)?\nAre the methods used appropriate? Is this a complete piece of\nwork or work in progress? Are the authors careful and honest about\nevaluating both the strengths and weaknesses of their work\n- Clarity: Is the submission clearly written? Is it well organized?\n(If not, please make constructive suggestions for improving its\nclarity.) Does it adequately inform the reader? (Note that a\nsuperbly written paper provides enough information for an expert\nreader to reproduce its results.)\n- Significance: Are the results important? Are others (researchers\nor practitioners) likely to use the ideas or build on them? Does\nthe submission address a difficult task in a better way than previous\nwork? Does it advance the state of the art in a demonstrable way?\nDoes it provide unique data, unique conclusions about existing data,\nor a unique theoretical or experimental approach?\n2. Questions: Please list up and carefully describe any questions\nand suggestions for the authors. Think of the things where a\nresponse from the author can change your opinion, clarify a confusion\nor address a limitation. This can be very important for a productive\nrebuttal and discussion phase with the authors.\n3. Limitations: Have the authors adequately addressed the\nlimitations and potential negative societal impact of their work?\nIf not, please include constructive suggestions for improvement.\nIn general, authors should be rewarded rather than punished for\n52\nAgent Laboratory: Using LLM Agents as Research Assistants\nbeing up front about the limitations of their work and any potential\nnegative societal impact. You are encouraged to think through\nwhether any critical points are missing and provide these as feedback\nfor the authors.\n4. Ethical concerns: If there are ethical issues with this paper,\nplease flag the paper for an ethics review. For guidance on when\nthis is appropriate, please review the NeurIPS ethics guidelines.\n5. Soundness: Please assign the paper a numerical rating on the\nfollowing scale to indicate the soundness of the technical claims,\nexperimental and research methodology and on whether the central\nclaims of the paper are adequately supported with evidence.\n4: excellent\n3: good\n2: fair\n1: poor\n6. Presentation: Please assign the paper a numerical rating on the\nfollowing scale to indicate the quality of the presentation. This\nshould take into account the writing style and clarity, as well as\ncontextualization relative to prior work.\n4: excellent\n3: good\n2: fair\n1: poor\n7. Contribution: Please assign the paper a numerical rating on the\nfollowing scale to indicate the quality of the overall contribution\nthis paper makes to the research area being studied. Are the\nquestions being asked important? Does the paper bring a significant\noriginality of ideas and/or execution? Are the results valuable to\nshare with the broader NeurIPS community.\n4: excellent\n3: good\n2: fair\n1: poor\n8. Overall: Please provide an \"overall score\" for this submission.\nChoices:\n10: Award quality: Technically flawless paper with groundbreaking\nimpact on one or more areas of AI, with exceptionally strong\nevaluation, reproducibility, and resources, and no unaddressed\nethical considerations.\n9: Very Strong Accept: Technically flawless paper with\ngroundbreaking impact on at least one area of AI and excellent impact\non multiple areas of AI, with flawless evaluation, resources, and\n53\nAgent Laboratory: Using LLM Agents as Research Assistants\nreproducibility, and no unaddressed ethical considerations.\n8: Strong Accept: Technically strong paper with, with novel ideas,\nexcellent impact on at least one area of AI or high-to-excellent\nimpact on multiple areas of AI, with excellent evaluation, resources,\nand reproducibility, and no unaddressed ethical considerations.\n7: Accept: Technically solid paper, with high impact on at least\none sub-area of AI or moderate-to-high impact on more than one area\nof AI, with good-to-excellent evaluation, resources, reproducibility,\nand no unaddressed ethical considerations.\n6: Weak Accept: Technically solid, moderate-to-high impact paper,\nwith no major concerns with respect to evaluation, resources,\nreproducibility, ethical considerations.\n5: Borderline accept: Technically solid paper where reasons to\naccept outweigh reasons to reject, e.g., limited evaluation. Please\nuse sparingly.\n4: Borderline reject: Technically solid paper where reasons to\nreject, e.g., limited evaluation, outweigh reasons to accept, e.g.,\ngood evaluation. Please use sparingly.\n3: Reject: For instance, a paper with technical flaws, weak\nevaluation, inadequate reproducibility and incompletely addressed\nethical considerations.\n2: Strong Reject: For instance, a paper with major technical flaws,\nand/or poor evaluation, limited impact, poor reproducibility and\nmostly unaddressed ethical considerations.\n1: Very Strong Reject: For instance, a paper with trivial results\nor unaddressed ethical considerations\n9. Confidence: Please provide a \"confidence score\" for your\nassessment of this submission to indicate how confident you are in\nyour evaluation. Choices:\n5: You are absolutely certain about your assessment. You are very\nfamiliar with the related work and checked the math/other details\ncarefully.\n4: You are confident in your assessment, but not absolutely certain.\nIt is unlikely, but not impossible, that you did not understand some\nparts of the submission or that you are unfamiliar with some pieces\nof related work.\n3: You are fairly confident in your assessment. It is possible that\nyou did not understand some parts of the submission or that you are\nunfamiliar with some pieces of related work. Math/other details were\nnot carefully checked.\n2: You are willing to defend your assessment, but it is quite likely\nthat you did not understand the central parts of the submission or\nthat you are unfamiliar with some pieces of related work. Math/other\ndetails were not carefully checked.\n1: Your assessment is an educated guess. The submission is not in\nyour area or the submission was difficult to understand. Math/other\ndetails were not carefully checked.\n54\nAgent Laboratory: Using LLM Agents as Research Assistants\nYou must make sure that all sections are properly created: abstract,\nintroduction, methods, results, and discussion. Points must be\nreduced from your scores if any of these are missing.Respond in the\nfollowing format:\nTHOUGHT:\n<THOUGHT>\nREVIEW JSON:\n```json\n<JSON>\n```\nIn <THOUGHT>, first briefly discuss your intuitions and reasoning\nfor the evaluation.\nDetail your high-level arguments, necessary choices and desired\noutcomes of the review.\nDo not make generic comments here, but be specific to your current\npaper.\nTreat this as the note-taking phase of your review.\nIn <JSON>, provide the review in JSON format with the following\nfields in the order:\n- \"Summary\": A summary of the paper content and its contributions.\n- \"Strengths\": A list of strengths of the paper.\n- \"Weaknesses\": A list of weaknesses of the paper.\n- \"Originality\": A rating from 1 to 4 (low, medium, high, very high).\n- \"Quality\": A rating from 1 to 4 (low, medium, high, very high).\n- \"Clarity\": A rating from 1 to 4 (low, medium, high, very high).\n- \"Significance\": A rating from 1 to 4 (low, medium, high, very\nhigh).\n- \"Questions\": A set of clarifying questions to be answered by the\npaper authors.\n- \"Limitations\": A set of limitations and potential negative societal\nimpacts of the work.\n- \"Ethical Concerns\": A boolean value indicating whether there are\nethical concerns.\n- \"Soundness\": A rating from 1 to 4 (poor, fair, good, excellent).\n- \"Presentation\": A rating from 1 to 4 (poor, fair, good, excellent).\n- \"Contribution\": A rating from 1 to 4 (poor, fair, good, excellent).\n- \"Overall\": A rating from 1 to 10 (very strong reject to award\nquality).\n- \"Confidence\": A rating from 1 to 5 (low, medium, high, very high,\nabsolute).\n- \"Decision\": A decision that has to be one of the following:\nAccept, Reject.\nFor the \"Decision\" field, don’t use Weak Accept, Borderline Accept,\n55\nAgent Laboratory: Using LLM Agents as Research Assistants\nBorderline Reject, or Strong Reject. Instead, only use Accept or\nReject.\nThis JSON will be automatically parsed, so ensure the format is\nprecise.\nNeurIPS Reviewer Prompt\nOutlined in the following text is the research plan that the machine\nlearning engineer was tasked with building: {outlined_plan}\nThe following text is the research latex that the model produced:\n{latex}\nC. Survey questions\n56"
  },
  {
    "source": "cheatsheet-transformers-large-language-models.pdf",
    "content": "CME 295 – Transformers & Large Language Models https://cme295.stanford.edu\nVIP Cheatsheet:\nTransformers & Large Language Models\nAfshine Amidi and ShervineAmidi\nMarch 23, 2025\nThis VIP cheatsheet gives an overview of what is in the \"Super Study Guide: Transformers &\nLarge Language Models\" book, which contains∼600 illustrations over 250 pages and goes into\nthe following concepts in depth. You can find more details athttps: // superstudy. guide.\n1 Foundations\n1.1 Tokens\n❒ Definition – A token is an indivisible unit of text, such as a word, subword or character,\nand is part of a predefined vocabulary.\nRemark: The unknown token[UNK] represents unknown pieces of text while the padding token\n[PAD] is used to fill empty positions to ensure consistent input sequence lengths.\n❒ Tokenizer– Atokenizer T divides text into tokens of an arbitrary level of granularity.\nTthis teddy bear is reaaaally cute this cute [PAD][UNK]teddy bear is [PAD]...\nHere are the main types of tokenizers:\nType Pros Cons Illustration\nWord • Easy to interpret\n• Short sequence\n• Large vocabulary size\n• Word variations not handled\nteddy  bear\nSubword • Word roots leveraged\n• Intuitive embeddings\n• Increased sequence length\n• Tokenization more complex\nted ##dy bear\nCharacter\nByte\n• No out-of-vocabulary\nconcerns\n• Small vocabulary size\n• Much longer sequence length\n• Patterns hard to interpret\nbecause too low-level\nt e d d y  \nb e a r\n \nRemark: Byte-Pair Encoding (BPE) and Unigram are commonly-used subword-level tokenizers.\n1.2 Embeddings\n❒ Definition– Anembeddingis a numerical representation of an element (e.g. token, sentence)\nand is characterized by a vectorx∈Rn.\n❒ Similarity – Thecosine similaritybetween two tokenst1,t2 is quantified by:\nsimilarity(t1,t2) = t1 · t2\n||t1||||t2||= cos(θ) ∈[−1,1]\nThe angleθ characterizes the similarity between the two tokens:\nSimilar Dissimilar Independent\nteddy bear\ncute\nteddy bear\nunpleasant\nteddy bear\nairplane\nRemark: Approximate Nearest Neighbors (ANN) and Locality Sensitive Hashing (LSH) are\nmethods that approximate the similarity operation efficiently over large databases.\n2 Transformers\n2.1 Attention\n❒ Formula– Given aquery q, we want to know whichkey k the query should pay \"attention\"\nto with respect to the associatedvalue v.\na cute is reading .\nteddy bear\na cute teddy bear is reading .\nteddy bear\nteddy bear\ncutea is reading .\nteddy bearcutea is reading .v v v v v v\nkT kT kT kT kT kT\nq\nAttention can be efficiently computed using matricesQ,K,V that contain queriesq, keyskand\nvalues v respectively, along with the dimensiondk of keys:\nattention = softmax\n(\nQKT\n√dk\n)\nV\n❒ MHA – AMulti-Head Attention (MHA) layer performs attention computations across mul-\ntiple heads, then projects the result in the output space.\nInput queries\nInput keys\nInput values\nOutputW O\nProjection\nW Q\n1\nW K\n1\nW V\n1\nAttention head 1\nAttention head h\nW Q\nh\nW K\nh\nW V\nh . . .\nIt is composed ofh attention heads as well as matricesWQ,WK,WV that project the input\nto obtain queriesQ, keysK and valuesV. The projection is done using matrixWO.\nRemark: Grouped-Query Attention (GQA) and Multi-Query Attention (MQA) are variations\nof MHA that reduce computational overhead by sharing keys and values across attention heads.\n2.2 Architecture\n❒ Overview – Transformer is a landmark model relying on the self-attention mechanism and\nis composed of encoders and decoders. Encoders compute meaningful embeddings of the input\nthat are then used by decoders to predict the next token in the sequence.\nStanford University 1 Spring 2025\nCME 295 – Transformers & Large Language Models Shervine Amidi & Afshine Amidi\nEncoder\n...\nEncoder\nDecoder\n...\nDecoder\nmy teddy bear is cute . mon ours en peluche[BOS]\nest\nen-US\n fr-FR\nRemark: Although the Transformer was initially proposed as a model for translation tasks, it\nis now widely used across many other applications.\n❒ Components – The encoder and decoder are two fundamental components of the Trans-\nformer and have different roles:\nEncoder Decoder\nEncoded embeddings encapsulate meaning\nof input\nDecoded embeddings encapsulate meaning\nof both input and output predicted so far\nSelf-Attention\nFeed-Forward Neural Network\n+\nQueryKeyValue\n+\nMasked Self-Attention\nCross-Attention\n+\nQueryKeyValue\n+\nQueryKeyValue\nFeed-Forward Neural Network\n+\n...\n❒ Position embeddings– Position embeddingsinform where the token is in the sentence and\nare of the same dimension as the token embeddings. They can either be arbitrarily defined or\nlearned from the data.\nRemark: Rotary Position Embeddings (RoPE) are a popular and efficient variation that rotate\nquery and key vectors to incorporate relative position information.\n2.3 Variants\n❒ Encoder-only – Bidirectional Encoder Representations from Transformers (BERT) is a\nTransformer-based model composed of a stack of encoders that takes some text as input, and\noutputs meaningful embeddings, which can be later used in downstream classification tasks.\nEncoderN ×\nLinear + Softmax\npositive\nmy teddy bear is cute[CLS]\nA [CLS] token is added at the beginning of the sequence to capture the meaning of the sentence.\nIts encoded embedding is often used in downstream tasks, such as sentiment extraction.\n❒ Decoder-only–GenerativePre-trainedTransformer(GPT)isanautoregressiveTransformer-\nbased model that is composed of a stack of decoders. Contrary to BERT and its derivatives,\nGPT treats all problems as text-to-text problems.\nDecoderN ×\nLinear + Softmax\ncute\n[BOS] my teddy bear is\nMost of the current state-of-the-art LLMs rely on a decoder-only architecture, such as the GPT\nseries, LLaMA, Mistral, Gemma, DeepSeek, etc.\nRemark: Encoder-decoder models, like T5, are also autoregressive and share many character-\nistics with decoder-only models.\n2.4 Optimizations\n❒ Attention approximation– Attention computations are inO(n2), which can be costly as\nthe sequence lengthn increases. There are two main methods to approximate computations:\n• Sparsity: Self-attention does not happen through the whole sequence but only between\nmore relevant tokens.\n• Low-rank: The attention formula is simplified as the product of low-rank matrices, which\nbrings down the computation burden.\n❒ Flash attention– Flash attentionis an exact method that optimizes attention computations\nby cleverly leveraging GPU hardware, using the fastStatic Random-Access Memory (SRAM)\nfor matrix operations before writing results to the slowerHigh Bandwidth Memory (HBM).\nRemark: In practice, this reduces memory usage and speeds up computations.\nStanford University 2 Spring 2025\nCME 295 – Transformers & Large Language Models Shervine Amidi & Afshine Amidi\n3 Large language models\n3.1 Overview\n❒ Definition – A Large Language Model (LLM) is a Transformer-based model with strong\nNLP capabilities. It is \"large\" in the sense that it typically contains billions of parameters.\n❒ Lifecycle – An LLM is trained in 3 steps: pretraining, finetuning and preference tuning.\nPretraining Finetuning Preference \ntuning\nLearn generalities about language Learn speciﬁc tasks Demote bad answers\nFinetuning and preference tuning are post-training approaches that aim ataligning the model\nto perform certain tasks.\n3.2 Prompting\n❒ Context length– The context length of a model is the maximum number of tokens that\ncan fit into the input. It typically ranges from tens of thousands to millions of tokens.\n❒ Decoding sampling– Token predictions are sampled from the predicted probability distri-\nbution pi, which is controlled by the hyperparameter temperatureT.\nx\nT ≪ 1\np\npi =\nexp\n(xi\nT\n)\nn∑\nj=1\nexp\n(xj\nT\n)\nT ≪ 1\nx\np\nRemark: High temperatures lead to more creative outputs whereas low temperatures lead to\nmore deterministic ones.\n❒ Chain-of-thought – Chain-of-Thought (CoT) is a reasoning process in which the model\nbreaks down a complex problem into a series of intermediate steps. This helps the model to\ngenerate the correct final response.Treeof Thoughts (ToT) is a more advanced version of CoT.\nRemark: Self-consistency is a method that aggregates answers across CoT reasoning paths.\n3.3 Finetuning\n❒ SFT – SupervisedFineTuning (SFT) is a post-training approach that aligns the behavior of\nthe model with an end task. It relies on high-quality input-output pairs aligned with the task.\nRemark: If the SFT data is about instructions, then this step is called \"instruction tuning\".\n❒ PEFT– Parameter-Efficient FineTuning (PEFT) is a category of methods used to run SFT\nefficiently. In particular,Low-Rank Adaptation (LoRA) approximates the learnable weightsW\nby fixingW0 and learning low-rank matricesA,B instead:\nW 0Wd\nk\n+\nk\nd B\nr\nd≈\nAr\nk\n×\nRemark: Other PEFT techniques include prefix tuning and adapter layer insertion.\n3.4 Preference tuning\n❒ Reward model– A Reward Model (RM) is a model that predicts how well an outputˆy\naligns with desired behavior given the inputx. Best-of-N (BoN) sampling, also calledrejection\nsampling, is a method that uses a reward model to select the best response amongN generations.\nRMfx , , ..., ̂y1 ̂y2 ̂yN k = argmax r (x, ̂yi )\ni ∈[[1, N ]]\n❒ Reinforcement learning– ReinforcementLearning(RL) is an approach that leverages RM\nand updates the modelf based on rewards for its generated outputs. If RM is based on human\npreferences, this process is calledReinforcement Learning fromHuman Feedback(RLHF).\nRM r (x, ̂ y)fx ̂ y\nProximalPolicy Optimization (PPO)isapopularRLalgorithmthatincentivizeshigherrewards\nwhile keeping the model close to the base model to prevent reward hacking.\nRemark: There are also supervised approaches, like Direct Preference Optimization (DPO),\nthat combine RM and RL into one supervised step.\n3.5 Optimizations\n❒ Mixture of experts– AMixture of Experts (MoE) is a model that activates only a portion\nof its neurons at inference time. It is based on a gateG and expertsE1,...,E n.\n...\nx × ̂y\nE1\nE2\nEn\nG\nˆy=\nn∑\ni=1\nG(x)iEi(x)\nMoE-based LLMs use this gating mechanism in their FFNNs.\nRemark: Training an MoE-based LLM is notoriously challenging, as mentioned in the LLaMA\npaper whose authors chose to not use this architecture despite its inference-time efficiency.\n❒ Distillation – Distillation is a process where a (small) student modelS is trained on the\nprediction outputs of a (big) teacher modelT. It is trained using the KL divergence loss:\nKL(ˆyT ||ˆyS) =\n∑\ni\nˆy(i)\nT log\n(\nˆy(i)\nT\nˆy(i)\nS\n)\nRemark: Training labels are considered as \"soft\" labels since they represent class probabilities.\nStanford University 3 Spring 2025\nCME 295 – Transformers & Large Language Models Shervine Amidi & Afshine Amidi\n❒ Quantization– Model quantizationis a category of techniques that reduces the precision of\nmodel weights while limiting its impact on the resulting model performance. As a result, this\nreduces the model’s memory footprint and speeds up its inference.\nRemark: QLoRA is a commonly-used quantized variant of LoRA.\n4 Applications\n4.1 LLM-as-a-Judge\n❒ Definition – LLM-as-a-Judge (LaaJ) is a method that uses an LLM to score given outputs\naccording to some provided criteria. Notably, it is also able to generate a rationale for its score,\nwhich helps with interpretability.\nItem to score\nLaaJ\nCriteria Rationale\nScore\nCuteness\nTeddy bear 10/10\nTeddy bears are the cutest\nContrary to pre-LLM era metrics such asRecall-Oriented Understudy for Gisting Evaluation\n(ROUGE), LaaJ does not need any reference text, which makes it convenient to evaluate on any\nkind of task. In particular, LaaJ shows strong correlation with human ratings when it relies on\na big powerful model (e.g. GPT-4), as it requires reasoning capabilities to perform well.\nRemark: LaaJ is useful to perform quick rounds of evaluations but it is important to monitor\nthe alignment between LaaJ outputs and human evaluations to make sure there is no divergence.\n❒ Common biases– LaaJ models can exhibit the following biases:\nPosition bias Verbosity bias Self-enhancement bias\nProblem Favors first position in\npairwise comparisons\nFavors more verbose\ncontent\nFavors outputs generated\nby themselves\nSolution Average metric on\nrandomized positions\nAdd a penalty on the\noutput length\nUse a judge built from\na different base model\nA remedy to these issues can be to finetune a custom LaaJ, but this requires a lot of effort.\nRemark: The list of biases above is not exhaustive.\n4.2 RAG\n❒ Definition – Retrieval-Augmented Generation (RAG) is a method that allows the LLM to\naccess relevant external knowledge to answer a given question. This is particularly useful if we\nwant to incorporate information past the LLM pretrained knowledge cut-off date.\nQ LLM A\n𝒟\nGiven a knowledge baseD and a question, aRetriever fetches the most relevant documents,\nthen Augments the prompt with the relevant information beforeGenerating the output.\nRemark: The retrieval stage typically relies on embeddings from encoder-only models.\n❒ Hyperparameters – The knowledge baseD is initialized by chunking the documents into\nchunks of sizenc and embedding them into vectors of sizeRd.\nnc\n1\nd\n4.3 Agents\n❒ Definition – Anagent is a system that autonomously pursues goals and completes tasks on\na user’s behalf. It may use different chains of LLM calls to do so.\n❒ ReAct– Reason +Act (ReAct) is a framework that allows for multiple chains of LLM calls\nto complete complex tasks:\nInput Output\nAct\nObserve\nPlan\nThis framework is composed of the steps below:\n• Observe: Synthesize previous actions and explicitly state what is currently known.\n• Plan: Detail what tasks need to be accomplished and what tools to call.\n• Act: Perform an action via an API or look for relevant information in a knowledge base.\nRemark: Evaluating an agentic system is challenging. However, this can still be done both at\nthe component level via local inputs-outputs and at the system level via chains of calls.\n4.4 Reasoning models\n❒ Definition– Areasoning modelis a model that relies on CoT-based reasoning traces to solve\nmore complex tasks in math, coding and logic. Examples of reasoning models include OpenAI’s\no series, DeepSeek-R1 and Google’s Gemini Flash Thinking.\nRemark: DeepSeek-R1 explicitly outputs its reasoning trace between<think> tags.\n❒ Scaling – Two types of scaling methods are used to enhance reasoning capabilities:\nDescription Illustration\nTrain-time\nscaling\nRun RL for longer to let the model learn\nhow to produce CoT-style reasoning\ntraces before giving an answer\nRL steps\nPerformance\nTest-time\nscaling\nLet the model think longer before\nproviding an answer with budget\nforcing keywords such as \"Wait\"\nCoT length\nPerformance\nStanford University 4 Spring 2025"
  },
  {
    "source": "2412.15605v2.pdf",
    "content": "arXiv:2412.15605v2  [cs.CL]  23 Feb 2025\nDon’t Do RAG:\nWhen Cache-Augmented Generation is All You Need for\nKnowledge Tasks\nBrian J Chan ∗\nChao-Ting Chen∗\nJui-Hung Cheng ∗\nDepartment of Computer Science\nNational Chengchi University\nTaipei, Taiwan\n{110703065,110703038,110703007}@nccu.edu.tw\nHen-Hsen Huang\nInsititue of Information Science\nAcademia Sinica\nTaipei, Taiwan\nhhhuang@iis.sinica.edu.tw\nAbstract\nRetrieval-augmented generation (RAG) has gained traction as a\npowerful approach for enhancing language models by integra ting\nexternal knowledge sources. However, RAG introduces chall enges\nsuch as retrieval latency, potential errors in document sel ection,\nand increased system complexity. With the advent of large la n-\nguage models (LLMs) featuring signiﬁcantly extended conte xt win-\ndows, this paper proposes an alternative paradigm, cache-a ugmented\ngeneration (CAG) that bypasses real-time retrieval. Our me thod in-\nvolves preloading all relevant resources, especially when the docu-\nments or knowledge for retrieval are of a limited and managea ble\nsize, into the LLM’s extended context and caching its runtim e pa-\nrameters. During inference, the model utilizes these prelo aded pa-\nrameters to answer queries without additional retrieval st eps. Com-\nparative analyses reveal that CAG eliminates retrieval lat ency and\nminimizes retrieval errors while maintaining context rele vance. Per-\nformance evaluations across multiple benchmarks highligh t sce-\nnarios where long-context LLMs either outperform or comple ment\ntraditional RAG pipelines. These ﬁndings suggest that, for certain\napplications, particularly those with a constrained knowl edge base,\nCAG provide a streamlined and eﬃcient alternative to RAG, ac hiev-\ning comparable or superior results with reduced complexity .\nCCS Concepts\n• Computing methodologies → Discourse, dialogue and prag-\nmatics; Natural language generation ; • Information systems\n→ Specialized information retrieval .\nKeywords\nCache Augmented Generation, Retrieval Augmented Generati on,\nRetrieval-Free Question Answering, Large Language Models\n∗Three authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of thi s work for personal or\nclassroom use is granted without fee provided that copies ar e not made or distributed\nfor proﬁt or commercial advantage and that copies bear this n otice and the full cita-\ntion on the ﬁrst page. Copyrights for components of this work owned by others than\nthe author(s) must be honored. Abstracting with credit is pe rmitted. To copy other-\nwise, or republish, to post on servers or to redistribute to l ists, requires prior speciﬁc\npermission and/or a fee. Request permissions from permissi ons@acm.org.\nWWW Companion ’25, April 28-May 2, 2025, Sydney, NSW, Australia\n© 2025 Copyright held by the owner/author(s). Publication r ights licensed to ACM.\nACM ISBN 979-8-4007-1331-6/2025/04\nhttps://doi.org/10.1145/3701716.3715490\n1 Introduction\nThe advent of retrieval-augmented generation (RAG) [2, 5] h as\nsigniﬁcantly enhanced the capabilities of large language m odels\n(LLMs) by dynamically integrating external knowledge sour ces. RAG\nsystems have proven eﬀective in handling open-domain quest ions\nand specialized tasks, leveraging retrieval pipelines to p rovide con-\ntextually relevant answers. However, RAG is not without its draw-\nbacks. The need for real-time retrieval introduces latency , while\nerrors in selecting or ranking relevant documents can degra de the\nquality of the generated responses. Additionally, integra ting re-\ntrieval and generation components increases system comple xity,\nnecessitating careful tuning and adding to the maintenance over-\nhead.\nThis paper proposes an alternative paradigm, cache-augmen ted\ngeneration (CAG), leveraging the capabilities of long-con text LLMs\nto address these challenges. Instead of relying on a retriev al pipeline,\nas shown in Figure 1, our approach involves preloading the LL M\nwith all relevant documents in advance and precomputing the key-\nvalue (KV) cache [9], which encapsulates the inference stat e of\nthe LLM. The preloaded context enables the model to provide r ich,\ncontextually accurate answers without the need for additio nal re-\ntrieval during runtime. This approach eliminates retrieva l latency,\nmitigates retrieval errors, and simpliﬁes system architec ture, all\nwhile maintaining high-quality responses by ensuring the m odel\nprocesses all relevant context holistically.\nRecent advances in long-context LLMs have extended their ab il-\nity to process and reason over substantial textual inputs. F or exam-\nple, Llama 3.1 [1] was trained with a 128K context length, and its\neﬀective context length is 32K in Llama 3.1 8B and 64K in Llama\n3.1 70B [3]. This 32K to 64K context window is suﬃcient for sto r-\ning knowledge sources such as internal company documentati on,\nFAQs, customer support logs, and domain-speciﬁc databases , mak-\ning it practical for many real-world applications. By accom modat-\ning larger context windows, these models can assimilate ext ensive\ninformation in a single inference step, making them well-su ited\nfor tasks like document comprehension, multi-turn dialogu e, and\nsummarization of lengthy texts. This capability eliminate s the de-\npendency on real-time retrieval, as all necessary informat ion can\nWWW Companion ’25, April 28-May 2, 2025, Sydney, NSW, Australia Brian J Chan, Chao-Ting Chen, Jui-Hung Cheng, and Hen-Hse n Huang\nIR ModelQuery\nKnowledge\nSource\nLLM\nContext /u1D45E /u1D45F\nRetrieval-Augmented Generation\nKnowledge\nSource\nQuery\nLLM\nContext\nCacheOﬄine Preloading\n/u1D45E /u1D45F\nCache-Augmented Generation\nActive during inference\nFigure 1: Comparison of Retrieval-Augmented Generation\n(RAG) and our Cache-Augmented Generation (CAG) Work-\nﬂows: The pink-shaded components represent the processes\nactive during real-time inference. In RAG (top section), th e\nIR model retrieves relevant information from the knowl-\nedge source, and both the retrieved knowledge and query\nare processed by the LLM during inference, introducing re-\ntrieval latency. In contrast, CAG (bottom section) preload s\nand caches knowledge oﬀline, allowing the LLM to process\nonly the query during inference, eliminating retrieval ove r-\nhead and ensuring a more eﬃcient generation process.\nbe preloaded into the model. These developments create oppo rtu-\nnities to streamline workﬂows for knowledge-intensive tas ks, po-\ntentially reducing or even eliminating the need for traditi onal RAG\nsystems.\nRecent studies [4, 7] have investigated the performance of l ong-\ncontext models in RAG tasks, revealing that state-of-the-a rt mod-\nels like GPT-o1, GPT-4, and Claude 3.5 can eﬀectively proces s large\namounts of retrieved data, outperforming traditional syst ems in\nmany scenarios. Findings suggest that as long as all documen ts\nﬁt within the extended context length, traditional RAG syst ems\ncan be replaced by these long-context models. Similarly, Lu et al.\n[8] has demonstrated the beneﬁts of precomputed KV caching t o\nimprove eﬃciency, albeit with the need for position ID rearr ange-\nment to enable proper functioning. Nonetheless, these meth ods re-\nmain vulnerable to retrieval failures inherent to RAG syste ms.\nThrough a series of experiments comparing traditional RAG w ork-\nﬂows with our proposed approach, we identify scenarios wher e\nlong-context LLMs outperform RAG in both eﬃciency and accu-\nracy. By addressing the technical and practical implicatio ns, this\npaper aims to provide insights into when and why CAG may serve\nas a streamlined, eﬀective alternative to RAG, particularl y for cases\nwhere the documents or knowledge for retrieval are of limite d,\nmanageable size. Our ﬁndings challenge the default relianc e on\nRAG for knowledge integration tasks, oﬀering a simpliﬁed, r obust\nsolution to harness the growing capabilities of long-conte xt LLMs.\nOur contributions are threefold as follows:\n• Eﬃcient Alternative to RAG : We introduced a novel ap-\nproach leveraging long-context LLMs with preloaded docu-\nments and precomputed KV caches, mitigating retrieval la-\ntency, errors, and system complexity.\n• Quantitative Analysis: We conducted extensive experiments\nshowing scenarios where long-context LLMs outperform tra-\nditional RAG systems, especially with manageable knowl-\nedge bases.\n• Practical Insights: This work provided actionable insights\ninto optimizing knowledge-intensive workﬂows, demonstra t-\ning the viability of retrieval-free methods for speciﬁc app li-\ncations. Our CAG framework is released publicly. 1\n2 Methodology\nOur CAG framework leverages the extended context capabilit ies of\nlong-context LLMs to enable retrieval-free knowledge inte gration.\nBy preloading external knowledge sources, such as a collect ion of\ndocuments D = { /u1D4511,/u1D4512, . . .} , and precomputing the KV cache CKV,\nwe address the computational challenges and ineﬃciencies i nher-\nent to real-time retrieval in traditional RAG systems. The o peration\nof our framework is divided into three phases:\n(1) External Knowledge Preloading\nIn this phase, a curated collection of documents D relevant\nto the target application is preprocessed and formatted to\nﬁt within the model’s extended context window. The LLM\nM, with parameters /u1D703, processes D, transforming it into a\nprecomputed KV cache:\nCKV = KV-Encode(D) (1)\nThis KV cache, which encapsulates the inference state of\nthe LLM, is stored on disk or in memory for future use. The\ncomputational cost of processing D is incurred only once,\nregardless of the number of subsequent queries.\n(2) Inference\nDuring inference, the precomputed KV cache CKV is loaded\nalongside the user’s query /u1D45E. The LLM utilizes this cached\ncontext to generate responses:\n/u1D45F= M (D ⊕ /u1D45E) = M ( /u1D45E| C KV) (2)\nBy preloading the external knowledge, this phase eliminate s\nretrieval latency and reduces risks of errors or omissions\nthat arise from dynamic retrieval. The combined prompt\nD ⊕ /u1D45Eensures a uniﬁed understanding of both the external\nknowledge and the user query.\n(3) Cache Reset\nTo maintain system performance across multiple inference\nsessions, the KV cache, stored in memory, can be reset eﬃ-\nciently. As the KV cache grows in an append-only manner\nwith new tokens /u1D45E= ( /u1D4611, /u1D4612, . . . , /u1D461/u1D458) sequentially appended,\nresetting involves truncating these new tokens. This allow s\nfor rapid reinitialization without reloading the entire ca che\nfrom disk, ensuring sustained speed and responsiveness.\nThe proposed methodology oﬀers several signiﬁcant advanta ges\nover traditional RAG systems:\n1https://github.com/hhhuang/CAG\nDon’t Do RAG:\nWhen Cache-Augmented Generation is All You Need for KnowledgeTasks WWW Companion ’25, April 28-May 2, 2025, Sydney, NSW, A ustralia\n• Reduced Inference Time: By eliminating the need for real-\ntime retrieval, the inference process becomes faster and mo re\neﬃcient, enabling quicker responses to user queries.\n• Uniﬁed Context : Preloading the entire knowledge collec-\ntion into the LLM provides a holistic and coherent under-\nstanding of the documents, resulting in improved response\nquality and consistency across a wide range of tasks.\n• Simpliﬁed Architecture : By removing the need to inte-\ngrate retrievers and generators, the system becomes more\nstreamlined, reducing complexity, improving maintainabi l-\nity, and lowering development overhead.\nLooking forward, our approach is poised to become even more\npowerful with the anticipated advancements in LLMs. As futu re\nmodels continue to expand their context length, they will be able\nto process increasingly larger knowledge collections in a s ingle in-\nference step. Additionally, the improved ability of these m odels to\nextract and utilize relevant information from long context s will\nfurther enhance their performance. These two trends will si gniﬁ-\ncantly extend the usability of our approach, enabling it to h andle\nmore complex and diverse applications. Consequently, our m ethod-\nology is well-positioned to become a robust and versatile so lution\nfor knowledge-intensive tasks, leveraging the growing cap abilities\nof next-generation LLMs.\n3 Experiments\n3.1 Experimental Setup\nTo evaluate the eﬀectiveness of our proposed method, we cond ucted\nexperiments using two widely recognized question-answeri ng bench-\nmarks: the Stanford Question Answering Dataset (SQuAD) 1.0 [10]\nand the HotPotQA dataset [11]. These datasets provide compl e-\nmentary challenges, with SQuAD focusing on precise, contex t-aware\nanswers within single passages and HotPotQA emphasizing mu lti-\nhop reasoning across multiple documents. Each of both datas ets\nconsists of documents D = { /u1D4511,/u1D4512, . . .} paired with questions Q =\n{ /u1D45E1, /u1D45E2, . . .} and golden responses R = { /u1D45F1, /u1D45F2, . . .} . These datasets\nprovide a robust platform for assessing both single-contex t com-\nprehension and complex multi-hop reasoning.\nTo investigate how diﬀerent levels of reference text length im-\npact retrieval diﬃculty, we created three test sets for each dataset,\nvarying the size of the reference text. For example, in the Ho tPotQA-\nsmall conﬁguration, we sampled 16 documents D/u1D460⊂ D from the\nHotPotQA document set to form a long reference text. QA pairs as-\nsociated with D/u1D460were selected as test instances. The same method-\nology was applied to create test sets for SQuAD.\nThe dataset statistics are summarized in Table 1. As the numb er\nof documents (and hence the length of the reference text) inc reases,\nthe task becomes more challenging, particularly for RAG sys tems.\nLonger reference texts increase the diﬃculty of accurately retriev-\ning the correct information, which is crucial for LLMs to gen erate\nhigh-quality responses.\nThe primary task involves generating accurate and contextu ally\nrelevant answers ˆR = { ˆ/u1D45F1, ˆ/u1D45F2, . . .} for the SQuAD and HotPotQA\nquestions, based on the respective preloaded passages. By l everag-\ning the precomputed key-value cache CKV = KV-Encode(D) , our\nsystem generates responses ˆ/u1D45F/u1D456= M ( /u1D45E/u1D456| C KV) without relying on\nTable 1: The SQuAD and HotPotQA test sets with varying ref-\nerence text lengths, highlighting the number of documents,\nquestions, and associated responses for each conﬁguration .\nSource Size # Docs # Tokens # QA Pairs\nHotPotQA\nSmall 16 21k 1,392\nMedium 32 43k 1,056\nLarge 64 85k 1,344\nSQuAD\nSmall 3 21k 500\nMedium 4 32k 500\nLarge 7 50k 500\nretrieval mechanisms during inference. This uniﬁed approa ch al-\nlows for direct performance comparisons against tradition al RAG\nsystems, highlighting the strengths and limitations of our method\nacross diverse QA challenges.\nThe experiments were executed on Tesla V100 32G × 8 GPUs.\nFor all experiments, we used Llama 3.1 8B [1] as the underlyin g\nLLM across all systems, including both the RAG baselines and our\nproposed method. This model supports input sizes of up to 128 k\ntokens, enabling the processing of extensive contexts. For our pro-\nposed method, the context of each dataset was preloaded into the\nmodel via a precomputed KV cache. For SQuAD, the documents\nDS were encoded into a KV cache CS\nKV = KV-Encode(D S) . For\nHotPotQA, similarly, the documents DH were encoded into CH\nKV =\nKV-Encode(D H) . These caches were stored oﬄine and loaded dur-\ning inference to eliminate the need for real-time retrieval , ensuring\ncomprehensive access to all relevant information for each d ataset.\nOur experiments were conducted on both the SQuAD and Hot-\nPotQA datasets to evaluate the performance of diﬀerent syst ems\nin terms of similarity to ground-truth answers, measured us ing\nBERTScore [12]. Each dataset—SQuAD and HotPotQA—was evalu -\nated separately, with retrieval systems conﬁgured to fetch passages\nexclusively from the respective dataset to ensure focused a nd fair\nevaluation.\n3.2 Baseline Systems\nThe baseline RAG systems were implemented using the LlamaIn -\ndex framework,2 employing two retrieval strategies: BM25 for sparse\nretrieval and OpenAI Indexes for dense retrieval. The detai ls of\neach baseline system are as follows:\n(1) Sparse Retrieval System (BM25) : The ﬁrst baseline sys-\ntem employed BM25 indexes for retrieval. BM25, a sparse re-\ntrieval algorithm, ranks documents based on term frequency -\ninverse document frequency (TF-IDF) and document length\nnormalization. Given a query /u1D45E/u1D456, BM25 retrieves the top- /u1D458\npassages P/u1D458= { /u1D45D1, /u1D45D2, . . . , /u1D45D/u1D458} from the indexed collection\nD. These passages were then passed to the generator, M,\nto synthesize answers:\nˆ/u1D45F/u1D456= M ( /u1D45E/u1D456| P /u1D458) (3)\nBM25 provides a robust and interpretable retrieval mecha-\nnism, suited for tasks involving keyword matching.\n2https://www.llamaindex.ai/framework\nWWW Companion ’25, April 28-May 2, 2025, Sydney, NSW, Australia Brian J Chan, Chao-Ting Chen, Jui-Hung Cheng, and Hen-Hse n Huang\nTable 2: Experimental Results\nHotPotQA SQuAD\nSize System Top- /u1D458 BERT-Score BERT-Score\nSmall\nSparse RAG\n1 0.6788 0.7214\n3 0.7626 0.7616\n5 0.7676 0.7608\n10 0.7521 0.7584\nDense RAG\n1 0.7164 0.6216\n3 0.7582 0.7106\n5 0.7481 0.7334\n10 0.7576 0.7586\nCAG (Ours) 0.7951 0.7695\nMedium\nSparse RAG\n1 0.6592 0.6902\n3 0.7546 0.7301\n5 0.7633 0.7298\n10 0.7458 0.7262\nDense RAG\n1 0.6973 0.5871\n3 0.7432 0.6702\n5 0.7322 0.6890\n10 0.7308 0.7310\nCAG (Ours) 0.7821 0.7383\nLarge\nSparse RAG\n1 0.6616 0.7254\n3 0.7463 0.7634\n5 0.7535 0.7658\n10 0.7345 0.7613\nDense RAG\n1 0.7020 0.6070\n3 0.7409 0.7018\n5 0.7234 0.7286\n10 0.7374 0.7590\nCAG (Ours) 0.7407 0.7734\n(2) Dense Retrieval System (OpenAI Indexes) : The second\nbaseline utilized OpenAI indexes, 3 which employ dense em-\nbeddings to represent both documents and queries in a shared\nsemantic space. For a query /u1D45E/u1D456, dense retrieval selects the\ntop-/u1D458passages P/u1D458that semantically align with the query,\noﬀering improved contextual understanding compared to\nsparse methods. These passages were similarly passed to\nthe generator for answer synthesis as Equation 3. This sys-\ntem is particularly eﬀective for questions requiring nuanc ed\ncontextual matching beyond exact term overlap.\nFor the RAG baselines, the top-1, top-3, top-5, and top-10 re -\ntrieved passages were used for inference. In contrast, our C AG uti-\nlized the preloaded context speciﬁc to each dataset to gener ate an-\nswers without retrieval constraints.\n3.3 Results\nAs shown in Table 2, the experimental results highlight key d is-\ntinctions between our proposed CAG approach and RAG systems .\nCAG consistently achieved the highest BERTScore in most cas es,\noutperforming both sparse and dense RAG methods. By preload ing\nthe entire reference text from the test set, our method is imm une to\n3https://cookbook.openai.com/examples/evaluation/evaluate_rag_with_llamaindex\nTable 3: Response Time (Seconds) Comparison on HotPotQA\nSize System Retrieval Generation\nSmall\nSparse RAG, Top-3 0.0008 0.7406\nSparse RAG, Top-10 0.0012 1.5595\nDense RAG, Top-3 0.4849 1.0093\nDense RAG, Top-10 0.3803 2.6608\nCAG - 0.8512\nIn-Context Learning - 9.3197\nMedium\nSparse RAG, Top-3 0.0008 0.7148\nSparse RAG, Top-10 0.0012 1.5306\nDense RAG, Top-3 0.4140 0.9566\nDense RAG, Top-10 0.4171 2.6361\nCAG - 1.4078\nIn-Context Learning - 26.3717\nLarge\nSparse RAG, Top-3 0.0008 0.6667\nSparse RAG, Top-10 0.0012 1.5175\nDense RAG, Top-3 0.4123 0.9331\nDense RAG, Top-10 0.4100 2.6447\nCAG - 2.2631\nIn-Context Learning - 92.0824\nretrieval errors, ensuring holistic reasoning over all rel evant infor-\nmation. This advantage is particularly evident in scenario s where\nRAG systems struggle with retrieving incomplete or irrelev ant pas-\nsages, leading to suboptimal answer generation.\nHowever, as the data size increases, the performance gap be-\ntween CAG and RAG narrows slightly, aligning with prior ﬁndi ngs\nthat long-context LLMs may experience degradation when han -\ndling very long contexts [6]. Additionally, the fact that sp arse RAG\noutperforms dense RAG suggests that the datasets may not be s uf-\nﬁciently challenging, allowing traditional sparse retrie val to eﬀec-\ntively capture most relevant information without requirin g deeper\nsemantic retrieval. Despite these factors, the results und erscore\nthe robustness and eﬃciency of CAG, particularly for tasks t hat\nrequire a uniﬁed understanding of the source material. By fu lly\nleveraging the long-context capabilities of Llama 3.1, our approach\nbypasses retrieval challenges and maintains superior perf ormance\nin retrieval-free knowledge integration.\nTable 3 and Figure 2 show the retrieval and generation time ac ross\ndiﬀerent HotPotQA knowledge sizes for various RAG methods a nd\nCAG. CAG eliminates retrieval time entirely, whereas spars e and\ndense retrieval-based RAG systems require additional retr ieval steps,\nwith dense retrieval incurring higher latency. Sparse RAG e xhibits\ninsigniﬁcant retrieval latency in the experiments, but sti ll requires\nretrieval before generation. As the knowledge size increas es, gener-\nation time grows across all methods, including CAG, highlig hting\nthe computational cost of handling longer contexts. Howeve r, CAG\nremains more eﬃcient than dense RAG, as it avoids retrieval o ver-\nhead while maintaining comparable or superior response tim es.\nTable 3 also compares our CAG approach with standard in-cont ext\nlearning, where the reference text is provided dynamically dur-\ning inference, requiring real-time KV-cache computation. The re-\nsults demonstrate that CAG dramatically reduces generatio n time,\nDon’t Do RAG:\nWhen Cache-Augmented Generation is All You Need for KnowledgeTasks WWW Companion ’25, April 28-May 2, 2025, Sydney, NSW, A ustralia\n0 1 2 3\n0 1 2 3\n0 1 2 3\nHotPotQA (Small)\nSparse RAG, Top-3\nSparse RAG, Top-10\nDense RAG, Top-3\nDense RAG, Top-10\nCAG\nHotPotQA (Medium)\nSparse RAG, Top-3\nSparse RAG, Top-10\nDense RAG, Top-3\nDense RAG, Top-10\nCAG\nHotPotQA (Large)\nSparse RAG, Top-3\nSparse RAG, Top-10\nDense RAG, Top-3\nDense RAG, Top-10\nCAG\nRetrieval time\nGeneration time\nFigure 2: Response Time Comparison on HotPotQA (Sec-\nonds). The x-axis represents response time in seconds acros s\ndiﬀerent knowledge sizes. CAG eliminates retrieval over-\nhead, while dense RAG incurs longer retrieval and genera-\ntion times due to retrieving and feeding longer text chunks\ninto the LLM. Sparse RAG retrieves shorter text spans, re-\nsulting in faster generation. As the knowledge size increas es,\ngeneration time grows for all methods, but CAG remains\ncompetitive while bypassing retrieval completely.\nparticularly as the reference text length increases. This e ﬃciency\nstems from preloading the KV-cache, which eliminates the ne ed to\nprocess the reference text on the ﬂy.\n4 Conclusion\nAs long-context LLMs evolve, we present a compelling case fo r\nrethinking traditional RAG workﬂows. While our work empha-\nsizes eliminating retrieval latency, there is potential fo r hybrid ap-\nproaches that combine preloading with selective retrieval . For ex-\nample, a system could preload a foundation context and use re -\ntrieval only to augment edge cases or highly speciﬁc queries . This\nwould balance the eﬃciency of preloading with the ﬂexibilit y of\nretrieval, making it suitable for scenarios where context c omplete-\nness and adaptability are equally important.\nLimitations\nOur method requires loading all relevant documents into the mod-\nels context, making it well-suited for use cases such as inte rnal\nknowledge bases of small companies, FAQs, and call centers, where\nthe knowledge source is of a manageable size. However, this a p-\nproach becomes impractical for signiﬁcantly larger datase ts. Fortu-\nnately, as LLMs continue to expand their context lengths and hard-\nware capabilities advance, this limitation is expected to d iminish,\nenabling broader applicability in the future.\nAcknowledgments\nThis work was partially supported by National Science and Te ch-\nnology Council (NSTC), Taiwan, under the grant 112-2221-E- 001-\n016-MY3, by Academia Sinica, under the grant 236d-1120205, and\nby National Center for High-performance Computing (NCHC), Na-\ntional Applied Research Laboratories (NARLabs), and NSTC u nder\nthe project “Trustworthy AI Dialog Engine, TAIDE. ” We thank Dis-\ncover AI4 and the many individuals who have introduced, shared,\nand discussed our work, contributing to its broader visibil ity.\nReferences\n[1] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhis hek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Y ang, Angela Fan,\net al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783(2024).\n[2] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu P an, Yuxi Bi, Yi Dai,\nJiawei Sun, and Haofen Wang. 2023. Retrieval-augmented gen eration for large\nlanguage models: A survey. arXiv preprint arXiv:2312.10997(2023).\n[3] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu A charya, Dima\nRekesh, Fei Jia, and Boris Ginsburg. 2024. RULER: What’s the Real Context\nSize of Your Long-Context Language Models?. In First Conference on Language\nModeling. https://openreview.net/forum?id=kIoBbc76Sy\n[4] Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, and Michael Carbin. 2024.\nLong Context RAG Performance of Large Language Models. arXiv preprint\narXiv:2411.03538 (2024).\n[5] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Pe troni, Vladimir\nKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen- tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation for k nowledge-intensive nlp\ntasks. Advances in Neural Information Processing Systems33 (2020), 9459–9474.\n[6] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. 2024. Long-\ncontext LLMs Struggle with Long In-context Learning. arXiv :2404.02060 [cs.CL]\nhttps://arxiv.org/abs/2404.02060\n[7] Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Mi chael Ben-\ndersky. 2024. Retrieval Augmented Generation or Long-Cont ext LLMs?\nA Comprehensive Study and Hybrid Approach. In Proceedings of the 2024\nConference on Empirical Methods in Natural Language Process ing: Industry\nTrack. Association for Computational Linguistics, Miami, Flori da, US, 881–893.\nhttps://doi.org/10.18653/v1/2024.emnlp-industry.66\n[8] Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, and Yaohua T ang.\n2024. TurboRAG: Accelerating Retrieval-Augmented Genera tion with Pre-\ncomputed KV Caches for Chunked Text. arXiv:2410.07590 [cs. CV]\nhttps://arxiv.org/abs/2410.07590\n[9] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jaco b Devlin, James Brad-\nbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeﬀ De an. 2023. Ef-\nﬁciently scaling transformer inference. Proceedings of Machine Learning and\nSystems 5 (2023), 606–624.\n[10] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension of Tex t. In Proceed-\nings of the 2016 Conference on Empirical Methods in Natural Language Processing,\nJian Su, Kevin Duh, and Xavier Carreras (Eds.). Association for Computational\nLinguistics, Austin, Texas, 2383–2392. https://doi.org/ 10.18653/v1/D16-1264\n[11] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, Wi lliam W. Cohen, Rus-\nlan Salakhutdinov, and Christopher D. Manning. 2018. Hotpo tQA: A Dataset for\nDiverse, Explainable Multi-hop Question Answering. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\n[12] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinbe rger, and Yoav Artzi.\n[n. d.]. BERTScore: Evaluating Text Generation with BERT. I n International Con-\nference on Learning Representations.\n4https://www.youtube.com/watch?v=NaEf_uiFX6o\nThis figure \"acm-jdslogo.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2412.15605v2\nThis figure \"sample-franklin.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2412.15605v2"
  },
  {
    "source": "document_1.pdf",
    "content": "Dataset Description \nCOCO-VID Dataset: \nCOCO-Video Dataset is an extension of COCO dataset, designed specifically for video -\nbased tasks such as object detection . It contains video clips with annotations that span \nacross frames, offering temporal continuity, and scene context. Unlike the original COCO \ndataset, which focuses on static images, COCO -Video provides annotations that are \nconsistent across frames, enabling tasks such as object tracking and action recognition. \nObjects are labeled with instance IDs across consecutive frames. The dataset includes a \ndiverse set of scenes, object categories, and motion patterns, reflecting the  real-world \nenvironments. The annotations include bounding boxes, segmentation masks, and object \nclasses for each frame in the video clips. \n \nWaymo Open: \nWaymo Open Dataset is a large-scale dataset curated by Waymo, a leading company in \nautonomous driving technology. The dataset contains diverse real -world driving scenarios \ncaptured from Waymo’s self -driving vehicles, featuring high -resolution sensor data and \ndetailed annotations. The dataset includes data from a variety of sensors, including LiDAR, \ncameras, and radar . Each frame in the dataset is annotated with labels for objects like \nvehicles, pedestrians, cyclists, and road signs, along with their bounding boxes, \nsegmentation masks, and trajectories. \n \nUCF101: \nThe UCF101 Dataset has been adapted to object detection tasks. UCF101 contains over \n13,000 video clips spanning 101 categories, such as sports, human interactions, and daily \nactivities. When adapted for object detection, the focus shifts to identifying and localizing \nobjects of interest within each video frame, enabling detailed analysis of the spatial and \ntemporal relationships in dynamic scenes. This adaptation involves annotating objects \nacross frames to generate bounding boxes and class labels. \nDataset Categories Training Set Test Set \nUCF101 101 9,537 3,783 \nCOCO 80 118K 41K \nWaymo Open 4 1000+ 200+ \n "
  },
  {
    "source": "2407.04153v1.pdf",
    "content": "Mixture of A Million Experts\nXu Owen He hexu@google.com\nGoogle DeepMind\nAbstract\nThe feedforward (FFW) layers in standard transformer architectures incur a linear increase\nin computational costs and activation memory as the hidden layer width grows. Sparse\nmixture-of-experts (MoE) architectures have emerged as a viable approach to address this\nissue by decoupling model size from computational cost. The recent discovery of the fine-\ngrained MoE scaling law shows that higher granularity leads to better performance. How-\never, existing MoE models are limited to a small number of experts due to computational\nand optimization challenges. This paper introduces PEER (parameter efficient expert re-\ntrieval), a novel layer design that utilizes the product key technique for sparse retrieval\nfrom a vast pool of tiny experts (over a million). Experiments on language modeling tasks\ndemonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms\nof performance-compute trade-off. By enabling efficient utilization of a massive number of\nexperts, PEER unlocks the potential for further scaling of transformer models while main-\ntaining computational efficiency.\n(a) 6e18 FLOPs\n (b) 2e19 FLOPs\nFigure 1: Isoflop comparison on the C4 dataset between PEER and other baselines with two different FLOP\nbudgets (6e18 and 2e19 FLOPs). The x axis is in log scale.\n1 Introduction\nThe past few years have seen the power of scaling (Kaplan et al., 2020; Hoffmann et al., 2022): increasing\nthe number of parameters, amount of training data, or the computational budget has proven to be a reliable\n1\narXiv:2407.04153v1  [cs.LG]  4 Jul 2024\nway to improve model performance. Notably, feedforward (FFW) layers, responsible for storing factual\nknowledge(Gevaetal.,2021;Daietal.,2022), accountfortwo-thirdsofthetotalparametersinatransformer.\nHowever, one drawback of these dense FFWs is that their computational footprint (FLOPs and device\nmemory consumption) is linearly proportional to their parameter count.\nTo break the coupling between computational cost and parameter count, many recent works (Shazeer et al.,\n2017; Lepikhin et al., 2020; Fedus et al., 2022; Zhou et al., 2022) have adopted the Mixture-of-Experts (MoE)\narchitecture, which uses a set of sparsely activated expert modules (often FFWs) in place of a single dense\nFFW. Clark et al. (2022) studied the scaling law of MoE language models and showed that increasing the\nnumber of experts is an effective way to improve performance without increasing the inference cost. However,\ntheir experiments showed that the efficiency gains provided by MoEs plateau after a certain model size is\nreached. More recently, Krajewski et al. (2024) discovered that this plateau was caused by using a fixed\nnumber of training tokens. When the number of training tokens is compute-optimal, MoEs consistently\noutperform dense models in terms of FLOP efficiency. Moreover, they introduced granularity (the number\nof active experts) as a new scaling axis and empirically showed that using higher granularity improves\nperformance. Extrapolating this fine-grained MoE scaling law suggests that continued improvement of\nmodel capacity will ultimately lead to a large model with high granularity, corresponding to an architecture\nof an immense number of tiny experts.\nBeyond efficient scaling, another reason to have a vast number of experts is lifelong learning, where MoE\nhas emerged as a promising approach (Aljundi et al., 2017; Chen et al., 2023; Yu et al., 2024; Li et al.,\n2024). For instance, Chen et al. (2023) showed that, by simply adding new experts and regularizing them\nproperly, MoE models can adapt to continuous data streams. Freezing old experts and updating only new\nones prevents catastrophic forgetting and maintains plasticity by design. In lifelong learning settings, the\ndata stream can be indefinitely long or never-ending (Mitchell et al., 2018), necessitating an expanding pool\nof experts.\nAlthough both efficient scaling and lifelong learning require MoE designs capable of handling a vast number\nof experts, to the best of our knowledge, the only architecture supporting more than ten thousands of experts\nis the Mixture of Word Experts (MoWE) (dos Santos et al., 2023). However, MoWE is language-specific\nand uses a fixed routing scheme. Theoretical and empirical evidence (Clark et al., 2022; Dikkala et al., 2023)\nhighlights the advantages of learned routers over non-trainable ones. Thus, an MoE design with a learned\nrouter scalable to over a million experts remains an open area for exploration.\nThis work introduces the Parameter Efficient Expert Retrieval (PEER) architecture, leveraging product\nkey retrieval (Lample et al., 2019) for efficient routing to an extremely large number of experts, decoupling\ncomputational cost from parameter count. This design demonstrates a superior compute-performance trade-\noff in our experiments, positioning it as a competitive alternative to dense FFW layers for scaling foundation\nmodels. The main contributions of this work are:\n• Exploration of Extreme MoE Setting:Deviating from the focus on a small number of large experts\nin previous MoE research, this work investigates the under-explored case of numerous tiny experts.\n• LearnedIndexStructureforRouting: Demonstratingforthefirsttimethatalearnedindexstructure\n(Kraska et al., 2018) can efficiently route to over a million experts.\n• New Layer Design: Combining product key routing with single-neuron experts, we introduce the\nPEER layer that expands layer capacity without significant computational overheads. Empirical re-\nsults demonstrate its superior efficiency compared to dense FFW, coarse-grained MoEs and Product\nKey Memory (PKM) layers.\n• Comprehensive Ablation Studies: We investigate the impact of different design choices of PEER\nsuch as number of experts, active parameters, number of heads and query batch normalization on\nlanguage modeling tasks.\n2\nParameter Efficient Experts \nInput \nRetrieval\nQuery \nTransformer\nBackbone\n Top k Indices\nCompute \nSimilarity Product Keys\nMixture\nFigure 2: Illustration of the PEER layer. A PEER layer can be inserted in the middle of a transformer\nbackbone or can be used to replace FFW layers. Given the state vectorx from the previous layer, a query\nnetwork q maps it to a query vectorq(x), which is then compared with the product keys to compute the\nrouter scores and to retrieve the topk experts e1,...,e k. After the retrieved experts make their predictions\nei(x), their outputs are linearly combined using the softmax-normalized router scores as weights.\n2 Method\nIn this section, we introduce the Parameter Efficient Expert Retrieval (PEER) layer, which is a Mixture\nof Experts architecture using product keys (Lample et al., 2019) in the router and single-neuron MLPs as\nexperts. Fig. 2 illustrates the computational process within a PEER layer.\nPEER Overview Formally, a PEER layer is a functionf : Rn →Rm that consists of three parts: a pool of\nN experts E := {ei}N\ni=1, where each expertei : Rn →Rm shares the same signature asf, a corresponding\nset of N product keys K := {ki}N\ni=1 ⊂Rd, and a query networkq : Rn →Rd that maps the input vector\nx∈Rn to a query vectorq(x). LetTk denote the top-k operator. Given an inputx, we first retrieve a subset\nof k experts whose corresponding product keys have the highest inner products with the queryq(x).\nI = Tk\n(\n{q(x)Tki}N\ni=1\n)\n# Retrieve topk experts (1)\nThen we apply nonlinear activations (such as softmax or sigmoid) to the query-key inner products of these\ntop k experts to obtain the router scores.\ngi(x) =s(q(x)Tki) # Compute router scores (2)\nFinally, we compute the output by linearly combining the expert outputs weighted by the router scores.\nf(x) =\n∑\ni∈I\ngi(x)ei(x) # Aggregate expert outputs (3)\nProduct Key Retrieval Since we intend to use a very large number of experts (N ≥106), naively computing\nthe topk indices in Eq. 1 can be very expensive. Hence we apply the product key retrieval technique here.\nInstead of using N independent d-dimensional vectors as our keyski, we create them by concatenating\nvectors from two independent sets ofd\n2 -dimensional sub-keysC,C′ ⊂R\nd\n2 :\nK = {\n[c\nc′\n]\n|c∈C,c′ ∈C′} (4)\nNote that hereC,C′ have cardinality\n√\nN and c,c′ have dimensionality d\n2 . So in practice, we chooseN to\nbe a perfect square andd to be an even number.\n3\nThis Cartesian product structure ofK allows us to find the topk experts efficiently. Instead of comparing\nq(x) to all N keys inK and selecting the top k matches, we can split the query vectorq(x) into two sub-\nqueries q1 and q2 and apply the top k operations to the inner products between the sub-queries and sub-keys\nrespectively:\nIC = Tk\n(\n(qT\n1 ci)\n)\n, IC′ = Tk\n(\n(qT\n2 c′\nj)\n)\n(5)\nThis results in a set ofk2 candidate keysK′ := {\n[ci\ncj\n]\n|i∈IC,j ∈I′\nC}, and it is mathematically guaranteed\nthat thek most similar keys toq(x) from K are in this candidate set. Moreover, the inner product between\nthe candidate key and q(x) is simply the sum of inner products between the sub-keys and sub-queries:\nq(x)T\n[ci\ncj\n]\n= qT\n1 ci + qT\n2 cj. Hence we can apply the top-k operator again to thesek2 inner products to get\nthe top k matching keys from the original set of product keysK. As explained in Lample et al. (2019). This\nreduces the complexity of top k expert retrieval in Eq. 1 fromO(Nd) as done naively by exhaustive search\nto O((\n√\nN + k2)d).\nParameter Efficient Experts and Multi-Head Retrieval Unlike other MoE architectures, which often set\nthe hidden layer of each expert to the same size as other FFW layers, in PEER, every expertei is a singleton\nMLP, in other words, it has only one hidden layer with a single neuron:\nei(x) :=σ(uT\ni x)vi (6)\nwhere vi,ui are not matrices but vectors with the same dimension asx, and σ is a nonlinear activation\nfunction such as ReLU or GELU. We omit bias terms here for brevity.\nInstead of varying the size of individual experts, we adjust the expressiveness of a PEER layer by using multi-\nhead retrieval, similar to the multi-head attention mechanism in transformers and the multi-head memory\nin PKMs. In particular, we useh independent query networks instead of one, each computes its own query\nand retrieves a separate set ofk experts. However, different heads share the same pool of experts with the\nsame set of product keys. The outputs of theseh heads are simply summed up:\nf(x) :=\nh∑\ni=1\nfi(x) =\nh∑\ni=1\n∑\nj∈Ii\ngj(x)ej(x) (7)\nOne can verify that when only one expert is retrieved (k = 1) per head, using a PEER layer withh heads\nis the same as using one expert withh hidden neurons:\nf(x) =\nh∑\ni=1\nei(x) =\nh∑\ni=1\nσ(uT\ni x)vi = Vσ(WTx); (8)\nwhere W = [u1,··· ,uh],V = [v1,··· ,vh]. In other words, PEER dynamically assembles an MLP withh\nneurons by aggregatingh singleton MLPs retrieved from a shared repository. Compared to existing MoE\napproaches that use MLPs with multiple hidden neurons as experts, this design allows shared hidden neurons\namong experts, enhancing knowledge transfer and parameter efficiency.\nAlgorithm 1 shows a simplified implementation of the PEER forward pass, storing parameter-efficient expert\nweights in embedding layers and combining them with einsum operations. This implementation can be\neasily extended to experts of the GLU variants (Shazeer, 2020) by adding additional linear gating weights.\nIn practice, an efficient implementation may require specialized hardware kernels to accelerate embedding\nlookup and fusion with the einsum operations.\nWhy A Large Number of Small Experts?Given an MoE layer, we can characterize it by three hyperparam-\neters: the total number of parametersP, the number of active parameters per tokenPactive and the size of a\nsingle expertPexpert. Krajewski et al. (2024) showed that the scaling law of MoE models has the following\nform:\nL(P,D,G ) =c+ ( g\nGγ + a) 1\nPα + b\nDβ, (9)\n4\nwhere Lis the final test loss,a,b,g,γ,α,β are constants,D is the total number of training tokens and the\ngranularity G is the number of active experts:\nG:= Pactive\nPexpert\n(10)\nIn order to improve model performance, we need to scale upP,D,G . On the other hand, it is essential to\nlimit Pactive because the computational and memory costs are primarily determined by the active parameters\nduring training and inference. Notably, the memory footprint corresponding toPactive has to be multiplied\nby the number of tokens in a batch, while the memory cost ofP is independent of the batch size and sequence\nlength because only one copy of the model needs to be stored.\nAs a result, we want to increaseP,G but not Pactive. Since the expert size Pexpert = Pactive/G and the\nnumber of experts N = P/Pexpert = P ·G/Pactive, this implies that we should decrease the size of each\nexpert, Pexpert, and increase the number of expertsN. Hence we need a large number of small experts.\nIn general, for experts that are MLPs with a single hidden layer.Pexpert = (2dmodel + 1)dexpert and Pactive =\n(2dmodel +1)dactive, wheredmodel, dexpert and dactive are the hidden dimension of the transformer, the number\nofhiddenneuronsusedinoneexpertandthetotalnumberofhiddenneuronsactivatedpertoken, respectively.\nIn the case of PEER, we use the smallest expert size possible by settingdexpert = 1, and the number of\nactivated neurons is the number of retrieval heads multiplied by the number of experts retrieved per head:\ndactive = hk. Consequently, the granularity of PEER is alwaysG= Pactive/Pexpert = dactive/dexpert = hk.\n1 def peer_forward (self , x):\n2 # Embedding layers storing the down /up projection weights of all experts\n3 self . w_down_embed = nn. Embed ( num_embeddings = self . n_experts , features = self . d_model )\n4 self . w_up_embed = nn. Embed ( num_embeddings = self . n_experts , features = self . d_model )\n5\n6 # Retrieve the weights of the top matching experts using product keys\n7 # indices and scores have the shape ’bthk ’, where h is the number of heads\n8 indices , scores = self . get_indices ( self . query_proj (x), self . sub_keys , top_k = self .k)\n9 w_down = self . w_down_embed ( indices )\n10 w_up = self . w_up_embed ( indices )\n11\n12 # Compute weighted average of expert outputs\n13 x = jnp . einsum (’btd , bthkd -> bthk ’, x, w_down )\n14 x = self . activation (x)\n15 x = x * nn. softmax ( scores )\n16 x = jnp . einsum (’bthk , bthkd -> btd ’, x, w_up )\n17 return x\nAlgorithm 1: Pseudo code implementation of a PEER layer forward pass. An example implementation of\nthe get_indices and query_proj functions in Pytorch can be found in Lample et al. (2021)\n3 Experiments\n3.1 Pretraining isoFLOP Analysis\nWe compare PEER with various baselines using isoFLOP analysis (Borgeaud et al., 2022b). We chose a\nfixed FLOP budget (6e18 and 2e19) and jointly varied the model size and the number of training tokens\nfrom the C4 dataset (Raffel et al., 2020) to obtain isoFLOP curves. Each point on an isoFLOP curve has\nthe same computational cost, and we plot them in terms of their model size and final validation perplexity\non C4.\nFor the dense baselines, we varied their size by changing the number of layers, attention heads and model\ndimensions. For MoE, PKM and PEER methods, we took each of the dense models considered and replaced\nthe FFW layer in the middle block (e.g. in a 12 block transformer, we replace the FFN in block 6) by a\nlayer of MoE, PKM and PEER, respectively.\nIn MoE, we used the expert-choice (Zhou et al., 2022) routing algorithm, which effectively addresses the\nexpert load imbalance issue and generally outperforms token-choice MoEs (see Section 4 for a review and\n5\ncomparison of these approaches). Each expert has the same size as the original MLPs in the corresponding\ndense model, and we use128 experts to cover the same range of model sizes as our PEER models. This\ntype of MoE represents standard coarse-grained MoE approaches, which consist of a small number of large\nexperts.\nIn PKM, we used10242 memories withh= 8heads and topk = 32memories were selected per head. We\nalso applied query batch normalization, as recommended in the original PKM paper (Lample et al., 2019),\nto enhance memory usage.\nIn PEER, we used10242 experts with h = 8 heads and topk = 16 experts per head. By default, we also\nenabled query BatchNorm to increase expert usage. Ablation studies in subsection 3.3 investigate the effect\nof these hyperparameters. Unlike the expert-choice MoE baseline, PEER represents a fine-grained approach\nwhere a large number of small experts are employed.\nAcross all model sizes and methods, we maintained a consistent batch size (128) and sequence length (2048).\nWe calculated the number of training steps by dividing the total compute budget by the FLOPs per training\nstep. Fig. 1 presents the isoFLOP profiles. Compared to the dense FFW baseline, the sparse alternatives\nshift the isoFLOP curves downward and to right because they introduce a larger number of total parameters\nP but utilize a smaller or equal number of active parametersPactive. Given the same compute budget, a\nPEER model achieves the lowest compute-optimal perplexity.\n3.2 Evaluation on Language Modeling Datasets\nAfter determining the compute-optimal model for each method based on the isoFLOP curves, we evaluated\nthe performance of these pretrained models on several popular language modeling datasets, including Cura-\ntion Corpus (Curation, 2020), Lambada (Paperno et al., 2016), the Pile (Gao et al., 2020), Wikitext (Merity\net al., 2016) and the pretraining dataset C4. Table 1 presents a summary of the evaluation results. We\ngrouped the models based on their FLOP budgets used during training.\nTable 1: Perplexities of the compute-optimal models of each method on language modeling datasets.\nMethod Curation Lambada Pile Wikitext C4\nCorpus\nDense (6e18) 23.26 21.95 24.55 29.14 23.84\nMoE (6e18) 20.98 19.09 23.26 26.10 21.41\nPKM (6e18) 21.80 19.39 20.49 27.09 21.92\nPEER (6e18) 20.68 17.65 19.01 25.48 20.63\nDense (2e19) 17.70 12.28 18.19 21.21 18.31\nMoE (2e19) 16.88 12.97 17.41 20.28 17.12\nPKM (2e19) 17.03 11.18 16.34 20.26 17.36\nPEER (2e19) 16.34 10.33 14.99 19.09 16.45\n3.3 Ablations\nVarying the Number of Total Experts The models in the isoFLOP plot depicted in Fig. 1 all have over\na million (10242) experts. Here we conduct an ablation study on the effect of the number of expertsN,\nwhich determines the total parameter countP in Eq. 9. We selected the model at the isoFLOP-optimal\nposition and vary the number of experts (N = 1282,2562,5122,10242) in the PEER layer while keeping the\nnumber of active experts constant (h = 8,k = 16). The results are shown in Fig. 3 (a). As can be seen,\nthe isoFLOP curve interpolates between the PEER model with10242 experts and the corresponding dense\nbackbone without replacing the FFW layer in the middle block by a PEER layer. This demonstrates that\nsimply increasing the number experts can improve model performance.\nVarying the Number of Active ExpertsWe also conducted an ablation study on the effect of the number\nof active experts hk, which equals the granularityG in Eq. 9. We systematically varied the number of\n6\n(a) Varying Total Expert Num\n (b) Varying Active Expert Num\nFigure 3: We conduct two ablation studies using the same PEER model configuration. In (a), we vary the\ntotal number of expertsN while keeping the same number of active expertshk = 128. In (b), we vary the\nnumber of active expertsG= hk by jointly changingh and k while keeping the total number of experts at\nN = 10242.\nactive experts (hk = 32,64,128,256,512) while keeping the number of total experts constant (N = 10242).\nFurthermore, for a givenhk, we jointly variedh and k to identify the optimal composition. The resulting\nisoFLOP curves, plotted over the number of heads (h), are shown in Fig. 3 (b).\nThe results indicate that, within the range of values considered, higherhk generally leads to improved per-\nformance. Notably, the optimalh increases ashk increases. However, the performance gradually saturates,\nand increasing the number of active experts also increases device memory consumption and may necessitate\nadditional accelerator devices. Thus in practice, the appropriatehk values should be selected based on the\ntrade-off between performance, device number and computational resource requirements.\nTable 2:KL and expert usage for different memory sizes, with and without query BN.Similar to the findings\nin PKM, using query BN results in a more balanced usage of the experts.\nExpert numN 16k 65k 262k 1M\nBatchNorm No Yes No Yes No Yes No Yes\nPerplexity 23.47 23.47 22.61 22.55 21.54 21.47 20.73 20.64\nExpert Usage (%) 100.0 100.0 100.0 100.0 100.0 100.0 99.8 100.0\nUnevenness (↓) 0.45 0.30 0.63 0.44 0.97 0.66 1.52 1.06\nExpert Usage and Query Batch Normalization Given the presence of over a million experts in the PEER\nlayer, it is natural to inquire how many of these experts are actually selected during inference and whether\ntheir usage is evenly distributed. To analyze this, we kept an accumulated router score, denoted asz′\ni =∑\nxgi(x) for each expertei across all tokensx within the C4 validation set. Heregi(x) is the router score\nused to aggregate the expert output when tokenxis given as input, withgi(x) = 0if expertei is not selected.\nFrom these accumulated router scores, we can obtain an empirical probability distribution vector, denoted\nas z = z′/||z′||1, representing the distribution of all experts over the C4 validation set. Then we computed\nthe following metrics proposed by Lample et al. (2019) to assess the usage and distribution of experts:\n7\n• Expert Usage: the fraction of experts retrieved during inference:#{zi ̸= 0}\n• Unevenness: KL divergence betweenz and the uniform distribution:log(N) +∑\nizilog(zi)\nwhere N is the number of total experts.\nBy default, we also added a batch normalization (BN) layer on top of the query network, as proposed by\nLample et al. (2019) to increase the expert usage during training. Here we study the effect of adding this\nBN layer on the above-mentioned metrics.\nTable 2 presents the expert usage and unevenness for varying numbers of experts, with and without BN. We\ncan see that even for 1M experts, the expert usage is close to100%, and using BN can lead to more balanced\nutilization of the experts and lower perplexities. These findings demonstrate the effectiveness of the PEER\nmodel in utilizing a large number of experts.\nFigure 4: Query BatchNorm Ablation. IsoFLOP curves of a PEER model with 1M experts on the C4\ndataset, with and without query BatchNorm.\nWe additionally compared isoFLOP curves with and without BN. Fig. 4 shows that the PEER model with\nBN generally achieves lower perplexities. While the difference is not significant, it is most pronounced around\nthe isoFLOP-optimal region.\n4 Related Works\nMixture of Expert Since Shazeer et al. (2017) demonstrated the effectiveness of sparsely-gated Mixtures\nof Experts (MoEs) in efficiently increasing model capacity on GPU clusters, MoEs have emerged as a pop-\nular technique for scaling large models efficiently. Subsequent research (Fedus et al., 2022; Lepikhin et al.,\n2020; Du et al., 2022) has proposed variations to address challenges such as load balancing, communication\noverhead, and training instability. These methods usually replace feedforward (FFW) layers in certain Trans-\nformer blocks with sparsely-gated MoE layers, which consist of multiple FFW layers as experts. Typically\neach expert matches the size of the regular dense FFW layer. Gating scores are calculated for each expert\nand token, and only the top k experts are activated for each token. These methods are known as token-choice\nmethods. More recently, Zhou et al. (2022) introduced the Expert Choice routing method, where experts\nchoose the top k tokens instead of tokens selecting experts. However, both token-choice and expert-choice\nmethods require the top-k operator on a gating score matrix of sizeN ×M (N: number of experts, M:\nnumber of tokens), resulting in a routing cost of at leastO(N). This limits their practical application to a\nsmall number of experts (typically less than 128).\n8\nInstead of using the top-k operator, some works also proposed using deterministic hash tables as routers\n(Roller et al., 2021; dos Santos et al., 2023). WithO(1) average lookup complexity, these methods offer\npotential scalability to a large number of experts. However, these routers are fixed and not learned. Clark\net al. (2022) showed that deterministic routing does not scale as well as trainable routers. Furthermore,\nDikkala et al. (2023) proved theoretically that learned routers offer non-trivial advantages over their fixed\ncounterparts, such as removing spurious directions and identifying latent clusters in data. In contrast to\nprevious works, the proposed PEER layer employs a learned router with sublinear (O(\n√\nN)) complexity.\nSince PEER uses lightweight experts, our work is also related to recent studies on parameter-efficient MoEs\n(Wang et al., 2022; Zadouri et al., 2024). These methods utilize parameter efficient fine-tuning (PEFT)\nadapters as experts instead of full-sized FFWs. Their focus is on minimizing the number of parameters\nupdated during fine-tuning, allowing storage of only one copy of the large backbone model. In PEER,\nparameter efficiency refers to the small number of active parameters in the MoE layer, which directly affects\nFLOPs and activation memory consumption during pre-training and inference. However, PEER could\npotentially be adapted to retrieve a large number of PEFT adapters.\nRetrieval-Augmented Models Our proposed method, with its retrieval mechanism for a large number of\nexperts, aligns with the emerging field of retrieval-augmented models. These models facilitate large model\nmemorization by retrieving knowledge from external databases, leading to improved accuracy and efficiency\non knowledge-intensive tasks. Some notable works in this domain include ones by Khandelwal et al. (2019);\nBorgeaud et al. (2022a); Guu et al. (2020). While these methods retrieve data in various formats, for instance,\ntokens (Khandelwal et al., 2019), chunks (Borgeaud et al., 2022b) or knowledge graphs (Kang et al., 2023)\n(see (Gao et al., 2023) for a comprehensive survey on this topic), they differ from the proposed method in\nthat they retrieve data rather than learned functions (experts). This distinction sets our parameter-efficient\nexpert retrieval approach apart from existing retrieval-augmented models.\nEfficient Feedforward Layers Enhancing the efficiency of feedforward networks has been a long-standing\narea of research. Similar to PEER, most approaches are based on the idea of conditional computation\n(Bengio, 2013), where a gating mechanism is trained to determine which subset of neurons to compute. For\ninstance, Davis & Arel (2013) utilized low-rank weight matrix approximation to estimate the sign of pre-\nnonlinearity activations. Neurons with negative activations are omitted as they will produce zeros after the\nnonlinearity. Bengio et al. (2015) explored reinforcement learning to develop an activation-dependant policy\nfordroppingblocksofneurons. Morerecently, Belcak&Wattenhofer(2023)introducedtheFastFeedForward\n(FFF) layer that employs a differentiable balanced binary tree to select a neuron block for computation.\nDuring inference, only one leaf (corresponding to one block) is selected, hence it hasO(log(N)) complexity,\nwhere N is the total number of blocks in the tree. However, during training, all leaves and intermediate\nnodes are activated for gradient calculation, imposing a training complexity ofO(N) and limiting the total\nnumber of blocks. The most relevant work to ours is the Product Key Memory (PKM) (Lample et al., 2019),\nwhose retrieval technique is utilized as the router in the PEER layer. However, PKM retrieves memory\nvectors instead of functions, thus their values cannot vary according to the inputs. As we show in Section 3,\nby changing the memory vectors to input-dependent expert networks, PEER can achieve significantly higher\nefficiency than PKM. Finally, Csordás et al. (2023) presented a unified view encompassing FFW, MoE and\nPKM and proposed to change the router normalization function in MoE and PKM from softmax to sigmoid\nor ReLU.\n5 Conclusion\nThis work introduces a fine-grained MoE architecture that decomposes an extremely wide dense feedforward\nlayer into a large number of small experts. This design is supported by the recent discovery of the fine-\ngrained MoE scaling law. To overcome the computational overhead of routing to a large number of experts,\nwe apply the product keys to efficiently select a small subset of hidden neurons within a wide MLP layer.\nEmpirical analysis using language modeling tasks demonstrate that given the same compute budget, PEER\nsignificantly outperforms dense transformers, coarse-grained MoEs and product key memory layers.\n9\nAcknowledgments\nThe author would like to thank Adam Santoro, Arthur Guez, Arthur Szlam, Andrei Rusu, Marc’aurelio\nRanzato, Simon Schug, Utku Evci, Doina Precup and Razvan Pascanu for their insightful discussions and\ninvaluable advice. The author is also grateful to Zhitao Gong, Daniel Toyama, Qixuan Feng and Jiajun Shen\nfor their technical assistance. Special thanks are due to Adam Santoro for sharing the isoFLOP analysis\nscripts and to Andy Brock for building and maintaining the internal codebase used to train the models.\nReferences\nRahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network\nof experts. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 3366–\n3375, 2017.\nPeter Belcak and Roger Wattenhofer. Fast feedforward networks.arXiv preprint arXiv:2308.14711, 2023.\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural\nnetworks for faster models.arXiv preprint arXiv:1511.06297, 2015.\nYoshua Bengio. Deep learning of representations: Looking forward. InInternational conference on statistical\nlanguage and speech processing, pp. 1–37. Springer, 2013.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas,\nAurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones,\nAlbin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen\nSimonyan, Jack Rae, Erich Elsen, and Laurent Sifre. Improving language models by retrieving from\ntrillions of tokens. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu,\nand Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, vol-\nume 162 of Proceedings of Machine Learning Research, pp. 2206–2240. PMLR, 17–23 Jul 2022a. URL\nhttps://proceedings.mlr.press/v162/borgeaud22a.html.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving\nlanguage models by retrieving from trillions of tokens. InInternational conference on machine learning,\npp. 2206–2240. PMLR, 2022b.\nWuyang Chen, Yanqi Zhou, Nan Du, Yanping Huang, James Laudon, Zhifeng Chen, and Claire Cui. Life-\nlong language pretraining with distribution-specialized experts. InInternational Conference on Machine\nLearning, pp. 5383–5395. PMLR, 2023.\nAidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan\nDamoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language\nmodels. In International Conference on Machine Learning, pp. 4057–4086. PMLR, 2022.\nRóbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. Approximating two-layer feedforward networks for\nefficient transformers. In The 2023 Conference on Empirical Methods in Natural Language Processing,\n2023.\nCuration. Curation corpus base, 2020.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained\ntransformers. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 8493–8502, 2022.\nAndrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation in deep\nneural networks.arXiv preprint arXiv:1312.4461, 2013.\n10\nNishanth Dikkala, Nikhil Ghosh, Raghu Meka, Rina Panigrahy, Nikhil Vyas, and Xin Wang. On the benefits\nof learning to route in mixture-of-experts models. In The 2023 Conference on Empirical Methods in\nNatural Language Processing, 2023. URLhttps://openreview.net/forum?id=QV79qiKAjD.\nCicero Nogueira dos Santos, James Lee-Thorp, Isaac Noble, Chung-Ching Chang, and David Uthus. Memory\naugmented language models through mixture of word experts, 2023.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P Bosma, Zongwei Zhou,\nTao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju\nDuke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. GLaM: Efficient\nscaling of language models with mixture-of-experts. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song,\nCsaba Szepesvari, Gang Niu, and Sivan Sabato (eds.),Proceedings of the 39th International Conference\non Machine Learning, volume 162 ofProceedings of Machine Learning Research, pp. 5547–5569. PMLR,\n17–23 Jul 2022. URLhttps://proceedings.mlr.press/v162/du22c.html.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models\nwith simple and efficient sparsity.The Journal of Machine Learning Research, 23(1):5232–5270, 2022.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace\nHe, AnishThite, NoaNabeshima, ShawnPresser, andConnorLeahy. ThePile: An800gbdatasetofdiverse\ntext for language modeling.arXiv preprint arXiv:2101.00027, 2020.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint\narXiv:2312.10997, 2023.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-\nvalue memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.),\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 5484–5495,\nOnline and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.\ndoi: 10.18653/v1/2021.emnlp-main.446. URL https://aclanthology.org/2021.emnlp-main.446.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language\nmodel pre-training. InInternational conference on machine learning, pp. 3929–3938. PMLR, 2020.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models.arXiv preprint arXiv:2203.15556, 2022.\nMinki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. Knowledge graph-augmented language\nmodels for knowledge-grounded dialogue generation.arXiv preprint arXiv:2305.18846, 2023.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.arXiv preprint\narXiv:2001.08361, 2020.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through\nmemorization: Nearest neighbor language models. InInternational Conference on Learning Representa-\ntions, 2019.\nJakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pióro, Michał Krutul, Szymon Antoniak,\nKamil Ciebiera, Krystian Król, Tomasz Odrzygóźdź, Piotr Sankowski, et al. Scaling laws for fine-grained\nmixture of experts.arXiv preprint arXiv:2402.07871, 2024.\nTim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index\nstructures. In Proceedings of the 2018 international conference on management of data, pp. 489–504,\n2018.\n11\nGuillaumeLample, AlexandreSablayrolles, Marc’AurelioRanzato, LudovicDenoyer, andHervéJégou. Large\nmemory layers with product keys.Advances in Neural Information Processing Systems, 32, 2019.\nGuillaume Lample, Alexandre Sablayrolles, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. Min-\nimalist implementation of a product-key memory layer.https://github.com/facebookresearch/XLM/\nblob/main/PKM-layer.ipynb, 2021.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation\nand automatic sharding.arXiv preprint arXiv:2006.16668, 2020.\nHongbo Li, Sen Lin, Lingjie Duan, Yingbin Liang, and Ness B. Shroff. Theory on mixture-of-experts in\ncontinual learning, 2024. URLhttps://arxiv.org/abs/2406.16437.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models,\n2016.\nTomMitchell, WilliamCohen, EstevamHruschka, ParthaTalukdar, BishanYang, JustinBetteridge, Andrew\nCarlson, Bhavana Dalvi, Matt Gardner, Bryan Kisiel, et al. Never-ending learning.Communications of\nthe ACM, 61(5):103–115, 2018.\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, MarcoBaroni, GemmaBoleda, andRaquelFernández. TheLAMBADAdataset: Wordprediction\nrequiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 1525–1534, Berlin, Germany, August 2016.\nAssociation for Computational Linguistics. doi: 10.18653/v1/P16-1144. URLhttps://www.aclweb.org/\nanthology/P16-1144.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nJournal of machine learning research, 21(140):1–67, 2020.\nStephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models.Advances\nin Neural Information Processing Systems, 34:17555–17566, 2021.\nNoam Shazeer. Glu variants improve transformer, 2020. URLhttps://arxiv.org/abs/2002.05202.\nNoam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. InInternational\nConference on Learning Representations, 2017. URLhttps://openreview.net/forum?id=B1ckMDqlg.\nYaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadal-\nlah, and Jianfeng Gao. AdaMix: Mixture-of-adaptations for parameter-efficient model tuning. In Yoav\nGoldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pp. 5744–5760, Abu Dhabi, United Arab Emirates, De-\ncember 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.388. URL\nhttps://aclanthology.org/2022.emnlp-main.388.\nJiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang, Huchuan Lu, and You He. Boosting continual\nlearning of vision-language models via mixture-of-experts adapters. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 23219–23230, June 2024.\nTed Zadouri, Ahmet Üstün, Arash Ahmadian, Beyza Ermis, Acyr Locatelli, and Sara Hooker. Pushing\nmixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. InThe Twelfth\nInternational Conference on Learning Representations, 2024. URLhttps://openreview.net/forum?id=\nEvDeiLv7qc.\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le,\nJames Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information\nProcessing Systems, 35:7103–7114, 2022.\n12"
  },
  {
    "source": "Pathfinding (private).pdf",
    "content": "Problem  Statement  Mrs.  Qiu  is  planning  to  practice  orienteering.  The  area  where  she'll  \npractice\n \nis\n \na\n \nrectangular\n \nfield\n \ndivided\n \ninto\n \nunit\n \nsquares.\n \nYou\n \nare\n \ngiven\n \nits\n \ndescription\n \nas\n \na\n \nString[]\n \nfield.\n \nEach\n \ncharacter\n \nin\n \nfield\n \nis\n \n'.'\n \n(a\n \nperiod),\n \n'*'\n \n(an\n \nasterisk),\n \nor\n \n'#'\n \n(a\n \nnumber\n \nsign).\n \nEach\n \n'.'\n \nrepresents\n \na\n \npassable\n \nsquare\n \nwithout\n \na\n \ncheckpoint,\n \neach\n \n'*'\n \nrepresents\n \na\n \npassable\n \nsquare\n \nwith\n \na\n \ncheckpoint,\n \nand\n \neach\n \n'#'\n \nrepresents\n \nan\n \nimpassable\n \nobstacle.\n \nIt\n \nis\n \nguaranteed\n \nthat\n \nall\n \npassable\n \nsquares\n \n(i.e.,\n \nall\n \n'.'s\n \nand\n \n'*'s)\n \nform\n \na\n \n4-connected\n \ntree\n \n(see\n \nnotes\n \nfor\n \nformal\n \ndefinition).\n \nThe\n \nnumber\n \nof\n \ncheckpoints\n \nis\n \nat\n \nmost\n \n300.\n \n  In  order  to  practice,  Mrs.  Qiu  chooses  K  of  the  checkpoints  uniformly  \nat\n \nrandom.\n \nAfterwards,\n \nshe\n \nwill\n \nfind\n \nthe\n \nshortest\n \nsequence\n \nof\n \nsquares\n \nthat\n \npasses\n \nthrough\n \nall\n \nchosen\n \ncheckpoints.\n \nThe\n \nsequence\n \ncan\n \nstart\n \nat\n \nany\n \nsquare,\n \nend\n \nat\n \nany\n \nsquare\n \n(possibly\n \nother\n \nthan\n \nthe\n \nstarting\n \none),\n \nand\n \nvisit\n \neach\n \nsquare\n \nany\n \nnumber\n \nof\n \ntimes.\n \nEach\n \npair\n \nof\n \nconsecutive\n \nsquares\n \nin\n \nthe\n \nsequence\n \nmust\n \nhave\n \na\n \ncommon\n \nside.\n \nThe\n \nlength\n \nof\n \nthe\n \nsequence\n \nis\n \nthe\n \nnumber\n \nof\n \nmoves\n \nMrs.\n \nQiu\n \nwill\n \nhave\n \nto\n \nmake.\n \n(So,\n \nfor\n \nexample,\n \na\n \nsequence\n \nthat\n \nconsists\n \nof\n \n7\n \nsquares\n \nhas\n \nlength\n \n6.)\n \n  You  are  given  the  String[]  field  and  the  int  K.  Return  the  expected  \nlength\n \nof\n \nMrs.\n \nQiu’s\n \nsequence.\n  \nDefinition  Method  signature:  def  expected_length(field:  List[str],  K:  int)  ->  \nfloat\n  \nNotes  -  A  set  S  of  squares  is  said  to  form  a  4-connected  tree  if  for  any  \ntwo\n \nsquares\n \nA\n \nand\n \nB\n \nfrom\n \nS,\n \nthere\n \nexists\n \nexactly\n \none\n \nway\n \nto\n \nwalk\n \nfrom\n \nA\n \nto\n \nB\n \nwhile\n \nvisiting\n \nonly\n \nthe\n \nsquares\n \nfrom\n \nS\n \nand\n \nnot\n \nvisiting\n \nthe\n \nsame\n \nsquare\n \nmore\n \nthan\n \nonce.\n \nFrom\n \na\n \ngiven\n \nsquare,\n \nit\n \nis\n \npossible\n \nto\n \nwalk\n \ninto\n \nany\n \nsquare\n \nthat\n \nshares\n \na\n \ncommon\n \nside\n \nwith\n \nit.\n \nConstraints  -  field  will  contain  between  1  and  50  elements,  inclusive.  \n-  Each  element  of  field  will  contain  between  1  and  50  characters,  \ninclusive.\n -  Each  element  of  field  will  contain  the  same  number  of  characters.  -  Each  character  in  field  will  be  '*',  '.',  or  '#'.  -  '*'  and  '.'  form  a  4-connected  tree.  -  K  will  be  between  2  and  300,  inclusive.  -  field  will  contain  between  K  and  300  '*',  inclusive.   \nExamples   Example  #1:  field  =  [   \"*#..#\",   \".#*#.\",   \"*...*\"]  K  =  2  Returns:  3.8333333333333353   Explanation:  Let  (i,j)  be  the  square  represented  by  the  j-th  character  of  the  i-th  \nelement\n \nof\n \nfield\n \n(both\n \nnumbers\n \nare\n \n0-based).\n \n  If  she  chooses  (0,0)  and  (1,2),  one  of  the  optimal  sequences  is  (0,0)  \n->\n \n(1,0)\n \n->\n \n(2,0)\n \n->\n \n(2,1)\n \n->\n \n(2,2)\n \n->\n \n(1,2).\n If  she  chooses  (0,0)  and  (2,0),  one  of  the  optimal  sequences  is  (0,0)  \n->\n \n(1,0)\n \n->\n \n(2,0).\n If  she  chooses  (0,0)  and  (2,4),  one  of  the  optimal  sequences  is  (0,0)  \n->\n \n(1,0)\n \n->\n \n(2,0)\n \n->\n \n(2,1)\n \n->\n \n(2,2)\n \n->\n \n(2,3)\n \n->\n \n(2,4).\n If  she  chooses  (1,2)  and  (2,0),  one  of  the  optimal  sequences  is  (1,2)  \n->\n \n(2,2)\n \n->\n \n(2,1)\n \n->\n \n(2,0).\n If  she  chooses  (1,2)  and  (2,4),  one  of  the  optimal  sequences  is  (1,2)  \n->\n \n(2,2)\n \n->\n \n(2,3)\n \n->\n \n(2,4).\n If  she  chooses  (2,0)  and  (2,4),  one  of  the  optimal  sequences  is  (2,0)  \n->\n \n(2,1)\n \n->\n \n(2,2)\n \n->\n \n(2,3)\n \n->\n \n(2,4).\n If  she  chooses  (0,0),  (1,2),  and  (2,4)  So  the  expected  length  of  her  sequences  is:    (5  +  2  +  6  +  3  +  3  +  4)  /  6  =  23  /  6  =  3.8333333333333353    Example  #2:  field  =  [  \n \"*#..#\",   \".#*#.\",   \"*...*\"]  K  =  4  Returns:  8.0   Explanation:  Mrs.  Qiu  chooses  all  four  checkpoints.  One  of  the  shortest  sequences  \nis\n \n(0,0)\n \n->\n \n(1,0)\n \n->\n \n(2,0)\n \n->\n \n(2,1)\n \n->\n \n(2,2)\n \n->\n \n(1,2)\n \n->\n \n(2,2)\n \n->\n \n(2,3)\n \n->\n \n(2,4).\n   Example  #3:  field  =  [   \"#.#**\",   \"....#\",   \"#*#**\",   \"**#*#\",   \"#..##\",   \"*#..#\",   \".#.#.\",   \"....*\"]  K  =  3  Returns:  10.825000000000024    Example  #4:  field  =  [        \"###################\",   \"#*###############*#\",   \"#.....#######.....#\",   \"#*###*.#.*.#.*###*#\",   \"#*####*.*#*.*####*#\",   \"#*#####*###*#####*#\",   \"###################\"]  K  =  9  Returns:  30.272233648704244    Example  #5:  field  =  [        \"**##*.**#..#.*...*#...*#..#.##..#..#.#*...#.##*##.\",   \".#..###..#..#.#.##..#.#.*#.*..#..#.#*..##.#*...*..\",  \n \"..#.....###.#*.##..#.#.#*..#.#..#....#..#...#*####\",   \".#.##*#.*#..#*#*.#.#...*.#.*#.#.##.#*.##.#.#..*...\",   \"..*.*#*.###.#..#.#..##.##.*#..#.....#.....#..#.#.#\",   \".#.##.#..##..*#..#.#...#*##*#*..#.#.#.#.##.##.#.#*\",   \"..##....#..#.#*#...*.##...#.#.####...#.#*.....#...\",   \".#.*#.##.*#*.#*.#.#.#..#.#..#.#*#.###..##.##.#.##*\",   \".*.#*..*.#.#...#.*##.#.**.#.*...**..*#..#.#.#*.#..\",   \".#*.#*##....##.#.#*..*.###.#.##.##.#.#.#....#.#*.#\",   \"*.#..#*#.#*#*....#.#.#..*#**...##.#.#.**#*##.*.#..\",   \".#*.##..##..##.#.#..#.#.###.###...#...#*#..##*#.#.\",   \"#..#*.#..*.###..#.#...#.###.#.#*#.#.#**##.#...*.#*\",   \"..#..#.#.##.#..#.**.##*#.#**.**..#.#..#...#.##*#..\",   \".#*#.#.*..#.*#...#.#...#...#.##.#..*#*.##*....###.\",   \".*.#.#.#.#*#..*##.**.##*##..#.*#.#*###..*.#.##.#..\",   \".#......#...#.#.*#.#.#..#..#.#*#....#*.#*#.*#..*.#\",   \"#..####..#*#...#*.#..#.###...#.#.#.###*#..##*##.#.\",   \".#.*..#.#...#.#..#.##...#..#.#.#.#.###..##..*.*.*.\",   \".#.#.#.#..##.*..#.*.#.##.#..##*...#.#..#.#.##.#.##\",   \".#..#*.#.#..#.##..##..#.*..#.*#.#...##....#...###.\",   \".#.#.#.#*.#.#..#.#..#..#.#.*#...#.##...#.##.##.*..\",   \".#...#.#.##.#.#..*#.*#..###..#.#.#*###.##...#*.##.\",   \".#.##.*.......*.#.*#.#.#*###..*...*..#.*.##.#.#..#\",   \"...###*####*#.#..##*...#..#..##.#.#.#..##*#*.*.*#.\",   \"#.#.#....*#..#.#.#.#.##..#*.#...#..#.#*#...#.##.*.\",   \"..*.#*##.#.#*#.###...#..##.#.#.#*###*#.*#.#.*###.#\",   \"##*##..##...#.....##.#.#.**#..#*.....##.#..#*.#.*.\",   \".....#.*.##..##.##*.*#...#.#.#.##.#*#.**..#..#.#.#\",   \"##.#.#*##.#.#.*.*.#.#*#.#.#....*...#*##*##.#....#.\",   \"*.**#**....*..##.#*.*.**..##.###.##.....##...##.**\",   \"#.####.##*#*##..#.*#*#.##*...#.##..#.##....#*..##.\",   \"....#...##.#...#*.#..##.##.#*..*.#....##.#.*##...#\",   \"#.#..*##*..#.#..#..#..#*....#.##..##.#*##.##.*##..\",   \"..#.#*.*.##.#.#*#.#*##.###.##...#............#*.#.\",   \"#.#.##.#....*....*..##..*#.#.#.###.#.#.#.###..#..#\",   \".#**..#*#.#*#*#.#.#...*##....##.#*..#..#*..*#..#..\",   \"...#*#.....#..#.#..#*#.*##.#..#.#.##..#.*#*#.#...#\",   \".#*.###.#.#.#.#.*#*##.##..#.#*..#...#.#.#..#*.*#..\",   \"#*.#.#.#..#..#..#....*#.*##..##.#.#..#...##.#.#..#\",   \"*.#..#..#...#..##.#*#..#.#*#.#.#.###..#.#*...#.#..\",   \"#...#.#...#.#.#..#.*.#*.....**.*..#*##.#*.##....##\",   \"#*#....#*#..#.*.###*#..#*##.##.#.#...#.*.##.##.##.\",   \"..##*##*..#*#.#..#*.*##*.##.#...#.#.#.#.#..*#.##..\",   \"#...#*##.#*#**.##.*#.*.##..*.#*#**....#**##...*.*#\",  \n \"*#.##......*#.##.#.#.##**.#.#.#.#.#.##..#...#*#*#*\",   \"*....##.#.#..#.....#..##.#....*....#.#.##.#.#.##**\",   \"#.##*#...#..#.#.##..#..##.##.##.##........##.#*#.#\",   \"..#...#.#*#*..*#..*#.*#.#......##.#.#.#*#..#..****\",   \".###.#..#...#.#..#..#.#...#.#.#...**.#..*#*.*##*#.\"]  K  =  150  Returns:  1309.4951033725558       "
  },
  {
    "source": "Purdue Word thesis template_v. 01-16-2024.docx",
    "content": "by\nA \n, Indiana\nAdd or Delete Department\nAdd or Delete Department\n \nThis page is OPTIONAL and is used for personal dedications. This page is formatted using italicized font, 1.5 line spacing, and centered horizontal and vertical alignment. If you do not intend to personally dedicate this document to anyone, delete this page.\nACKNOWLEDGMENTS\nThis page is OPTIONAL. The acknowledgements section is typically reserved for professional dedications. You may also include funding information or any other copyright acknowledgments. If you do not intend to professionally dedicate this document to anyone, delete this page. \nTABLE OF CONTENTS\nLIST OF TABLES\t7\nLIST OF FIGURES\t8\nEXTRA HEADINGS\t9\nABSTRACT\t10\n1. THESIS FORMATTING GUIDE\t11\n1.1\tThesis/dissertation styles\t11\n1.2\tControlled vs. Non-controlled thesis or dissertation\t11\n1.3\tDraft review information\t12\n1.4\tMargins\t12\n1.5\tTable of Contents\t13\n1.6\tPage Breaks and Section Breaks\t13\n1.7\tPage numbering\t14\n1.8\tFont style, size, and spacing\t15\n1.9\tLine spacing\t15\n1.10\tParagraph indentations\t15\n1.11\tParagraph alignment\t15\n1.12\tHeadings and Subheadings\t15\n1.12.1\tHeading styles\t16\n1.12.2\tSubheading styles\t16\n1.13\tFigures, Tables, and Captions\t17\n1.13.1\tExamples of formatted Figures, Tables, and Captions\t18\n1.14\tCross-Referencing Tables and Figures\t20\n1.15\tEquations\t20\n1.16\tBlock quotes\t20\n1.17\tFootnotes\t21\n1.18\tCitation publication references\t21\n1.18.1\tPermission to reuse published content\t22\n1.19\tAppendix\t23\n1.20\tReferences\t23\n1.21\tDocument Accessibility Statement\t24\n2. TYPE YOUR CHAPTER TITLE DESCRIPTION\t25\n2.1\tFirst level subheading\t25\n2.1.1\tSecond level subheading\t25\nThird level subheading\t25\nFourth level subheading\t25\n3. TYPE YOUR CHAPTER TITLE DESCRIPTION\t26\n3.1\tFirst level subheading\t26\n3.1.1\tSecond level subheading\t26\nThird level subheading\t26\nFourth level subheading\t26\nAPPENDIX A. TYPE YOUR APPENDIX TITLE DESCRIPTION\t27\nAPPENDIX B. TYPE YOUR APPENDIX TITLE DESCRIPTION\t28\nREFERENCES (OPTION 1 – NUMERICAL)\t29\nREFERENCES (OPTION 2 - ALPHABETICAL)\t30\nPUBLICATIONS\t31\nVITA\t32\nThis existing Table of Contents is already formatted for your use. Do not delete or modify it. After you have added your headings and subheadings throughout your document, right-click on the Table of Contents and select “update field,” then click “update entire field” and the entire table will update to reflect all your major headings, chapter title headings, subheadings, and page numbers. All major headings and chapter title headings should appear in all caps. Subheadings should appear in sentence case.\nLIST OF TABLES\nTable 1.1. This is an example of a Purdue formatted table caption. Use 24-point Before and 6-point After spacing for the caption.\t19\nTable 1.2. This is an example of a Purdue formatted table caption with a separate full caption description underneath.\t19\nThis existing List of Tables is already formatted for your use. Do not delete it. After you have properly captioned all your tables in the document, right-click on the list above and select “update field”, then click “update entire field” and the entire list will update to reflect all your table captions. If your document does not include any tables, delete this page.\nLIST OF FIGURES\nFigure 1.1. This is an example of a Purdue formatted figure caption. Use 6-point Before and 24-point After spacing for the caption.\t18\nFigure 1.2. This is an example of a Purdue formatted figure caption with a separate full caption description underneath.\t18\nThis existing List of Figures is already formatted for your use. Do not delete it. After you have properly captioned all your tables in the document, right-click on the list above and select “update field”, then click “update entire field” and the entire list will update to reflect all your table captions. If your document does not include any tables, delete this page.\nEXTRA HEADINGS\nThis page may be used for a Glossary, List of Abbreviations, List of Symbols, or Nomenclature. Rename the heading above for what is being included on this page, then replace this paragraph text with your contents. Contents should be spaced consistently and should be sorted in alphabetical order.\nDo not use this page to include more than one type of data (Glossary, List of Abbreviations, List of Symbols, or Nomenclature). Instead, create a new page for each type and use the MAJOR HEADINGS heading style.\nIf you do not have a Glossary, List of Abbreviations, List of Symbols, or Nomenclature section, delete this page.\nABSTRACT\nAn abstract is a concise summary of your thesis, which is REQUIRED to be included in this document. A typical thesis abstract should be a single un-indented paragraph. Although Purdue does not place a limit on the length of an abstract, try to limit it to 250 words. Please summarize your research and be efficient in your writing when creating your abstract.\nTHESIS FORMATTING GUIDE\nStudents must use this template to write their thesis or dissertation. This template provides formatting information and guidance. We suggest that you keep the Show/Hide tool (¶) found on the Home ribbon turned on so that you can see formatting within your document (spacing, page breaks, section breaks, etc.). If you have questions regarding the template or formatting, feel free to contact us for help at thesishelp@purdue.edu.\nThesis/dissertation styles\nThree types of thesis/dissertation styles are accepted by the Graduate School:\nTraditional.\nArticle-Based\nCreative Work\nFor information on thesis styles and requirements, please see VII. Administering Graduate Degree Programs, C. Thesis and Dissertation Policies, 6. Thesis and Dissertation Structures and Formatting in the Purdue University Catalog (https://catalog.purdue.edu/content.php?catoid=16andnavoid=19690#theses). Thesis structures may vary by department. Please consult with your committee for specific departmental thesis formatting requirements.\nControlled vs. Non-controlled thesis or dissertation\nTheses/dissertations containing content and data subject to Export Administration Regulations (EAR), International Traffic in Arms Regulations (ITAR), Nuclear Regulatory Commission (NRC), Department of Energy (DOE), and/or other controlled unclassified information (CUI) must meet specific federal security requirements and cannot be handled or stored in standard university systems. Theses with these security designations and requirements must be specially handled and cannot be submitted to the Hammer Research Repository, cannot be submitted through authenticate, and cannot be shared electronically through fax, email, links, etc. We cannot determine whether you have controlled data in your document. It is up to you, your sponsors, sources, and/or committee to determine whether you have controlled content in your document.\nIf your thesis or dissertation does not contain any of the information mentioned above, then it is non-controlled.\nDraft review information\nAll students must have their completed thesis or dissertation draft reviewed as early in the semester as possible. We will cease formatting reviews and consultations 2 weeks prior to the deposit deadline. The formatting review method is based on whether the thesis or dissertation is controlled or non-controlled. Please see below for :\nNon-controlled thesis/dissertation - When you have a completed draft, email to us at thesishelp@purdue.edu. If you used the Purdue Microsoft Word template to create your draft, please send your thesis/dissertation to us as a Word document. If you used the Purdue Latex/Overleaf template to create your draft, please send your thesis/dissertation as a PDF document. We will review it as quickly as possible and provide you with any formatting feedback.\nControlled thesis/dissertation - Do NOT email your draft or share it electronically. Instead, an in-person formatting review is required to ensure that your document remains confidential and secure (this means that you would need to bring your laptop with your document on it to the format review appointment). Follow the process steps and instructions for controlled thesis/dissertation reviews and submissions in this web link: https://www.purdue.edu/research/oevprp/regulatory-affairs/export-controls/guidance-documents/graduate-theses.php.\nMargins\nThe Title Cover Page and the Statement of Committee Approval page both require a 1.5” top margin and 1.0” for the left, right, and bottom page margins. All other pages require 1.0” margins on all sides.\nTable of Contents\nIf you are working within this template to create your thesis/dissertation, you should not delete and create a new Table of Contents. Instead, keep the existing Table of Contents and just use the “Update Field” function to update the existing Table of Contents to reflect your headings/subheadings. \nTo update an existing Table of Contents, right-click anywhere in the body of the existing Table of Contents to open a formatting options box. Click on “Update field” in the formatting options box, then click on “Update entire table”, and then click “OK”. The Table of Contents should update and populate all your headings, subheadings, and page numbers.\nPage Breaks and Section Breaks\nUse “page breaks” (instead of using paragraph spaces) at the end of a page when you need to start a new chapter or major section on the next page.\nUse a “section break (next page)” if you need to put tables and figures on pages by themselves so that you can apply center vertical alignment to center them on the page (both at the end of the prior page and at the end of the page to be centered).\nDO NOT USE “section break (continuous)” as it can cause formatting issues.\nPage numbering\nPage numbering is set up and formatted in this template. Page numbering begins on page 2 (the Statement of Committee Approval page).\nFor portrait orientation pages, page numbers are located in the center of the page footer using Times New Roman 12-point font. Page numbering is already set for portrait orientation pages, so please do not change the page number formatting on those pages. \nFor landscape orientation pages, the page number will need to be moved so that it is centered in the left page margin using the following steps:\nOn the landscape layout page, place the cursor in the page header, then click the “link to previous” button in the Navigation section of the Header and Footer tool ribbon. Then put the cursor in the footer and click the “link to previous button”. Doing this should unlink the header and footer for that page.\nRepeat the first step above for the first portrait orientation page located immediately after the landscape orientation page.\nGo back to the footer of the landscape page, place the cursor in the footer, click the Page Number drop-down in the Header and Footer tool ribbon, select Page Margins from the drop-down, and then select “Border Left”. You may have to scroll down to find “Border Left”. This will insert the page number in the left page margin of the landscape page.\nWhile still in the footer, click on the page number that is now located in the left margin, then go to the Home tool ribbon and click center alignment.\nKeep the cursor on the page number in the left margin, then go to the Home tool ribbon, click on Shape Format tool ribbon, click the Text Direction drop-down and select “Rotate all text 90o”.\nClick on the page number in the footer of the landscape page and then delete the page number from the page footer.\nFont style, size, and spacing\nThe font style and size for all paragraphs throughout your document should be Times New Roman with 12-point font size. See headings, subheadings, and captions sections for additional font and formatting requirements.\nLine spacing\nThe Title Cover Page, Statement of Committee Approval, Table of Contents, and Dedications pages are already set using 1.5 line spacing and should not be changed. \nAll other paragraph line spacing in this document can either be 1.5 line spacing or 2.0 (double) line spacing.\nParagraph indentations\nPlease use only ONE of the following paragraph indentations and spacing options:\nIf you are using indented paragraphs, the first line of each paragraph must be indented to 0.5” and there should not be any paragraph spaces between each new paragraph. Ensure that your indentions are consistent and do not use varying indentation levels.\nIf you choose not to indent your paragraphs, then there must be only one paragraph space between each new paragraph.\nParagraph alignment\nPurdue uses of justified paragraph alignment as it gives the document a clean, streamlined appearance. If your department specifically requires you to use left paragraph alignment, then use that alignment.\nHeadings and Subheadings\nHeading and subheading styles are already formatted, set up, and included in the Styles section of the Home tool ribbon. You should use them throughout your document so that they will populate accurately in the Table of Contents. You must type all headings in ALL CAPS. Subheadings must be typed in sentence case. All styles have built in spacing, so please do not add paragraph spaces above or below headings and subheadings. \nHeading styles\nNormal headings –The ACKNOWLEDGMENTS and TABLE OF CONTENTS headings are considered normal headings because they should not be included in the Table of Contents. These headings use the following formatting: single line spacing, 14-point font size, bold font, center alignment, 0-point Before spacing, and 36-point After spacing. Do not use any indentations. These headings should not be numbered.\nMajor headings – Major headings include, but are not limited to, the ABSTRACT, LIST OF TABLES, LIST OF FIGURES, LIST OF ABBREVIATIONS, NOMENCLATURE, GLOSSARY, REFERENCES, APPENDIX, VITA, and PUBLICATIONS headings. Please use the “Major Headings” style from the Styles section of the Home tool ribbon for these headings. The formatting for this heading style is already set up using single line spacing, 14-point font size, bold font, center alignment, 0-point Before spacing, and 36-point After spacing, and 0” indentations. These headings should not be numbered. The appendix heading should include an appendix letter.\nChapter title headings – All chapter title headings should use the “1. Heading 1, Chapter heading” style. The formatting for this heading style is already set up using single line spacing, 14-point font size, bold font, center alignment, 0-point Before spacing, and 36-point After spacing, and 0” indentations. These headings must be numbered.\nSubheading styles\nThis template includes built-in subheading styles. Subheadings must be typed in sentence case, and they should not be numbered. Formatting for all subheadings must be single line spacing, Times New Roman font, and 12-point font size with 24-point Before spacing and 12-point After spacing. Ensure that “Widow/Orphan Control”, “Keep with next”, and “Keep lines together” in paragraph settings (found in the Layout tab, Line and Page Break settings) are applied to all subheading styles. All four subheading styles are shown and described in the next chapter of this template.\nFigures, Tables, and Captions\nUse the “Insert Caption” function in the References tool ribbon to properly insert captions. Do not insert captions into text boxes or within tables. Caption descriptions should be brief.\nCaption formatting is already set using Times New Roman 12-point font size, single line spacing, and caption numbering. See the APPENDIX section for guidance on caption numbering for tables and figures located in an appendix.\nUse center alignment for captions consisting of only one line. Use justified alignment for captions consisting of more than one line.\nFont size for text inside of tables and figures and captions cannot be less than 10-point and not more than 12-point.\nNotes underneath tables should use 10-point font size, italicized font, single line spacing, left alignment, and 0-point Before and 24-point After spacing.\nFigures and tables should be inserted either before or after a paragraph or list (not within a paragraph or list).\nUse center alignment for figure pictures and tables. \nUse single line spacing for figure pictures, table contents, and figure and table notes.\nEnsure that there is 24-point After spacing applied after tables.\nEnsure that the “Wrap Text” setting for figure pictures, shapes, and graphics are set to “In line with text” in the Picture Format tools.\nEnsure that the “Wrap Text” for tables is set to “None” in Table Properties in Table Layout tools.\nFigures and tables (with captions) that are too wide or too lengthy to fit entirely on the same page can be placed on a page by themselves. Tables and figures located on a page by themselves (with no headings/subheadings and paragraph text) must be centered both horizontally and vertically on the page. To accommodate length and/or width, you may also change the page orientation to landscape orientation and/or increase the page size to either 8.5” x 14” or 11” x 17”.\nExamples of formatted Figures, Tables, and Captions\nNOTE: Notes should be located underneath the figure using a 10-point font size, italicized font, single line spacing, justified alignment, and apply 0-point Before and 12-point After spacing.\nFigure 1.1. This is an example of a Purdue formatted figure caption. Use 6-point Before and 24-point After spacing for the caption.\nFigure 1.2. This is an example of a Purdue formatted figure caption with a separate full caption description underneath.\nThe caption is split with the shortened caption description listed above, and the full caption description separately listed below the caption description. Spacing for the caption number and shortened caption description should be 6-point Before and 12-point After. Spacing for the separate full caption description should be 12-point Before and 24-point After.\nTable 1.1. This is an example of a Purdue formatted table caption. Use 24-point Before and 6-point After spacing for the caption.\nNOTE: Notes should be located underneath the table using a 10-point font size, italicized font, single line spacing, justified alignment, and apply 0-point Before and 24-point After spacing.\nTable 1.2. This is an example of a Purdue formatted table caption with a separate full caption description underneath.\nThe caption is split with the shortened caption description listed above, and the full caption description separately listed below the caption description. Spacing for the caption number and shortened caption description should be 24-point Before and 12-point After. Spacing for the separate full caption description should be 12-point Before and 6-point After. \nCross-Referencing Tables and Figures\nWhen using tables and figures, you should always cross-reference them within your paragraph text. To insert a caption cross-reference, put your cursor in the text where you want to insert a table or figure cross-reference, click on the References tool ribbon, click on “Cross-Reference” (located in the Captions section), and a box will open for you to select your cross-reference options. Select the correct “Reference Type” (figure or table), select “Insert Reference To” as “only label and number”, and then select the correct table or figure number from “For which caption”, click the “Insert” button, and then click the “Cancel” button when done.\nEquations\nCambria Math 12-point font size and double line spacing should be used for equations. Equations should be numbered so that readers may refer to them in your document. Use right paragraph alignment to keep the equation number along the right margin and then tab until the equation is centered horizontally. If you need to use additional spacing above or below the equation, do not use more than 6-point Before and After spacing for equations. The first digit of the equation number represents the chapter number, and the second digit represents the equation number in the chapter. See the example below:\n\t\t\t(1.1)\nBlock quotes\nBlock quotes are long quotes consisting of either 40 words or more, or 3 lines of text or more. Please see the block quote example below:\nThis is an example of a formatted block quote. Both the line spacing and the paragraph alignment should match what was used for other paragraphs in your document (either 1.5 or 2.0 line spacing, and either justified or left alignment). Spacing above and below the block quote is not required, but if you choose to add spacing, please do not use more than 6-12 point Before and 6-12 point After spacing. Use 0.5” left alignment. Do not use quotation marks for your block quote. The parenthetical citation should come after the closing punctuation mark.\nFootnotes\nFootnotes may be used for brief, concise explanatory and/or bibliographic notes. Do not use footnotes for extensive or digressive notes, block quotes, or lengthy explanations or clarifications. Do not use footnotes in headings or subheadings. To insert a footnote, place the cursor in the paragraph sentence where you wish to use insert the footnote reference. Then, go to the Footnotes section of the References tool ribbon and click the corner arrow to open the footnote formatting options box. Footnotes must be numbered (do not use Roman numerals, symbols, etc.) in a consecutive and continuous manner throughout your document. Footnote numbering should not start over at “1” in each chapter. Footnotes should be formatted using Times New Roman 10-point font, 0-point Before & After spacing, justified or left alignment, and paragraph settings of “widow/orphan control” and “keep lines together”. Footnotes begin and end on the same page and should never carry over from one page to the next.\nCitation publication references\nSee below for clarification on what you need to include when reusing your published content (as an author or co-author) in your document:\nIf your chapter has been submitted to a publisher for review, but it is not published yet, then you will need to include a publication citation reference (using single line spacing) underneath the chapter title heading.\nWhen you are reusing only a few items (such as a table, figure, and/or paragraph) from a published journal article, a publication citation reference underneath the chapter title heading is not required. Instead, you should include a short cross-reference in the caption(s) and/or paragraph(s), and you will need also need to include the published article as a full reference in the references section of your document.\nIf part or most of your chapter consists of published information, then you must include a publication citation reference (using single line spacing and 24-point After spacing) underneath the chapter title heading, short cross-references in the areas where publication information was used, and you will need also need to include the published article as a full reference in the references section of your document. A publication citation reference example can be found underneath the second chapter title heading in this template.\nIf all of your chapter consists of published information, then you must include a publication citation reference (using single line spacing) underneath the chapter title heading, short cross-references in the areas where publication information was used, and you will need also need to include the published article as a full reference in the references section of your document. A publication citation reference example can be found underneath the third chapter title heading in this template.\nChapter 2 includes an example of a publication citation reference for a pending publication. Chapter 3 includes an example of a publication citation reference for a published publication.\nPermission to reuse published content\nWhen you are using content in your document that you have authored or co-authored (whether submitted for publication or already published), you need to make sure that you have documented the following below:\nPublished article – You must obtain written permission from the publisher to reuse your published article content and that you retain the written permission for your records.\nSubmitted article (in review / pending) - Although your article content has been submitted for review & is not yet published, find out from the publisher whether they require you to obtain written permission from them to reuse your article while it is in review / pending.\nIf they do not require written permission while it is under review, and the article remains unpublished at the time of your final dissertation deposit approval, then you do not need written permission to reuse the article.\nIf they do require written permission while it is under review, but the article is published prior to final dissertation deposit approval, then you must obtain written permission from the publisher to reuse your article content and retain it for your records.\nAppendix\nYou can either include an appendix section as a subchapter at the end of each chapter, or you can choose to use a separate appendix section at the end of the document. An appendix section Please use only one of the following formats when using an appendix:\nIf using an appendix at the end of your document, use the “Major Headings” heading style and type the heading title all in caps. An example of an appendix subheading title is APPENDIX A. SUPPLEMENTAL INFORMATION. See the APPENDIX section near the end of this template for more information.\nIf using an in-chapter appendix instead of using a separate appendix at the end of your document, use the first-level subheading style and type the subheading title in sentence case. Unlike appendices located at the end of your document, you should only have one appendix subchapter per chapter. An example of an appendix subheading title is: 1.1  Supplemental information.\nReferences\nYou can either include a reference section as a subchapter at the end of each chapter, or you can list all references in the separate reference section at the end of the document. Please use only one method; do not use both methods. Please note that if you are using an article-based thesis format, you must include in-chapter references in each chapter (not at the end of your document). A DOI or website link must be included with all references.\nUse the following formatting for all references:\nReferences should either be listed numerically or alphabetically (by last name). If they are listed numerically, they must be in numerical order. If they are listed alphabetically, then references must be sorted in alphabetical order by the author’s last name.\nLine spacing can be either 1.0 (single), 1.5, or 2.0 (double) line spacing.\nIf using 1.0 (single) line spacing for your references, apply 0-point Before and no more than 12-point After spacing to all references. If using 1.5 or 2.0 line spacing for your references, apply 0-point Before and 0-point After spacing.\nAlignment for references should match the alignment used for your paragraphs in your document (either justified or left alignment).\nUse a 0.5” hanging indentation.\nApply both “widow/orphan control” and “keep lines together” from paragraph settings.\nSee the REFERENCES section of this document for two different examples of reference formatting (numerical and alphabetical).\nDocument Accessibility Statement\nThis section is for your information only and should be deleted when using this template to create your thesis/dissertation. According to Purdue University Policy, Electronic Information, Communication and Technology Accessibility (S-5): As a public university and federal contractor, Purdue University is required to adhere to Sections 504 and 508 of the Rehabilitation Act of 1973 and Title II of the Americans with Disabilities Act. This standard specifies the means by which the University ensures compliance with these laws.\nUse Microsoft’s accessibility guide to help create an accessible document. When you initiate Form 9 Electronic Thesis Acceptance Form from your online Plan of Study, you will need to certify that in the preparation of this thesis/dissertation, you have to the best of your ability, created an accessible document that is in compliance with sections 504 and 508 of the Rehabilitation Act of 1973 and Title II of the Americans with Disabilities Act.\nTYPE YOUR CHAPTER TITLE DESCRIPTION\nContent in this chapter has been submitted for review and is pending publication with Psychology of Popular Media Culture:\nGrady, J. S., Her, M., Moreno, G., Perez, C., & Yelinek, J. (2024). Emotions in storybooks: A comparison of storybooks that represent ethnic and racial groups in the United States. Psychology of Popular Media Culture. Manuscript submitted for publication (in review, pending).\nBelow are all four subheading levels so that you can see how they are formatted.\nFirst level subheading\nThis appears in the heading style ribbon as 1.1 Heading 2, Subheading 1. You will note that the first-level subheading is centered, has bold font, and is numbered.\nSecond level subheading\nThis appears in the heading style ribbon as 1.1.1 Heading 3, Subheading 2. You will note that the second-level subheading is left aligned, has bold font, and is numbered.\nThird level subheading\nThis appears in the heading style ribbon as Heading 4, Subheading 3. You will note that the third-level subheading is left aligned, has bold and italic font, and is not numbered.\nFourth level subheading\nThis appears in the heading style ribbon as Heading 5, Subheading 4. You will note that the fourth-level subheading is left aligned, has italic font, and is not numbered.\nTYPE YOUR CHAPTER TITLE DESCRIPTION\nContent in this chapter is published in Psychology of Popular Media Culture and is reprinted with permission:  \nGrady, J. S., Her, M., Moreno, G., Perez, C., & Yelinek, J. (2019). Emotions in storybooks: A comparison of storybooks that represent ethnic and racial groups in the United States. Psychology of Popular Media Culture, 8(3), 207–217. https://doi.org/10.1037/ppm0000185 \nBelow are all four subheading levels so that you can see how they are formatted.\nFirst level subheading\nThis appears in the heading style ribbon as 1.1 Heading 2, Subheading 1. You will note that the first-level subheading is centered, has bold font, and is numbered.\nSecond level subheading\nThis appears in the heading style ribbon as 1.1.1 Heading 3, Subheading 2. You will note that the second-level subheading is left aligned, has bold font, and is numbered.\nThird level subheading\nThis appears in the heading style ribbon as Heading 4, Subheading 3. You will note that the third-level subheading is left aligned, has bold and italic font, and is not numbered.\nFourth level subheading\nThis appears in the heading style ribbon as Heading 5, Subheading 4. You will note that the fourth-level subheading is left aligned, has italic font, and is not numbered\nAPPENDIX A. TYPE YOUR APPENDIX TITLE DESCRIPTION\nAn appendix section is OPTIONAL and is not required. If your document will not include an appendix, delete this page. Some students use an appendix for survey information, data collection information, supplemental data, etc. Formatting in an appendix should be the same as formatting the used in your chapters. Please note that if figures and tables are included in an appendix section in the back of your document, the caption numbering is different than those located in chapters. In an appendix, the caption number must begin with the appendix letter and numbering starts over back at 1 in each appendix. Appendix figures and tables are not required to be included in the List of Figures and List of Tables sections because the numbering is different than caption numbering used throughout your chapters. Microsoft Word only recognizes one type of numbering in the List of Tables and List of Figures, which is why appendix captions will not populate in those lists when they are updated as they are numbered differently than captions in chapters. See below for an example of a formatted appendix caption.\nFigure A.1. This is an example of a formatted appendix figure and caption.\nAPPENDIX B. TYPE YOUR APPENDIX TITLE DESCRIPTION\nAn appendix section is OPTIONAL and is not required. If your document will not include an appendix, delete this page. Some students use an appendix for survey information, data collection information, supplemental data, etc. Formatting in an appendix should be the same as formatting the used in your chapters. Please note that if figures and tables are included in an appendix section in the back of your document, the caption numbering is different than those located in chapters. In an appendix, the caption number must begin with the appendix letter and numbering starts over back at 1 in each appendix. Appendix figures and tables are not required to be included in the List of Figures and List of Tables sections because the numbering is different than caption numbering used throughout your chapters. Microsoft Word only recognizes one type of numbering in the List of Tables and List of Figures, which is why appendix captions will not populate in those lists when they are updated as they are numbered differently than captions in chapters. See below for an example of a formatted appendix caption.\nTable B.1. This is an example of a formatted appendix figure and caption.\nREFERENCES (OPTION 1 – NUMERICAL)\nAmerican Psychiatric Association. (2022). Diagnostic and statistical manual of mental disorders (5th ed., text rev.). https://doi.org/10.1176/appi.books.9780890425787.\nGrady, J. S., Her, M., Moreno, G., Perez, C., and Yelinek, J. (2019). Emotions in storybooks: A comparison of storybooks that represent ethnic and racial groups in the United States. Psychology of Popular Media Culture, 8(3), 207–217. https://doi.org/10.1037/ppm0000185.\nJackson, L. M. (2019). The psychology of prejudice: From attitudes to social action (2nd ed.). American Psychological Association. https://doi.org/10.1037/0000168-000.\nMiranda, C. (2019). Exploring the lived experiences of foster youth who obtained graduate level degrees: Self-efficacy, resilience, and the impact on identity development (Publication No. 27542827) [Doctoral dissertation, Pepperdine University]. PQDT Open. https://pqdtopen.proquest.com/doc/2309521814.html?FMT=AI.\nNational Cancer Institute. (2019). Taking time: Support for people with cancer (NIH Publication No. 18-2059). U.S. Department of Health and Human Services, National Institutes of Health. https://www.cancer.gov/publications/patient-education/takingtime.pdf.\nOuellette, J. (2019, November 15). Physicists capture first footage of quantum knots unraveling in superfluid. Ars Technica. https://arstechnica.com/science/2019/11/study-you-can-tie-a-quantum-knot-in-a-superfluid-but-it-will-soon-untie-itself/.\nSapolsky, R. M. (2017). Behave: The biology of humans at our best and worst. Penguin Books.\nSvendsen, S., and Løber, L. (2020). The big picture/Academic writing: The one-hour guide (3rd digital ed.). Hans Reitzel Forlag. https://thebigpicture-academicwriting.digi.hansreitzel.dk/.\nZambrano-Vazquez, L. (2016). The interaction of state and trait worry on response monitoring in those with worry and obsessive-compulsive symptoms [Doctoral dissertation, University of Arizona]. UA Campus Repository. https://repository.arizona.edu/handle/10150/620615.\nREFERENCES (OPTION 2 - ALPHABETICAL)\nAmerican Psychiatric Association. (2022). Diagnostic and statistical manual of mental disorders  (5th ed., text rev.). https://doi.org/10.1176/appi.books.978089042578.\nGrady, J. S., Her, M., Moreno, G., Perez, C., and Yelinek, J. (2019). Emotions in storybooks: A comparison of storybooks that represent ethnic and racial groups in the United States. Psychology of Popular Media Culture, 8(3), 207–217. https://doi.org/10.1037/ppm0000185.\nJackson, L. M. (2019). The psychology of prejudice: From attitudes to social action (2nd ed.). American Psychological Association. https://doi.org/10.1037/0000168-000.\nMiranda, C. (2019). Exploring the lived experiences of foster youth who obtained graduate level degrees: Self-efficacy, resilience, and the impact on identity development (Publication No. 27542827) [Doctoral dissertation, Pepperdine University]. PQDT Open. https://pqdtopen.proquest.com/doc/2309521814.html?FMT=AI.\nNational Cancer Institute. (2019). Taking time: Support for people with cancer (NIH Publication No. 18-2059). U.S. Department of Health and Human Services, National Institutes of Health. https://www.cancer.gov/publications/patient-education/takingtime.pdf.\nOuellette, J. (2019, November 15). Physicists capture first footage of quantum knots unraveling in superfluid. Ars Technica. https://arstechnica.com/science/2019/11/study-you-can-tie-a-quantum-knot-in-a-superfluid-but-it-will-soon-untie-itself/.\nSapolsky, R. M. (2017). Behave: The biology of humans at our best and worst. Penguin Books.\nSvendsen, S., and Løber, L. (2020). The big picture/Academic writing: The one-hour guide (3rd digital ed.). Hans Reitzel Forlag. https://thebigpicture-academicwriting.digi.hansreitzel.dk/.\nZambrano-Vazquez, L. (2016). The interaction of state and trait worry on response monitoring in those with worry and obsessive-compulsive symptoms [Doctoral dissertation, University of Arizona]. UA Campus Repository. https://repository.arizona.edu/handle/10150/620615.\nPUBLICATIONS\nIf you are an author or co-author of a publication or publications (both published or submitted/pending publications), publication references should be included in this section. If you have only one publication, the heading cannot be plural and must be revised to show as PUBLICATION. Publications should be formatted the same way as references are formatted.\nIf you do not have any publications, delete this page.\nVITA\nA VITA section is OPTIONAL and is not required. If you are unsure about what to include in your VITA, consult with your department and/or major professor. The VITA can be written as a narrative or in Curriculum VITA (CV) form. Do not include any personal, private, or confidential information such as your home address, personal email address, personal phone number, date of birth, Social Security number, family member names and addresses, etc. \nIf your document will not include a VITA, delete this page."
  },
  {
    "source": "Timeline and Workflow.pdf",
    "content": "Timeline  and  Workflow   This  is  an  in-depth  overview  of  how  I  would  utilize  my  first  90  days(~13  weeks).  This  report  will  \ngive\n \nan\n \nidea\n \nof\n \nthe\n \nareas\n \nand\n \nmethods\n \nI\n \nwould\n \nprioritize\n \nassuming\n \nbeta\n \nproduct\n \nrelease\n \nby\n \nthe\n \nend\n \nof\n \nsummer.\n  Task-1:  (Understanding  the  problem  statement  and  gathering  data)  [2  -  3  weeks]  The  first  step  is  to  get  a  clear  mental  understanding  of  the  problem  I  would  be  trying  to  solve,  so  \nthat\n \nI\n \ncan\n \ndesign\n \na\n \nsystem\n \nthat\n \nis\n \nefficient\n \nin\n \ndealing\n \nwith\n \nthe\n \ntype\n \nof\n \ninput,\n \nprocessing\n \nload\n \nthat\n \nwill\n \nbe\n \nput\n \non\n \nthe\n \nsystem\n \nand\n \nthe\n \ntype\n \nof\n \noutput.\n \nBy\n \ndoing\n \nthis,\n \neven\n \nif\n \nI\n \nneed\n \nto\n \nimplement\n \nany\n \nfeatures\n \nin\n \nthe\n \nnear\n \nfuture,\n \nI\n \ncan\n \ndo\n \nso\n \nby\n \nmaking\n \nminimal\n \nchanges.\n  After  coming  up  with  the  system  plan(usually  takes  about  3  -  4  days),  I  will  start  working  on  the  \ndataset\n \ncollection.\n \nI\n \nwill\n \ngather\n \nwhatever\n \nthe\n \ndata\n \nis\n \nalready\n \npresent\n \nwithin\n \nmy\n \naccess\n \nfirst\n \nand\n \nthen\n \nconsult\n \ndermatologists\n \nto\n \nobtain\n \nmore.\n \nSimultaneously,\n \nI\n \nwill\n \nkeep\n \neducating\n \nmyself\n \nwith\n \nthe\n \nknowledge\n \ndomain\n \nof\n \nskin\n \ndiseases\n \nand\n \nconditions.\n \nMost\n \nimportantly,\n \nI\n \nwill\n \nresearch\n \nwhat\n \ntypes\n \nof\n \nskin\n \ndiseases\n \ncan\n \nactually\n \nbe\n \ncontrolled\n \nor\n \nmanaged\n \nby\n \nover\n \nthe\n \ncounter\n \nskin\n \ncare\n \nproducts\n \nsince\n \nproviding\n \nrecommendations\n \nof\n \nskin\n \ncare\n \nproducts\n \nbased\n \non\n \nthe\n \nperson’s\n \ncondition\n \nwould\n \nbe\n \none\n \nof\n \nthe\n \nmain\n \nobjectives\n \nof\n \nNanu\n \nBeauty.\n  The  dataset  is  going  to  be  very  huge,  we  would  be  requiring  atleast  50,000  images  to  get  \nstarted\n \nfor\n \nthe\n \nfirst\n \npart\n \nand\n \nit\n \nwill\n \nkeep\n \non\n \nincreasing\n \nto\n \n100,000\n \nconsidering\n \nvarious\n \nconditions\n \nand\n \nclasses\n \nif\n \nI’m\n \nguessing\n \nright.\n \nThen,\n \nI’ll\n \nmonitor\n \nthe\n \ndata\n \nand\n \nresearch\n \non\n \ntypes\n \nof\n \ntransformations\n \nrequired\n \nto\n \nthe\n \nimages\n \nin\n \norder\n \nto\n \nget\n \na\n \ngood\n \nperforming\n \nmodel\n \nat\n \nthe\n \nend.\n \nFlag\n \nlow-incidence\n \nclasses\n \nduring\n \ninitial\n \naudit.\n \nDeveloping\n \na\n \nGAN-based\n \nor\n \ndiffusion-based\n \naugmentation\n \nfor\n \nthose\n \nclasses.\n \nAlong\n \nwith\n \nthe\n \nimages\n \nand\n \nlabels,\n \nI’ll\n \nalso\n \nrequest\n \ndermatologists\n \nto\n \nprovide\n \nsome\n \nmetadata\n \nfor\n \neach\n \ncorresponding\n \nimage\n \ncollected\n \nin\n \nthe\n \ndataset.\n \nWith\n \nthis\n \ninformation,\n \nI’ll\n \ncome\n \nup\n \nwith\n \nstrategies\n \nto\n \nmaximize\n \nthe\n \nuse\n \nof\n \nmetadata\n \nwith\n \nthe\n \nimages\n \nto\n \ntrain\n \nan\n \nefficient\n \nDL\n \nmodel\n \ntaking\n \nits\n \nperformance\n \nto\n \nthe\n \nthe\n \nutmost\n \npossible\n \nevaluation\n \nmetric\n \non\n \ntest\n \ndatasets.\n \nThis\n \npart\n \nwould\n \nactually\n \nbe\n \nthe\n \n“preprocessing”\n \nwhere\n \nI\n \napply\n \ntransformations,\n \ncleaning\n \nmetadata\n \nand\n \ndevelop\n \na\n \npipeline\n \nto\n \npreprocess\n \nthe\n \ndata\n \nobtained.\n \nThis\n \nis\n \nthe\n \n1st\n \ncrucial\n \npart\n \nin\n \nmy\n \nopinion\n \nas\n \nthis\n \ndefines\n \nour\n \nmodel’s\n \nperformance.\n  Task-2:  (Researching  pre-trained  models  and  tweaks)  [3  -  4  weeks]  The  next  task  I  would  be  working  on  is  researching  if  there  are  any  pre-trained  models  for  the  \nclasses\n \nthat\n \nare\n \npresent\n \nin\n \nour\n \ndataset\n \nand\n \nhow\n \nthose\n \nmodels\n \ntackled\n \nthe\n \nproblem.\n \nFor\n \nexample,\n \npre-trained\n \nresnet\n \nmay\n \nperform\n \nbetter\n \non\n \none\n \nclass\n \nlike\n \nmelanoma,\n \nefficientnet\n \nmay\n \nperform\n \nbetter\n \non\n \nclasses\n \nwith\n \nhyperpigmentation,\n \nvision\n \ntransformers\n \non\n \nother\n \ngroups\n \nof\n \nclasses.\n \nWhile\n \nresearching\n \nthis,\n \nI\n \nwill\n \nalso\n \ntry\n \nto\n \nidentify\n \nwhich\n \nmodels\n \ncan\n \nbe\n \nused\n \nacross\n \nvarious\n \ngroups\n \nof\n \nclasses\n \nto\n \nreduce\n \nthe\n \ndiversity\n \nof\n \nmodels\n \nwhile\n \nensembling\n \nthe\n \noutputs\n \ntogether\n \nfor\n \nfinal\n \npredictions\n \n(discussed\n \nmore\n \nin\n \nTask-3).\n \nAfter\n \ngetting\n \nsome\n \ninitial\n \nidea\n \nof\n \nwhich\n \npre-trained\n \nmodels\n \nto\n \nuse\n \non\n \nwhich\n \nclasses,\n \nI’ll\n \nperform\n \na\n \ndry\n \nrun\n \nof\n \nthese\n \nmodels\n \nwithout\n \nany\n \ntweaks\n \nor\n \nmodifications\n \non\n \nour\n \ndataset\n \nin\n \nmini\n \nbatches\n \nand\n \nlook\n \nat\n \nthe\n \nresults.\n \nBy\n \ndoing\n \nso,\n \nI’ll\n \nget\n \nan\n \nunderstanding\n \nof\n \nwhat\n \nparameters\n \nand\n \ntype\n \nof\n \nmodifications\n \nI\n \nneed\n \nto\n \ndo\n \nfor\n \ninitial\n \ntraining.  Then  I’ll  start  performing  initial  training  of  those  models  on  the  subsets  of  our  dataset  \nover\n \nseveral\n \niterations\n \neach\n \ntime\n \nmaking\n \na\n \nnote\n \nof\n \nmodifications\n \nand\n \nchanges\n \nI\n \nneed\n \nto\n \nmake\n \nat\n \nwhat\n \nspecific\n \nlayers\n \nof\n \nthe\n \npre-trained\n \nmodels\n \nfor\n \nall\n \nclasses.\n \nIn\n \ngeneral\n \nterms,\n \nI’ll\n \nconduct\n \nA/B\n \ntesting\n \non\n \nthe\n \nmodels.\n \nI\n \nwill\n \nhave\n \nto\n \nchoose\n \nthe\n \nparameters\n \ncarefully\n \nas\n \nthis\n \nwill\n \ngreatly\n \nhelp\n \nin\n \nfinal\n \ntraining\n \nin\n \nboth\n \naspects\n \nof\n \ncomputational\n \nresources\n \nand\n \ntime.\n \nThis\n \nis\n \nthe\n \n2nd\n \ncrucial\n \npart\n \nof\n \nthe\n \nentire\n \nprocess.\n  Table  1.  Metrics  I’d  be  dealing  with  during  this  task   Metric  Why?  \nAUC  For  imbalance  classes  \nF1  score  For  balanced  classes  \nSensitivity  Clinical  safety  \nSpecificity  Overfitting   Task-3:  (Final  training  and  feedback)  [2  -  3  weeks]  After  deciding  on  the  final  modifications  to  tweaks  to  the  pre-trained  models,  I’ll  get  the  now  \nsaved\n \nmodels\n \nready\n \nfor\n \nfinal\n \ntraining.\n \nI’ll\n \nthen\n \nwrite\n \na\n \npipeline\n \nto\n \nperform\n \ntraining\n \non\n \nall\n \nof\n \nthe\n \nsubsets\n \npresent\n \nin\n \nour\n \ndataset\n \nbased\n \non\n \nthe\n \ncollected\n \ndata\n \nfrom\n \nthe\n \nprevious\n \niterations.\n \nI’ll\n \nwrite\n \nthe\n \npipeline\n \nin\n \nsuch\n \na\n \nway\n \nthat\n \nit\n \nautomatically\n \nchooses\n \nthe\n \ntype\n \nof\n \nsaved\n \nmodel\n \nto\n \nuse\n \non\n \nthe\n \nspecific\n \ndataset\n \nso\n \nwe\n \ncan\n \nspend\n \nthat\n \ntime\n \nelsewhere.\n \nNow\n \ncomes\n \nthe\n \nnext\n \npart\n \nthat\n \nI\n \ndiscussed\n \nin\n \nthe\n \ninterview.\n \nWhile\n \npredicting\n \nthe\n \nfinal\n \nprobabilities,\n \nI’ll\n \nensemble\n \nthe\n \noutputs\n \nof\n \nall\n \nthese\n \nmodels\n \ninto\n \na\n \nsingle\n \noutput.\n \nThe\n \nreason\n \nI\n \nwould\n \nsuggest\n \ngoing\n \nwith\n \nthis\n \ntechnique\n \nat\n \nthe\n \ntime\n \nof\n \npredicting\n \nfinal\n \noutput\n \nis\n \nthat\n \nmultiple\n \nmodels\n \nlearn\n \ndifferent\n \naspects\n \nof\n \nthe\n \ndata,\n \nand\n \nby\n \naggregating\n \ntheir\n \npredictions,\n \nwe\n \ncan\n \nreduce\n \nvariance,\n \nbias,\n \nand\n \noverfitting.\n \nThis\n \nalso\n \nsaves\n \na\n \nlot\n \nof\n \ntime\n \nand\n \ncomputational\n \nresources\n \nas\n \nwe\n \nwon’t\n \nbe\n \nstarting\n \nfrom\n \nscratch\n \nwhich\n \ngenerally\n \ntakes\n \na\n \nlot\n \nof\n \ntime\n \nand\n \nthere\n \nmight\n \nbe\n \nhigh\n \nprobability\n \nwe\n \nrun\n \ninto\n \nseveral\n \nfailures\n \nwhile\n \ntraining.\n \nI’ll\n \nwrite\n \nan\n \nensembling\n \npipeline\n \nwhich\n \nbasically\n \ncollects\n \nprobabilities\n \nfrom\n \nall\n \nmodels\n \nand\n \nthen\n \naggregates\n \nthem\n \nusing\n \nan\n \nensembling\n \nlogic.\n \nThere\n \nare\n \nseveral\n \nwe\n \ncan\n \nchoose\n \nfrom,\n \nbut\n \nI\n \ncan\n \nchoose\n \nthe\n \nbest\n \none\n \nafter\n \nTask-2,\n \nit’ll\n \ngive\n \na\n \nclear\n \nconclusion\n \non\n \nwhich\n \nlogic\n \nto\n \nchoose\n \nfrom.\n Some  of  them  are:  1.  Soft  voting  2.  Weighted  soft  voting  3.  Stacking  4.  Hybrid  of  these  ensembling  logics  Each  logic  has  its  own  pros  and  cons,  but  I  highly  recommend  going  with  a  hybrid  of  ensembling  \nlogic\n \nfor\n \nour\n \nproblem\n \nstatement.\n \nI\n \nbelieve\n \nfor\n \nour\n \ndataset\n \nwhich\n \nis\n \ngoing\n \nto\n \nbe\n \nvery\n \nhuge\n \nand\n \nseveral\n \nclasses,\n \na\n \nlogic\n \nof\n \nsoft\n \nvoting\n \n+\n \nCAM\n \nvisualizations\n \nwould\n \nbe\n \nthe\n \nbest\n \noption\n \nbut\n \nI’ll\n \ndecide\n \nthis\n \nlater.\n  \nAfter  we  have  the  final  outputs  on  a  test  dataset,  I’ll  take  those  predictions  and  consult  \ndermatologists\n \nfor\n \ntheir\n \nfeedback\n \non\n \nthe\n \nmodels\n \nperformance\n \nand\n \nreceive\n \nany\n \nsuggestions.\n \nWe\n \nobviously\n \nwon’t\n \nhave\n \ntime\n \nto\n \nimplement\n \nall\n \nthose\n \nsuggestions\n \nso\n \nI’ll\n \nrank\n \nthem\n \nand\n \nimplement\n \nthe\n \nmost\n \nimportant\n \nones\n \nthat\n \ncan\n \nprovide\n \na\n \nboost\n \nin\n \nmodel\n \nperformance.\n \nThen\n \nover\n \nthe\n \ncourse\n \nof\n \nnext\n \nmonth,\n \nI’ll\n \nimplement\n \nthe\n \nother\n \nsuggestions\n \nwhile\n \ncontinuing\n \nto\n \ncollect\n \nfeedback.\n  Task-4:  (Developing  a  recommendation  system)  [1  -  2  weeks]  Now  I’d  be  in  the  final  stages  of  reaching  our  main  objective  i.e.,  recommending  skin  care  \nproducts\n \nbased\n \non\n \nthe\n \nfinal\n \npredictions\n \nof\n \nthe\n \nDL\n \nmodel.\n \nBased\n \non\n \nthe\n \nseverity\n \nof\n \neach\n \nclass\n \nI’ll\n \nset\n \na\n \nconfidence\n \nthreshold\n \nand\n \ncompare\n \nit\n \nwith\n \nthe\n \npredicted\n \nprobabilities\n \nfrom\n \nthe\n \nmodel\n \nand\n \nsuggest\n \nthe\n \nrecommendations.\n \nSince\n \nI\n \nwon’t\n \nhave\n \nmuch\n \ndata\n \nhere,\n \nI’d\n \nstart\n \nwith\n \na\n \nsimple\n \nrule\n \nbased\n \nrecommendation\n \nsystem\n \nwith\n \nif-else\n \nstatements.\n \nAfter\n \nintegrating\n \nthis\n \nlayer\n \nin\n \nour\n \nfinal\n \nmodel,\n \nI’d\n \ncollect\n \nfeedback\n \nfrom\n \ndermatologists\n \nif\n \nthese\n \nrecommendations\n \nare\n \na\n \ngood\n \nstarting\n \npoint\n \nor\n \nshould\n \nI\n \nchange\n \nsome\n \naspects.\n \nWhile\n \ndoing\n \nthis,\n \nI’ll\n \ncollect\n \nlabeled\n \nrecommended\n \ndata\n \nfrom\n \nthem\n \nif\n \npossible\n \nand\n \ntry\n \nto\n \ncome\n \nup\n \nwith\n \na\n \nML\n \nbased\n \nrecommender\n \nsystem.\n \nIf\n \nnot,\n \nI’ll\n \njust\n \nproceed\n \nwith\n \nrule\n \nbased\n \nfirst\n \nand\n \nthen\n \ncollect\n \nenough\n \ndata\n \nafter\n \nseveral\n \niterations\n \nof\n \nthe\n \nmodel\n \noutputs\n \nand\n \nfeedback\n \nand\n \nthen\n \nI’ll\n \nuse\n \ndata\n \nfrom\n \nfeedback\n \nto\n \ndevelop\n \na\n \nML\n \nrecommender\n \nsystem.\n \n  \n Fig  1.  Overview  flowchart  of  the  workflow   Task-5:  (Rigorous  testing  and  final  changes)  [1  week]  By  this  time,  we  would  have  a  well  developed  model  capable  of  solving  our  problem  statement.  \nSo,\n \nI’ll\n \nput\n \nthat\n \nmodel\n \nto\n \ntest\n \nin\n \nvarious\n \nscenarios\n \nand\n \nunseen\n \nimages\n \nof\n \nvarious\n \nclasses\n \nand\n \n\nevaluate  it  and  make  small  necessary  changes  without  disturbing  the  model  architecture.  The  \nmodel\n \nshould\n \nthen\n \nbe\n \nready\n \nfor\n \nbeta\n \nrelease.\n  This  is  the  best  approach  I  would  proceed  with  in  the  first  90  days.  But  for  some  reason,  let’s  \nsay\n \nthat\n \nwhile\n \nconducting\n \nresearch\n \nduring\n \nTask-2,\n \nif\n \nI\n \never\n \nfind\n \nevidence\n \nthat\n \ntraining\n \na\n \nmodel\n \narchitecture\n \nlike\n \nCNN,\n \nTransformer\n \nfrom\n \nscratch\n \nwill\n \nproduce\n \nbetter\n \nresults,\n \nI’ll\n \nweigh\n \nthe\n \napproach\n \nover\n \nseveral\n \nfactors\n \nand\n \nbased\n \non\n \nthe\n \nfinal\n \narguments\n \nover\n \nthose\n \nfactors,\n \nI’ll\n \nchange\n \nthe\n \ntasks\n \n2\n \n&\n \n3.\n \nSome\n \nof\n \nthe\n \nfactors\n \nI’ll\n \nconsider\n \nare:\n 1.  Based  on  the  dataset  size,  how  feasible  is  it  to  implement  compared  to  the  present  \napproach?\n 2.  What  would  be  ideal  fail  case  scenarios  since  I  would  be  training  from  scratch?  3.  What  additional  transformations  should  I  perform  excluding  Task-1  for  this  approach?  4.  If  all  the  above  factors  favour  the  new  approach,  I  need  to  decide  on  developing  the  \nlayers\n \nfrom\n \nscratch\n \nwhich\n \nis\n \ngoing\n \nto\n \ntake\n \ntime.\n \nSo,\n \nis\n \nthis\n \njustified\n \nfor\n \nthe\n \nmodel’s\n \nperformance\n \nboost\n \ncompared\n \nto\n \nthe\n \npresent\n \napproach?\n 5.  What  would  be  the  cost  of  computational  resources?  6.  If  I  need  to  make  any  modifications,  how  deep  into  layers  should  I  modify  to  implement  \nthem?\n These  are  just  the  basic  arguments,  and  while  conducting  research  there  will  be  a  high  \npossibility\n \nthese\n \narguments\n \nmight\n \nincrease.\n  So  after  weighing  all  these  factors,  if  I  find  the  new  approach  justifiable  to  the  performance  \nboost\n \ncompared\n \nto\n \nthe\n \npresent\n \napproach,\n \nI’ll\n \nproceed\n \nwith\n \nit.\n \nBy\n \ndoing\n \nthis,\n \nthe\n \ntasks\n \n2\n \n&\n \n3\n \nmight\n \ntake\n \na\n \nbit\n \nmore\n \ntime\n \nthan\n \nexpected.\n    "
  },
  {
    "source": "1. AI-Powered Game Strategy Analyze.txt",
    "content": "1. AI-Powered Game Strategy Analyzer\nProblem: Teams want competitive edges from data. Project Idea:\n\nUse computer vision (tracking players' movement) and ML algorithms to analyze opponent strategies, strengths, and weaknesses.\nProvide a visual playbook with strategy recommendations.\nBusiness Insight Angle: Demonstrate how data-informed strategies could improve win rates and fan engagement.\n\n\n2. AI-Powered Class and Facility Utilization Optimizer\nProblem: Many sports facilities struggle to maximize facility space and class attendance.\n\nProject Idea:\n\nAnalyze historical booking data, attendance, seasonal trends, and cancellations.\nBuild a predictive model that recommends optimal class schedules, resource allocations, and staffing levels to maximize usage.\nDeliver insights via a dashboard with utilization heatmaps and revenue forecasts.\nBusiness Impact:\n\nIncreases facility profitability and customer satisfaction.\nHelps Upper Hand clients make data-driven scheduling decisions.\n\n\n3. Revenue Forecasting Dashboard for Sports Businesses\nProblem: Clients want to plan ahead with confidence.\n\nProject Idea:\n\nBuild time-series models (e.g., Prophet, ARIMA) for monthly revenue forecasting.\nInclude factors like bookings, promotions, events, and seasonality.\nDeliver insights through visual dashboards and business scenario simulations.\nBusiness Impact:\n\nHelps clients budget, invest, and grow strategically.\nAdds value to Upper Hand’s business analytics offering.\n\n4. SQL Query Optimization & ETL Pipeline Design (Hands-On Project)\nObjective: Design efficient data pipelines to handle user and financial data.\n\nKey Components:\nDesign and document ETL/ELT workflows (using tools like Airflow, dbt, or custom Python scripts).\nOptimize SQL queries for performance.\nAutomate data quality checks and report generation.\nBusiness Impact: Supports scalability, improves data accuracy, and speeds up insight delivery.\n\n5. Product Feature Impact Analysis on Retention and Revenue\nObjective: Quantify which product features drive higher retention and revenue.\n\nKey Components:\nA/B test feature usage and apply causal inference (e.g., uplift modeling).\nIdentify high-impact features and underused ones.\nVisualize correlations with revenue and churn metrics.\nBusiness Impact: Informs feature prioritization and product roadmap decisions.\n\n6. ML-Powered User Retention Predictor (End-to-End Deployment)\nWhy It Fits:\nSolves a key business challenge: retaining users.\nShows skills in model deployment & monitoring, cloud tools, and presentation of insights.\nProject Scope:\nTrain classification models (XGBoost, Random Forest, Neural Nets) to predict user churn based on usage and financial data.\nDeploy using AWS SageMaker or GCP Vertex AI.\nMonitor model drift with MLflow or Prometheus.\nCreate dashboards showing retention metrics and actionable recommendations.\n\n7. Sports Business Intelligence Platform (End-to-End Project)\nWhy It Fits:\nCombines data science, BI, and cloud deployment.\nAligns with executive-level presentations and strategic alignment.\nProject Scope:\nAggregate data on revenue, bookings, user activity, and market trends.\nBuild a data warehouse (BigQuery or Redshift) and ETL workflows.\nCreate ML models for KPI forecasting and performance benchmarks.\nPresent via dashboards + storytelling decks for stakeholders."
  },
  {
    "source": "2502.19204v2.pdf",
    "content": "Distill Any Depth:\nDistillation Creates a Stronger Monocular Depth Estimator\nXiankang He∗1,2 Dongyan Guo∗1 Hongji Li2,3 Ruibo Li4 Ying Cui1 Chi Zhang†2\n1Zhejiang University of Technology 2 AGI Lab, Westlake University\n3Lanzhou University 4Nanyang Technological University\n{hexiankang577, 3420670269neon}@gmail.com {guodongyan,cuiying}@zjut.edu.cn\nruibo001@e.ntu.edu.sg chizhang@westlake.edu.cn\nhttps://distill-any-depth-official.github.io/\nFigure 1: Zero-shot prediction on in-the-wild images. Our model, distilled from Genpercept [49] and DepthAnythingv2 [51],\noutperforms other methods by delivering more accurate depth details and exhibiting superior generalization for monocular\ndepth estimation on in-the-wild images.\nAbstract\nRecent advances in zero-shot monocular depth estima-\ntion(MDE) have significantly improved generalization by\nunifying depth distributions through normalized depth rep-\nresentations and by leveraging large-scale unlabeled data\nvia pseudo-label distillation. However, existing methods that\nrely on global depth normalization treat all depth values\nequally, which can amplify noise in pseudo-labels and re-\nduce distillation effectiveness. In this paper, we present a\nsystematic analysis of depth normalization strategies in the\ncontext of pseudo-label distillation. Our study shows that,\nunder recent distillation paradigms (e.g., shared-context dis-\ntillation), normalization is not always necessary—omitting\nit can help mitigate the impact of noisy supervision. Fur-\nthermore, rather than focusing solely on how depth informa-\ntion is represented, we propose Cross-Context Distillation,\n*denotes co-first authorship. This work was done while Xiankang He\nwas a visiting student at the AGI Lab, Westlake University.\n† denotes corresponding author.\nwhich integrates both global and local depth cues to enhance\npseudo-label quality. We also introduce an assistant-guided\ndistillation strategy that incorporates complementary depth\npriors from a diffusion-based teacher model, enhancing su-\npervision diversity and robustness. Extensive experiments\non benchmark datasets demonstrate that our approach sig-\nnificantly outperforms state-of-the-art methods, both quanti-\ntatively and qualitatively.\n1. Introduction\nMonocular depth estimation (MDE) predicts scene depth\nfrom a single RGB image, offering greater flexibility com-\npared to stereo or multi-view methods, and benefiting a\nwide range of applications, such as autonomous driving and\nrobotic perception [ 11, 13, 18, 52, 30]. Recent research\non zero-shot MDE models [37, 55, 47, 24] aims to handle\ndiverse scenarios, but training such models requires large-\nscale, diverse depth data, which is often limited by the need\nfor specialized equipment [31, 54]. A promising solution is\narXiv:2502.19204v2  [cs.CV]  21 Apr 2025\nusing large-scale unlabeled data, which has shown success in\ntasks like classification and segmentation [27, 63, 48]. Stud-\nies like DepthAnything [50] highlight the effectiveness of\nusing pseudo labels from teacher models for training student\nmodels.\nTo enable training on such a diverse, mixed dataset, most\nstate-of-the-art methods [51, 40, 55] employ scale-and-shift\ninvariant (SSI) depth representations for loss computation.\nThis approach normalizes raw depth values within an image,\nmaking them invariant to scaling and shifting, and ensures\nthat the model learns to focus on relative depth relationships\nrather than absolute values. The SSI representation facilitates\nthe joint use of diverse depth data, thereby improving the\nmodel’s ability to generalize across different scenes [ 38,\n5]. Similarly, during evaluation, the metric depth of the\nprediction is recovered by solving for the unknown scale and\nshift coefficients of the predicted depth using least squares,\nensuring the application of standard evaluation metrics.\nDespite its advantages, using SSI depth representation\nfor pseudo-label distillation in MDE models presents sev-\neral issues. Specifically, the inherent normalization process\nin SSI loss makes the depth prediction at a given pixel not\nonly dependent on the teacher model’s raw prediction at\nthat location but also influenced by the depth values in other\nregions of the image. This becomes problematic because\npseudo-labels inherently introduce noise. Even if certain\nlocal regions are predicted accurately, inaccuracies in other\nregions can negatively affect depth estimates after global\nnormalization, leading to suboptimal distillation results. As\nshown in Fig. 2, we empirically demonstrate that normaliz-\ning depth maps globally tends to degrade the accuracy of\nlocal regions, as compared to only applying normalization\nwithin localized regions during evaluation.\nBuilding on this insight, in this paper, we investigate the\nissue of depth normalization in pseudo-label distillation. We\nanalyze various depth normalization strategies, including\nglobal normalization, local normalization, hybrid global-\nlocal approaches, and the absence of normalization. Through\nempirical experiments, we explore how each depth repre-\nsentation impacts the performance of different distillation\ndesigns, especially when using pseudo-labels for training.\nRather than focusing solely on pseudo-label represen-\ntation, we introduce Cross-Context Distillation, a method\nthat integrates both global and local depth cues to enhance\npseudo-label quality. Our findings reveal that local regions,\nwhen used for distillation, produce pseudo-labels that capture\nhigher-quality depth details, improving the student model’s\ndepth estimation accuracy. However, relying solely on local\nregions may overlook broader contextual relationships in the\nimage. To address this, we combine both local and global\ninputs within a unified distillation framework. By leveraging\nthe context-specific advantages of local distillation alongside\nthe broader understanding provided by global methods, our\nGlobal Least-\nsquare\nLocal Least-\nsquare\nCrop\nGlobal\nDepth GT\nLocal \nDepth GT\nAffine-invariant \nDepth Prediction\nCrop\n①\nError Map\n②\n7.2 6.2\nKITTI NYU ScanNet DIODE\n3.8 3.1 3.7 3.2\n14.0\n8.3\n3.9 3.1\nETH3D\nGlobal Least-square\nLocal Least-square\nAbsRel\n ①\n②\n(a) Least-square strategy\n(b) Metric of different way of Least-square \nError\nFigure 2: Issue with Global Normalization (SSI).In (a), we\ncompare two alignment strategies for the centralw/2, h/2 re-\ngion: (1) Global Least-Square, where alignment is applied to\nthe full image before cropping, and (2) Local Least-Square,\nwhere alignment is performed on the cropped region. Met-\nrics are computed on the cropped region. As shown in (b),\nthe outperformed local strategy demonstrates that global\nnormalization degrades local accuracy compared to local\nnormalization.\napproach yields more detailed and reliable depth predictions.\nTo harness the strengths of both, we propose using a\ndiffusion-based model as the teacher assistant to generate\npseudo-labels, which are then used to supervise the student\nmodel. This strategy enables the student model to learn\nfrom the detailed depth information provided by diffusion-\nbased models, while also benefiting from the precision and\nefficiency of encoder-decoder models.\nTo validate the effectiveness of our design, we conduct\nextensive experiments on various benchmark datasets. The\nempirical results show that our method significantly out-\nperforms existing baselines qualitatively and quantitatively.\nThe contributions can be summarized below: 1) We system-\natically analyze the role of different depth normalization\nstrategies in pseudo-label distillation, providing insights into\ntheir effects on MDE performance. 2) To enhance the quality\nof pseudo-labels, we propose Cross-Context Distillation, a\nhybrid local-global framework that leverages fine-grained\ndetails and global depth relationships; a teacher assistant that\nharnesses the complementary strengths of diverse depth esti-\nmation models to further improve robustness and accuracy.\n3) We conduct extensive experiments on benchmark datasets,\ndemonstrating that our method outperforms state-of-the-art\napproaches both quantitatively and qualitatively.\n2. Related Work\n2.1. Monocular Depth Estimation\nMonocular depth estimation (MDE) has evolved from\nhand-crafted methods to deep learning, significantly im-\nproving accuracy [ 11, 29, 12, 16, 62, 38]. Architectural\nrefinements, such as multi-scale designs and attention mech-\nanisms, have further enhanced feature extraction [21, 6, 61].\nHowever, most models remain reliant on labeled data and\nstruggle to generalize across diverse environments. Zero-\nshot MDE improves generalization by leveraging large-\nscale datasets, geometric constraints, and multi-task learning\n[37, 55, 57, 60, 58]. Metric depth estimation incorporates in-\ntrinsic data for absolute depth learning [2, 56, 22, 35, 46, 4],\nwhile generative models such as Marigold refine depth de-\ntails using diffusion priors [ 24, 49, 17, ?]. Despite these\nadvances, effectively utilizing unlabeled data remains a chal-\nlenge due to pseudo-label noise and inconsistencies across\ndifferent contexts. DepthAnything [51] explores large-scale\nunlabeled data but struggles with pseudo-label reliability.\nPatchFusion [9, 32] improves depth estimation by refining\nhigh-resolution image representations but lacks adaptabil-\nity in generative settings. To address these issues, we pro-\npose Cross-Context and Multi-Teacher Distillation, which\nenhances pseudo-label supervision by leveraging diverse\ncontextual information and multiple expert models, improv-\ning both accuracy and generalization ability.\n2.2. Semi-supervised Monocular Depth Estimation\nSemi-supervised depth estimation has garnered increas-\ning attention, primarily leveraging temporal consistency to\nutilize unlabeled data more effectively [28, 19]. Some ap-\nproaches [1, 44, 7, 53, 15] integrate stereo geometric con-\nstraints, enforcing left-right consistency in stereo video to\nenhance depth accuracy. Others incorporate additional su-\npervision, such as semantic priors [36, 20]or generative ad-\nversarial networks (GANs). For instance, DepthGAN [23]\nrefines depth predictions through adversarial learning. How-\never, these methods often rely on temporal cues, stereo con-\nstraints, or other auxiliary information, limiting their ap-\nplicability to broader and more general scenarios. Recent\nwork [34] has explored pseudo-labeling for semi-supervised\nmonocular depth estimation (MDE), but it lacks generative\nmodeling capabilities, restricting its generalization across\ndiverse environments. DepthAnything [ 50] demonstrates\nthe effectiveness of large-scale unlabeled data in improv-\ning generalization; however, pseudo-label reliability remains\na challenge. In contrast, our approach focuses on single-\nimage depth estimation, improving pseudo-label reliabil-\nity and maximizing its effectiveness. By relying solely on\nunlabeled data without additional constraints, our method\nachieves a more accurate and generalizable MDE model.\n3. Method\nIn this section, we introduce a novel distillation frame-\nwork designed to leverage unlabeled images for training\nzero-shot Monocular Depth Estimation (MDE) models. We\nbegin by exploring various depth normalization techniques in\nSection 3.1, followed by detailing our proposed distillation\nmethod in Section 3.2, which combines predictions across\nmultiple contexts. The overall framework is illustrated in\nFig. 3. Finally, we introduce an assistant-guided distillation\nscheme, in which a diffusion-based model acts as an aux-\niliary teacher to provide additional supervision for student\ntraining.\n3.1. Depth Normalization\nDepth normalization is a crucial component of our frame-\nwork as it adjusts the pseudo-depth labelsdt from the teacher\nmodel and the depth predictions ds from the student model\nfor effective loss computation. To understand the influence\nof normalization techniques on distillation performance, we\nsystematically analyze several approaches commonly em-\nployed in prior works. These strategies are visually illus-\ntrated in Fig. 4.\nGlobal Normalization: The first strategy we examine is\nthe global normalization [ 50, 51] used in recent distillation\nmethods. Global normalization [ 37] adjusts depth predic-\ntions using global statistics of the entire depth map. This\nstrategy aims to ensure scale-and-shift invariance by normal-\nizing depth values based on the median and mean absolute\ndeviation of the depth map. For each pixel i, the normalized\ndepth for the student model and pseudo-labels are computed\nas:\n˜ds\ni = Nglo(ds) = ds\ni − med(ds)\n1\nM\nPM\nj=1\n\f\fds\nj − med(ds)\n\f\f\n˜dt\ni = Nglo(dt) = dt\ni − med(dt)\n1\nM\nPM\nj=1\n\f\fdt\nj − med(dt)\n\f\f,\n(1)\nwhere med(ds) and med(dt) are the medians of the pre-\ndicted depth and pseudo depth, respectively. The final regres-\nsion loss for distillation is computed as the average absolute\ndifference between the normalized predicted depth and the\nnormalized pseudo depth across all valid pixels M:\nLDis = 1\nM\nMX\ni=1\n\f\f\f˜ds\ni − ˜dt\ni\n\f\f\f. (2)\nHybrid Normalization: In contrast to global normalization,\nHierarchical Depth Normalization [ 59] employs a hybrid\nnormalization approach by integrating both global and local\ndepth information. This strategy is designed to preserve\nboth the global structure and local geometry in the depth\nmap. The process begins by dividing the depth range into S\nS T\nT T\nTeacher\nTeacher\nStudent\nStudent\nS\nRandom\nCrop\nCrop following \nTeacher stage\nT\n(1) Shared-Context Distillation\n(2) Local-Global Distillation\nLocal-Global loss\nFigure 3: Overview of Cross-Context Distillation. Our method combines local and global depth information to enhance the\nstudent model’s predictions. It includes two scenarios: (1) Shared-Context Distillation, where both models use the same image\nfor distillation; and (2) Local-Global Distillation, where the teacher predicts depth for overlapping patches while the student\npredicts the full image. The Local-Global loss Llg (Top Right) ensures consistency between local and global predictions,\nenabling the student to learn both fine details and broad structures, improving accuracy and robustness.\nGlobal Norm. \nLocal Norm.\nHybrid Norm. \nNo Norm.\nNorm.\nArea\nPixel\nFigure 4: Normalization Strategies. We compare four nor-\nmalization strategies: Global Norm [37], Hybrid Norm [59],\nLocal Norm, and No Norm. The figure visualizes how each\nstrategy processes pixels within the normalization region\n(Norm. Area). The red dot represents any pixel within the\nregion.\nsegments, where S is selected from {1, 2, 4}. When S = 1,\nthe entire depth range is normalized globally, treating all\npixels as part of a single context, akin to global normal-\nization. In the case of S = 2, the depth range is divided\ninto two segments, with each pixel being normalized within\none of these two local contexts. Similarly, for S = 4, the\ndepth range is split into four segments, allowing normal-\nization to be performed within smaller, localized contexts.\nBy adapting the normalization process to multiple levels\nof granularity, hybrid normalization achieves a balance be-\ntween global coherence and local adaptability. For each\ncontext u, the normalized depth values for the student model\nNu(ds\ni ) and pseudo-labels Nu(dt\ni) are calculated within the\ncorresponding depth range. The loss for each pixel i is then\ncomputed by averaging the losses across all contexts Ui to\nwhich the pixel belongs:\nLi\nDis = 1\n|Ui|\nX\nu∈Ui\n\f\fNu(ds\ni ) − Nu(dt\ni)\n\f\f, (3)\nwhere |Ui| denotes the total number of groups (or contexts)\nthat pixel i is associated with. To obtain the final loss LDis,\nwe average the pixel-wise losses across all valid pixels M:\nLDis = 1\nM\nMX\ni=1\nLi\nDis. (4)\nLocal Normalization: In addition to global and hybrid nor-\nmalization, we investigate Local Normalization, a strategy\nthat focuses exclusively on the finest-scale groups used in hy-\nbrid normalization. This approach isolates the smallest local\ncontexts for normalization, emphasizing the preservation of\nfine-grained depth details without considering hierarchical\nor global scales. Local normalization operates by dividing\nthe depth range into the smallest groups, corresponding to\nS = 4 in the hybrid normalization framework, and each\npixel is normalized within its local context. The loss for\neach pixel i is computed using a similar formulation as in\nhybrid normalization, but with ui now representing the local\ncontext for pixel i, defined by the smallest four-part group:\nLDis = 1\nM\nMX\ni=1\n\f\fNui (ds\ni ) − Nui (dt\ni)\n\f\f. (5)\nNo Normalization: As a baseline, we also consider a direct\ndepth regression approach with no explicit normalization.\nThe absolute difference between raw student predictions and\nteacher pseudo-labels is used for loss computation:\nLDis = 1\nM\nMX\ni=1\n\f\fds\ni − dt\ni\n\f\f, (6)\nThis approach eliminates the need for normalization, as-\nsuming pseudo-depth labels naturally reside in the same\ndomain as predictions. It provides insight into whether nor-\nmalization enhances distillation effectiveness or if raw depth\nsupervision suffices.\n3.2. Distillation Pipeline\nIn this section, we introduce an enhanced distillation\npipeline that integrates two complementary strategies: Cross-\nContext Distillation andassistant-guided distillation. Both\nstrategies aim to improve the quality of pseudo-label distilla-\ntion, enhance the model’s fine-grained perception.\nCross-context Distillation. A key challenge in monocular\ndepth distillation is the trade-off between local detail preser-\nvation and global depth consistency. As shown in Fig. 5,\nproviding a local crop of an image as input to the teacher\nmodel enhances fine-grained details in the pseudo-depth la-\nbels, but it may fail to capture the overall scene structure.\nConversely, using the entire image as input preserves the\nglobal depth structure but often lacks fine details. To ad-\ndress this limitation, we propose Cross-Context Distillation,\na method that enables the student model to learn both local\ndetails and global structures simultaneously. Cross-context\ndistillation consists of two key strategies:\n1) Shared-Context Distillation: In this setup, both the\nteacher and student models receive the same cropped re-\ngion of the image as input. Instead of using the full image,\nwe randomly sample a local patch of varying sizes from\nthe original image and provide it as input to both models.\nThis encourages the student model to learn from the teacher\nRGB Global Depth Local Depth\nLocal Crop\nFigure 5: Different Inputs Lead to Different Pseudo La-\nbels. Global Depth: The teacher model predicts depth using\nthe entire image, and the local region’s prediction is cropped\nfrom the output. Local Depth: The teacher model directly\ntakes the cropped local region as input, resulting in more\nrefined and detailed depth estimates for that area, capturing\nfiner details compared to using the entire image.\nmodel across different spatial contexts, improving its ability\nto generalize to varying scene structures. For the loss of\nshared-context distillation, the teacher and student models\nreceive identical inputs and produce each depth prediction,\ndenoted as dt\nlocal and ds\nlocal:\nLsc = LDis\n\u0000\nds\nlocal, dt\nlocal\n\u0001\n, (7)\nThis loss encourages the student model to refine its fine-\ngrained predictions by directly aligning with the teacher’s\noutputs at local scales.\n2) Local-Global Distillation: In this approach, the teacher\nand student models operate on different input contexts. The\nteacher model processes local cropped regions, generating\nfine-grained depth predictions, while the student model pre-\ndicts a global depth map from the entire image. To ensure\nknowledge transfer, the teacher’s local depth predictions\nsupervise the corresponding overlapping regions in the stu-\ndent’s global depth map. This strategy allows the student\nto integrate fine-grained local details into its holistic depth\nestimation. Formally, the teacher model produces multiple\ndepth predictions for cropped regions, denoted as dt\nlocaln ,\nwhile the student generates a global depth map, ds\nglobal. The\nloss for Local-Global distillation is computed only over over-\nlapping areas between the teacher’s local predictions and the\ncorresponding regions in the student’s global depth map:\nLlg = 1\nN\nNX\nn=1\nLDis\n\u0000\nCrop(ds\nglobal), dt\nlocaln\n\u0001\n, (8)\nwhere Crop(·) extracts the overlapping region from the stu-\ndent’s depth prediction, andN is the total number of sampled\npatches. This loss ensures that the student benefits from the\ndetailed local supervision of the teacher model while main-\ntaining global depth consistency. The total loss function\nintegrates both local and cross-context losses along with ad-\nditional constraints, including feature alignment and gradient\npreservation, as proposed in prior works [51]:\nLtotal = Lsc + λ1 · Llg + λ2 · Lfeat + λ3 · Lgrad. (9)\nHere, λ1, λ2, and λ3 are weighting factors that balance the\ndifferent loss components. By incorporating cross-context\nsupervision, this framework effectively allows the student\nmodel to integrate both fine-grained details from local crops\nand structural coherence from global depth maps.\nAssistant-Guided Distillation. In addition to cross-context\ndistillation, we propose an assistant-guided distillation strat-\negy to further enhance the quality and robustness of the\ndistilled depth knowledge. This approach pairs a primary\nteacher [51] with a single auxiliary assistant, selected as a\ndiffusion-based depth estimator [49], which leverages gener-\native priors to complement the primary teacher’s predictions.\nThis design leverages their complementary strengths: the\nprimary teacher excels in providing efficient and globally\nconsistent supervision, while the assistant offers fine-grained\ndepth cues derived from its generative modeling capabili-\nties. By drawing supervision from two distinct architectures\ntrained with different paradigms(e.g., optimization strate-\ngies or data distributions), the student benefits from diverse\nknowledge sources, effectively mitigating biases and limi-\ntations inherent to a single teacher model. Formally, let M\nand Ma denote the primary and assistant models, respec-\ntively. During training, a probabilistic sampling mechanism\ndetermines whether supervision for each iteration is drawn\nfrom M or Ma. This stochastic guidance encourages the\nstudent to adapt to multiple supervision styles, fostering\nricher and more generalizable depth representations. Over-\nall, this assistant-guided scheme introduces complementary\nand diversified pseudo-labels, reducing over-reliance on any\nsingle teacher and improving both generalization and depth\nestimation performance.\n4. Experiment\n4.1. Experimental Settings\nDatasets. We train our proposed distillation framework\non a subset of 200,000 unlabeled images from the SA-1B\ndataset [26], following the training protocol of DepthAny-\nthingv2 [51]. For evaluation, we assess the distilled student\nmodel on five widely used depth estimation benchmarks. All\ntest datasets are kept unseen during training, enabling a zero-\nshot evaluation of generalization performance. The chosen\nbenchmarks include: NYUv2 [43], KITTI [14], ETH3D [42],\nScanNet [8], and DIODE [45]. Additional dataset details are\nprovided in the Appendix.\nMetrics. We assess depth estimation performance using\ntwo key metrics: the mean absolute relative error (AbsRel)\nand δ1 accuracy. Following previous studies [37, 56, 24] on\nzero-shot MDE, we align predictions with ground truth in\nboth scale and shift before evaluation.\nImplementation. Our experiments use state-of-the-art\nmonocular depth estimation models as teachers to gener-\nate pseudo-labels, supervising various student models in a\ndistillation framework with only RGB images as input. In\nshared-context distillation, both teacher and student receive\nthe same global region, extracted via random cropping from\nthe original image. The crop maintains a 1:1 aspect ratio\nand is sampled within a range from 644 pixels to the shortest\nside of the image, then resized to 560 × 560 for prediction.\nIn global-local distillation, the global region is cropped into\noverlapping local patches, each sized 560 × 560, for the\nteacher model to predict pseudo-labels. For assistant-guided\ndistillation, we adopt a probabilistic sampling strategy where\nthe primary teacher and the assistant model are selected with\na ratio of 7:3, respectively. We train our best student model\nusing the distillation pipeline for 20,000 iterations with a\nbatch size of 8 on a single NVIDIA V100 GPU, initialized\nwith pre-trained DAv2-Large weights. The learning rate is\nin tune with that of the corresponding student model. For\nDAv2 [51], the decoder learning rate is set to 5 × 10−5. For\nthe total loss function, we set the parameters as follows:\nλ1 = 0.5, λ2 = 1.0 and λ3 = 2.0.\n4.2. Analysis\nFor the ablation study and analysis, we sample a subset of\n50K images from SA-1B [26] as our training data, with an\ninput image size of 560 × 560 for the network. We conduct\nexperiments on two of the most challenging benchmarks,\nDIODE [45] and ETH3D [42], which include both indoor\nand outdoor scenes.\nAnalysis of Normalization across Cross-Context Distilla-\ntion. We evaluate the impact of different depth normalization\nstrategies on Cross-Context Distillation, as shown in Table 1.\nThe results reveal that the optimal normalization method\ndepends on the specific distillation design. In shared-context\ndistillation, where all pseudo-labels are generated by a sin-\ngle teacher model, hybrid normalization achieves the best\nperformance, closely followed by no normalization. The\nconsistent domain across supervision signals reduces the\nneed for normalization, enabling the model to better pre-\nserve local depth relationships. Unlike ground-truth-based\ntraining—where normalization is essential to align depth dis-\ntributions across datasets captured by heterogeneous sensors\nor represented in varying formats (e.g., sparse vs. dense, rel-\native vs. absolute)—pseudo-labels from a single model are\ninherently more uniform. Therefore, direct L1 loss without\nnormalization can more faithfully supervise pixel-level depth\nwithout distortion from global rescaling. In contrast, global\nRGB DepthAnythingv2 Marigold Ours\nGenpercept\nMiDaS v3.1\nFigure 6: Qualitative Comparison of Relative Depth Estimations. We present visual comparisons of depth predictions from\nour method (”Ours”) alongside other classic depth estimators (”MiDaS v3.1” [3], and models using DINOv2 [33] or SD as\npriors (”DepthAnythingv2 [51]”, ”Marigold” [24], ”Genpercept” [49]). Compared to state-of-the-art methods, the depth map\nproduced by our model, particularly at the position indicated by the black arrow, exhibits finer granularity and more detailed\ndepth estimation.\nnormalization introduces undesirable inter-pixel dependen-\ncies, while local normalization discards global structural\ncoherence. In local-global distillation, hybrid normalization\nagain proves most effective, likely due to its hierarchical\ndesign that enforces consistency across both local and global\ndepth predictions. The relatively small gap between hybrid\nand global normalization suggests that our framework, which\nuses local cues to refine global predictions, effectively miti-\ngates the limitations of global normalization. However, no\nnormalization leads to a notable performance drop compared\nto the shared-context setting, indicating that localized re-\ngions in this case come from distinct depth domains, making\ndirect L1 supervision less reliable. Local normalization, as\nbefore, sacrifices global consistency and thus underperforms.\nAblation Study of Cross-Context Distillation. To further\nvalidate the effectiveness of our distillation framework, we\nconduct ablation studies by removing Shared-Context Dis-\ntillation and Local-Global Distillation in Table 2. Without\nboth components, the model degrades to a conventional dis-\ntillation setup, resulting in significantly lower performance.\nIntroducing Shared-Context Distillation with Hybrid Nor-\nmalization notably improves accuracy, highlighting the bene-\nfits of a better normalization strategy with consistent context\nsupervision. When using only Local-Global Distillation,\nthe model still performs well, showing the effectiveness of\nregion-wise depth refinement even without global context in-\nformation. Combining both strategies yields the best results,\nTable 1: Analysis of Normalization Strategies. Perfor-\nmance comparison of different normalization strategies\nacross Shared-Context Distillation and Local-Global Dis-\ntillation.\nMethod Normalization ETH3D DIODE\nAbsRel↓ AbsRel↓\nShared-Context\nDistillation\nGlobal Norm. 0.064 0 .259\nNo Norm. 0.057 0.239\nLocal Norm. 0.070 0 .245\nHybrid Norm. 0.057 0.238\nLocal-Global\nDistillation\nGlobal Norm. 0.065 0 .239\nNo Norm. 0.273 0 .300\nLocal Norm. 0.076 0 .244\nHybrid Norm. 0.064 0.238\nconfirming that both components contribute significantly\nto improving the student model’s ability to utilize pseudo-\nlabels, demonstrating the robustness of our approach.\nCross-Architecture Distillation. To highlight the limita-\ntions of previous state-of-the-art distillation approaches em-\nploying global normalization, we compare their performance\nagainst the Hybrid Normalization strategy, which we utilize\nin our distillation framework, across diverse model architec-\ntures. To demonstrate the generalizability of our approach,\nTable 2: Effect of Cross-context Distillation. Performance\ncomparison of various combinations of Shared-Context Dis-\ntillation and Local-Global distillation on the ETH3D [ 42]\nand DIODE [ 45] datasets. The baseline corresponds to a\nsimple shared-context approach with no random cropping.\nWhen neither method is applied, the model defaults to this\nbaseline.\nShared-Context\nDistillation\nLocal-Global\nDistillation\nETH3D DIODE\nAbsRel↓ AbsRel↓\n✗ ✗ 0.075 0.270\n✗ ✓ 0.064(−14.6%) 0.238(−13.3%)\n✓ ✗ 0.058(−22.6%) 0.237(−12.2%)\n✓ ✓ 0.056(−25.3%) 0.232(−14.1%)\nTable 3: Comparison in Cross-Architecture Distillation.\nEvaluation of our distillation pipeline in the context of Cross-\nArchitecture Distillation. We adopt different architectures as\nteacher and student models, where the Base represents the\nprevious distillation method [51]. Our method consistently\nimproves the performance of the distilled student models.\nTeacher Student Training\nLoss\nDIODE ETH3D\nAbsRel↓ AbsRel↓\nDA-L DA-S Base 0.290 0.110\nOurs 0.262(−9.6%) 0.098(−10.9%)\nDA-L Midas-L Base 0.313 0.147\nOurs 0.295(−5.7%) 0.126(−14.3%)\nMidas-L Midas-S Base 0.303 0.150\nOurs 0.272(−10.2%) 0.120(−20.0%)\nwe conduct cross-architecture distillation experiments on\nboth the state-of-the-art DepthAnything [ 51] and the clas-\nsic MiDaS [ 37] architecture. Experiments are conducted\nusing MiDaS [37] and DepthAnything [51] in four configu-\nrations (DA-L, MiDaS-L, DA-S, MiDaS-S), as shown in Ta-\nble 3. Our method consistently outperforms previous global\nnormalization-based distillation on both the DIODE [45] and\nETH3D [42] datasets. These results demonstrate superior\nperformance both within and across architectures, underscor-\ning the limitations of global normalization in pseudo-label\ndistillation.\nEffect of Assistant-Guided Distillation. To validate the\neffectiveness of our proposedassistant-guided distillation\nstrategy, we design a comparative experiment that intro-\nduces an additional assistant model to the conventional\nteacher-student distillation framework. Specifically, we\nadopt DepthAnything v2 as the primary teacher and Gen-\nPercept—a diffusion-based model—as the assistant. The\nstudent model shares the same architecture as DAv2-Large\nand is initialized with its pre-trained weights. This setup al-\nlows us to investigate whether supervision from two diverse\nTable 4: Effect of Assistant-Guided Distillation. Bold\nvalues indicate the best performance. Our method inte-\ngrates a primary teacher, DepthAnything v2 (denoted as\n‘D’), with a diffusion-based assistant, GenPercept (denoted\nas ‘G’), leveraging their complementary strengths to produce\nhigher-quality pseudo-labels. The student model, trained un-\nder this assistant-guided distillation framework, consistently\nachieves better accuracy than when distilled from the DAv2-\nLarge teacher alone.\nMethod Assistant-Guided\nStrategy\nETH3D DIODE\nAbsRel↓ AbsRel↓\nDepthAnything v2 w/o 0.131 0.262\nGenpercept(Disparity) w/o 0.096 0.226\nD + G Avg. 0.228 0.371\nD + G Select. 0.054 0.258\narchitectures—trained under different paradigms—can offer\ncomplementary guidance that enhances both generalization\nand depth estimation performance. To explore the most\neffective way to combine pseudo-labels from the primary\nteacher and the assistant, we compare two assistant-guided\nstrategies: (1) a weighted averaging approach (Avg.), which\nassigns greater weight to pixels where the two teachers ex-\nhibit high agreement, and (2) a selection-based strategy (Se-\nlect.), which probabilistically samples the supervision signal\nfrom either teacher. While the averaging strategy attempts\nto leverage consistency between teachers, it often performs\npoorly due to conflicting pseudo-labels, where averaging\ncan amplify errors. In contrast, the selection-based strategy\nallows the student to selectively absorb the strengths of each\nteacher, avoiding error reinforcement. As shown in Table 4,\nthe Select. strategy significantly outperforms both individual\nteachers and the averaging method on the ETH3D bench-\nmark, demonstrating the effectiveness ofassistant-guided\ndistillation in delivering robust and diverse supervision.\n4.3. Comparison with State-of-the-Art\nQuantitative Analysis. As shown in Table 5, our method\nachieves state-of-the-art performance across a diverse range\nof zero-shot depth estimation benchmarks. These in-\nclude both structured indoor scenes (e.g., NYUv2 [ 43],\nScanNet [8]) and challenging outdoor environments (e.g.,\nKITTI [ 14], DIODE [ 45], ETH3D [ 42]), demonstrating\nstrong generalization across domains with varying scene\nstructures, lighting conditions, and depth statistics. To fur-\nther validate the effectiveness and scalability of our distilla-\ntion framework, we conduct evaluations on two representa-\ntive model architectures: DepthAnythingv2, a recent state-\nof-the-art model based on DINOv2, and MiDaS, a classic\nand widely adopted encoder-decoder framework. For each\nsetup, the student model is initialized with the correspond-\ning pre-trained encoder and distilled using pseudo-labels\nTable 5: Quantitative comparison with other affine-invariant depth estimators on several zero-shot benchmarks. The\nbold values indicate the best performance, and underscored represent the second-best results.\nMethod NYUv2 KITTI DIODE ScanNet ETH3D\nAbsRel ↓ δ1↑ AbsRel ↓ δ1↑ AbsRel ↓ δ1↑ AbsRel ↓ δ1↑ AbsRel ↓ δ1↑\nDiverseDepth [55] 0.117 0.875 0.190 0.704 0.376 0.631 0.108 0.882 0.228 0.694\nMiDaS [37] 0.111 0.885 0.236 0.630 0.332 0.715 0.111 0.886 0.184 0.752\nLeReS [47] 0.090 0.916 0.149 0.784 0.271 0.766 0.095 0.912 0.171 0.777\nOmnidata [10] 0.074 0.945 0.149 0.835 0.339 0.742 0.077 0.935 0.166 0.778\nHDN [59] 0.069 0.948 0.115 0.867 0.246 0.780 0.080 0.939 0.121 0.833\nDPT [39] 0.098 0.903 0.100 0.901 0.182 0.758 0.078 0.938 0.078 0.946\nDepthAnything v2 [50] 0.045 0.979 0.074 0.946 0.262 0.754 0.042 0.978 0.131 0.865\nGenPercept [49] 0.058 0.969 0.080 0.934 0.226 0.741 0.063 0.960 0.096 0.959\nMarigold [25] 0.055 0.961 0.099 0.916 0.308 0.773 0.064 0.951 0.065 0.960\nMiDaS v3.1 [3] - 0.980 - 0.949 - - - - 0.061 0.968\nOurs † 0.046 0.985 0.063 0.972 0.142 0.788 0.049 0.980 0.057 0.976\nOurs ∗ 0.043 0.981 0.070 0.949 0.233 0.753 0.043 0.980 0.054 0.981\n† Cross-Context distillation applied to MiDaS v3.1, using a pre-trained MiDaS v3.1 model as the teacher.\n∗ Cross-Context distillation applied to DepthAnythingv2-Large, using a pre-trained DAv2 model as the teacher.\ngenerated by the teacher model. Our approach yields con-\nsistent improvements over both teacher models across all\nbenchmarks, highlighting its effectiveness in learning from\npseudo-labels. Notably, it establishes new state-of-the-art\nresults in most cases, outperforming existing affine-invariant\ndepth estimators and demonstrating the robustness of our\nmethod in both DAv2 and MiDaS settings. These results\nconfirm that our distillation framework is broadly applicable\nand can effectively transfer knowledge across model scales\nand depth distributions, enabling the student model to sur-\npass its teacher in both accuracy and generalization under\nzero-shot evaluation.\nQualitative analysis. We present a qualitative comparison\nof depth estimations from different models in Fig. 6, in-\ncluding recent state-of-the-art approaches and our student\nmodel, which shares the same architecture as DAv2 but is\ntrained using our distillation framework. Compared with\nDAv2 [51], our method clearly preserves finer structural de-\ntails—particularly in regions highlighted by arrows—thanks\nto the proposed cross-context distillation strategy and the as-\nsistant model’s enhanced detail perception. While diffusion-\nbased MDE methods such as Marigold [ 24] and GenPer-\ncept [49] generate visually rich depth maps by leveraging\ngenerative priors, they are trained on a limited amount of\nsynthetic data, which hinders their ability to maintain cor-\nrect relative depth ordering in real-world scenes. This issue\narises from the inherent stochasticity and creativity of their\ngeneration paradigms, which, although capable of producing\naccurate depth ordering in certain regions, may introduce\ninconsistencies in others. In contrast, our student model\neffectively balances detail preservation and structural con-\nsistency. with shared-context distillation and local-global\ndistillation, it achieves more reliable and robust depth estima-\ntions that are both locally detailed and globally consistent.\n5. Conclusion\nIn this work, we investigate pseudo-label distillation\nstrategies for MDE. We observe that the commonly used\nglobal normalization scheme tends to amplify noise in\nteacher-generated pseudo-labels, thereby impairing local\ndepth accuracy. To address this issue, we propose Cross-\nContext Distillation, which combines local refinement with\nglobal consistency through a more effective normaliza-\ntion strategy. This enables the model to learn both fine-\ngrained details and high-level structural context. Further-\nmore, ourassistant-guided distillation framework integrates\ndiffusion-based generative priors as complementary guid-\nance to traditional encoder-decoder networks, achieving\nstate-of-the-art performance across multiple benchmarks.\nReferences\n[1] Ali Jahani Amiri, Shing Yan Loo, and Hong Zhang. Semi-\nsupervised monocular depth estimation with left-right consis-\ntency using deep neural network. 2019 IEEE International\nConference on Robotics and Biomimetics (ROBIO) , pages\n602–607, 2019. 3\n[2] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,\nand Matthias M¨uller. Zoedepth: Zero-shot transfer by com-\nbining relative and metric depth. arXiv preprint arXiv:\n2302.12288, 2023. 3\n[3] Reiner Birkl, Diana Wofk, and Matthias M¨uller. Midas v3.1 –\na model zoo for robust monocular relative depth estimation,\n2023. 7, 9\n[4] Aleksei Bochkovskii, Ama¨el Delaunoy, Hugo Germain, Mar-\ncel Santos, Yichao Zhou, Stephan R. Richter, and Vladlen\nKoltun. Depth pro: Sharp monocular metric depth in less than\na second. arXiv, 2024. 3, 13\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 9650–9660, 2021. 2\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Rethinking atrous con-\nvolution for semantic image segmentation. arXiv preprint\narXiv:1706.05587, 2017. 3\n[7] Jaehoon Cho, Dongbo Min, Youngjung Kim, and Kwanghoon\nSohn. A large rgb-d dataset for semi-supervised monocular\ndepth estimation. arXiv preprint arXiv: 1904.10230, 2019. 3\n[8] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nießner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 5828–5839. IEEE, 2017. 6, 8, 13\n[9] John Doe and Jane Smith. Patchfusion: Multi-scale feature\nfusion for enhanced depth estimation. International Journal\nof Computer Vision, 131:1234–1250, 2023. 3, 13\n[10] Adnan Eftekhar, Mate Balog, et al. Omnidata: A pipeline\nfor building synthetic data of complex 3d scenes. In Proceed-\nings of the IEEE/CVF International Conference on Computer\nVision (ICCV), 2021. 9, 13\n[11] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map\nprediction from a single image using a multi-scale deep net-\nwork. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 2366–2374, 2014. 1, 3\n[12] Huazhu Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-\nmanghelich, and Dacheng Tao. Deep ordinal regression net-\nwork for monocular depth estimation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 2002–2011, 2018. 3\n[13] Ravi Garg, Vijay Kumar BG, Gustavo Carneiro, and Ian Reid.\nUnsupervised learning of depth and ego-motion from video.\nIn European Conference on Computer Vision, pages 556–573.\nSpringer, 2016. 1\n[14] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Are we ready for autonomous driving? the kitti\nvision benchmark suite. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 3354–3361.\nIEEE, 2012. 6, 8, 13\n[15] Cl´ement Godard, Oisin Mac Aodha, Michael Firman, and\nGabriel J Brostow. Monodepth2: Self-supervised monocular\ndepth estimation with left-right consistency. In Proceedings\nof the IEEE International Conference on Computer Vision\n(ICCV), pages 168–176, 2019. 3\n[16] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J Brostow.\nUnsupervised monocular depth estimation with left-right con-\nsistency. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 270–279, 2017.\n3\n[17] Ming Gui, Johannes Schusterbauer, Ulrich Prestel, Pingchuan\nMa, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas\nBaumann, Vincent Tao Hu, and Bj¨orn Ommer. Depthfm: Fast\nmonocular depth estimation with flow matching, 2024. 3\n[18] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, and Adrien\nGaidon. 3d packing for self-supervised monocular depth\nestimation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 2485–2494,\n2020. 1\n[19] Vitor Guizilini, Jie Li, Rares Ambrus, Sudeep Pillai, and\nAdrien Gaidon. Robust semi-supervised monocular depth\nestimation with reprojected distances. In Conference on robot\nlearning, pages 503–512. PMLR, 2020. 3\n[20] Lukas Hoyer, Dengxin Dai, Qin Wang, Yuhua Chen, and Luc\nVan Gool. Improving semi-supervised and domain-adaptive\nsemantic segmentation with self-supervised depth estimation.\nInternational Journal of Computer Vision, 131(8):2070–2096,\n2023. 3\n[21] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\nworks. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 7132–7141,\n2018. 3\n[22] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long,\nHao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and\nShaojie Shen. Metric3d v2: A versatile monocular geomet-\nric foundation model for zero-shot metric depth and surface\nnormal estimation. arXiv preprint arXiv:2404.15506, 2024. 3\n[23] Rongrong Ji, Ke Li, Yan Wang, Xiaoshuai Sun, Feng Guo,\nXiaowei Guo, Yongjian Wu, Feiyue Huang, and Jiebo Luo.\nSemi-supervised adversarial monocular depth estimation.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 42:2410–2422, 2020. 3\n[24] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Met-\nzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurpos-\ning diffusion-based image generators for monocular depth\nestimation. 2024. 1, 3, 6, 7, 9, 13\n[25] Qianli Ke, Hanxiao Lu, Yingcong Zhang, et al. Marigold:\nMulti-modal 3d perception with diffusion models. arXiv\npreprint arXiv:2402.04567, 2024. 9\n[26] Alexander Kirillov, Eric Mintun, et al. Sa-1b: Seg-\nment anything 1-billion mask dataset. https://\nsegment-anything.com, 2023. 6, 13\n[27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and\nRoss Girshick. Segment anything. arXiv preprint arXiv:\n2304.02643, 2023. 2\n[28] Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semi-\nsupervised deep learning for monocular depth map prediction.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 6647–6655, 2017. 3\n[29] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-\nerico Tombari, and Nassir Navab. Deeper depth prediction\nwith fully convolutional residual networks. In Proceedings\nof the Fourth International Conference on 3D Vision (3DV),\npages 239–248, 2016. 3\n[30] Yanhua Li, Qixing Zhang, and Liqian Zhang. Ar shadow:\nReal-time 3d object tracking and shadow rendering for mobile\naugmented reality. IEEE Transactions on Visualization and\nComputer Graphics, 26(9):2871–2881, 2020. 1\n[31] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,\nDaniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A\nlarge dataset to train convolutional networks for disparity,\noptical flow, and scene flow estimation. In Proceedings of the\nIEEE conference on computer vision and pattern recognition,\npages 4040–4048, 2016. 1\n[32] S. M. H. Miangoleh, Sebastian Dille, Long Mai, Sylvain Paris,\nand Ya˘gız Aksoy. Boosting monocular depth estimation mod-\nels to high-resolution via content-adaptive multi-resolution\nmerging. Computer Vision and Pattern Recognition, 2021. 3\n[33] Maxime Oquab, Timoth´ee Darcet, Theo Moutakanni, Huy V .\nV o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell\nHowes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li,\nWojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas,\nGabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal,\nPatrick Labatut, Armand Joulin, and Piotr Bojanowski. Di-\nnov2: Learning robust visual features without supervision.\nTMLR, 2024. 7\n[34] Andra Petrovai and Sergiu Nedevschi. Exploiting pseudo\nlabels in a self-supervised learning framework for improved\nmonocular depth estimation. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages\n1578–1588, 2022. 3\n[35] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia\nSegu, Siyuan Li, Luc Van Gool, and Fisher Yu. UniDepth:\nUniversal monocular metric depth estimation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2024. 3\n[36] Pierluigi Zama Ramirez, Matteo Poggi, Fabio Tosi, S. Mat-\ntoccia, and L. D. Stefano. Geometry meets semantics for\nsemi-supervised monocular depth estimation. Asian Confer-\nence on Computer Vision, 2018. 3\n[37] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. To-\nwards robust monocular depth estimation: Mixing datasets\nfor zero-shot cross-dataset transfer. In IEEE Transactions on\nPattern Analysis and Machine Intelligence (TPAMI), 2020. 1,\n3, 4, 6, 8, 9, 13\n[38] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 12179–12188, 2021. 2, 3\n[39] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 9\n[40] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 44(3):1623–1637, 2020. 2\n[41] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Ku-\nmar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and\nJoshua M. Susskind. Hypersim: A photorealistic synthetic\ndataset for holistic indoor scene understanding. In ICCV,\n2021. 13\n[42] Thomas Sch¨ops, Torsten Sattler, and Marc Pollefeys. A multi-\nview stereo benchmark with high-resolution images and multi-\ncamera videos. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 3260–3269. IEEE, 2017.\n6, 8, 13\n[43] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\nFergus. Indoor segmentation and support inference from\nrgbd images. In European Conference on Computer Vision\n(ECCV), pages 746–760. Springer, 2012. 6, 8, 13\n[44] Nikolai Smolyanskiy, Alexey Kamenev, and Stan Birchfield.\nOn the importance of stereo for accurate depth estimation:\nAn efficient semi-supervised deep neural network approach.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition workshops, pages 1007–1015, 2018.\n3\n[45] Igor Vasiljevic, Ayan Chakrabarti, Vladlen Koltun, and Jack\nTumblin. Diode: A dense indoor and outdoor depth dataset. In\nIEEE International Conference on Computer Vision (ICCV),\npages 896–905. IEEE, 2019. 6, 8, 13\n[46] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang,\nYu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking\naccurate monocular geometry estimation for open-domain\nimages with optimal training supervision, 2024. 3, 13, 16\n[47] Zhenyu Wei, Andreas Geiger, et al. Leres: Learning-based\nmonocular depth estimation for all scenes. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) Workshops, 2021. 1, 9\n[48] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le.\nSelf-training with noisy student improves imagenet classifica-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 10687–10698,\n2020. 2\n[49] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan,\nKangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen.\nWhat matters when repurposing diffusion models for general\ndense perception tasks? arXiv preprint arXiv:2403.06090,\n2024. 1, 3, 6, 7, 9, 13\n[50] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi\nFeng, and Hengshuang Zhao. Depth anything: Unleashing\nthe power of large-scale unlabeled data. 2024. 2, 3, 9\n[51] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao-\ngang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything\nv2. arXiv preprint arXiv:2406.09414, 2024. 1, 2, 3, 6, 7, 8, 9,\n13, 14\n[52] Nan Yang, Rui Wang, J ¨org St ¨uckler, and Daniel Cremers.\nD3vo: Deep depth, deep pose and deep uncertainty for monoc-\nular visual odometry. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n1281–1292, 2020. 1\n[53] Nan Yang, Rui Wang, J. St ¨uckler, and D. Cremers. Deep\nvirtual stereo odometry: Leveraging deep depth prediction for\nmonocular direct sparse odometry. European Conference on\nComputer Vision, 2018. 3\n[54] Weicong Yin, Jianping Shi, and Yao Feng. Learning to re-\ncover 3d scene shape from a single image. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2042–2051, 2021. 1\n[55] Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian,\nSongcen Xu, Changming Sun, and Renyin Dou. Diversedepth:\nAffine-invariant depth prediction using diverse data. arXiv\npreprint arXiv:2002.00569, 2020. 1, 2, 3, 9\n[56] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaix-\nuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d:\nTowards zero-shot metric 3d prediction from a single image.\nIEEE International Conference on Computer Vision, 2023. 3,\n6, 13\n[57] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long\nMai, Simon Chen, and Chunhua Shen. Learning to recover\n3d scene shape from a single image. Computer Vision and\nPattern Recognition, 2020. 3\n[58] Ge Yongtao, Xu Guangkai, Zhao Zhiyue, Huang zheng, Sun\nlibo, Sun Yanlong, Chen Hao, and Shen Chunhua. Geobench:\nBenchmarking and analyzing monocular geometry estimation\nmodels. arXiv preprint arXiv:2406.12671, 2024. 3\n[59] Chi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu, and\nChunhua Shen. Hierarchical normalization for robust monoc-\nular depth estimation. In Advances in Neural Information\nProcessing Systems 35: Annual Conference on Neural Infor-\nmation Processing Systems 2022, NeurIPS 2022, New Or-\nleans, LA, USA, November 28 - December 9, 2022, 2022. 3,\n4, 9\n[60] Chi Zhang, Wei Yin, Gang Yu, Zhibin Wang, Tao Chen, Bin\nFu, Joey Tianyi Zhou, and Chunhua Shen. Robust geometry-\npreserving depth estimation using differentiable rendering.\nIEEE International Conference on Computer Vision, 2023. 3\n[61] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 2881–2890, 2017. 3\n[62] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G\nLowe. Unsupervised learning of depth and ego-motion from\nvideo. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 1851–1858,\n2017. 3\n[63] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, and\nQuoc V Le. Rethinking pre-training and self-training. In Ad-\nvances in Neural Information Processing Systems, volume 33,\npages 3833–3845, 2020. 2\nA. Appendix\nB. Dataset Details\nB.1. Datasets.\nOur model is trained on SA-1B [26], a large-scale dataset\ncomprising high-quality RGB images of diverse indoor and\noutdoor scenes. These high-fidelity images facilitate the\ngeneration of more detailed pseudo-labels, enabling robust\ndepth estimation and fine-grained detail learning for real-\nworld applications. SA-1B [26] is also the dataset employed\nin DAv2 [51]. For evaluation, we use established monocular\ndepth benchmarks:\n• NYUv2 [43]: Indoor depth estimation and semantic\nsegmentation dataset. We evaluate on the official test\nsplit of 654 samples.\n• KITTI [14]: Autonomous driving dataset with outdoor\nscenes and high-quality LiDAR ground truth depth. Fol-\nlowing prior work, we use the 697-image test split.\n(Corrected number of images based on standard KITTI\nbenchmark)\n• ETH3D [42]: High-resolution stereo images for indoor\nand outdoor depth estimation and 3D reconstruction.\nWe evaluate on all 454 images.\n• ScanNet [8]: Large-scale RGB-D dataset for 3D scene\nreconstruction and semantic segmentation. We use a\ntest split of 1000 samples.\n• DIODE [45]: Dense, high-quality depth maps for both\nindoor and outdoor environments. However, as noted\nin MoGe [46], this dataset exhibits artifacts in the depth\nvalues of the ground truth near the boundaries of the\nobject.\nFor visualization, we also use images from Dav2 [ 51],\nGenPercept [ 49], PatchFusion [ 9], Hypersim [ 41], Om-\nnidata [10], Depth Pro [4] and Gen-3.\nB.2. Metrics.\nWe evaluate depth estimation using mean absolute relative\nerror (AbsRel) and δ1 accuracy. AbsRel is defined as:\nAbsRel = 1\nM\nMX\ni=1\n|di − d∗\ni |\nd∗\ni\n(10)\nwhere di is the predicted depth, d∗\ni is the ground truth, and\nM is the total number of depth values. δ1 accuracy measures\nthe percentage of pixels where:\nδ1 = max\n\u0012di\nd∗\ni\n, d∗\ni\ndi\n\u0013\n< 1.25 (11)\nindicating prediction accuracy within a specific tolerance.\nFollowing Metric3D [37, 56, 24], we align predictions with\nground truth in scale and shift before evaluation.\nC. More Experiments\nC.1. Implementation Details.\nFor visualization, our model uses Dav2 as the student\nmodel and is fine-tuned with Dav2 parameters as the pre-\ntrained weights. Since our training iterations and dataset size\nare relatively small, leveraging the strong prior knowledge of\nDav2 allows us to achieve significant visual improvements\nquickly. Regarding Table 5, the model is fine-tuned with\nDav2 but only uses the backbone parameters from Dav2.\nOther components, such as the DPT head, are initialized\nrandomly. We found that training entirely with Dav2’s\npre-trained parameters does not directly demonstrate the\neffectiveness of our method. By retaining only the encoder\nand training the decoder from scratch, the accuracy clearly\nshows the improvement in pseudo-label utilization due to\nour normalization strategy, as well as the effectiveness of\nour cross-context distillation approach.\nC.2. Effect of Data Scaling.\nTo investigate the impact of dataset size on model perfor-\nmance, we conducted experiments with progressively larger\ntraining sets and compared our method against the SSI Loss\nbaseline across five popular benchmarks. The results are\naveraged over these benchmarks. As shown in Fig. 7, we\nreport the Absolute Relative Error (AbsRel) as the dataset\nsize increases from 10K to 200K images. Our distillation\npipeline consistently outperforms the traditional SSI-based\nglobal normalization approach across all dataset sizes. No-\ntably, the performance gap between our method and the\nbaseline widens as more training data is introduced. More-\nover, our approach enables the student model to surpass the\nteacher’s performance using significantly less training data,\nhighlighting its data efficiency.\nC.3. Distilling Generative Models vs. DepthAny-\nthingv2.\nBeyond distilling encoder-decoder depth models, we ex-\ntend our approach to generative models, specifically GenPer-\ncept [49], aiming to transfer their superior detail preservation\nto a more efficient student model. While diffusion-based\ndepth estimators achieve fine-grained depth reconstruction,\ntheir high computational cost limits practical applications.\nWe investigate whether their depth estimation capability can\nbe effectively distilled into a lightweight DPT-based model.\nExperimental results in Fig. 8 show that compared to using\nDepthAnythingv2 as the teacher, distilling from a diffusion-\nbased model yields a student model with significantly en-\nhanced fine-detail prediction.\nC.4. Qualitative Comparison with Baseline Distil-\nlation.\nWe present a qualitative comparison between our method\nand the previous distillation method [ 51], where the Base\nmodel relies solely on global normalization. We analyze\nthe depth map details and the distribution differences be-\ntween predicted and ground truth depths. The red diagonal\nlines represent the ground truth, with results closer to these\nlines indicating better performance. As shown in Fig. 9,\nour method produces smoother surfaces, sharper edges, and\nmore detailed depth maps.\nC.5. Additional Results on 3D reconstruction in the\nWild.\nBenefiting from MoGe’s advances in geometry-\npreserving depth estimation, we align the relative depth pre-\ndicted by our model with MoGe’s outputs. Using the camera\nparameters estimated by MoGe, we project the aligned depth\ninto 3D space to obtain visualizable point clouds. As shown\nin Fig. 10, these visualizations demonstrate the effectiveness\nand practical applicability of our model in unconstrained,\nreal-world scenarios. Remarkably, our method performs\nwell even on stylized or synthetic content, such as anime-\nstyle images, making it potentially useful for downstream\ntasks like virtual character modeling. Similarly, our model\ngenerates high-quality reconstructions for images from game\nengines. In real-world photographs—captured by consumer\ndevices such as smartphones—the reconstructed point clouds\npreserve meaningful geometric structures and fine details.\nFurthermore, even in abstract scenes like sketches with miss-\ning visual cues, our model can infer plausible relative depth\nand recover a semantically coherent 3D layout of the entire\nscene.\nC.6. Qualitative Comparison: Additional Results\non Depth Estimation in the Wild.\nAs shown in Fig. 11, our model demonstrates strong gen-\neralization and robustness across a wide range of scenarios,\nincluding real-world indoor and outdoor environments, styl-\nized virtual content such as anime and game engine renders,\nand information-sparse inputs like sketches or line draw-\nings. Even in unconventional perspectives such as bird’s-eye\ncityscapes, the model preserves accurate relative depth and\nstructural coherence. These results highlight its ability to\ndeliver detailed and semantically meaningful depth predic-\ntions in both natural and synthetic domains, enabling practi-\ncal applications in 3D reconstruction, content creation, and\ndownstream tasks across real and virtual worlds.\nFigure 7: Comparison of Data Scaling . Perfor-\nmance comparison of our model with SSI Loss as\nthe dataset size increases, measured by the average\nAbsRel. The results indicate that our method con-\nsistently outperforms the baseline method.\nDav2 as Teacher Genpercept as Teacher\nRGB\nFigure 8: Distilled Generative Models: Instead of just distilling classical depth models, we also apply distillation to diffusion-\nbased generative models, aiming for the student model to learn the rich details inherent in these models, which are often not\nfully reflected in standard accuracy metrics.\nPredicted Depth\nInputs Base Ours Base Ours\nGround Truth Ground Truth\nGround Truth\nGround Truth\nGround Truth\nGround Truth\nGround Truth\nGround Truth\nPredicted DepthPredicted DepthPredicted Depth\nPredicted DepthPredicted DepthPredicted DepthPredicted Depth\nFigure 9: Qualitative Comparison with Baseline Distillation. We compare our method with the baseline as the previous\ndistillation method, which uses only global normalization. The red diagonal lines represent the ground truth, with results\ncloser to the lines indicating better performance. Our method produces smoother surfaces, sharper edges, and more detailed\ndepth maps.\nRGB Front View Top View Left View\nFigure 10: Additional results on 3D reconstruction from in-the-wild RGB images. We present point clouds generated from\nour model’s predicted depth maps, aligned with geometry-preserving depth from MoGe [46]. These visualizations demonstrate\nthe effectiveness and practical applicability of our model in unconstrained, real-world scenarios.\nRGB Depth Map RGB RGBDepth Map Depth Map\nFigure 11: Additional Results on Depth Estimation in the Wild. We showcase more depth maps generated by our model on\nin-the-wild scenes, highlighting its robustness and precision."
  },
  {
    "source": "2407.02485v1.pdf",
    "content": "RankRAG: Unifying Context Ranking with\nRetrieval-Augmented Generation in LLMs\nYue Yu∗\nGeorgia Tech\nWei Ping ∗\nNVIDIA\nZihan Liu\nNVIDIA\nBoxin Wang\nNVIDIA\nJiaxuan You\nNVIDIA\nChao Zhang\nGeorgia Tech\nMohammad Shoeybi\nNVIDIA\nBryan Catanzaro\nNVIDIA\nAbstract\nLarge language models (LLMs) typically utilize the top-k contexts from a retriever\nin retrieval-augmented generation (RAG). In this work, we propose a novel instruc-\ntion fine-tuning framework RankRAG, which instruction-tunes a single LLM for\nthe dual purpose of context ranking and answer generation in RAG. In particular,\nthe instruction-tuned LLMs work surprisingly well by adding a small fraction\nof ranking data into the training blend, and outperform existing expert ranking\nmodels, including the same LLM exclusively fine-tuned on a large amount of\nranking data. For generation, we compare our model with many strong baselines,\nincluding GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced\nmodel with the state-of-the-art performance on RAG benchmarks. Specifically,\nour Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4\nmodels on nine knowledge-intensive benchmarks. In addition, it also performs\ncomparably to GPT-4 on five RAG benchmarks in the biomedical domain without\ninstruction fine-tuning on biomedical data, demonstrating its superb capability for\ngeneralization to new domains.\n1 Introduction\nRetrieval-augmented generation (RAG) (Lewis et al., 2020; Izacard & Grave, 2021; Lin et al., 2024;\nWang et al., 2024) is a widely used technique for customizing large language models (LLMs) to handle\nlong-tail knowledge (Mallen et al., 2023; Asai et al., 2024b), provide up-to-date information (Kasai\net al., 2023), and adapt to specific domains and tasks (Xiong et al., 2024) without modifying the\nmodel weights. In general, a dense embedding based retriever (Karpukhin et al., 2020; Lin et al.,\n2023; Wang et al., 2022) first retrieves top-k chunked contexts from a collection documents or external\ndatabase for a given question. Then, LLM reads the top-k contexts to generate the answer.\nHowever, the current RAG pipeline has the following limitations:i) LLMs are not good at reading too\nmany chunked contexts (e.g., top-100) even with the long-context window, not only due to efficiency\nreasons, but also because a shorter list of top-k (e.g., 5, 10) contexts usually leads to higher accuracy\nof generation (e.g., see Table 5 in Xu et al., 2024b). ii) Given a small k, one needs a mechanism to\nensure the high recall of relevant contents. Relying solely on a retrieval model may be inadequate\ndue to challenges in learning effective local alignments across the entire embedding space to support\naccurate matching (Luan et al., 2021). In practice, a separate ranking model (Nogueira et al., 2020;\nGlass et al., 2022; Ma et al., 2023) that cross-encodes question and candidate context can work better\nthan a dense embedding-based retriever for obtaining the most relevant top-k contexts from top-N\ncandidates (N ≫ k). iii) However, the zero-shot generalization capability of the expert ranking model\ncan be relatively limited compared to the versatile LLM itself.\n∗Yue Yu did this work during an internship at NVIDIA. Correspondence to: Yue Yu <yueyu@gatech.edu>,\nWei Ping <wping@nvidia.com>.\narXiv:2407.02485v1  [cs.CL]  2 Jul 2024\nBased on the above considerations, our goal is to design an RAG instruction tuning pipeline that\nuses a single language model to achieve both high-recall context extraction and high-quality content\ngeneration. In previous study, instruction-tuned LLMs demonstrate a strong ability to extract answers\nfrom relevant context for a given question (e.g., OpenAI, 2023; Liu et al., 2024; Lin et al., 2024).\nThis capability can be viewed as the “dual capability” of determining whether a chunk of context\nis relevant to the question thus is useful for generating the answer. We hypothesize that these\ncapabilities mutually enhance each other. Motivated by this insight, we propose RankRAG, which\nintruction-tunes a single LLM for both context ranking and answer generation in RAG framework.\nFurthermore, RankRAG expands upon existing instruction-tuning data by incorporating context-rich\nQA, retrieval-augmented QA and ranking datasets, enhancing the LLM’s ability to filter out irrelevant\ncontexts during both the retrieval and generation phases of RAG.\nOur contribution can be summarized as follows:\n• We propose RankRAG, a novel framework that enhances LLM’s RAG capability through simul-\ntaneously instructing the LLM on context ranking and answer generation. During training, we\ndesign a specialized task focused on identifying relevant contexts or passages for a given question.\nThis task is structured for ranking and framed as regular question answering with instruction,\naligning more effectively with retrieval-augmented generation tasks. At inference, the LLM first\nreranks the retrieved contexts, then generates answer based on the refined top- k (e.g., 5). This\nframework is readily applicable to diverse knowledge-intensive NLP tasks.\n• Remarkably, we observe that integrating a small fraction of ranking data into the instruction tuning\nblend of LLM works surprisingly well on the evaluations of ranking associated with the RAG\ntasks, even surpassing the LLMs fine-tuned with 10× more ranking data. We attribute this success\nto the transferable design of RankRAG training.\n• We extensively compare the proposed RankRAG method with several strong baselines, including\nthe open-sourced ChatQA-1.5. On nine general-domain and five biomedical knowledge-intensive\nbenchmarks for RAG, Llama3-RankRAG-8B and Llama3-RankRAG-70B outperforms Llama3-\nChatQA-1.5-8B and Llama3-ChatQA-1.5-70B by a margin, respectively.\nIn the remainder of the paper, we discuss related work in § 2. We introduce problem setup in § 3 and\nRankRAG method in § 4. We present the experimental setup in § 5, and conclude the paper in § 6.\n2 Related Work\nRetrieval-augumented generation (RAG) has been established for knowledge-intensive NLP\ntasks (Lewis et al., 2020; Borgeaud et al., 2022; Izacard et al., 2023; Izacard & Grave, 2021).\nIn the standard process, a standalone dense-embedding-based retriever (e.g., Karpukhin et al., 2020)\nfirst retrieves relevant information from an external corpus, which the LLM then utilizes in the\ngeneration process. To improve this pipeline, recent research has focused on aligning retrievers to\nthe needs of LLMs for generation (Shi et al., 2024; Lin et al., 2024), designing multi-step retrieval\nprocesses (Trivedi et al., 2023; Jiang et al., 2023; Jeong et al., 2024; Shao et al., 2023), or filtering\nirrelevant contexts (Wang et al., 2023c; Yoran et al., 2024; Xu et al., 2024a). To improve generation,\nseveral studies have designed instruction-tuning methods dedicated to enhancing the search (Ma et al.,\n2023; Zhu et al., 2024; Muennighoff et al., 2024) and RAG capability of LLMs (Liu et al., 2024; Lin\net al., 2024; Luo et al., 2023; Asai et al., 2024a; Wang et al., 2024).\nAlthough strong retrievers have been introduced (e.g., Lin et al., 2023; Yu et al., 2022; Wang et al.,\n2022, 2023a; Lee et al., 2024), one potential approach to improve retriever is optimizing it along with\nLLM in an end-to-end manner (e.g., Guu et al., 2020; Shi et al., 2024; Sachan et al., 2021; Izacard\net al., 2023). However, this requires surrogate loss for optimization and complicates the training\npipeline, especially when the embedding database needs to be re-indexed frequently due to the update\nof the embedding model (i.e., retriever).\nRanking serves as an intermediate step to improve the quality of information retrieval (Mitra et al.,\n2018), and has been applied to RAG pipeline for improving generation quality (Glass et al., 2022;\nRam et al., 2023). However, these methods still rely on an additional moderate-sized model (e.g.\nBERT, T5) for ranking, which is often insufficient to capture the relevance between query and contexts\nand may lack the zero-shot generalization capability. Although recent studies have demonstrated the\nstrong ability of LLMs at ranking tasks (Khalifa et al., 2023; Qin et al., 2024; Sun et al., 2023), how\nto harvest this ability for the RAG pipeline remains underexplored.\n2\n5 10 20\n40\n42Exact Match\nChatQA-1.5 8B\n5 10 2044\n46\n48ChatQA-1.5 70B\nContext Size k\n(a) NQ\n5 10 2078\n80\n82Exact Match\nChatQA-1.5 8B\n5 10 2082\n84\n86ChatQA-1.5 70B\nContext Size k (b) TriviaQA\n5 10 2046\n48\n50\n52\n54Exact Match\nChatQA-1.5 8B\n5 10 2048\n49\n50\n51\n52ChatQA-1.5 70B\nContext Size k (c) PopQA\n5 10 2089\n90\n91\n92Exact Match\nChatQA-1.5 8B\n5 10 2090\n91\n92\n93ChatQA-1.5 70B\nContext Size k (d) FEVER\nFigure 1: Performance of ChatQA-1.5, one of the strongest RAG model, on different context size k.\nWe observe a trade-off of selecting top-k contexts: a smaller k compromises the recall, while a larger\nk could introduce irrelevant or noisy context and mislead the LLM generation.\n3 Preliminaries\nIn this section, we first introduce the preliminaries of retrieval-augmented generation as well as the\nproblem setup. Then we present the limitations in the current RAG pipeline, which motivates the\nproposed RankRAG method.\n3.1 Problem Setup\nIn retrieval-augmented generation, a collection of documents or contexts (e.g. Wikipedia) is given,\nproviding the grounded knowledge. Given a question q, the retriever R (e.g., a parameterized\nembedding model) first retrieves top- k contexts C = {c1, ··· , ck} that are most relevant to the\nquestion. Subsequently, the language model produces the final answer where the answer can either\nbe a short phrase or a long sentence, depending on the type of the target task. Our focus is on\nautoregressive language models (OpenAI, 2022, 2023; Meta-AI, 2024), which is the most common\narchitectures for LLMs.\n3.2 Limitation of Current RAG Pipelines\nBefore formally introducing RankRAG, we would like to first pinpoint several limitations of the\ncurrent “retrieve-then-generate” pipeline with large language models.\nLimited Capacity of Retriever. Current RAG systems usually employ sparse retrieval (e.g.\nBM25 (Robertson et al., 2004)) or moderate-size (e.g. BERT-based) embedding model (Karpukhin\net al., 2020; Lin et al., 2023; Wang et al., 2022) as the retriever R, mainly due to efficiency con-\nsideration as there are often millions of, if not more, documents need to be indexed. These models\nencode questions and documents independently and calculate the similarity between question and\ndocuments using vector similarity metrics. However, the limited capacity of embedding model and\nindependent processing of query and documents constrain the ability to estimate textual relevance\nbetween question q and documents d, reducing their effectiveness in new tasks or domains, verified\nby both theoretical (Menon et al., 2022) and empirical (Luan et al., 2021; Thakur et al., 2021) studies.\nTrade-off of Picking Top-k Contexts. Although the state-of-the-art long-context LLM can take\nmany retrieved contexts as input for answer generation, the performance quickly saturates with\nincreased k in practice. For example, Xu et al. (2024b) finds the optimal number of chunked context\nk is around 10 for long document QA tasks. As illurstrated in Figure 1, we perform evaluation\non ChatQA-1.5 (Liu et al., 2024), one of the strongest RAG model with open weights, and find\nthe saturation of accuracy when k = 10. In general, a smaller k often fails to capture all relevant\ninformation, compromising the recall, given the limited expressibility of retriver. In contrast, a larger\nk improves recall but at the cost of introducing irrelevant content that hampers the LLM’s ability to\ngenerate accurate answers (Yoran et al., 2024; Yu et al., 2023b).\n4 RankRAG\nTo address the limitations mentioned in the previous section, we propose the RankRAG method\nto enhance the LLM’s ability for retrieval-augmented generation. Specifically, we instruction-tune\nthe LLM to simultaneously capture the relevance between the question and context and utilize the\nretrieved context for answer generation. The details are introduced as follows.\n3\nQueryCorpus\n1.2.……𝑁.\nTop-NDocs0.730.46……0.57\n1.2.…𝑘.\nTop-KDocs0.940.73…0.63Relevancescores\n1.2.……𝑁.\nTop-NDocsRetriever(e.g.,DPR,Dragon)\nRankRAG\nRerankGenerateAnswerSynthetic InstructionsSelf-Instruct,  Unnatural InstructionConversationSODA, Dolly,OpenAssistantLong-form QAELI5Chain-of-thoughtFLAN\nStage-I:SFT\nConversationSynthetic Conversation, HumanAnnotatedConvQARetrieval-augmentedQA&RankingSQuAD,WebQuestionReading ComprehensionNarrativeQA, DROP,Quoref, NewsQA,TAT-QA, ROPESContext RankingMS MarcoSynthetic Conversation\nStage-II:RankRAG Instruction-Tuning\nLLMInferenceTraining\nRankRAG\nNewlyIntroducedTasks\nFigure 2: Two-stage instruction tuning framework for RankRAG.\n4.1 Stage-I: Supervised Fine-Tuning (SFT)\nIt is observed that general instruction-tuning or supervised fine-tuning (SFT) often significantly\nimproves the ability of LLMs to follow instructions, thus improving zero-shot results on various\ndownstream tasks (Wei et al., 2022; Ouyang et al., 2022). As such, we follow existing works (Chung\net al., 2024; Wang et al., 2024; Liu et al., 2024) to first leverage SFT on a blend of high quality\ninstruction following datasets, including: i) a private crowd-sourced conversational dataset and\npublic conversation datasets: OpenAssistant (Köpf et al., 2023), Dolly (Conover et al., 2023), and\nSODA (Kim et al., 2023), ii) a long-form QA dataset ELI5 that requires elaborate answers (Fan et al.,\n2019), iii) LLM-generated instructions: Self-Instruct (Wang et al., 2023b) and Unnatural Instructions\n(Honovich et al., 2023), iv) FLAN and Chain-of-thought datasets (Chung et al., 2024).\nThere are overall 128K SFT examples in total. We make sure that there is no overlap between SFT\ndata and data from evaluation tasks. For each sample in the instruction-following dataset, we take the\nmulti-turn conversational format, use the previous turns of conversation between the user and the\nassistant as the context, and compute the loss only at the last response from the assistant.\n4.2 Stage-II: Unified Instruction-Tuning for Ranking and Generation\nThe Stage-I SFT enpowers the LLMs with basic instruction-following capabilities; however, their\nperformance on RAG tasks often remains suboptimal, as the LLMs are not optimized for extracting\nanswers from retrieved context for a given question. Although recent studies (Lin et al., 2024; Liu\net al., 2024; Zhang et al., 2024) enhance the RAG capability of LLM by instruction tuning it on\ncontext-rich generation tasks, these approaches can still be ineffective with poor initial retrieval\nresults. RankRAG instruction tunes the LLM for both retrieval-augmented generation and context\nranking. In particular, the context ranking capability is crucial to obtain more relevant top-k context\nwith imperfect retriever.\nTo achieve this goal, the instruction tuning blend of Stage-II consists the following five parts:\n1) SFT data from Stage-I. This part is included to maintain LLM’s instruction-following capability.\n2) Context-rich QA data. We first follow Liu et al. (2024) to leverage multiple context-rich QA tasks\nto enhance the LLM’s capability of using context for generation. The training blend we use consists of:\ni) standard QA and reading comprehension datasets: DROP (Dua et al., 2019), NarrativeQA (Koˇcisk`y\net al., 2018), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), NewsQA (Trischler et al.,\n2017), TAT-QA (Zhu et al., 2021), which contains a question,a golden context and an answer. ii)\nconversational QA datasets: HumanAnnotatedConvQA and SyntheticConvQA open-sourced by Liu\net al. (2024), which contains a conversation between user and assistant, as well as one background\ndocument. The model needs to generate an answer given the conversation history and document.\n3) Retrieval-augmented QA data. In addition to the above QA datasets used in Liu et al. (2024),\nwe add two datasets with not only gold context but also the top-retrieved context using BM25. Note\nthat, it is crucial to improve LLM’s robustness over irrelevant context at generation. Being aware of\nthis, we consider two QA tasks, namely SQuAD (Rajpurkar et al., 2016) and WebQuestions (Berant\net al., 2013). For each question with the answer, we combine the gold context with the top-retrieved\ncontexts using BM25, ensuring a total of five contexts. Note that some retrieved contexts may not\ncontain the answer, and could be the “hard-negative” contexts.\n4) Context ranking data. To empower LLMs with ranking capabilities, we use the popular\nMS MARCO passage (context) ranking dataset (Bajaj et al., 2016). We treat the gold query-passage\n4\nTable 1: The instruction template for Stage-II. It is worth noting that all the tasks can be unified in the\n(x, c, y) format, which is able to facilitate effective knowledge transfer across tasks.\nTask Questionx Contextc Answery\nContext-rich QA Answer the following question from context.{question}Passage:{Passage}(1 Psg.) A phrase/sentence\nRetrieval-augmented QAAnswer the following question from context.{question}Passage 1:{Passage 1}... A phrase/sentencePassage 5:{Passage 5}(5 Psg. total)\nContext ranking For the question{question}, access whether the passageis relevant to the question. Passage:{Passage}(1 Psg.) True/False\nRetrieval-augmented rankingFor the question{question}, find all passages from Passage 1:{Passage 1}... Passage Indexesthe context that are relevant to the question. Passage 5:{Passage 5}(5 Psg. total)\npairs (q, c+) as relevant while using hard negative passages (q, c−) mined via BM25 as irrelevant\npairs. The LLM need to generate “True” or “False” given the corresponding query-passage pair,\nwhere the question along with the task-specific instruction is “For the question {question}, access\nwhether the passage is relevant to the question.”.\nWe want to handle ranking in conversational senario as well. While MS MARCO spans various topics,\nthe questions are only single-turn short sentences. However, ranking data are only available, if any, at\na small amount for conversation QA. To overcome this limitation, we repurpose the conversational\nQA pairs to generate pseudo relevance pairs. As each conversation is only associated with one\ndocument d, we cut each document into 150-word chunks (c1, c2, . . . , cn). We compute the 4-gram\nrecall score between each chunk ci and the ground-truth answer a, considering segments with a recall\nscore above 0.5 as relevant and those below 0.1 as irrelevant for the corresponding conversation. Note\nthat, each sample contains one question-context pair for this ranking dataset. In total, there are around\n50k ranking pairs from MS MARCO ranking and synthetic conversations for instruction finetuning.\n5) Retrieval-augmented ranking data. We aim to train the LLM with the capability of determining\nthe relevance of multiple contexts simultaneously given a question, which is closer to the test-time\nbehavior of RAG with top-k contexts. As before, we utilize two QA datasets, SQuAD (Rajpurkar\net al., 2016) and WebQuestions (Berant et al., 2013). We combine the gold context with the top-\nretrieved contexts using BM25, ensuring a total of five contexts. The contexts containing the answer\nare considered relevant, and the LLM is trained to explicitly identify all relevant contexts for the\nquestion.\nUnifying RAG and ranking with instruction tuning. It is worth noting that, despite the variety\nof datasets and tasks described, they can all be cast into a standardized QA format (x, c, y), where\nx is the question, c is the corresponding context, and y is the target output answer. For example,\nfor the retrieval-augmented ranking data, the question is “For the question <question>, find all the\npassages from the context that are relevant to the question.” Table 1 exhibits how to cast different\ntasks into a unified format. Despite its simplicity, this approach has the following advantages: i) It\nempowers the LLM with the ranking capability by adding relatively small amount of ranking data. ii)\nBy standardizing these tasks into a unified format, they can mutually enhance each other. After that,\nwe obtain the final RankRAG model that can be applied to various knowledge-intensive NLP tasks.\n4.3 RankRAG Inference: Retrieve-Rerank-Generate Pipeline\nAs RankRAG incorporates an additional reranking step, the inference pipeline for each question\nis modified as a retrieve-rerank-generate pipeline, described as follows: (1) the retriever R first\nretrieves top-N contexts from the corpus. (2) the RankRAG model calculates the relevance score\nbetween the question and retrieved N contexts as the probability of generating the answer as True\nusing the prompt in Table 1, then reranks contexts to only retain top-k (k ≪ N) contexts, which are\nthen used as the input for the generation step. (3) The top-k contexts, along with the question, are\nconcatenated and fed back into the RankRAG model to generate the final answer.\nEfficiency Discussion. We are aware that the addition of a reranking step introduces extra processing\ntime. In practice, for each question, denote the time for indexing and retrieval ast1, the time for using\nLLM to calculate the relevance score as t2 and the time for generation as t3, then the ratio of added\ntime overhead is N∗t2\nt1+t3\n. In practice, calculating relevance typically requires generating just one token\nand involves much shorter inputs compared to the generation step with top-k contexts. We provide\nefficiency study in §5.5.\n5\n5 Experiments\nIn this section, we conduct comprehensive experiments on a variety of knowledge-intensive NLP\ntasks to demonstrate the zero-shot capabilities of RankRAG.\n5.1 Experiment Setup\nTasks and Datasets. We consider 3 types of tasks in experiments: (1)Open-domain QA (OpenQA),\nwhich includes NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), PopQA (Mallen et al.,\n2023), HotpotQA (Yang et al., 2018) and 2WikimQA (Ho et al., 2020). The first three are single-\nhop QA tasks, while the last two are multi-hop QA datasets. For NQ, TriviaQA, and HotpotQA,\nwe use the split from KILT benchmark (Petroni et al., 2021) 2. (2) Fact verification, where we\nuse FEVER (Thorne et al., 2018) from KILT benchmark. (3) Conversational QA (ConvQA), we\nconsider three datasets including Doc2Dial (Feng et al., 2020), TopiOCQA (Adlakha et al., 2022)\nand INSCIT (Wu et al., 2023), which have long documents that cannot be fitted directly into LLMs\nthus necessitates retrieval and ranking. The detailed dataset information is in Appendix A.1.\nBaselines. We consider the following baselines: (1) Baseline LLMs without RAG, where we con-\nsider LLMs trained with proprietary data including InstructGPT (Ouyang et al., 2022), PaLM\n2 (Anil et al., 2023), FLAN-LaMDA (Longpre et al., 2023), GLaM (Du et al., 2022), Claude\n2 (Anthropic, 2023), Mixtral-8x22B-Instruct (Mistral, 2024), DeepSeek-V2 Chat (DeepSeek, 2024)\nand only use the official reported results. We also consider two ChatGPT-series models, namely\nGPT-3.5-turbo (gpt-3.5-turbo-0613) (OpenAI, 2022) and GPT-4 (gpt-4-0613) (OpenAI, 2023).\n(2) Baselines with retrieval, we evaluate models augmented with retrieval. Specifically, we include\nAtlas (Izacard et al., 2023) and Raven (Huang et al., 2023), two RAG models based on encoder-\ndecoder LMs. For decoder-only models, we consider Self-RAG (Asai et al., 2024a), RECOMP (Xu\net al., 2024a), InstructRetro (Wang et al., 2024), RePlug (Shi et al., 2024), RA-DIT (Lin et al., 2024),\nLlama-3-instruct (Meta-AI, 2024) and ChatQA-1.5 (Liu et al., 2024). We also list the result of RAG\npipelines using InstructGPT (175B parameters) as the backbone including GenRead (Yu et al., 2023a),\nRetrieve-read (Lazaridou et al., 2022) and ReFeed (Yu et al., 2024), but mainly for reference. Other\nreported numbers are directly comparable if they follow the standard zero-shot settings.\nEvaluation Metrics. For OpenQA datasets, we use Exact Match (EM) as the main metric but also\nreport Accuracy for TriviaQA and PopQA and F1 score for HotpotQA and 2WikimQA as it is used\nin several studies (Asai et al., 2024a; Mallen et al., 2023). For FEVER, we use accuracy as the metric.\nFor ConvQA datasets, we follow (Liu et al., 2024; Wang et al., 2024) to use F1 score as the metric.\nImplementation Details. We use Llama3 8B and 70B (Meta-AI, 2024) as the backbone in our main\nexperiments. For the two-stage instruction tuning, we set the batch size to 128 and train the model for\n1000 steps with learning rate 5e-6 in Stage-I. Then, we reduce the learning rate to 3e-7 for 8B and 2e-7\nfor 70B model, set the batch size to 64, and train the model for 3300 steps (around 1 epoch). We use\nthe Adam optimizer (Kingma & Ba, 2014) with β1 = 0.9 and β2 = 0.98. During the inference stage,\nwe use the December 2018 Wikidump as the corpus index for NQ, TQA, HotpotQA, 2WikimQA, and\nuse the December 2020 Wikidump for PopQA, following (Asai et al., 2024a). By default, we follow\n(Wang et al., 2024; Lin et al., 2024; Liu et al., 2024) to use the Dragon retriever (Lin et al., 2023) as\ndefault and retrieve top-N (100 for 8B and 30 for 70B) documents for ranking, but RankRAG can be\nadapted to various retrievers and different N (see § 5.3 and 5.5). To ensure a fair comparison, we test\nthe performance of k ∈ {5, 10, 20} and report the best performance for baselines. For generation, we\nkeep temperature T = 0and set the maximum number of generated token to be 32 for OpenQA, 128\nfor ConvQA and 8 for others. Training RankRAG-8B uses 32 NVIDIA A100 GPUs for 10 hours\n(4 hours for Stage-I and 6 hours for Stage-II finetuning), while training RankRAG-70B uses 128\nNVIDIA A100 GPUs for 16 hours (4 hours for Stage-I and 12 hours for Stage-II Finetuning).\nData Contamination Issues. One possible issue for the zero-shot evaluation is the test set contami-\nnation, where some of the task-specific examples overlap with the instruction fine-tuning data (Oren\net al., 2024). To address this issue, we have performed a string match-based analysis where we do not\nobserve any overlap between the train data and data from target tasks.\n5.2 Main Experiments\nTable 2 presents results of RankRAG and baselines. The findings are summarized as follows:\n2The results of NQ and TriviaQA using the split from DPR (Karpukhin et al., 2020) are in Appendix F.\n6\nTable 2: Results of RankRAG and baselines on 9 datasets. Unless specified, all results are under\nzero-shot evaluation without additional demonstrations. Results unavailable in public reports are\nmarked as “–”. We use NQ, TriviaQA, and HotpotQA from the KILT benchmark for Llama3-Instruct,\nLlama3-ChatQA-1.5, and Llama3-RankRAG. Note that†: GPT-4 and GPT-4-turbo may refuse to\nanswer the question when retrieved passages do not contain relevant information, thus the EM /\naccuracy drops after including RAG on TriviaQA, HotpotQA and 2WikimQA.\nTask (Zero-shot) NQ TriviaQA PopQA HotpotQA 2WikimQA FEVER Doc2Dial TopiOCQA Inscit Avg.\nMetric EM EM / Acc. EM / Acc. EM / F1 EM / F1 Acc. F1 F1 F1 –\nWithout Retrieval-Augmented Generation\nInstructGPT (Ouyang et al.) 29.9 65.8 / 73.2 – / – 26.0 / 38.2 27.2 / 34.8 77.6 – – – –PaLM2 540B (0 shot, Anil et al.)21.2 76.9 / – – / – – / – – / – – – – – –PaLM2 540B (5 shot, Anil et al.) 37.1 86.1 / – – / – – / – – / – – – – – –GLaM 64B (0 shot, Du et al.) 37.5 71.3 / – – / – – / – – / – – – – – –FLAN-LaMDA 137B (Wei et al.)20.7 68.1 / – – / – – / – – / – – – – – –Claude 2 (5 shot, Anthropic) – 87.5/ – – / – – / – – / – – – – – –Mixtral-8x22B-Instruct (5 shot, Mistral)40.1 82.2 / – – / – – / – – / – – – – – –DeepSeek-V2 236B (5 shot, DeepSeek)53.4 86.7 / – – / – – / – – / – – – – – –GPT-3.5-turbo-1106 (OpenAI)38.6 82.9 / 91.7 28.4 / 32.2 29.9 / 42.0 23.9 / 30.4 82.7 20.1 28.5 27.2 38.5GPT-4-0613 (OpenAI) 40.3 84.8 /94.5 31.3 / 34.8 34.5 / 46.9 29.8 / 36.6 87.7 27.6 30.1 27.0 42.0GPT-4-turbo-2024-0409 (OpenAI)41.5 80.0 / 94.3 25.0 / 33.5 26.6 / 43.8 24.1 / 35.5 87.0 27.6 26.4 24.4 38.6\nWith Retrieval-Augmented Generation\nAtlas 11B (Izacard et al.) 26.7 56.9 / – – / – 34.7 / – – / – 77.0 – – – –Raven 11B (Huang et al.) 29.6 65.7 / – – / – – / – – / – – – – – –Self-RAG 7B (Asai et al.) – – / 66.4 – / 54.9 – / – – / – – – – – –Self-RAG 13B (Asai et al.) – – / 69.3 – / 55.8 – / – – / – – – – – –RECOMP 20B (Xu et al.) 37.0 59.0 / – – / – 30.4 / 40.1 – / – – – – – –InstructRetro 43B (Wang et al.)38.9 78.3 / – – – / – – / – – 36.0 – – –RePlug 65B (Shi et al.) 28.8 72.6 / – – / – 32.0 / – – / – 73.3 – – – –RA-DIT 65B (Lin et al.) 35.2 75.4 / – – / – 39.7 / – – / – 80.7 – – – –Llama3-Instruct 8B (Meta-AI)30.9 70.7 / 80.4 34.9 / 55.8 26.0 / 35.8 9.6 / 25.2 88.9 33.6 44.9 32.6 40.8Llama3-Instruct 70B (Meta-AI)42.7 82.4 / 89.3 45.3 / 56.4 35.5 / 43.3 13.5 / 27.9 91.4 37.9 49.7 36.2 47.1Llama3-ChatQA-1.5 8B (Liu et al.)42.4 81.0 / 87.6 52.6 / 59.8 33.4 / 44.6 26.8 / 31.9 90.9 39.3 49.9 30.1 49.6Llama3-ChatQA-1.5 70B (Liu et al.)47.0 85.6/ 91.450.9 / 58.3 42.2/ 54.4 34.9/ 37.4 92.7 41.3 55.6 32.3 53.6\nLlama3-RankRAG 8B (0 shot) 50.6 82.9 / 89.557.6/ 64.1 35.3 / 46.731.4 / 36.992.0 40.4 50.4 33.3 52.6Llama3-RankRAG 70B (0 shot) 54.2 86.5/92.3 59.9/65.4 42.7/55.4 38.2/43.9 93.8 41.5 52.8 35.2 56.1\nFor reference:Using InstructGPT or CodeX (∼175B) (Ouyang et al., 2022) as the Backbone LLM.\nGenRead (Yu et al.) 32.5 66.2 / – 46.0 / – 36.4 / 39.9 – / – 80.4 – – – –Retrieve-Read (Lazaridou et al.)31.7 61.4 / – – / – 35.2 / 38.0 27.7 / – 82.7 – – – –ReFeed (Yu et al.) 39.6 68.9 / – – / – 41.5 / 45.1 – / – – – – – –GPT-3.5-turbo-1106 RAG (OpenAI)46.7 79.7 / 88.0 49.9 / 57.0 31.2 / 41.2 27.2 / 32.2 90.8 34.8 44.3 35.3 46.8GPT-4-0613 RAG†(OpenAI) 40.4 75.0 / 88.5 44.3 / 61.4 27.6 / 38.1 14.4 / 17.6 92.6 34.2 45.1 36.4 43.5GPT-4-turbo-2024-0409 RAG†(OpenAI)40.3 70.2 / 91.1 39.5 / 58.4 8.1 / 17.9 22.8 / 39.2 92.2 35.4 48.3 33.8 41.6\nRankRAG outperforms existing RAG methods.With 8B scale, RankRAG consistently outperforms\nChatQA-1.5 8B, one of the most recent open-sourced model with state-of-the-art performance on\nmany RAG benchmarks. RankRAG 8B is also competitive when compared with baseline models\nwith much more parameters. For example, it significantly outperforms InstructRetro (5× parameters),\nRA-DIT 65B (8× paramters), and even outperforms Llama3-instruct 70B (8× parameters) on NQ\nand TriviaQA tasks. With more parameters, RankRAG 70B outperforms the strong ChatQA-1.5 70B\nmodel, and largely outperforms previous RAG baselines with InstructGPT as the underlying LLM.\nRankRAG demonstrates larger improvement on more challenging datasets. We observe that\nthe performance gains of RankRAG over baselines are more pronounced for more challenging QA\ndatasets. For example, on long-tailed QA (PopQA) and multi-hop QA (2WikimQA) tasks, we\nachieve more than 10% improvement over ChatQA-1.5. These findings suggest that in challenging\nOpenQA datasets where top documents from retrievers are less relevant to the answer, context ranking\neffectively enhances performance. In this work we focus on improving single-time retrieval for QA\ntasks. How to effectively combine multi-round RAG pipelines (Jiang et al., 2023; Khattab et al.,\n2022; Jeong et al., 2024) with RankRAG is an interesting avenue of future work.\n5.3 Ablation Studies\nEffect of Designed Components. Table 3 shows the ablations of RankRAG with Llama3 8B as the\nbackbone on nine general-domain datasets. Overall, we observe all of the proposed components\ncontribute to the final performance. Removing context ranking hurts performance on all tasks,\njustifying its efficacy in selecting the most relevant contexts for the target question. Besides, the\nretrieval-augmented QA (RQA) and retrieval-augmented ranking (RAR) designed for instruction fine-\ntuning improve outcomes on most tasks by helping the model explicitly pinpoint relevant contexts.\nOn the contrary, the RAFT method used in (Lin et al., 2024) treats each retrieved context separately\nduring instruction finetuning, which yields suboptimal results when compared to RankRAG with the\nsame training data.\nPerformance with Different LLMs. Table 4 reports the performance of RankRAG and the most\n7\nTable 3: Ablation study of RankRAG. We use Llama3-8B as the backbone. Where ‘RQA’ and ‘RAR’\nstands for retrieval-augmented QA and retrieval-augmented ranking data, respectively. For ‘w/o\nreranking’, we do not perform ranking in the inference stage.\nTask (Zero-Shot) NQ TriviaQA PopQA HotpotQA 2WikimQA FEVER Doc2Dial TopiOCQA Inscit Avg.\nMetric EM EM / Acc. EM / Acc. EM / F1 EM / F1 Acc. F1 F1 F1 –\nRankRAG 8B 50.6 82.9 / 89.557.6 / 64.135.3 /46.7 31.4 / 36.9 92.0 40.4 50.4 33.3 52.6w/o reranking 48.0 80.3 / 86.8 49.3 / 59.0 31.3 / 41.6 26.4 / 30.5 91.1 39.7 49.4 30.9 49.8w/o RQA 49.4 82.0 / 88.9 55.1 / 62.935.6/ 45.9 31.8 / 37.5 92.1 39.4 46.8 32.4 51.6w/o RAR 48.6 82.2 / 89.1 56.0 / 62.6 35.1 / 45.2 31.2 / 35.7 91.4 39.6 48.6 33.5 51.8\nw/ RAFT (Lin et al.)43.3 80.8 / 87.6 48.9 / 56.3 30.5 / 41.8 25.2 / 29.6 91.2 36.8 46.4 30.1 48.1w/ Stage-I SFT Only38.3 63.7 / 76.6 49.8 / 54.6 26.5 / 40.3 18.0 / 25.9 85.7 33.3 33.7 30.5 42.2\nTable 4: Zero-shot evaluation using Llama2 (Touvron et al., 2023) model as the backbone.\nTask (Zero-Shot) NQ TriviaQA PopQA HotpotQA 2WikimQA FEVER Doc2Dial TopiOCQA Inscit Avg.\nMetric EM EM / Acc. EM / Acc. EM / F1 EM / F1 Acc. F1 F1 F1 –\nLlama2-70B (Touvron et al.)25.3 82.4 / – – / – – / – – / – – – – – –\nLlama2-ChatQA-1.0 7B (Liu et al.)41.4 77.8 / 86.5 46.7 / 55.0 28.9 / 40.3 24.0 / 27.5 85.9 37.9 45.5 31.0 46.6Llama2-ChatQA-1.0 13B (Liu et al.)47.9 80.9 / 87.6 51.8 / 56.2 32.9 / 43.2 27.6 / 31.1 87.6 38.1 48.9 30.8 49.6Llama2-ChatQA-1.0 70B (Liu et al.)49.5 83.2 / 89.7 52.1 / 56.6 39.0 / 49.4 28.9 / 34.1 91.7 38.9 51.0 31.9 51.8\nLlama2-RankRAG 7B 46.9 84.0 / 89.655.9 / 61.332.2 / 43.226.8 / 30.7 86.6 38.6 49.2 32.3 50.3Llama2-RankRAG 13B 50.5 84.5 / 91.058.0 / 63.936.4 / 47.329.5 / 34.2 91.7 39.5 49.2 33.4 52.5Llama2-RankRAG 70B 53.2 85.8 / 92.158.7 / 64.541.8 / 53.133.8 / 38.8 91.9 41.2 52.9 35.8 55.0\nrecent baseline ChatQA using Llama2 with backbone having varying amounts of parameters. Notably,\nthere exist consistent gains in terms of the average performance (7.8%/6.4%/6.3% on 7B/13B/70B\nvariants respectively), justifying the advantage of RankRAG across different LLM types and scales.\nNQ TQA PopQA20\n40\n60Exact Match\nChatQA-1.5 RankRAG\n(a) DPR\nNQ TQA PopQA20\n40\n60\n80Exact Match\nChatQA-1.5 RankRAG (b) Contriever\nFigure 3: Performance with different retrievers.\nThe performance of Recall is in Appendix E.1.\nPerformance with Different Retrievers. Figure 3\nexhibits the performance of RankRAG and ChatQA-\n1.5 with different dense retrievers on three represen-\ntative tasks, where we consider DPR (Karpukhin\net al., 2020) and Contriever-MS MARCO (Izac-\nard et al., 2022) as two variants. We note that al-\nthough the initial retrieved result is not good enough,\nRankRAG still surpasses ChatQA-1.5 by more than\n10% for both retrievers on average. To summarize,\nRankRAG is robust to the choice of retrievers.\n5.4 Experiment on Domain-specific RAG Benchmarks\nTable 5: The performance of RankRAG on Mirage, a zero-shot\nbiomedical RAG benchmark. RankRAG and baselines use re-\ntrieval by default. Most of numbers are from (Xiong et al., 2024).\nDatasets MMLU-med PubmedQA BioASQ MedQA MedMCQA Avg.\nGPT-4-0613 (OpenAI) 87.24 70.60 92.56 82.80 66.65 79.97GPT-3.5 (OpenAI) 75.48 67.40 90.29 66.61 58.04 71.56Mixtral 8*7B (Jiang et al.) 75.85 67.60 87.54 60.02 56.42 69.49Llama2 70B (Touvron et al.) 54.55 50.40 73.95 44.93 43.08 53.38Meditron 70B (Chen et al.) 65.38 56.40 76.86 49.57 52.67 60.18PMC-llama 13B (Wu et al.) 52.53 42.58 48.29 56.00 65.21 52.92Llama3-ChatQA-1.5 8B (Liu et al.) 61.40 66.40 82.69 42.36 46.97 59.96Llama3-ChatQA-1.5 70B 80.51 74.80 83.17 68.89 62.54 73.98\nLlama3-RankRAG 8B 64.55 65.00 84.44 48.86 56.90 63.95Llama3-RankRAG 70B81.44 79.80 90.76 69.21 69.11 78.06\nTo demonstrate that RankRAG\ncan adapt to specialized domains,\nwe conduct experiments on Mi-\nrage (Xiong et al., 2024), a re-\ncently introduced RAG bench-\nmark for the biomedical field.\nWe follow Xiong et al. (2024)\nto employ MedCPT (Jin et al.,\n2023) as the retriever R with\nMedCorp3 as the corpus D.\nThe experiment results of RankRAG and baselines are shown in Table 5. From the table, we observe\nthat RankRAG, even without fine-tuning on the biomedical domain, excels at medical QA tasks.\nNotably, RankRAG 8B surpasses Meditron 70B—a leading open-source LLM for the medical\ndomain—by 6.3%. Besides, RankRAG 70B attains more than 98% performance of GPT-4. These\nresults justify RankRAG’s capacity to be readily applied to new domains without extra post-training.\n5.5 A Closer Look at the Ranking Module\nAs the context ranking serves as a core step in RankRAG, we take a closer look at this component.\nAll the studies are done using Llama3-8B as the backbone.\n3Link: https://huggingface.co/MedRAG. Detailed dataset information is in Appendix A.2.\n8\nTable 6: Ranking performance with different ranking models. Unless specified, all baselines are used\nto rank the top 100 retrieved passages. RankRAG achieves better performance despite using fewer\nranking data. ∗ NQ, TriviaQA and HotpotQA are used for training the BGE-Reranker model. †: Our\nre-implementation. ‡ We only rerank top-30 passages for GPT-4 due to budget constraint.\nTask # Rank Data NQ TriviaQA PopQA HotpotQA Inscit\nRecall R@5 R@10 R@20 R@5 R@10 R@20 R@5 R@10 R@20 R@5 R@10 R@20 R@5 R@10 R@20\nBackbone RetrieverDragon (Lin et al.) – 74.9 80.3 84.3 89.0 92.9 95.3 69.6 76.9 82.6 47.5 52.4 60.1 43.4 56.0 64.9\nFinetuned Baseline Ranking ModelRankBERT 110M (Glass et al.)∼503k 73.5 79.3 84.0 88.4 92.0 95.5 78.7 82.8 85.5 54.6 59.8 63.7 45.6 57.1 66.7monoT5 3B (Nogueira et al.)∼503k 75.6 80.9 84.9 90.7 93.6 95.9 81.0 83.6 85.9 54.8 60.2 63.3 48.6 59.4 68.8BGE-Rerank-v2-m3 568M (Chen et al.)∼1.6M 78.0∗ 82.8∗ 85.6∗ 91.6∗ 94.5∗ 97.1∗ 79.6 84.5 86.9 58.5∗ 61.8∗ 65.0∗ 51.3 59.8 69.7RankLLaMA 8B†(Ma et al.) ∼503k 77.8 83.1 86.0 91.2 93.1 96.4 80.1 84.3 86.8 57.1 62.1 64.8 57.8 62.1 71.3ChatQA-1.5 8B (Liu et al.) N/A 68.2 75.7 82.0 85.4 91.1 94.0 67.3 76.7 83.5 37.4 45.0 53.6 32.3 42.6 54.9\nOff-the-shelf LLM RerankerGPT-3.5 (top 100, OpenAI) Unk. 77.8 82.5 85.7 91.1 94.4 96.7 77.4 82.0 85.5 52.1 56.6 62.4 50.2 59.1 68.6GPT-4‡(top 30, OpenAI) Unk. 79.3 83.2 85.1 92.8 95.5 96.8 79.3 83.6 86.2 53.2 57.0 61.0 52.3 61.7 70.0\nOur ModelRankRAG 8B(top 100) ∼50k 80.3 84.0 86.3 93.2 95.4 97.3 81.6 84.9 87.0 57.6 61.8 65.2 60.9 65.7 73.5RankRAG 70B(top 30) ∼50k 80.6 84.0 85.4 93.6 95.9 97.1 81.8 84.6 86.5 56.3 59.7 62.2 61.3 66.4 74.6\n0k 5k 10k 50k\n# Ranking Data\n40\n45\n50Exact Match\nChatQA-1.5 (NQ) RankRAG (NQ)\nFigure 4: Performance\nw.r.t. # Ranking Data\n0 2 4 6 8 10\nInference Time\n42\n45\n48\n51Exact Match\nN=20\nN=30\nN=40\nN=50\nN=100\nChatQA-1.5\nRankRAG\n(a) NQ\n0 2 4 6 8 10\nInference Time\n81\n82\n83Exact Match\nN=20\nN=30\nN=40\nN=50\nN=100\nChatQA-1.5\nRankRAG (b) TriviaQA\n0 2 4 6 8 10\nInference Time\n34\n35Exact Match\nN=20\nN=30\nN=40\nN=50\nN=100\nChatQA-1.5\nRankRAG (c) HotpotQA\nFigure 5: Performance v.s. Efficiency analysis for RankRAG.\nRankRAG is Data-efficient. Previous approaches that infuse context ranking into the RAG pipeline\nusually involve a separate reranking model. To compare our model with these baselines, we evaluate\nfour models (BERT (Glass et al., 2022)/T5 (Nogueira et al., 2020)/Llama3 (Ma et al., 2023)) fine-tuned\non the full MS MARCO passage ranking dataset, a strong off-the-shelf reranker model BGE-ranker,\nand two OpenAI GPT-series models. For the GPT-series models, we use the token probability of\n‘True’ as a proxy for the relevance score 4. These models are then used to rerank top-retrieved\npassages by Dragon, similar to our approach. Surprisingly, as shown in Table 6, RankRAG achieves\nbetter recall over dedicated ranking models trained on10× more ranking data for most cases. Besides,\nRankRAG can still outperform the BGE-ranker on most tasks, which has been extensively trained\non more than 1 million ranking pairs, including some that overlap with our evaluation tasks. This\nadvantage is likely due to the adaptable nature of our model’s training, where the ranking data closely\nresembles the general RAG fine-tuning data. Directly using ChatQA-1.5 to rank passages hurts the\nperformance, indicating the necessity of incorporating ranking data into instruction fine-tuning.\nWe further study the relation between the number of context ranking data and final performance. As\nshown in Figure 4, with 5k ranking data only (∼ 1% of the MS MARCO dataset), RankRAG can\nalready obtain very compelling results, while further increasing the number of ranking data to 50k\nyields non-marginal gains. This finding confirms RankRAG’s data efficiency – achieving effective\nperformance with a modest amount of ranking data and maintaining adaptability across various tasks.\nPerformance v.s. Time-efficiency for RankRAG. One specific caveat for scaling up model size\nis the increment in the latency overhead — as mentioned in §4.3, it requires sample-wise ranking\nwhich incurs additional time. To study the relation between the time efficiency and performance, we\nchange the N used in reranking and plot the relation of N and final accuracy in Figure 5, from which\nwe observe that even with N = 20, RankRAG still improve the baseline model without reranking.\nWhile reranking across N = 20to 100 improves the exact match score by 5.9% to 9.1% across three\ntasks, it incurs an additional 0.9× to 6.0× increase in time – significantly less than the 20× to 100×\nincrease one might expect.\n5.6 Case Study\nTable 7 presents a case study on NQ dataset, where we observe that using retriever only yield noisy\ncontexts, as there are several distractors, and some contexts (e.g. Passage 4/5 for ChatQA-1.5) are\n4https://platform.openai.com/docs/api-reference/chat/create#chat-create-logprobs\n9\nTable 7: A case study on the top-retrieved context and predictions on NQ dataset, illustrating the\neffectiveness of RankRAG-8B over ChatQA-1.5-8B. Red text denotes distractors, while green stands\nfor evidences. RankRAG is able to find the correct answer via extract more evidence with reranking.\nQ: who hosted and won the inagural world cup?A: Uruguay\nChatQA-1.5\nPassage 1: FIFA World Cup second round on home soil in 1982. England (1966) won its only title while playing as a host nation. Uruguay (1930), Italy(1934), Argentina (1978) and France (1998) won their first titles as host nations but have gone on to win again, while Germany (1974) won their secondtitle on home soil...Passage 2: FIFA World Cup hosts country is now chosen in a vote by FIFA’s Congress ... Only Mexico, Italy, France, Germany (West Germany) untilshortly after the 1990 World Cup) and Brazil have hosted the event on two occasions.Passage 3: CONCACAF hosts, beating the bids of Canada and the United States, and thereby became the first nation to host two World Cups. Thissecond World Cup in Mexico came 16 years after the first one in 1970...Passage 4: 1998 FIFA World Cup Africa made their first appearances in the finals. France was awarded the 1998 World Cup on 2 July 1992 by theexecutive committee of FIFA during a general meeting in Zürich, Switzerland. They defeated Morocco by 12 votes to 7.Passage 5: 2026 FIFA World Cup be hosted by one of the remaining four confederations: CONCACAF (North America; last hosted in 1994), CAF(Africa; last hosted in 2010), CONMEBOL (South America; last hosted in 2014), or OFC (Oceania, never hosted before)...Prediction:Germany(✘)\nRankRAG\nPassage 1: FIFA World Cup second round on home soil in 1982. England (1966) won its only title while playing as a host nation. Uruguay (1930), Italy(1934), Argentina (1978) and France (1998) won their first titles as host nations but have gone on to win again, while Germany (1974) won their secondtitle on home soil...Passage 2: Timeline of association football penalty kicks. Thirteen teams enter the first World Cup, held in Uruguay. The hosts beat Argentina 4–2 inthe final. Contested between the top national teams of continental Europe, Dr. Gerö Cup’ first edition is won by Italy.Passage 3: The Uruguay national football team represents Uruguay in international association football and is controlled by the Uruguayan FootballAssociation. They have won the Copa América 15 times, the most successful national team in the tournament, the most recent title being the 2011edition. The team has won the FIFA World Cup twice, including the first World Cup in 1930 as hosts, defeating Argentina 4–2 in the final.Passage 4: FIFA World Cup hosts country is now chosen in a vote by FIFA’s Congress. The decision is currently made roughly seven years in advanceof the tournament, though the hosts for the 2022 tournament were chosen at the same time as those for the 2018 tournament.Passage 5: CONCACAF hosts, beating the bids of Canada and the United States, and thereby became the first nation to host two World Cups. Thissecond World Cup in Mexico came 16 years after the first one in 1970...Prediction:Uruguay(✓)\nunhelpful. However, the utilization of reranking uncovers two additional relevant passages, aiding\nthe model in providing the correct answer. More case studies are provided in Appendix G.\n6 Conclusion\nIn this work, we introduce a new RAG framework, RankRAG, which instruction-tunes a single LLM\nfor both ranking and answer generation. We find that the instruction tuned LLMs can outperform\nexisting expert ranking models by only adding a small fraction of ranking data into the training blend.\nWe compare our RankRAG with the state-of-the-art RAG models on comprehensive knowledge-\nintensive benchmarks and demonstrate RankRAG significantly outperform all of them on nine\ngeneral-domain and five biomedical benchmarks for RAG.\nReferences\nAdlakha, V ., Dhuliawala, S., Suleman, K., de Vries, H., and Reddy, S. Topiocqa: Open-domain\nconversational question answering with topic switching. TACL, 2022.\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey,\nP., Chen, Z., et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.\nAnthropic. Model card and evaluations for claude models. 2023.\nAsai, A., Wu, Z., Wang, Y ., Sil, A., and Hajishirzi, H. Self-RAG: Learning to retrieve, generate, and\ncritique through self-reflection. In ICLR, 2024a.\nAsai, A., Zhong, Z., Chen, D., Koh, P. W., Zettlemoyer, L., Hajishirzi, H., and Yih, W.-t. Reliable,\nadaptable, and attributable language models with retrieval. arXiv preprint arXiv:2403.03187,\n2024b.\nBajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R., McNamara, A., Mitra,\nB., Nguyen, T., et al. Ms marco: A human generated machine reading comprehension dataset.\narXiv preprint arXiv:1611.09268, 2016.\nBerant, J., Chou, A., Frostig, R., and Liang, P. Semantic parsing on freebase from question-answer\npairs. In EMNLP, 2013.\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche,\nG. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by retrieving from\ntrillions of tokens. In ICML. PMLR, 2022.\n10\nChen, J., Xiao, S., Zhang, P., Luo, K., Lian, D., and Liu, Z. Bge m3-embedding: Multi-lingual,\nmulti-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023a.\nChen, Z., Cano, A. H., Romanou, A., Bonnet, A., Matoba, K., Salvi, F., Pagliardini, M., Fan, S.,\nKöpf, A., Mohtashami, A., et al. Meditron-70b: Scaling medical pretraining for large language\nmodels. arXiv preprint arXiv:2311.16079, 2023b.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y ., Fedus, W., Li, Y ., Wang, X., Dehghani, M.,\nBrahma, S., et al. Scaling instruction-finetuned language models. JMLR, 25(70), 2024.\nConover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M.,\nand Xin, R. Free Dolly: Introducing the world’s first truly open instruction-tuned llm, 2023.\nDasigi, P., Liu, N. F., Marasovi´c, A., Smith, N. A., and Gardner, M. Quoref: A reading comprehension\ndataset with questions requiring coreferential reasoning. In EMNLP, 2019.\nDeepSeek. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model,\n2024.\nDu, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y ., Krikun, M., Zhou, Y ., Yu, A. W., Firat,\nO., et al. Glam: Efficient scaling of language models with mixture-of-experts. In ICML, 2022.\nDua, D., Wang, Y ., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. Drop: A reading compre-\nhension benchmark requiring discrete reasoning over paragraphs. In NAACL, 2019.\nFan, A., Jernite, Y ., Perez, E., Grangier, D., Weston, J., and Auli, M. Eli5: Long form question\nanswering. In ACL, 2019.\nFeng, S., Wan, H., Gunasekara, C., Patel, S., Joshi, S., and Lastras, L. doc2dial: A goal-oriented\ndocument-grounded dialogue dataset. In EMNLP, 2020.\nGlass, M., Rossiello, G., Chowdhury, M. F. M., Naik, A., Cai, P., and Gliozzo, A. Re2G: Retrieve,\nrerank, generate. In NAACL, 2022.\nGuu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model\npre-training. In ICML, 2020.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring\nmassive multitask language understanding. In ICLR, 2021.\nHo, X., Nguyen, A.-K. D., Sugawara, S., and Aizawa, A. Constructing a multi-hop qa dataset for\ncomprehensive evaluation of reasoning steps. In COLING, 2020.\nHonovich, O., Scialom, T., Levy, O., and Schick, T. Unnatural instructions: Tuning language models\nwith (almost) no human labor. In ACL, 2023.\nHuang, J., Ping, W., Xu, P., Shoeybi, M., Chang, K. C.-C., and Catanzaro, B. Raven: In-context learn-\ning with retrieval augmented encoder-decoder language models. arXiv preprint arXiv:2308.07922,\n2023.\nIzacard, G. and Grave, E. Leveraging passage retrieval with generative models for open domain\nquestion answering. In EACL, 2021.\nIzacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsuper-\nvised dense information retrieval with contrastive learning. TMLR, 2022.\nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A.,\nRiedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented language models.\nJMLR, 24(251):1–43, 2023.\nJeong, S., Baek, J., Cho, S., Hwang, S. J., and Park, J. C. Adaptive-rag: Learning to adapt retrieval-\naugmented large language models through question complexity. In NAACL, 2024.\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas,\nD. d. l., Hanna, E. B., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\n11\nJiang, Z., Xu, F. F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y ., Callan, J., and Neubig, G.\nActive retrieval augmented generation. In EMNLP, 2023.\nJin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., and Szolovits, P. What disease does this\npatient have? a large-scale open domain question answering dataset from medical exams. Applied\nSciences, 11(14):6421, 2021.\nJin, Q., Dhingra, B., Liu, Z., Cohen, W., and Lu, X. Pubmedqa: A dataset for biomedical research\nquestion answering. In EMNLP, 2019.\nJin, Q., Kim, W., Chen, Q., Comeau, D. C., Yeganova, L., Wilbur, W. J., and Lu, Z. Medcpt:\nContrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical\ninformation retrieval. Bioinformatics, 39(11), 2023.\nJoshi, M., Choi, E., Weld, D., and Zettlemoyer, L. TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. In ACL, 2017.\nKarpukhin, V ., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense\npassage retrieval for open-domain question answering. In EMNLP, 2020.\nKasai, J., Sakaguchi, K., yoichi takahashi, Bras, R. L., Asai, A., Yu, X. V ., Radev, D., Smith, N. A.,\nChoi, Y ., and Inui, K. Realtime QA: What’s the answer right now? InNeurIPS, 2023.\nKhalifa, M., Logeswaran, L., Lee, M., Lee, H., and Wang, L. Few-shot reranking for multi-hop QA\nvia language model prompting. In ACL, 2023.\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. Demonstrate-\nsearch-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv\npreprint arXiv:2212.14024, 2022.\nKim, H., Hessel, J., Jiang, L., Lu, X., Yu, Y ., Zhou, P., Bras, R. L., Alikhani, M., Kim, G., Sap, M.,\net al. Soda: Million-scale dialogue distillation with social commonsense contextualization. In\nEMNLP, 2023.\nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nKoˇcisk`y, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. The\nnarrativeqa reading comprehension challenge. TACL, 2018.\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D.,\nPolosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question answering\nresearch. TACL, 2019.\nKöpf, A., Kilcher, Y ., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A.,\nDuc, N. M., Stanley, O., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantuluri, A., Maguire, A.,\nSchuhmann, C., Nguyen, H., and Mattick, A. Openassistant conversations - democratizing large\nlanguage model alignment. arXiv preprint arXiv: 2304.07327, 2023.\nLazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. Internet-augmented language\nmodels through few-shot prompting for open-domain question answering. arXiv preprint\narXiv:2203.05115, 2022.\nLee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W. Nv-embed: Improved\ntechniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428,\n2024.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., Küttler, H., Lewis, M., Yih,\nW.-t., Rocktäschel, T., et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nNeurIPS, 33, 2020.\nLin, K., Tafjord, O., Clark, P., and Gardner, M. Reasoning over paragraph effects in situations. In\nWorkshop on Machine Reading for Question Answering, 2019.\n12\nLin, S.-C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad, Y ., Yih, W.-t., and Chen, X. How to train your\ndragon: Diverse augmentation towards generalizable dense retrieval. In Findings of EMNLP, 2023.\nLin, X. V ., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G.,\nLewis, M., Zettlemoyer, L., and tau Yih, W. RA-DIT: Retrieval-augmented dual instruction tuning.\nIn ICLR, 2024.\nLiu, Z., Ping, W., Roy, R., Xu, P., Shoeybi, M., and Catanzaro, B. Chatqa: Surpassing gpt-4 on\nconversational qa and rag. arXiv preprint arXiv:2401.10225, 2024.\nLongpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y ., Zhou, D., Le, Q. V ., et al. The flan\ncollection: Designing data and methods for effective instruction tuning. In ICML, 2023.\nLuan, Y ., Eisenstein, J., Toutanova, K., and Collins, M. Sparse, dense, and attentional representations\nfor text retrieval. TACL, 2021.\nLuo, H., Chuang, Y .-S., Gong, Y ., Zhang, T., Kim, Y ., Wu, X., Fox, D., Meng, H., and Glass, J. Sail:\nSearch-augmented instruction learning. arXiv preprint arXiv:2305.15225, 2023.\nMa, X., Wang, L., Yang, N., Wei, F., and Lin, J. Fine-tuning llama for multi-stage text retrieval.arXiv\npreprint arXiv:2310.08319, 2023.\nMallen, A., Asai, A., Zhong, V ., Das, R., Khashabi, D., and Hajishirzi, H. When not to trust language\nmodels: Investigating effectiveness of parametric and non-parametric memories. In ACL, 2023.\nMenon, A., Jayasumana, S., Rawat, A. S., Kim, S., Reddi, S., and Kumar, S. In defense of dual-\nencoders for neural ranking. In ICML, 2022.\nMeta-AI. Llama 3 model card. 2024.\nMistral. Mixtral 8x22b. 2024. URL https://mistral.ai/news/mixtral-8x22b/.\nMitra, B., Craswell, N., et al. An introduction to neural information retrieval. Foundations and\nTrends® in Information Retrieval, 2018.\nMuennighoff, N., Su, H., Wang, L., Yang, N., Wei, F., Yu, T., Singh, A., and Kiela, D. Generative\nrepresentational instruction tuning. arXiv preprint arXiv:2402.09906, 2024.\nNogueira, R., Jiang, Z., Pradeep, R., and Lin, J. Document ranking with a pretrained sequence-to-\nsequence model. In Findings of EMNLP, 2020.\nOpenAI. Introducing ChatGPT, 2022.\nOpenAI. GPT-4, 2023.\nOren, Y ., Meister, N., Chatterji, N. S., Ladhak, F., and Hashimoto, T. Proving test set contamination\nin black-box language models. In ICLR, 2024.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S.,\nSlama, K., Ray, A., et al. Training language models to follow instructions with human feedback.\nNeurIPS, 35, 2022.\nPal, A., Umapathi, L. K., and Sankarasubbu, M. Medmcqa: A large-scale multi-subject multi-choice\ndataset for medical domain question answering. In CHIL, 2022.\nPetroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., De Cao, N., Thorne, J., Jernite, Y ., Karpukhin,\nV ., Maillard, J., Plachouras, V ., Rocktäschel, T., and Riedel, S. KILT: a benchmark for knowledge\nintensive language tasks. In NAACL, 2021.\nQin, Z., Jagerman, R., Hui, K., Zhuang, H., Wu, J., Shen, J., Liu, T., Liu, J., Metzler, D., Wang,\nX., et al. Large language models are effective text rankers with pairwise ranking prompting. In\nFindings of NAACL, 2024.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad: 100,000+ questions for machine\ncomprehension of text. In EMNLP, 2016.\n13\nRam, O., Levine, Y ., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham, Y .\nIn-context retrieval-augmented language models. TACL, 2023.\nRobertson, S., Zaragoza, H., and Taylor, M. Simple bm25 extension to multiple weighted fields. In\nCIKM, 2004.\nSachan, D. S., Reddy, S., Hamilton, W. L., Dyer, C., and Yogatama, D. End-to-end training of\nmulti-document reader and retriever for open-domain question answering. In NeurIPS, 2021.\nShao, Z., Gong, Y ., Shen, Y ., Huang, M., Duan, N., and Chen, W. Enhancing retrieval-augmented\nlarge language models with iterative retrieval-generation synergy. In Findings of EMNLP, 2023.\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.\nReplug: Retrieval-augmented black-box language models. In NAACL, 2024.\nSun, W., Yan, L., Ma, X., Wang, S., Ren, P., Chen, Z., Yin, D., and Ren, Z. Is ChatGPT good at\nsearch? investigating large language models as re-ranking agents. In EMNLP, 2023.\nThakur, N., Reimers, N., Rücklé, A., Srivastava, A., and Gurevych, I. Beir: A heterogeneous\nbenchmark for zero-shot evaluation of information retrieval models. In NeurIPS, 2021.\nThorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A. Fever: A large-scale dataset for fact\nextraction and verification. In NAACL, 2018.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\nBhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023.\nTrischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K. Newsqa: A\nmachine comprehension dataset. In RepL4NLP Workshop at ACL, 2017.\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. Interleaving retrieval with chain-of-\nthought reasoning for knowledge-intensive multi-step questions. In ACL, 2023.\nTsatsaronis, G., Balikas, G., Malakasiotis, P., Partalas, I., Zschunke, M., Alvers, M. R., Weissenborn,\nD., Krithara, A., Petridis, S., Polychronopoulos, D., et al. An overview of the bioasq large-scale\nbiomedical semantic indexing and question answering competition. BMC bioinformatics, 2015.\nWang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi, M., and Catanzaro, B. Instructretro:\nInstruction tuning post retrieval-augmented pretraining. In ICML, 2024.\nWang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F. Text\nembeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533,\n2022.\nWang, L., Yang, N., Huang, X., Yang, L., Majumder, R., and Wei, F. Improving text embeddings\nwith large language models. arXiv preprint arXiv:2401.00368, 2023a.\nWang, Y ., Kordi, Y ., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct:\nAligning language models with self-generated instructions. In ACL, 2023b.\nWang, Z., Araki, J., Jiang, Z., Parvez, M. R., and Neubig, G. Learning to filter context for retrieval-\naugmented generation. arXiv preprint arXiv:2311.08377, 2023c.\nWei, J., Bosma, M., Zhao, V . Y ., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V .\nFinetuned language models are zero-shot learners. In ICLR, 2022.\nWu, C., Lin, W., Zhang, X., Zhang, Y ., Xie, W., and Wang, Y . Pmc-llama: toward building open-\nsource language models for medicine. JAMIA, 2024.\nWu, Z., Parish, R., Cheng, H., Min, S., Ammanabrolu, P., Ostendorf, M., and Hajishirzi, H. Inscit:\nInformation-seeking conversations with mixed-initiative interactions. TACL, 2023.\nXiong, G., Jin, Q., Lu, Z., and Zhang, A. Benchmarking retrieval-augmented generation for medicine.\narXiv preprint arXiv:2402.13178, 2024.\n14\nXu, F., Shi, W., and Choi, E. RECOMP: Improving retrieval-augmented LMs with context compres-\nsion and selective augmentation. In ICLR, 2024a.\nXu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M.,\nand Catanzaro, B. Retrieval meets long context large language models. In ICLR, 2024b.\nYang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W., Salakhutdinov, R., and Manning, C. D.\nHotpotQA: A dataset for diverse, explainable multi-hop question answering. In EMNLP, 2018.\nYoran, O., Wolfson, T., Ram, O., and Berant, J. Making retrieval-augmented language models robust\nto irrelevant context. In ICLR, 2024.\nYu, W., Iter, D., Wang, S., Xu, Y ., Ju, M., Sanyal, S., Zhu, C., Zeng, M., and Jiang, M. Generate\nrather than retrieve: Large language models are strong context generators. In ICLR, 2023a.\nYu, W., Zhang, H., Pan, X., Ma, K., Wang, H., and Yu, D. Chain-of-note: Enhancing robustness in\nretrieval-augmented language models. arXiv preprint arXiv:2311.09210, 2023b.\nYu, W., Zhang, Z., Liang, Z., Jiang, M., and Sabharwal, A. Improving language models via plug-and-\nplay retrieval feedback, 2024.\nYu, Y ., Xiong, C., Sun, S., Zhang, C., and Overwijk, A. Coco-dr: Combating distribution shift in\nzero-shot dense retrieval with contrastive and distributionally robust learning. In EMNLP, 2022.\nZhang, T., Patil, S. G., Jain, N., Shen, S., Zaharia, M., Stoica, I., and Gonzalez, J. E. Raft: Adapting\nlanguage model to domain specific rag. arXiv preprint arXiv:2403.10131, 2024.\nZhu, F., Lei, W., Huang, Y ., Wang, C., Zhang, S., Lv, J., Feng, F., and Chua, T.-S. Tat-qa: A question\nanswering benchmark on a hybrid of tabular and textual content in finance. In ACL, 2021.\nZhu, Y ., Zhang, P., Zhang, C., Chen, Y ., Xie, B., Dou, Z., Liu, Z., and Wen, J.-R. Inters: Unlocking\nthe power of large language models in search with instruction tuning. In ACL, 2024.\n15\nA Dataset Description\nThe information for 14 datasets used in RankRAG is listed as follows.\nA.1 Main Experiments\n• NQ (Kwiatkowski et al., 2019) is a widely used question-answering dataset constructed with\nWikipedia. The questions are constructed from the Google search engine, and the answers are\nidentified as text spans in the Wikipedia article.\n• TriviaQA (Joshi et al., 2017) is a challenging QA dataset containing question-answer pairs from\ntrivia enthusiasts and independently gathered evidence documents.\n• PopQA (Mallen et al., 2023) is an entity-centric QA dataset concentrated on long-tail entities.\nFor PopQA, we follow (Asai et al., 2024a) to use the long-tail subset, consisting of questions on\n1399 rare entities whose monthly Wikipedia page views are less than 100.\n• HotpotQA (Yang et al., 2018) is a multi-hop QA dataset, where the goal is to answer complex\nquestions that require understanding and linking information from multiple documents.\n• 2WikimQA (Ho et al., 2020) is also a multi-hop QA designed to test machine understanding\nacross two different Wikipedia entities, evaluating the ability of systems to handle cross-lingual\nand cross-cultural retrieval and question answering.\n• FEVER (Thorne et al., 2018) is a fact verification dataset aimed at supporting research into the\nautomatic verification of factual claims. It consists of claims that are manually verified against\nevidence from Wikipedia, providing a benchmark for fact-checking systems.\n• Doc2Dial (Feng et al., 2020) is a document-grounded conversational QA dataset covering four\ndomains: DMV , SSA, V A, and Student Aid. Each sample comprises a dialogue where a user poses\nqueries regarding the document, and an agent responds those questions. The average document\nlength is around 101K words.\n• TopiOCQA (Adlakha et al., 2022) is grounded on the whole Wikipedia. It incorporates topic\nswitching and requires the agent to search the entire Wikipedia for answers to user questions.\n• INSCIT (Wu et al., 2023) is also grounded on the whole Wikipedia. It studies the case where\nuser questions are under-specified and require clarification.\nA.2 Biomedical Benchmarks\n• MMLU-med (Hendrycks et al., 2021) is a subset of six tasks related to biomedicine, including\nanatomy, clinical knowledge, professional medicine, human genetics, college medicine, and\ncollege biology. It contains 1089 questions in total.\n• MedQA (Jin et al., 2021) is collected from the US Medical Licensing Examination, contaiing\n1273 four-option multiple-choice questions focused on real-world scenarios from professional\nmedical board exams.\n• MedMCQA (Pal et al., 2022) includes multiple-choice questions derived from Indian medical\nentrance exams, covering 2400 healthcare topics across 21 medical subjects. We use the 4,183-\nquestion development set from MedMCQA, as the test set lacks provided ground truths.\n• PubmedQA (Jin et al., 2019) is a biomedical research QA dataset consisting of 1000 manually\nannotated questions based on PubMed abstracts. Answers in PubMedQA are structured as\nyes/no/maybe to reflect the validity of the questions.\n• BioASQ (Tsatsaronis et al., 2015) includes 618 questions constructed from biomedical litera-\nture without providing the ground truth snippets, challenging RAG systems to infer answers\nindependently.\nB Data Blending Details for Ranking-enhanced Instruction Finetuning\nThe dataset blending ratio for Stage-II is as follows:\n• Drop: 0.069\n16\n• narrativeqa: 0.09\n• quoref: 0.026\n• ropes: 0.026\n• Squad (Retrieval-augmented QA): 0.09\n• Squad (Retrieval-augmented Ranking): 0.02\n• WebQuestions (Retrieval-augmented QA): 0.09\n• WebQuestions (Retrieval-augmented Ranking): 0.02\n• newsqa: 0.09\n• tatqa-arithmetic: 0.15\n• tatqa-others: 0.08\n• ConvQA: 0.2\n• MS MARCO ranking: 0.15\n• ConvQA ranking: 0.03\n• SFT: 0.2\nThe ratio for each dataset is further normalized to ensure the total ratio equals to 1.\nC Prompt Formats of Instruction Tuning\nC.1 Stage I: Supervised Fine-tuning\nThe format template of LLM inputs in stage-I is as follows:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user’s questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nUser: {Question 1}\nAssistant: {Answer 1}\n...\nUser: {Latest Question}\nAssistant:\nC.2 Stage-II: Unified Instruction-Tuning for Ranking and Generation\nThe format template of LLM inputs in stage-II are as follows:\n1) Context-rich QA data\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user’s questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage: {(Gold) Passage containing relevant context for QA}\nUser: {Question 1}\nAssistant: {Answer 1}\n17\n...\nUser: {Latest Question}\nAssistant:\nWe tailor specific user instructions for various dataset types. For instance:\nFor datasets requiring short answers (such as DROP, NarrativeQA, Quoref, ROPES, SQuAD1.1,\nSQuAD2.0, NewsQA), we use: \"Answer the following question with a short span.\"\nFor datasets that necessitate long answers (such as Synthetic_ConvQA), we instruct: \"Please give a\nfull and complete answer for the question.\"\nFor datasets involving arithmetic calculations or number extraction from the context (such as TAT-\nQA), we specify: \"Answer the following question with a number from the context or through math\narithmetic.\"\nFor datasets that may require both short and long answers (such as TAT-QA-Others), we direct:\n\"Answer the following question with a short span, or a full and complete answer.\"\n2) Retrieval-augmented QA data\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user’s questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage 1: {(Shuffled) Passage 1}\nPassage 2: {(Shuffled) Passage 2}\nPassage 3: {(Shuffled) Passage 3}\nPassage 4: {(Shuffled) Passage 4}\nPassage 5: {(Shuffled) Passage 5}\n...\nUser: {Question}\nAssistant:\n3) Context ranking data\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user’s questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage: {Passage 1}\nUser: {For the question <question>, access whether the passage is relevant to the\nquestion. Return True if relevant, otherwise False. }\nAssistant:\n4) Retrieval-augmented ranking data\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user’s questions\nbased on the context. The assistant should also indicate when the answer cannot be\n18\nfound in the context.\nPassage 1: {(Shuffled) Passage 1}\nPassage 2: {(Shuffled) Passage 2}\nPassage 3: {(Shuffled) Passage 3}\nPassage 4: {(Shuffled) Passage 4}\nPassage 5: {(Shuffled) Passage 5}\nUser: {For the question <question>, access whether the above passages are relevant\nto the question. Return all the relevant passage id. }\nAssistant:\nD Prompt Formats of Target Tasks\nD.1 Context Ranking\nNQ/TriviaQA/HotpotQA/PopQA:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user’s questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage: {Passage}\nUser: {For the question <question>, access whether the passage is relevant to the\nquestion. Return True if relevant, otherwise False. }\nAssistant:\nFEVER:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user’s questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage: {Passage}\nUser: {For the claim <claim>, access whether the passage is relevant to the\nclaim. Return True if relevant, otherwise False. }\nAssistant:\nDoc2dial, Inscit, TopiocQA:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user’s questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage: {Passage}\nUser: {Question 1}\n19\nAssistant: {Answer 1}\n...\nUser: {For the question <latest question>, access whether the passage is relevant\nto the question. Return True if relevant, otherwise False. }\nAssistant:\nD.2 RAG\nNQ/TriviaQA/HotpotQA/PopQA:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user’s questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage 1: {Rerank Top Passage 1}\nPassage 2: {Rerank Top Passage 2}\nPassage 3: {Rerank Top Passage 3}\nPassage 4: {Rerank Top Passage 4}\nPassage 5: {Rerank Top Passage 5}\n...\nUser: {Question}. Answer the above question with a short phrase.\nAssistant:\nFever:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user’s questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage 1: {Rerank Top Passage 1}\nPassage 2: {Rerank Top Passage 2}\nPassage 3: {Rerank Top Passage 3}\nPassage 4: {Rerank Top Passage 4}\nPassage 5: {Rerank Top Passage 5}\n...\nUser: Answer the following question with True or False. Is the claim ’<claim>’ correct?\nAssistant:\nDoc2dial, Inscit, TopiOCQA:\nSystem: This is a chat between a user and an artificial intelligence assistant.\n20\nThe assistant gives helpful, detailed, and polite answers to the user’s questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage 1: {Rerank Top Passage 1}\nPassage 2: {Rerank Top Passage 2}\nPassage 3: {Rerank Top Passage 3}\nPassage 4: {Rerank Top Passage 4}\nPassage 5: {Rerank Top Passage 5}\nUser: {Question 1}\nAssistant: {Answer 1}\n...\nUser: {Latest Question}\nAssistant:\nE Additional Experiment Results\nE.1 Ranking Performance Using DPR and Contriever as Retrievers R\nTable 8 shows the ranking performance of RankRAG-8B using DPR (Karpukhin et al., 2020) and\nContriever (Izacard et al., 2022) on three datasets. There are consistent performance gains for all\ntasks, indicating that RankRAG can apply to many popular retrieval models to improve the quality of\nretrieved contents.\nTable 8: Answer Recall Comparison Before and After Ranking on 3 Representative Datasets.\nNQ DPR Contriever\nR@5 R@10 R@20 R@5 R@10 R@20\nBefore Ranking 69.50% 76.20% 81.00% 67.60% 75.24% 80.67%\nw/ RankRAG 77.95% 81.70% 84.56% 75.32% 80.18% 84.70%\nTriviaQA DPR Contriever\nR@5 R@10 R@20 R@5 R@10 R@20\nBefore Ranking 67.80% 74.20% 80.30% 81.95% 86.76% 90.08%\nw/ RankRAG 77.73% 79.40% 84.74% 88.71% 90.05% 92.59%\nPopQA DPR Contriever\nR@5 R@10 R@20 R@5 R@10 R@20\nBefore Ranking 43.60% 48.90% 54.25% 60.61% 65.54% 69.90%\nw/ RankRAG 50.32% 53.75% 57.76% 65.11% 68.41% 71.77%\nE.2 RAG Performance with Different k\nWe also show the performance of RankRAG with different context sizek in figure 6. From the result,\nwe observe that different from the trend of vanilla RAG approaches (without ranking), k = 5already\nworks well for most datasets. This effectiveness stems from the reranking step, which prioritizes the\nmost relevant contexts at the top, reducing the necessity to include additional contexts.\n21\n5 10 2046\n48\n50\n52Exact Match\nNQ\n5 10 2080\n82\n84 TriviaQA\n5 10 2050\n55\n60 PopQA\n5 10 2090\n91\n92\n93 FEVER\nContext Size k\nFigure 6: Performance of RankRAG on different context size k.\nF Performance of NQ and Trivia QA on DPR Splits\nWe observe that the NQ and TriviaQA datasets exist in two versions: one used by the DPR (Karpukhin\net al., 2020) and FiD (Izacard & Grave, 2021) papers, which include 3610 and 11316 questions\nfor NQ and TriviaQA, respectively. In contrast, the KILT benchmark (Petroni et al., 2021) utilizes\nonly subsets of these, comprising 2837 and 5355 examples for NQ and TriviaQA, respectively. It is\nnoteworthy that many recent studies report performance metrics on these datasets without clarifying\nwhich version was employed for evaluation.\nTo facilitate anhonest and fair comparison, we present the performance of RankRAG on both datasets\nusing the DPR splits in Table 9. Notably, regardless of the subset used, RankRAG consistently\noutperforms both ChatQA and Llama-3-instruct, our direct competitors, as well as other methods\nutilizing InstructGPT as backbones. We aim for these results to assist the community in making\naccurate comparisons when referring to the performance of RankRAG.\nTable 9: Performance Across Models.\nModel Model Configuration NQ EM (%) TriviaQA EM / Acc. (%)\nRepresentative Baselines\nOpenAI GPT\nGPT-3.5-0613 35.2 70.1 / 81.3\nGPT-3.5-0613 RAG 42.3 65.8 / 76.7\nGPT-4-0613 37.2 72.6 / 85.1\nGPT-4-0613 RAG 36.2 61.2 / 75.9\nGPT-4-turbo-2024-0409 38.3 68.0 / 84.5\nGPT-4-turbo-2024-0409 RAG 36.3 57.6 / 79.2\nUsing Llama-2 (Touvron et al., 2023) as the backbone LLM\nLlama-2-Chat Llama-2 RAG 70B 37.7 65.6 / –\nChatQA-1.0\nLlama-2 7B 37.0 62.4 / 74.3\nLlama-2 13B 43.9 66.6 / 76.9\nLlama-2 70B 45.0 69.8 / 80.2\nRankRAG\nLlama-2 7B 42.4 68.3 / 78.9\nLlama-2 13B 46.2 69.5 / 80.0\nLlama-2 70B 48.7 72.3 / 82.6\nUsing Llama-3 (Meta-AI, 2024) as the backbone LLM\nLlama-3-InstructLlama-3-Instruct RAG 8B 27.6 57.1 / 74.6\nLlama-3-Instruct RAG 70B 37.3 67.6 / 79.6\nChatQA-1.5 Llama-3 8B 44.1 65.4 / 75.8\nLlama-3 70B 46.0 69.0 / 80.4\nRankRAG Llama-3 8B 46.1 68.8 / 79.9\nLlama-3 70B 50.0 72.6 / 82.9\nG Additional Case Studies\nTables 10 and 11 provide additional examples from the PopQA and HotpotQA datasets, which focus\non long-tailed and multi-hop QA. These tasks are particularly challenging for retrievers, making it\ndifficult to obtain relevant context from the corpus. Consequently, ChatQA-1.5 often struggles to\nproduce the correct answers. However, the reranking step in RankRAG helps counteract poor initial\n22\nTable 10: A case study on the top-retrieved context and predictions on PopQA dataset, illustrating the\neffectiveness of RankRAG-8B over ChatQA-1.5-8B. Red text denotes distractors, while green stands\nfor evidences.\nQ: Who is the author of The Universe Around Us?A: James Hopwood Jeans / James Jeans / Sir James Jeans / Sir James Hopwood Jeans\nChatQA-1.5\nPassage 1: Hooper is the author of two books published by Smithsonian Books/HarperCollins. The first, Dark Cosmos: In Search ofour Universe’s Missing Mass and Energy (2006) was named a notable book by Seed Magazine. His second book, Nature’s Blueprint:Supersymmetry and the Search for a Unified Theory of Matter and Force (2008), was called \"essential reading\" by New Scientist...Passage 2: Fraknoi is the author or co-author of 14 books in the field of astronomy. He was the lead author of V oyages through theUniverse, an introductory college astronomy textbook published by Brooks-Cole, which went through three editions. In the 1980s, heco-edited with Byron Preiss two collections of science articles and science fiction stories, \"The Universe\" and \"The Planets.\" With SidneyWolff, Fraknoi founded and was co-editor of the first on-line journal devoted to astronomy education, \"Astronomy Education Review\"...Passage 3: The Universe\" and \"The Planets.\" With Sidney Wolff, Fraknoi founded and was co-editor of the first on-line journal devotedto astronomy education, \"Astronomy Education Review\". He edited two collections of resources for K-12 teachers, The Universe at YourFingertips and More Universe at Your Fingertips published through the Astronomical Society of...Passage 4: Lincoln Kinnear Barnett (1909–1979) was an editor and author, most notably at Life Magazine for many years. LincolnBarnett wrote a number of books, including \"The Universe and Doctor Einstein\"Passage 5: The Universe Maker is a science fiction novel by American author A.E. van V ogt, published in 1953 by Ace Books as an AceDouble with The World of Null-A. It is based on the author´s \"The Shadow Men\" (Startling Stories, 1950). Set 400 years into the future,\nthe main character is Morton Cargill, a U.S. Army officer who served in the Korean War...Prediction: Lincoln Barnett (✘)\nRankRAG\nPassage 1: The Universe Around Us is a science book written by English astrophysicist Sir James Jeans, first published in 1929 by theSyndics of the Cambridge University Press....Passage 2: These books made Jeans fairly well known as an expositor of the revolutionary scientific discoveries of his day, especially inrelativity and physical cosmology. In 1939, the Journal of the British Astronomical Association reported that Jeans was going to stand asa candidate forPassage 3: James Jeans books for the lay public, including \"The Stars in Their Courses\" (1931), \"The Universe Around Us\", \"ThroughSpace and Time\" (1934), \"The New Background of Science\" (1933), and \"The Mysterious Universe.Passage 4: The Universe Around Us no special scientific knowledge. Parts of the book cover the same ground as various lectures I haverecently delivered to University and other audiences, including a course of wireless talks I gave last autumn. It has been found necessaryto rewrite these almost in their entirety, so that very few sentences remain in their original form, but those who have asked me to publishmy lectures and wireless talks will find the substance of them in the present book.Passage 5:Lincoln Barnett Lincoln Kinnear Barnett (1909–1979) was an editor and author, most notably at Life Magazine for many years.Lincoln Barnett wrote a number of books, including \"The Universe and Doctor Einstein\", \"The World We Live In\", and \"The Treasure ofOur Tongue\". The Universe and Doctor Einstein is a layman´s introduction to the theory of relativity. It includes a foreword by Albert\nEinstein, and has been reprinted several times... Prediction: Sir James Jeans (✓)\nTable 11: A case study on the top-retrieved context and predictions on HotpotQA dataset, illustrating\nthe effectiveness of RankRAG-8B over ChatQA-1.5-8B. Red text denotes distractors, while green\nstands for evidences.\nQ: Which song did Eminem and Rihanna collaborate on after their other collaboration song in studio album \"Unapologetic?\"A: The Monster\nChatQA-1.5\nPassage 1: Unapologetic is the seventh studio album by Barbadian singer Rihanna. It was released on November 19, 2012, by Def JamRecordings and SRP Records. It was recorded between June and November 2012, during promotion of her sixth album, \"Talk That Talk\"(2011). As executive producer, Rihanna enlisted previous collaborators The-Dream, David Guetta, Chase & Status, and StarGate to workalongside new collaborators such as Parker Ighile, Mike Will Made-It, and Labrinth...Passage 2: Def Jam France announced via Twitter that Rihanna would release a new single the upcoming week while her seventh studioalbum was scheduled to be released in November 2012. On October 11, 2012, in one of her tweets revealed that the title of her new albumis \"Unapologetic\" alongside with its cover. \"What Now\" was written by British singer-songwriter Livvi Franc together with Rihanna,Parker Ighile and Nathan Cassells, while the production of the song was done by the latter two. Ighile and Cassells...Passage 3: Justin then went on to co write \"Stay\" with Mikky Ekko and recorded by Barbadian singer Rihanna for her seventh studioalbum, \"Unapologetic\" (2012). It features guest vocals by Mikky Ekko, and was released as the second single from the album on 7January 2013. The song reached the top five of twenty-four countries worldwide including number four in the UK and number three onthe US Billboard Hot 100, becoming Rihanna’s twenty-fourth top ten on the latter chart...Passage 4: Via her official Twitter account, Rihanna posted series of \"teasing\" tweets announcing her seventh studio album. On October11, 2012, in one of her tweets revealed that the title of her new album is \"Unapologetic\" alongside with its cover. \"Jump\" is the overallseventh and final single off Unapologetic. It was written by Kevin Cossom and M. B. Williams together with its producers StarGate(Mikkel S. Eriksen and Tor Erik Hermansen) and Chase & Status (Saul MiltonPassage 5: copies of the song were sold in the UK, making \"Love the Way You Lie\" the country’s biggest-selling song of 2010. The sameyear, a sequel to the single, titled \"Love the Way You Lie (Part II)\" was released as part of Rihanna’s fifth studio album \"Loud\"; it mainlyviews matters from the female protagonist perspective. In November 2012, Eminem and Rihanna collaborated again on \"Numb\", which\nwas included on Rihanna’s seventh album \"Unapologetic\"... Prediction: Love the Way You Lie (✘)\nRankRAG\nPassage 1: The Monster (song). Copies of the song were sold in the UK, making \"Love the Way You Lie\" the country’s biggest-sellingsong of 2010. The same year, a sequel to the single, titled \"Love the Way You Lie (Part II)\" was released as part of Rihanna’s fifth studioalbum \"Loud\"; it mainly views matters from the female protagonist perspective. In November 2012, Eminem and Rihanna collaboratedagain on \"Numb\", which was included on Rihanna’s seventh album \"Unapologetic\"....Passage 2: \"Numb\" is a song by Barbadian singer Rihanna from her seventh studio album \"Unapologetic\" (2012). It features guestvocals by American rapper Eminem, making it the pair’s third collaboration since the two official versions of \"Love the Way You Lie\".Following the album’s release, \"Numb\" charted on multiple charts worldwide including in Canada, the United Kingdom and the UnitedStates. \"Numb\" lasts for a duration of .Passage 3: Eminem also wanted to experiment with \"retro, vintage\" sounds such as beatbreaks and scratches, and he felt that Rubin couldhelp him \"take that to another level.\" Rihanna, with whom Eminem previously collaborated on \"Love the Way You Lie\" from Eminem’sprevious studio effort, \"Recovery\" (2010), was featured on the song \"The Monster\". On September 11, 2013, she hinted at the...Passage 4: together with Jay-Z, Bono and The Edge for the same campaign to alleviate the 2010 Haiti earthquake. In summer 2010,Rihanna collaborated with rapper Eminem on \"Love the Way You Lie\", which was a major worldwide success, reaching No. 1 in over 20countries. Reaching number 2, the song became the biggest-selling song of 2010 in the UK and the first of Rihanna’s singles to sell over amillion copies in the country. In October 2010, Rihanna switched managers ...Passage 5: Eminem asked for more tracks and subsequently heard \"Love the Way You Lie\". He chose it and told his manager PaulRosenberg he wanted to collaborate with the Barbadian singer Rihanna. Eminem told Skyrock, \"It’s one of those tracks that I felt likeonly she could pull it off.\" Rosenberg sent the track to Rihanna, who accepted Eminem’s request \"at the last moment.\" Eminem then\nwrote the rapped verses. Prediction: The Monster (✓)\nretrieval by finding more pertinent evidence. Coupled with RAG-oriented finetuning, RankRAG\neffectively filters out distracting entities and pinpoints the correct answers.\n23"
  },
  {
    "source": "Wang_Comments_An Effiecient Video Classification Deep Learning Algorithm for Object Detection and Autonomous Drones.pdf",
    "content": " \n \nAN EFFICIENT VIDEO CLASSIFICATION DEEP LEARNING \nALGORITHM FOR OBJECT DETECTION AND AUTONOMOUS \nDRONES \nby \nRiyaz Ahamed Shaik \n \nA Thesis \nSubmitted to the Faculty of Purdue University \nIn Partial Fulfillment of the Requirements for the degree of \n \nMaster of Science in Engineering \n \n \nDepartment of Electrical and Computer Engineering \nFort Wayne, Indiana \nDecember 2024 \n \n  \n \n2 \nTHE PURDUE UNIVERSITY GRADUATE SCHOOL \nSTATEMENT OF COMMITTEE APPROVAL \nDr. Guoping Wang, Chair \nDepartment of Electrical and Computer Engineering \nDr. Chao Chen, Committee Member \nDepartment of Electrical and Computer Engineering \nDr. Claudio Freitas, Committee Member \nDepartment of Electrical and Computer Engineering \n \nApproved by: \nDr. Chao Chen \n \n3 \nTABLE OF CONTENTS \nABSTRACT .................................................................................................................................... 7 \n1. INTRODUCTION ...................................................................................................................... 8 \n1.1 Background ....................................................................................................................... 9 \n1.2 Problem Statement .......................................................................................................... 10 \n1.3 Research Motivation ....................................................................................................... 11 \n1.4 Objectives ....................................................................................................................... 12 \n1.5 Research Contributions .................................................................................................. 13 \n2. LITERATURE REVIEW ......................................................................................................... 14 \n2.1 Evolution of Object Detection Techniques .................................................................... 15 \n2.1.1 Handcrafted Feature-Based Methods ...................................................................... 15 \nHistogram of Oriented Gradients (HOG) ............................................................................ 15 \nSupport Vector Machines (SVM) ....................................................................................... 16 \n2.1.2 The Emergence of Deep Learning in Object Detection .......................................... 17 \nIntroduction of CNNs for Object Detection ........................................................................ 17 \n2.1.3 Real-Time Object Detectors .................................................................................... 20 \nYOLO (You Only Look Once) Framework ........................................................................ 20 \nSSD (Single Shot Multibox Detector) ................................................................................. 22 \nComparison of YOLO and SSD .......................................................................................... 23 \nAdvances in Real-Time Object Detection ........................................................................... 24 \nChallenges in Real-Time Object Detection ......................................................................... 24 \nConclusion ........................................................................................................................... 25 \n2.2 CNN’s and Its Role in Object Detection ........................................................................ 26 \n2.2.1 Two-Stream Convolutional Neural Networks (CNNs) ........................................... 26 \n2.2.2 3D Convolutional Neural Networks (3D CNNs) .................................................... 27 \nC3D (Convolutional 3D Networks) .................................................................................... 27 \nI3D (Inflated 3D Networks) ................................................................................................ 27 \n2.2.3 SlowFast Networks ................................................................................................. 28 \n2.2.4 Integration of CNN in Object Detection ................................................................. 29 \n2.3 Advancements in Transformers for Video Analysis ...................................................... 31 \n \n4 \n2.3.1 Video Transformers (ViTs) ..................................................................................... 31 \nViTs Role in Video Analysis .............................................................................................. 31 \nApplications in Video Tasks ............................................................................................... 32 \n2.3.2 Video Swin Transformers ....................................................................................... 33 \nApplications ........................................................................................................................ 33 \n2.3.3 Video Transformers Without Convolutions ............................................................ 34 \n2.3.4 Other Transformer-Based Models ........................................................................... 35 \n2.3.5 Conclusion ............................................................................................................... 35 \n2.4 Applications and Challenges in Video Based Object Detection .................................... 36 \n2.4.1 Applications ............................................................................................................ 36 \nAutonomous Driving ........................................................................................................... 36 \nSurveillance Systems ........................................................................................................... 37 \nRobotics ............................................................................................................................... 38 \n2.4.2 Challenges in Video-Based Object Detection ......................................................... 39 \nOcclusions and Motion Blur ............................................................................................... 39 \nReal-Time Constraints and Computational Costs ............................................................... 40 \nDataset Limitations and Annotation Challenges ................................................................. 41 \nConclusion ........................................................................................................................... 42 \n2.5 Gaps in Existing Research .............................................................................................. 42 \n2.6 Conclusion ...................................................................................................................... 44 \n3. METHODOLOGY ................................................................................................................... 46 \n3.1 Dataset Description ........................................................................................................ 46 \n3.1.1 UCF101 Dataset ...................................................................................................... 46 \n3.1.2 COCO-VID Dataset ................................................................................................ 47 \n3.1.3 Waymo Open Dataset .............................................................................................. 47 \n3.2 Model Framework .......................................................................................................... 48 \n3.2.1 Task Description ..................................................................................................... 48 \n3.2.2 3D ResNet Backbone .............................................................................................. 48 \n3.2.3 Transformer Encoding Layers ................................................................................. 49 \n3.2.4 DeepStream SDK for Object Detection .................................................................. 50 \nModel Preparation for DeepStream ..................................................................................... 51 \n \n5 \nPipeline Development using DeepStream ........................................................................... 51 \nAdvantages of Using DeepStream SDK ............................................................................. 53 \n3.3 Autonomous Drone Setup .............................................................................................. 54 \n3.3.1 Drone Frame and Propulsion System ...................................................................... 54 \n3.3.2 Computing Unit ....................................................................................................... 55 \n3.3.3 Obstacle Detection and Depth Sensing ................................................................... 55 \n3.3.4 GPS Navigation ....................................................................................................... 55 \n3.3.5 Manual Control ....................................................................................................... 55 \n3.4 Implementation Details .................................................................................................. 57 \n3.4.1 Preprocessing .......................................................................................................... 57 \nI preprocessed the video data by sampling frames at 16 frames per second (fps) and resizing \nthem to 112×112 pixels. I normalized the pixel values to zero mean and unit variance for \nconsistent input representation. ............................................................................................. 57 \n3.4.2 Training ................................................................................................................... 57 \n3.4.3 Hyperparameter Optimization & Performance ....................................................... 57 \nLearning Rate: ..................................................................................................................... 57 \nBatch Size: ........................................................................................................................... 58 \nDropout Rate: ...................................................................................................................... 58 \nWeight Initialization: ........................................................................................................... 58 \nOptimizer: ............................................................................................................................ 59 \nNumber of Transformer Attention Heads: .......................................................................... 59 \nSummary of Optimized Hyperparameters:.......................................................................... 60 \n3.5 Evaluation ....................................................................................................................... 60 \n4. RESULTS & DISCUSSION..................................................................................................... 61 \n4.1.1 Dataset Performance ............................................................................................... 61 \n4.1.2 Ablation Study ......................................................................................................... 62 \n4.1.3 Object Detection Results ......................................................................................... 62 \n4.1.4 Performance Summary ............................................................................................ 65 \n4.1.5 Challenges and Limitations ..................................................................................... 65 \n5. CONCLUSION & FUTURE WORK ....................................................................................... 67 \n5.1 Achievements in Object Detection ................................................................................. 67 \n \n6 \n5.2 Object Detection Capabilities and Deployment ............................................................. 67 \n5.3 Future Work .................................................................................................................... 68 \nEnhancing Transformer Architectures: ............................................................................... 68 \nReal-Time and Low Latency Optimizations: ...................................................................... 68 \nApplication to Multi-Modal Learning: ................................................................................ 69 \nImproving Spatiotemporal Consistency in Object Detection: ............................................. 69 \nBroader Applications in Real-World Scenarios: ................................................................. 69 \n5.4 Concluding Reflections .................................................................................................. 70 \nREFERENCES ............................................................................................................................. 72 \n   \n \n7 \nABSTRACT \nAdvancements in deep learning and computer vision have enabled  the use of video \nclassification and object detection,  in autonomous systems. This thesis explores the use of a 3D \nResNet-Transformer model to achieve real -time object detection and navigation for autonomous \ndrones. The model combines the spatial feature extraction s of 3D convolutional neural networks \nwith the temporal modeling  capabilities of transformers, effective for video -fed applications. It \nperforms across multiple datasets, achieving 47.1 mAP on the COCO dataset, 67.2 mAP on the \nWaymo Open dataset, and 61.5 mAP on the UCF101 dataset. The model is deployed on NVIDIA \nJetson Nano, using the GPU-accelerated capabilities of NVIDIA’s DeepStream SDK for real-time \nprocessing. This integration enables the drones to detect, classify, and track objects in dynamic \nenvironments while autonomously navigating and avoiding obstacles.  The model can be in \napplications such as surveillance, search -and-rescue, and autonomous transportation. The model \nis generalized to detect objects and classify various categories as it’s been trained on three different \ndatasets at each layer.   \n \n8 \n1. INTRODUCTION \nObject detection plays an important role in computer vision and has advanced rapidly \nthanks to the increasing use of deep learning algorithms. Whether it's self-driving cars recognizing \npedestrians or surveillance systems monitoring safety in real time vide o footage object detection \nis essential for various groundbreaking applications. Although there have been improvements, in \ndetecting objects in still images , applying these techniques to video sequences presents distinct \nchallenges. Machines need to grasp the nature of video data by identifying objects in frames and \nunderstanding how they relate over time in sequences—a crucial yet challenging aspect of video-\nbased object detection research due to the need for spatial and temporal comprehension \nsimultaneously. The conventional methods for detecting objects in videos face difficulties \nbalancing accuracy, with efficiency and scalability. Crafting features manually has its restrictions \nas it struggles to adapt to situations whereas initial deep learning models show potential but \nstruggle to combine spatial and temporal data effectively enough leading to the necessity, for \ninnovative structures capable of handling video data efficiently while being computationally adept.  \n \nThis dissertation tackles these obstacles by  using a video classification method based on \ndeep learning that focuses on identifying objects efficiently. Using a combination of 3D \nConvolutional Neural Networks and Transformer structures enables the proposed method to merge \nspatial feature extraction and temporal analysis for top notch results. The mixed approach is tested \non demanding video datasets to showcase its ability to connect video categorization, with object \nidentification effectively.  \n \nIn the following chapters we will delve into the context needed for the study by first \npresenting the background information and then addressing the issue at hand as well as outlining \nthe research goals and impacts of this project. \n  \n \n9 \n1.1 Background \nDetecting objects in videos is a major task in computer vision and supports various useful \napplications in different fields compared to pictures that don't move much; videos offer a lot of \ntime-based contexts that helps systems study how objects move and interact as situations change \nover time. This function plays a role in tasks like smart surveillance where spotting and following \npotential dangers in real time boosts safety measures significantly. Likewise i n self -driving \nvehicles, video-based object detection ensures driving by always recognizing moving elements \nsuch as people  and traffic signs. Detecting objects in videos not  only aids human computer \ninteraction but also supports gesture recognition and improves augmented reality experiences and \nadaptive interfaces. Showcasing the significance of video object detection in both technical \nintricacy and its impact on decision making, within real world settings[1, 5, 13]. \n \nOver the twenty years or so there have been big changes in the field of video -based \ncomputer vision thanks to better technology and new ways of doing things with algorithms. In the \nbeginning they used methods based on features that were made by hand , like optical flow and \nmotion detection but they didn't work great in all situations. As technology got better at processing \nvideos, more things became possible like tracking movements to recognize actions and events as \nwell as breaking down videos into different sections based on actions. Video detection has evolved \nfrom identifying irregularities to comprehending intricate scene details and actions over time. In \nthe healthcare field as well as entertainment industry video analysis is employed for live tracking \nof patient movements and conditions and, for optimizing personalized suggestions and content \norganization. In robotics and autonomous systems , video analysis adoption has revolutionized \nvarious sectors by empowering machines to sense and engage with consta ntly changing \nenvironments proficiently [11, 12 ]. The development indicates an increasing need for video \nanalysis systems that are precise and responsive to context changes.  \n \nThe advancement of learning has transformed object detection by introducing \ncomprehensive frameworks that can learn complex features directly from data sources instead of \nrelying on preset patterns like traditional approaches did in the past. Deep learning  models have \nthe ability to dynamically extract representations from data which makes them extremely reliable \nin various situations and environments. Convolutional Neural Networks (CNNs) in particular has \n \n10 \nplayed a role in this evolution by demonstrating exceptional performance in capturing spatial \nfeatures, for detecting objects in still images. New standards in precision and efficiency for image \nrelated tasks have been achieved through advancements like R-CNN and YOLO (You Only Look \nOnce) as well as SSD (Single Shot Multibox Detector) [13, 14]. \n \nThe introduction of 3D Convolutional Neural Networks represented a breakthrough by \nfacilitating the combined modeling of spatial and temporal aspects simultaneously in video \nanalysis tasks —an advancement that heightened the precision of object detection in video \nsequences significantly under circumstances necessitating insight into motion patterns [9, 13, 14]. \nMoreover, Transformers have garnered attention as an innovation in sequence modeling due to \ntheir inclusion of self-focusing mechanisms that excel at capturing extensive connections over long \ndistances—a feature that has revolutionized the scope of video analysis, for a more comprehensive \nperspective [6, 7]. \n1.2 Problem Statement \nDetecting and analyzing objects in videos face a demanding task in computer vision as \nvideo data is constantly changing and dynamic in nature; this presents a challenge for object \ndetection techniques that are primarily tailored for static images and strug gle to handle the time \nelement of videos effectively as a result of this limitation models often fail to capture motion \npatterns and long term relationships between frames leading to less than optimal performance, in \ndynamic settings.  \n \nCurrent methods that try to tackle these issues often must balance between precision, \ncomputational speediness and scalability concerns. Convolutional Neural Networks (CNN), \nthough excellent for capturing characteristics, struggle with handling sequential patterns \neffectively. On the other hand, methods for temporal modeling such as Recurrent Neural Networks \n(RNN) or Long Short Term Memory (LSTM) networks add extra computational load and may not \nbe practical for extensive video datasets. Success in achieving efficient object detection in videos \nis hindered by the challenge of integrating spatial and temporal features seamlessly [3, 5].  \n \n \n11 \nFurthermore, videos frequently contain situations like objects blocking each other’s view \nas well as motion blurriness and changes, in the sizes of objects present; all of which make \ndetection tasks more difficult to perform accurately and efficiently. To handle these obstacles \neffectively requires a framework that can meticulously capture specific spatial characteristics \nwhile also understanding how elements change over time across different frames [6, 8].  \n \nTo tackle these concerns and challenges effectively, the presented dissertation in this \nresearch titled \"An Efficient Video Classification Deep Learning Algorithm for Object Detection  \nand Autonomous Drones\" a combination design that merges 3D ResNet for extracting features and \nTransformers for modeling temporal aspects to improve the classification of videos over time more \nefficiently. This strategy is designed to address current obstacles by managing spati otemporal \nconnections and extensive associations effectively while offering a reliable and precise method for \ndetecting objects, in video content. \n1.3 Research Motivation \nThe increasing need for effective identification of objects in videos for various purposes \nlike self -driving cars and security systems highlights the importance of using advanced \ncomputational techniques; however, the changing nature of videos presents challenges, in object \ndetection tasks . Although deep learning has greatly enhanced object detection in still images \nexisting methods struggle to perform well with video data as it necessitates a comprehensive \nunderstanding of spatial and temporal relationships \n \nRecent advancements in detecting objects in videos have shown the promise of combining \ntypes of architectures to fill this void effectively observed in the field of video analysis and \nunderstanding spatiotemporal elements better; for instance, incorporating 3D CNNs and \nTransformers has proven to be quite powerful in this regard. However, the exploration of these \nmodels in detecting objects within videos is still limited. This scenario offers a chance to adjust \nand enhance these models to tackle the difficulties posed by dynamic video environments like \nobstruction, by other objects or peop le (referred to as occlusions) different movement patterns \ndisplayed by objects captured on video footage and time constraints that require real time \nprocessing capabilities [8, 10].  \n \n12 \n \nThis study is driven by the necessity to formulate a solution that does not attain  high \nprecision but also maintains computational effectiveness for extensive practical use cases. This \nresearch endeavors to establish a framework by utilizing areas of expertise from 3D ResNet and \nTransformer structures to enhance video object detection capabilities significantly. The method \naims to present an efficient approach to bridge crucial deficiencies in current systems and pave the \nway for future advancements in this field. \n1.4 Objectives \nThe main objective of this research is to create and execute a n efficient deep learning \nalgorithm for detecting objects in videos using the advantages of video classification methods \neffectively. The main emphasis is placed on  dealing with the obstacles related to combining \ntemporal features and surpassing the shortcomings of current approaches, in dynamic video \nsettings. The detailed aims are as follows.  \n1. Design and Implementation of a 3D ResNet-Transformer for Object Detection: \nDevelop a hybrid framework that integrates 3D ResNet, for capturing spatial features and \nTransformer self-awareness mechanisms for understanding temporal patterns in videos with the \ngoal of offering a comprehensive solution to address the intricacies found in video datasets.  \n \n2. Enhancing Detection Accuracy Through Hybrid Feature Extraction: \nEnhance the accuracy of detecting objects in videos by tuning the balance between spatial \nand temporal features in the model design emphasizing the effective capture of dynamic object \ninteractions while also tackling issues, like occlusions and motion blur effectively.  \n \n3. Application to Complex and Dynamic Video Datasets: \nValidate the suggested framework on a variety of challenging benchmark datasets, like \nUCF101, COCO and Waymo Open to showcase how well the model can adapt and perform in life \ndynamic settings.  \n \n \n13 \n1.5 Research Contributions \nThis thesis makes the following significant contributions to the field of video-based object \ndetection: \n1. Integration of 3D ResNet for Spatial Feature Extraction: \nThis research presents the use of 3 D ResNet as a strong foundation for extracting spatial \nfeatures in video data effectively. With the incorporation of network architecture and 3D CNN \ntechniques, the suggested framework adeptly captures intricate spatial details in each frame \ntackling issues, like different object sizes and motion blurring. \n \n2. Adapting Transformer Self-Attention for Temporal -Spatial Correlations: \nThe research applies Transformer self-focus mechanisms to improve the understanding of \nspatial relationships in video sequences by including multi head attention and positional encoding, \nin the new framework to capture extended dependencies and changing ob ject interactions \neffectively while addressing the shortcomings of conventional temporal modeling techniques \neffectively.  \n \n3. Experimental Validation on Benchmark Datasets for Object Detection: \nThE model undergoes thorough assessment using known video datasets  that cover a wide \narray of intricate scenarios. The outcomes showcase detection precision and resilience revealing \nthe effectiveness of the combined 3D ResNet Transformer framework, in video object detection. \nThe study also offers an examination of fine tuning hyperparameters and subtractive experiments \nto confirm the significance of each element’s contributions.\n \n14 \n2. LITERATURE REVIEW \nThe realm of computer vision has seen progress in the past few years as object detection \nhas become a key focus of study due to its application in various fields like surveillance systems \nand human computer interactions along with autonomous technologies. Although conventional \ntechniques have played a role, in this field’s development so far; the introduction of deep learning \nhas transformed object detection by allowing models to grasp complex features hierarchically and \nattain remarkable levels of precision. The inclusion of video categorization methods within object \ndetection frameworks has broadened the scope of study more and opened up avenues to tackle the \ndistinctive hurdles presented by video data [9, 13].  \n \nVideos present a challenge for object detection due to their dynamic nature compared to \nstatic images; they require models that can analyze both spatial and temporal characteristics \neffectively without sacrificing computational efficiency. Many current met hods excel in either \ntemporal analysis but face difficulties seamlessly combining the two aspects; this limitation often \nresults in performance issues. To address this challenge and improve detection accuracy in videos \nresearchers are increasingly explorin g architectures that combine the strengths of different \napproaches, such as utilizing Convolutional Neural Networks (CNNs) for spatial understanding \nand Transformers, for capturing long range temporal relationships [6, 7, 8].  \n \nThis chapter offers an overview of the research already existing related to the thesis. It delves \ninto the progress of techniques for detecting objects and the importance of video classification in \npushing this field as well as the rise of combined frameworks that use both CNNs and Transformers \ntechnologies. The goal is to pinpoint areas wher e current research falls short and lay down the \ngroundwork for the proposed 3D ResNet Transformer framework designed to achieve efficient \nobject detection, in dynamic video settings. This review also addresses the difficulties and \npotentials of incorporati ng temporal characteristics while providing a context for the findings \npresented in this thesis. \n \n15 \n2.1 Evolution of Object Detection Techniques \nFor years now, in the world of computer vision research community , object detection has \nbeen a key focus area continuously upgraded to recognize and pinpoint objects within images or \nvideo frames effectively with time the techniques have advanced from traditional manual feature-\nbased methods to more recent deep learning strategies marking significant progress and boosting \nthe performance reliability and versatility of object detection systems along the way. In this \nanalysis, below we delve into the evolution of this field by examining  methods and models that \nhave influenced its development. numbered. \n2.1.1 Handcrafted Feature-Based Methods \nDuring the phases of object detection, development strategies heavily leaned on manually \ncrafted features. These methods required intervention to create algorithms that could extract \nrelevant details from images before employing machine learning classifiers, for identification \npurposes. Two notable techniques that characterized this period were the Histogram of Oriented \nGradients (HOG) and Support Vector Machines (SVM) [15]. \nHistogram of Oriented Gradients (HOG) \nDalal and Triggs introduced HOG in 2005 as a feature descriptor for detecting objects in \nimages by calculating gradient orientations within specific regions to capture shape and edge \ndetails essential for recognizing entities such as pedestrians. HOG becam e popular for its ability \nto adapt to changes in lighting and poses; however its dependence on predetermined features \nhindered its applicability to real world situations with a variety of object appearances [15]. \n \n16 \n \nFigure 2.1. Working of HOG [15] \nSupport Vector Machines (SVM) \nFrequently used alongside HOG techniques to detect objects were Support Vector \nMachines (SVMs). SVMs proved to be classifiers by identifying a hyperplane that separates \ndifferent classes in a feature space with many dimensions to achieve dependable results  for tasks \ninvolving binary detection. While SVMs demonstrated success in their performance for object \ndetection tasks they faced challenges when it came to scalability, in situations involving class \ndetection and were unable to capture hierarchical features effectively [15]. \nAlthough these techniques set the groundwork for object detection systems to emerge and \nevolve further in the field of AI development their progress was hindered by the need for \nhandcrafted features and inefficiencies in processing power. This led to a transition towards \nmethods that rely more on data analysis and machine learning algorithms [15]. \n\n \n17 \n \nFigure 2.2. Working of SVMs [15] \n \n2.1.2 The Emergence of Deep Learning in Object Detection \nThe rise of deep learning brought about a significant change in object detection techniques \nby leveraging improvements in technology capabilities and the abundance of extensive data sets \nalongside innovations in neural network designs. The prominence of Convolutional Neural \nNetworks (CNNs) played a role in this transformation by offering an efficient and structured \nmethod, for extracting features automatically [9, 13]. \nIntroduction of CNNs for Object Detection \nCNN architectures were initially introduced by LeCun and colleagues in the 1990S for \nrecognizing digits and later gained traction in object detection following the breakthrough of \nAlexNet in 2012 . AlexNet showcased the capacity of learning in categorizing images and \nmotivated researchers to customize CNN models, for object detection purposes unlike traditional \nmanual methods, CNN architectures are able to extract features directly from data which enhances \ntheir resilience and versatility [9]. \nEarly CNN-based object detection models, such as the Region-based Convolutional Neural \nNetwork (R-CNN) series, played a pivotal role: \n\n \n18 \no R-CNN (2014): R-CNN introduced a two-stage approach, where region proposals \nwere generated first, followed by CNN-based feature extraction and classification. \nDespite its high accuracy, the method was computationally expensive due to \nredundant feature extraction for overlapping regions [13]. \no Fast R -CNN (2015): Fast R -CNN addressed this inefficiency by sharing \nconvolutional features across regions, significantly improving speed while \nmaintaining accuracy [13].  \no Faster R -CNN (2015): Faster R -CNN introduced a Region Proposal Network \n(RPN), enabling end-to-end training and making region proposal generation more \nefficient. It set the benchmark for high-performance object detection models [13]. \nWhile the R -CNN series demonstrated the potential of CNNs, their computational demands \nhighlighted the need for real-time detection solutions [14]. \n \nFigure 2.3. Difference Between Traditional and Deep Learning Process [14] \n \n \nFigure 2.4. Architecture of R-CNN [14] \n\n \n19 \n \nFigure2.5. Architecture of Fast R-CNN [14] \n \n \nFigure 2.6. Architecture of Faster R-CNN [14] \n \n \n\n \n20 \n2.1.3 Real-Time Object Detectors \nReal time object detection is now a focus of study because its crucial in situations where \nquick and precise decision making  matters most. Like in autonomous driving systems , video \nsurveillance setups, robotics projects and augmented reality applications. Real time detectors are \ndifferent, from the two stage methods that aim for accuracy even if it means sacrificing speed; they \nprioritize efficiency without majorly affecting accuracy too much. The change has mostly been \ninfluenced by the advancement of single stage detection designs, like Yolo (You Look Once) and \nSSD (Single Shot Multibox Detector) which have reshaped the field of object detection [13, 14].  \nYOLO (You Only Look Once) Framework \nJoseph Redmon and his team introduced Yolo in 2016 as a method that changed the way \nobject detection is approached by treating it as a single regression problem that predicts bounding \nboxes and class probabilities simultaneously in one go. The design of th e architecture prioritizes \nspeed and accuracy making it ideal, for real time applications. \no Key innovations in YOLO: \n▪ Single Neural Network Architecture: YOLO processes the entire image \nwith a single neural network, dividing it into an S×SS \\times SS×S grid. \nEach grid cell predicts a fixed number of bounding boxes, confidence scores, \nand class probabilities, eliminating the need for region proposal generation. \n▪ Unified Detection Pipeline: By combining region proposal, feature \nextraction, and classification into a single pipeline, YOLO significantly \nreduces computation time. \n▪ Trade-offs Between Speed and Accuracy: While YOLO prioritizes speed, \nits initial version had difficulty detecting small objects or objects that \nappeared in clusters. However, subsequent versions addressed these \nlimitations [13]. \no Evolution in YOLO: \n• YOLOv2 (2017): Introduced anchor boxes for improved localization and \nused batch normalization for faster convergence. It also incorporated a \n \n21 \ncustom dataset called YOLO9000, enabling detection across 9,000 object \nclasses. \n• YOLOv3 (2018): Enhanced feature extraction with a deeper network and \nmulti-scale predictions, allowing better handling of objects at different sizes \n[13]. \n• YOLOv4 and Beyond (2020 –): Integrated cutting -edge advancements \nsuch as CSPDarknet as the backbone, spatial pyramid pooling, and \nimproved activation functions, achieving state -of-the-art accuracy and \nefficiency [14]. \nYOLOs flexibility and ability to scale have positioned it as a popular framework for real time \nobject detection in real world scenarios by effectively managing the balance, between precision, \ntime efficiency and computational demands. \n \nFigure 2.7. Base Architecture of YOLO Framework [14] \n\n \n22 \nSSD (Single Shot Multibox Detector) \nIn 2016 Liu et al introduced the SSD model to overcome shortcomings of Yolo focusing \non detecting small objects and those with different aspect ratios more effectively than before . \nThanks to its innovative design allowing for multi scale predictions, in diverse scenarios [14]. \no Key features of SSD: \n▪ Multi-Scale Predictions: SSDs utilize feature maps at resolutions to identify \nobjects of different sizes; lower resolution maps are for detecting larger \nobjects and higher resolution maps are, for smaller objects. \n▪ Default Anchor Boxes: Taking cues from the anchor idea in R-CNN model \nand applying it to SSD model involves setting up default anchor boxes with \ndifferent aspect ratios and scales, at different points on the feature map to \ncapture a wider array of object shapes and sizes and minimize the chances \nof overlooking detections. \n▪ Single Shot Architecture: In contrast to two step processes SSD foresees the \nlikelihoods of class and box locations at once in one go which noticeably \ndecreases the time, for inference [14]. \no Applications of SSD: \n• SSDs are especially useful for tasks that involve detecting to medium sized \nobjects efficiently like aerial surveillance systems that operate on the move \nor, in industrial automation settings [14]. \n \n23 \n \nFigure 2.8. Architecture of SSD [14] \nComparison of YOLO and SSD \nBoth YOLO and SSD are leaders in real -time object detection, but they excel in different \nscenarios: \nTable 2.1. Comparison of YOLO and SSD \nFeature YOLO SSD \nSpeed Fater due to single prediction \nlayer \nSlightly slower but still \nefficient \nAccuracy High for large, distinct objects Better for small and medium \nobjects \nArchitecture Single prediction grid Multi-scale feature maps \nEase of Training Requires fewer \nhyperparameters \nSlightly more complex \nApplication Autonomous driving, robots Mobile systems, industrial \nvision \n \n  \n\n \n24 \nAdvances in Real-Time Object Detection \nThe progress of real time object detection hasn't halted at Yolo and SSD; there have been \nenhancements that combine elements from these frameworks with new developments, in deep \nlearning technologies: \no EfficientDet (2020): \n▪ Combines EfficientNet as the backbone with a BiFPN (Bi -directional \nFeature Pyramid Network) for better multi-scale feature fusion. \n▪ Achieves state -of-the-art performance with a focus on computational \nefficiency, making it ideal for edge devices [14]. \no YOLOv5 and Beyond: \n• YOLOv5 integrates enhancements such as auto-learning anchor boxes and \nhyperparameter tuning. \n• It also emphasizes lightweight deployment, making it highly adaptable for \nmobile and embedded systems [13, 14]. \no Transformer-Based Real-Time Detectors: \n▪ Emerging approaches like Detection Transformers (DETR) leverage \nTransformers for global context modeling while maintaining real -time \nperformance. These models aim to overcome the limitations of \nconvolutions-based frameworks in complex scenarios [6, 7, 8]. \nChallenges in Real-Time Object Detection \n• Balancing Speed and Accuracy in Detection Tasks; When trying to speed up the process \nof detection tasks; there is usually a tradeoff, with the accuracy of detecting overlapping \nobjects [13].  \n• Optimizing models is crucial when installing real time detectors on edge devices with \nrestricted capacity due to resource limitations [14].  \n \n25 \n• Generalizing performance, across scenarios and datasets continues to present a challeng e \n[14]. \nConclusion \nReal time object detection systems such as YOLO and SSD have transformed the field by \nstriking a balance between speed and accuracy for use in real world settings with practical \napplications. Their designs have become benchmarks for effectiveness and scalability in inspiring \ndevelopments in the field. Through improvements in combined models and focus mechanisms, the \nfuture of real time object detection offers increased accuracy, flexibility and computational \nefficiency which will lead to its incorporation, into computer vision systems of the future. \n  \n \n26 \n2.2 CNN’s and Its Role in Object Detection \nConvolutional Neural Networks (CNNs) are a class of deep learning models designed for \nprocessing structured grid data, such as images. They are really effective in object detection due \nto their ability to automatically learn spatial features through convolutional layers. These layers \nextract low-level features (e.g., edges, corners) and build high-level representations (e.g., shapes, \nobjects). In object detection, CNNs play a crucial role in identifying and localizing objects within \nan image. Architectures like YOLO, Faster R -CNN, and SSD use CNNs as backbones to extract \nmeaningful features, which are then passed to classification and regression  layers to predict  \ncategories and bounding box coordinates [9, 14]. \n2.2.1 Two-Stream Convolutional Neural Networks (CNNs) \nSimonyan and Zisserman introduced the Two Stream CNN design in 2014 as a \ngroundbreaking approach to address both spatial and temporal analysis, in video data effectively. \nThe design comprises two parallel networks: \n• Spatial Stream:  Processes individual RGB frames to extract spatial features, \ncapturing static object appearance and context. \n• Temporal Stream: Processes stacked optical flow fields, which represent the motion \nbetween consecutive frames, to model temporal dynamics. \nThe results from both streams are combined either by adding them or by merging them to \ncreate forecasts, for action recognition assignments successfully separating spatial and temporal \naspects to extract features independently in each area [1]. \n \nFigure 2.9. Architecture of Two-Stream CNNs [1] \n \n\n \n27 \n \n2.2.2 3D Convolutional Neural Networks (3D CNNs) \nTraditional 2-dimensional convolutions are extended by 3D CNNs to include the aspect of \ntime as well as space to extract both spatial and temporal features simultaneously This method is \nespecially useful for representing intricate motion sequences and understanding the relationships, \nbetween space and time [3]. \nC3D (Convolutional 3D Networks) \nTran et al suggested the concept of  D3D which employs three convolutions across a series \nof frames to extract spatial and temporal characteristics straight from unprocessed video content. \nThe design includes layers of three-dimensional convolution and pooling operations along with \nfully connected layers, for categorization purposes. The success of D3D showcased the ability of \nthree convolutions to accurately grasp motion details and surpass the performance of conventional \ntwo-dimensional Convolution Neural Networks (CNN) in multiple object detection evaluations \n[3]. \nI3D (Inflated 3D Networks) \nCarreira and Zisserman introduced I3D in 2017 as an extension of the 2-dimensional CNN \nmodels by transforming all the 2-dimensional filters and pooling kernels into a 3-dimensional \nformat to enable the network to grasp spatiotemporal characteristics while making use of pre \ntrained 2 dimensional models for better performance in action recognition tasks using two streams. \nOne for RGB and another, for optical flow data [4]. \n \n28 \n \nFigure 2.10. Architecture of I3D Network [4] \n2.2.3 SlowFast Networks \nDeveloped by Feichtenhofer et al. in 2019, SlowFast networks address the need to capture \nobjects occurring at different temporal frequencies. The architecture comprises two pathways: \n• Spatial Stream:  Operates at a low frame rate, focusing on capturing spatial \nsemantics and slow-evolving features. \n• Temporal Stream: Operates at a high frame rate, capturing rapidly changing motion \ninformation. \nThe merging of the two pathways happens at stages to enable the system to grasp both \ngradual and rapid temporal aspects efficiently. This approach results in outcomes in tasks related \nto recognizing object categories by capturing diverse motion patterns effectively [5]. \n \n\n \n29 \n \nFigure 2.11. Architecture of SlowFast Network [5] \n \nIncorporating SlowFast networks into object detection systems enhances the ability to detect \nobjects with varying motion patterns, improving robustness in dynamic environments. \n2.2.4 Integration of CNN in Object Detection \nThe techniques created for detecting objects in images have had an impact on spotting \nobjects in videos as well. Using structures that can understand how things change over time helps \nobject detection systems deal with problems, like things blocking the view blurry movements and \nobjects looking different at times. Combining features related to space and time helps identify and \nplace objects accurately in complicated scenes [1, 3, 5]. \n \nTable 2.2. Comparison of Object Detection Models [1, 3, 5] \nModel Key Features Advantages Limitations \nTwo-Stream CNNs Separate spatial and \ntemporal streams \nEffective motion \nmodeling \nComputationally \nintensive due to \noptical flow \nC3D 3D convolutions over \nspatiotemporal data \nCaptures motion and \nappearance jointly \nHigh computational \ncost \nI3D Inflated 3D \nconvolutions with \npre-trained weights \nLeverages pre-trained \nmodels for improved \naccuracy \nRequires large \ndatasets for optimal \nperformance \nSlowFast Networks Dual pathways for \nslow and fast features \nModels multi -scale \ntemporal dynamics \nIncreased complexity \nand resource \nrequirements \n \n\n \n30 \n \nThe advancement of object detection models has significantly influenced the progress of \nobject detection systems in settings. Through structures such as Two Stream CNNs and SlowFast \nnetworks that cater to temporal features efficiently integrating these models into object detection \npipelines will improve the precision in identifying and categorizing objects, amidst movement and \ninteractions promoting the growth of smarter and more responsive computer vision applications.  \n  \n \n31 \n2.3 Advancements in Transformers for Video Analysis \nTransformers have been seen as a method in analyzing videos by tackling the complexities \nof spatial and temporal connections effectively. They were initially created for processing natural \nlanguage but are now utilized for video analysis thanks to their ability to handle long range \ndependencies using self-focus mechanisms. This section explores the development of Transformer \nmodels in video analysis with an emphasis on Vision Transformers (ViTs) , Video Swin \nTransformers well as VidTr and various cutting-edge Transformer based designs [7, 8]. \n2.3.1 Video Transformers (ViTs) \nDosovitskiy introduced the Vision Transformer (ViTs) in 2020 as a shift away from \ntraditional convolutional designs like CNNs that focus on local operations within images. Unlike \nCNNs that work with patches overlapped in images for processing the ViTs trea t image inputs as \nsequences of distinct and non-overlapping patches. Each patch is considered as a token  \ntransformed into a feature space before being passed through multiple Transformer layers allowing \nthe model to capture global connections across the entirety of the image using multi head self-\nattention [6]. \nViTs Role in Video Analysis \nViTs were first created for images but have since been adapted for analyzing videos by \nconsidering video frames as sequences of spatiotemporal patches that capture specific \nspatiotemporal features in a localized manner; furthermore the self -awareness mechanism takes \ninto account both within frame and, between frame relationships. \n• Advantages: Superior capability to model long -range dependencies compared to \nCNNs. Effective handling of global context across frames, essential for capturing \nmotion patterns and object interactions [6, 7]. \n• Limitations: Requires large -scale datasets for pretraining, such as ImageNet or \nKinetics. High computational cost due to the quadratic complexity of self-attention \n[6, 7]. \n \n32 \nApplications in Video Tasks \nViTs have proven effective in applications like identifying actions, in videos and \nclassifying them based on content over time by incorporating temporal dimensions into the patch-\nbased method for a stronger grasp of spatial and temporal connections [7].  \n \nFigure 2.12. Architecture of ViT [7] \n \nTable 2.3. Comparison of ViT and CNN Architectures in Video Analysis \nAspect Vision Transformer CNNs \nFeature Extraction Global context through self -\nattention \nLocal context through \nconvolutional filters \nTemporal Modeling Captures long -range \ndependencies via attention \nLimited to local temporal \ncontext \nScalability Requires large datasets and \ncomputational resources \n \nMore efficient with smaller \ndatasets \n\n \n33 \n2.3.2 Video Swin Transformers \nBuilding on the success of ViT, Swin Transformers introduced by Liu et al. in 2021 \nintroduced a hierarchical architecture and shifted windows to improve efficiency. Video Swin \nTransformers extend this architecture to spatiotemporal data, allowing localized attention through \n3D shifted windows. Some of the key features include: \n• Hierarchical Structure: Processes features at multiple scales, improving the \ndetection of objects and actions at varying resolutions. \n• Shifted Windows: Instead of global attention, it computes attention within localized \nwindows, reducing computational overhead while retaining critical spatial and \ntemporal relationships. \n• 3D Windows: The extension to 3D allows seamless integration of temporal \ninformation, essential for video tasks [7]. \nApplications \nThe Video Swin Transformer has shown state -of-the-art performance on datasets like \nKinetics and Something -Something V2, excelling in tasks such as action recognition, video \nsegmentation, and motion analysis. \n \nFigure 2.13. Architecture of Video Swin Transformer [7] \nTable 2.4. Performance of Video Swin Transformer vs Traditional CNNs [7, 4, 5] \nModel Top 1 Accuracy (%) Computational Cost (GFLOP) \nVideo Swin Transformer 82.7 282 \n3D ResNet-50 76.4 304 \nI3D 78.7 306 \n\n \n34 \n2.3.3 Video Transformers Without Convolutions \nVidTr represents a significant advancement in video analysis by eliminating convolutional \noperations and relying solely on Transformer architectures to model spatiotemporal data. Proposed \nby Li et al. in 2021, VidTr introduces a pure Transformer-based model for video classification. It \nprocesses video frames as sequences of patches and applies self -attention mechanisms to capture \nboth spatial and temporal dependencies. By removing convolutions, VidTr aims to leverage the \nfull potential of Transformers in modeling complex video data [8]. \n \nFigure 2.14. Architecture of VidTr [8] \nSome of the key features include: \n• Separable Attention : VidTr employs separable attention mechanisms to handle \nspatial and temporal dimensions independently, reducing computational \ncomplexity. \n• Scalability: The model can be scaled to handle longer video sequences by adjusting \nthe number of attention layers and patch sizes. \n• Performance: VidTr demonstrates competitive performance on video classification \nbenchmarks, highlighting the efficacy of Transformer-based architectures in video \nanalysis [8]. \n  \n\n \n35 \nTable 2.5. Comparison of VidTr and CNN-Based Models on Video Classification [8] \nModel Top 1 Accuracy (%) Parameters (Millions) \nVidTr 79.5 45 \nC3D 74.5 78 \nIED 77.9 50 \n2.3.4 Other Transformer-Based Models \nSeveral other Transformer-based architectures have contributed to video analysis: \n• TimeSformer: Models temporal relationships independently across frames, achieving \nhigh accuracy on action recognition tasks. \n• MViT (Multiscale Vision Transformers): Introduces a multiscale approach for both \nspatial and temporal modeling. \n• DETR (Detection Transformer): Adapts Transformer -based detection for video data, \nexcelling in temporal object tracking. \n2.3.5 Conclusion \nTransformer based  models have greatly improved video analysis by overcoming the \nconstraints of techniques in representing long range connections and time related behaviors \neffectively. Through examples, like ViTs and Video Swin Transformers , showcases how \nTransformers excel in grasping temporal associations enabling the development of more \nproductive video analysis systems. These advancements have laid the groundwork for studies \nfocused on refining Transformers for real-world video applications. \n  \n \n36 \n2.4 Applications and Challenges in Video Based Object Detection \nDetecting the objects in videos plays a role in contemporary computer vision technology \nby helping machines comprehend moving scenarios swiftly or for extended durations of time \nacross different environments. Its uses are diverse. From bolstering safety measures in self-driving \ncars to enhancing efficacy in monitoring and robotic tasks. Nevertheless, the changing nature of \nvideo information poses distinctive hurdles such as obstructions blurring caused by motion, \nlimitations, in computing capabilities and the shortage of meticulously annotated datasets that are \nof high quality. This part covers an examination of the uses and obstacles in identifying objects in \nvideos through visual analysis methods [3, 5, 7]. \n2.4.1 Applications \nAutonomous Driving \nObject detection plays a pivotal role in autonomous vehicles, enabling systems to perceive \nand navigate complex environments. Vehicles rely on real -time detection of objects such as \npedestrians, cyclists, vehicles, and road signs to make split-second decisions. \n• Key Functions: \no Detecting and tracking obstacles in real-time. \no Identifying traffic signs, signals, and lane markings for navigation. \no Monitoring surrounding vehicles to predict their trajectories and avoid collisions. \n• Challenges in Autonomous Driving: \no High-speed scenarios exacerbate motion blur, making it harder to detect objects \naccurately. \no Occlusions caused by large vehicles or environmental factors like fog and rain can \nobscure critical visual information. \n• Advancements: \no Leveraging hybrid architectures such as 3D CNNs and Transformers has improved \nspatiotemporal modeling, enabling better detection in dynamic environments [7]. \n \n \n37 \n \n \nFigure 2.15. Object Detection in a Tesla Car [7] \nSurveillance Systems \nVideo-based object detection is a cornerstone of modern surveillance systems, ensuring \nsafety in public spaces, workplaces, and private properties. \n• Applications in Surveillance: \no Identifying and tracking suspicious activities or individuals. \no Monitoring crowd behavior to detect anomalies like fights or stampedes. \no Ensuring compliance with security protocols, such as face mask detection during \npandemics. \n• Challenges in Surveillance: \no Real-time detection over prolonged periods requires high computational resources. \no Privacy concerns necessitate anonymization techniques, adding complexity to the \nsystem. \n• Advancements: \no Using multi -stream architectures like SlowFast networks has enhanced the \ndetection of fast and slow-moving objects in crowded environments [3, 5, 7]. \n \n \n \n\n \n38 \n \nFigure 2.16. Real-Time Object Detection in Surveillance Systems [7] \nRobotics \nIn robotics, object detection is essential for enabling robots to interact effectively with their \nenvironment. Applications include industrial automation, healthcare assistance, and autonomous \ndrones. \n• Key Functions: \no Detecting and manipulating objects in industrial settings. \no Navigating through complex terrains by recognizing obstacles. \no Assisting healthcare workers by detecting and delivering medical supplies. \n• Challenges in Robotics: \no Varying lighting conditions and object appearances can hinder detection accuracy. \no Real-time constraints require low -latency detection algorithms for effective \noperation. \n• Advancements: \no Lightweight detection models optimized for edge devices have enabled deployment \non resource-constrained robotic platforms [14]. \n \n \n\n \n39 \n \nFigure 2.17. Real-Time Object Detection in Robot [14] \n2.4.2 Challenges in Video-Based Object Detection \nOcclusions and Motion Blur \nOcclusions occur when objects are partially or fully blocked by other objects in the scene, \nmaking it difficult to detect or track them accurately. Motion blur, caused by rapid object \nmovement or camera motion, further complicates detection. \n• Impact on Detection: \no Occlusions can result in fragmented tracking or missed detections. \no Motion blur distorts object appearance, leading to classification errors. \n• Solutions: \no Utilizing temporal information across multiple frames helps infer occluded \nobjects' positions. \no Advanced architectures like VidTr leverage attention mechanisms to model \nobject trajectories effectively [15]. \n \n \n \n\n \n40 \n \nFigure 2.18. Illustration showing blurred and occluded objects in a dynamic scene [15] \nReal-Time Constraints and Computational Costs \nReal-time applications, such as autonomous driving and surveillance, demand high -speed \nprocessing to ensure timely decision -making. Achieving this requires significant computational \nresources, particularly for high-resolution video data. \n• Challenges: \no Balancing accuracy with inference speed. \no Handling large volumes of data in real -time without compromising \nperformance. \n• Solutions: \no Real-time detectors like YOLO and SSD prioritize speed through single -shot \narchitectures. \no Efficient Transformers, such as Video Swin Transformers, incorporate \nlocalized attention mechanisms to reduce computational overhead [15]. \n \n\n \n41 \nTable 2.6. Comparison of Real-Time Object Detectors [13, 14, 17] \nModel Speed (FPS) Accuracy (mAP) Computational Cost \n(GFLOPs) \nYOLOv5 45 50.7 28 \nSSD 22 48.5 50 \nVideo Swin 30 55.2 282 \n \nDataset Limitations and Annotation Challenges \nHigh-quality datasets are essential for training robust video-based object detection models. \nHowever, creating such datasets involves significant challenges, including: \n• Annotation Complexity: \no Annotating video frames is labor -intensive and time -consuming, especially for \nlarge datasets. \no Temporal annotations require precise labeling of object interactions and trajectories \nacross frames. \n• Dataset Bias: \no Existing datasets may lack diversity in object types, environments, or lighting \nconditions, limiting model generalization. \n• Solutions: \no Leveraging synthetic datasets generated by GANs or simulation environments can \nalleviate annotation bottlenecks. \no Semi-supervised learning methods can make better use of unannotated or partially \nannotated video data [3, 8]. \nTable 2.7. Popular Video Datasets for Object Detection \nDataset Number of Videos Classes Challenges \nKinetics 650,000 400 Diverse actions, high \ncomplexity \nActivityNet 20,000 200 Limited temporal \nannotations \nUCF101 13,320 101 Limited diversity, \nsmall scale \n \n42 \nConclusion \nVideo based object detection is used in driving, surveillance systems and robotics to \nenhance safety, efficiency and decision-making processes. However, there are obstacles like \nobstructions blurred movements, real time demands and limited data sets. To tackle these hurdles, \nwe need to improve model structures, optimize methods and expand data sets. By conquering these \nchallenges video-based object detection will progress further leading to smarter and more reliable \nsystems, across various fields. \n2.5 Gaps in Existing Research \nWhile there have been strides in video-based object detection technology advancements \nthere are still key challenges that prevent its seamless integration into real life situations leaving \nplenty of room for enhancement. One major obstacle is the inefficiencies tied to combining \ntemporal features. T he top tier techniques today heavily depend on Convolutional Neural \nNetworks (CNNs) to extract features effectively and excel at capturing detailed information, within \neach frame of a video segment. CNNs have a challenge when it comes to capturing how thin gs \nchange over time in videos due to their design limitations in modeling relationships between \nframes accurately for recognizing movements and trajectories of objects in dynamic video clips. \nAlthough methods like RNN and temporal convolutions have tried to improve modeling in videos \nthey often end up sacrificing the quality of spatial details causing a less comprehensive grasp of \nthe video context. For instance while RNN performs well with data it struggles with managing \nlarge amounts of data and maintaini ng detailed spatial information, over extended periods. \nExtending operations into the temporal domain with 3 dimensional CNN models has its limitations \nas it mainly focuses only short-term dependencies and may fall short when detecting objects, in \nintricate and lengthy activities. \n \nA major issue is that current video object detection models lack scalability due to their \ncomputational requirements and memory usage limitations which hinder their usability in \nenvironments with limited resources, like surveillance systems or autonomous vehicles that often \ndeal with high resolution video feeds. For example in self -driving cars real time identification of \nobjects necessitates models to handle video feeds concurrently and respond swiftly to make rapid \n \n43 \njudgements. However current designs are not tailored for these paced situations causing potential \nsafety risks. Moreover devices at the edge and portable gadgets commonly employed in the \nInternet of Things and embedded setups lack the computing power to accommodate these complex \nmodels which hinders their integration, into decentralized systems. The problem of scalability is \nworsened by the energy usage linked to cutting edge models that restricts their use in eco-friendly \nand energy efficient setups. \n \nVideo object detection models struggle to adapt to real world situations due to their limited \ntraining data that often lacks diversity in conditions, like lighting and camera angles as well as the \ntypes of objects included for identification purposes. Detecting objects in a surveillance video or \nin challenging weather conditions like fog or rain can be quite tough for models trained on daylight \ndata sets. Moreover, differences in how objects look. Such as size, position and partial blockages. \nMake it even harder often resulting in missed detections or incorrectly identifying objects. This \nchallenge is especially noticeable in city settings where objects often overlap, move erratically and \nhave different physical characteristics. \n \nRelying on annotated datasets can slow down progress in video object detection research \nas it takes a lot of effort and time to create high quality labels for video data compared to static \nimages labeling process. Temporal annotations do not need  object labels but also detailed \ntrajectory information across frames which can be quite complex, in dynamic settings. \nFurthermore, current datasets do not have a wide variety of object categories settings and \ncircumstances which also restricts the ability of models trained on them to apply broadly semi \nsupervised and unsupervised learning techniques have demonstrated potential in addressing these \nchallenges but their use, in video object detection is still relatively restricted. \n \nProcessing data efficiently remains a major hurdle to overcome in the field of computing. \nVision Transformers (ViTs) and Video Swin Transformers have shown progress in capturing \nrelationships and dependencies over long distances. However, their computation complexity \nincreases quadratically as the input size grows leading to costs especially for lengthy video \nsequences or high resolution frames. This drawback hinders their use, in real time processing or \nedge computing scenarios. Ensuring video object detection models can keep up with the demands \n \n44 \nof applications without compromising performance hinges, on developing more effective attention \nmechanisms and streamlined architectures. \nTable 2.8. Comparison of Current Approaches to Video-Based Object Detection \nApproach Integration of \nSpatial and \nTemporal \nFeatures \nScalability Generalization \nto Real-World \nScenarios \nGeneralization \nto Real-World \nScenarios \n2D CNN + RNN Moderate Low Low High \n3D CNN Limited \nTemporal \nIntegration \nLow Moderate Very High \nTransformer-\nBased Models \nStrong Moderate High Moderate \nHybrid \nArchitectures \nHigh Low Moderate High \n \nTable 2.9. Performance Challenges in Real-World Applications \nChallenge Impact Example Scenario \nInefficient Temporal \nModeling \nMissed detections in complex \nmotion \nFast-moving vehicles in \nautonomous driving \nHigh Computational Demand Limited deployment on edge \ndevices \nReal-time surveillance in \nremote areas \nDataset Bias Poor generalization to diverse \nenvironments \nObject tracking in varied \nlighting conditions \n \nThe gaps highlight the importance of advancements in merging space time characteristics \nand enhancing computational performance while creating tailored models that merge the benefits \nof recognizing actions and detecting objects together is essential for pushing forward video-based \nobject detection to cater to the requirements of practical applications, in various areas. \n2.6 Conclusion \nIn looking at studies on video-based object detection methods have shown progress but also \nareas that still need improvement. Convolutional and recurrent neural networks have helped with \ntemporal modeling but their structures are not seamless and struggle with scalability. Transformer \nbased models offer an option by effectively capturing distant connections and context links; \nhowever they come with high computational costs and struggles in adapting to specific object \n \n45 \ndetection needs. In this field there is a need for innovative models that can perform object detection \neffieiently.  \n \nThe identified shortcomings point to the need for a framework that can tackle issues such as \ninefficient spatial temporal integration and scalability limitations while also incorporating hybrid \narchitectures they lack currently. This study  uses a 3D ResNet-Transformer framework intended \nto unify spatial and temporal feature modeling challenges efficiently by harness the strong spatial \nfeature extraction abilities of 3D ResNet along, with the extensive temporal modeling capabilities \nof Transformer self-attention mechanisms the method seeks to address the drawbacks of current \nmodels effectively.  \n \nIncludes various temporal scales to enhance the precision of object detection, in different \nand changing settings. This mixed approach is created to expand efficiently for real life uses like \nself-driving cars, surveillance and robotics tackling both the i ssues related to processing power \nand practical deployment. Drawing from research findings and addressing the existing limitations \nidentified in the literature this study aims to progress the current standards of video-based object \ndetection by providing a reliable and adaptable solution, for challenging real life situations.   \n \n46 \n3. METHODOLOGY \nIn my research project I set out to tackle the difficulties associated with identifying objects \nin videos by fine-tuning a model that combines 3 dimensional convolutional neural networks \n(CNNs) and Transformers. This unique blend of technologies known as the 3 D ResNet \nTransformer aims to effectively capture both spatial and temporal characteristics, within video \nsequences without sacrificing computational speed. Through this process I outline the method I \nused to develop, execute and assess the effectiveness of this model. This model used the base \narchitecture of 3D ResNet – Transformer, effective adjustments are made in terms of various \nparameters to fine-tune the model for autonomous driving.  The base model architecture is taken \nform the paper: Integrating 3D convolutional neural networks and transformer for  video action \nrecognition by Yan Cheng, Lingfeng Wan [2]. \n \nThe main aim was to utilize the spatial and temporal feature extraction abilities of 3D ResNet \nand improve the overall temporal relationships with the self-attention mechanism of Transformers. \nIn order to validate the model’s performance a variety of experiments were carried out using  the \ndatasets, UFC101, COCO, Waymo Open. The subsequent sections provide in-depth information \non the datasets, model structure, application details and assessment methodology. \n3.1 Dataset Description \nTo train and evaluate th e 3D ResNet-Transformer, I used UCF101, COCO and Waymo \nOpen. These datasets are benchmarks in the  object recognition for autonomous driving domain \ndue to their diverse range of categories and challenging scenarios. \n3.1.1 UCF101 Dataset \nThis The UCF101 dataset contains 13, 320 video clips grouped into 10 1 categories \nincluding sports activities and interactions involving humans and objects or human to human \ninteractions. Divided into 9,537 training videos and 3, 783 test videos . Dealing with this dataset \npresented challenges like differences between classes, changing backgrounds and varying video \nquality levels. \n \n47 \n3.1.2 COCO-VID Dataset \nThe COCO-VID dataset is based on the well -known COCO dataset which was initially \nused for image based object detection and segmentation, to meet the need for object detection and \ntracking in video sequences . It includes temporal annotations which allow to analyze the \nmovement of objects and their interactions in adjacent frames.  COCO-VID has the same \ncharacteristics as COCO in terms of diversity since it includes various objects and complex scenes, \nbut it has been extended to include temporal information to support video object detection. This \ndataset is very relevant for the development and testing of algorithms that are intended to identify \nand follow objects in three dimensional spaces with much background. \n3.1.3 Waymo Open Dataset \nWaymo Open Dataset is a comprehensive dataset which aims at supporting autonomous \ndriving research with a large amount of high quality sensor data, such as LiDAR, camera and 3D \nannotations.  It includes a number of training categories which are important for the autonomous \nsystems to function in the real world such as vehicles, pedestrians, cyclists and signs. The dataset \ncontains both 2D and 3D bounding box labels and per frame object tracks for scene understanding. \nIt covers a number of different environments including urban, suburban and highway which makes \nit ideal for model training for various tasks like object detection, semantic  segmentation and \nbehavior prediction. Thus, the Waymo Open Dataset with its precise annotations and multimodal \ndata is essential for the development of the state -of-the-art perception algorithms in self-driving \nsystems. \n \n \nTable 3.1. Comparative summary of the datasets \nDataset Categories Training Set Test Set \nUCF101 101 9,537 3,783 \nCOCO 80 118K 41K \nWaymo Open 4 1000+ 200+ \n \n \n48 \n3.2 Model Framework \nTo address the challenges of spatiotemporal feature extraction and long-range dependency \nmodeling, I fine-tuned the 3D ResNet -Transformer model, which consists of two main \ncomponents: a 3D ResNet backbone and a Transformer encoding layer. Below, I describe the task \nand architecture in detail. \n3.2.1 Task Description \nThe object detection task involves predicting the class of a given video segment. Formally, \nlet D =  {(xI, yi)}{i=1}\nn  represent the dataset, where 𝑥𝑖  is a video segment and 𝑦𝑖 is the \ncorresponding action label [2]. My goal was to design a model g such that g(𝑥𝑖)= 𝑦𝑖 for any unseen \n𝑥𝑖  . The model outputs a probability distribution over all possible classes using a softmax classifier. \n3.2.2 3D ResNet Backbone \nThe 3D ResNet component of my model was responsible for extracting local \nspatiotemporal features from video data. I used a 3D ResNet -34 architecture, which applies 3D \nconvolutions to process video segments. The input to the 3D ResNet was a tensor of shape \n(N,C,T,H,W), where N is the batch size,  C is the number of channels (RGB), T is the temporal \ndimension, and H, W are the spatial dimensions. \n \nEach 3D convolutional layer applied a kernel K of size (kt, kh, kw) to capture both spatial \nand temporal features. The operation is defined as: \n𝑌(𝑖, 𝑗, 𝑘) =  ∑ ∑ ∑ ∑ 𝑊(𝑝, 𝑞, 𝑟)\n𝑅\n{𝑟=1}\n𝑄\n{𝑞=1}\n𝑃\n{𝑝=1}\n𝐶\n{𝑐=1}\n⋅ 𝑋(𝑖 + 𝑝, 𝑗 + 𝑞, 𝑘 + 𝑟) +  𝑏 \n \nHere, W represents the kernel weights, X is the input tensor, and b is the bias term. \nTo address vanishing gradients in deep networks, I incorporated residual connections, \nallowing gradients to flow more effectively during backpropagation. This design ensured that the \nnetwork could extract rich spatiotemporal features across layers [2]. \n \n49 \n3.2.3 Transformer Encoding Layers \nAfter processing the video through the 3D ResNet, I passed the extracted features to a \nTransformer encoding layer. This layer utilized a multi -head self-attention mechanism to model \nglobal temporal relationships. The self -attention mechanism calculates the  interaction between \ninput features as: \n                    Attention(𝑄, 𝐾, 𝑉) = softmax (\n𝑄𝐾𝑇\n√𝑑𝑘\n) 𝑉                                          3.2 \n \nwhere Q,  K, V are the query, key, and value matrices derived from the input, and 𝑑𝑘  is the \ndimensionality of K. To preserve temporal ordering, I added positional encodings to the input \nfeatures: \n                                           𝑃𝐸(pos, 2𝑖 + 1) = cos (pos ⋅\n10000\n2𝑖\n𝑑\n 1 )                                             3.3 \n                                           𝑃𝐸(pos, 2𝑖 + 1) = cos (pos ⋅\n10000\n2𝑖\n𝑑\n1 )                                             3.4 \nThe final output is a tensor of shape (N,  Classification_Num) where  Classification_Num \ncorresponds to the number of action classes [2]. \n \nFig 3.1. Methodology Overview \n\n \n50 \n \nFigure 3.2. Base Architecture of 3D ResNet-Transformer Model [2] \n3.2.4 DeepStream SDK for Object Detection \nWhen aiming to improve the object detection abilities of the suggested 3D ResNet -\nTransformer model I utilized NVIDIAs DeepStream SDK as a component of the deployment \nprocess.DeepStream SDK stands out as a robust, finely tuned platform designed for creating video \nanalytics applications powered by AI making it a great fit for carrying out real time object detection \ntasks.This segment goes over how DeepStream SDK was incorporated into the methodology \ndiscusses its advantages and highlights how it plays a role,  in enhancing the performance of the \nobject detection system. \n \nNVIDIA has created the DeepStream SDK to help with streaming analytics for AI \napplications such as video analysis in areas like retail and surveillance, in smart cities and \nautonomous machines. DeepStream makes use of NVIDIAs TensorRT to speed up learning \nmodels and utilizes GStreamer to handle video streams efficiently on NVIDIA GPUs for real time \n\n \n51 \ninference and data processing. For this purpose, all the data training and model building is done in \na GPU powered by NVIDIA named Jetson AGX Orin. \n• Key Features of DeepStream SDK: \no Scalable Video Analytics: Handles multiple video streams simultaneously with \nGPU acceleration. \no Hardware Efficiency: Provides support for Jetson devices and NVIDIA GPUs, \nenabling object detection applications to run on embedded systems. \no GStreamer Integration: Uses GStreamer plugins to manage streaming data, \nincluding input preprocessing, model inference, and post-processing stages. \nModel Preparation for DeepStream \nAfter completing the training of the 3 D ResNet-transformer model for recognizing video \nactions and detecting objects in videos successfully I proceeded to utilize TensorRT for converting \nthe trained model into a more efficient format suitable for seamless integration, with DeepStreams \noperations. \n• The conversion includes: \no Model Quantization: Model was quantized to use FP16 precision, which reduces \ncomputation while maintaining high accuracy. \no TensorRT Conversion: Model was converted into a TensorRT -optimized engine, \nenhancing inference speed through reduced precision operations and model \noptimization. \nPipeline Development using DeepStream \nI then developed a GStreamer -based DeepStream Pipeline to implement real -time object \ndetection. The pipeline consists of several components that process video frames from the input to \ngenerate detection outputs. Here are the stages of the pipeline: \n• Video Stream Input: \no The video data was sourced from a live feed and pre-recorded files. \n \n52 \no DeepStream’s nvstreammux plugin was used to merge multiple video feeds into a \nsingle stream, hence proceeds with efficient resource usage during processing. \n• Preprocessing: \no It involved resizing, normalization, and frame batching using the nvvideoconvert \nand nvdspreprocess plugins. This made the video data suitable for input to the 3D \nResNet-Transformer model. \n• Inference with Optimized Model: \no The optimized 3D ResNet -Transformer model was integrated using the nvinfer \nplugin. This plugin used the TensorRT engine for efficient real -time inference on \nincoming video frames. \no The self-attention mechanism of the Transformer layer was useful for enhancing \nfeature relationships in scenes involving overlapping or occluded objects. \n• Post-Processing and Object Tracking: \no The results were processed using DeepStream’s nvtracker plugin to maintain \nunique IDs for detected objects across frames. \no Additional custom plugins were used to implement post -processing logic, such as \nnon-maximum suppression (NMS), to refine bounding box predictions. \n• Output Display and Analytics: \no The processed video was displayed using the nveglglessink plugin, while analytics \ndata was logged for further evaluation. \n \n53 \n \nFigure 3.3. DeepStream Pipeline for the 3D ResNet-Transformer Model \nAdvantages of Using DeepStream SDK \nThe incorporation of DeepStream SDK into the object detection workflow offered many \nadvantages: \n• Real-Time Inference: \no DeepStream SDK, in combination with TensorRT optimization, enabled real -time \ninference for high-resolution video streams. \n• High Throughput and Scalability: \no DeepStream’s ability to process multiple streams simultaneously allowed the \nmodel to be deployed on multiple input sources concurrently, leading to high \nthroughput in video analytics. \n\n \n54 \no By running the system on NVIDIA Jetson AGX Orin, latency is greatly reduced \ncompared to running on traditional GPUs. \n• Efficient Resource Utilization: \no Using FP16 precision with TensorRT helped balance computational load while \nmaintaining accuracy. This made it easy to run the model on GPU -accelerated \ndevices with less computational resources. \no DeepStream's efficient memory management and GStreamer integration helped \nreduce the overhead involved in frame handling and batch processing. \n• Integrated Object Tracking: \no Using the nvtracker plugin for object tracking allowed the model to maintain \nconsistency in object IDs across frames. \n3.3 Autonomous Drone Setup \nThis section provides the hardware and software configuration for the autonomous drone \nsystem. The setup integrates multiple  components to achieve autonomous flight, navigation, and \nobstacle detection. Each system was carefully selected to meet the project's requirements for \nefficiency, reliability, and scalability. \n3.3.1 Drone Frame and Propulsion System \n  The drone is built on a robust quad -propeller chassis designed to accommodate all  \ncomponents securely while ensuring optimal aerodynamics and stability. This drone’s parts are  \nordered from an external source “Holybro”. The propulsion system includes four brushless motors \nand electronic speed controllers (ESCs), providing efficient thrust and maneuverability. Power is \nsupplied by a 5A LiPo battery pack. Drone's navigation and control is done by Pixhawk 6X flight \ncontroller, a powerful and versatile module for autonomous flight. The Pixhawk 6X is responsible \nfor stabilizing the drone, executing flight plans, and integrating data from sensors. It provides real-\ntime feedback to maintain stability during flight and interfaces seamlessly with other components. \nThis is connected to the Jetson Nano via ethernet port to communicate. \n \n \n55 \n3.3.2 Computing Unit \n  The drone uses a Jetson Nano as its onboard computing unit. This module is specifically \nchosen for its GPU-accelerated processing capabilities and lightweight, enabling the execution of \ndeep learning algorithms for real -time object detection and path planning. The Jetson Nano \nprocesses data and communicates with the Pixhawk 6X to adjust flight parameters dynamically.  \nFlight parameters are passed by generating a p roto.txt file and passing it to the flight controller. \nJetson Nano runs a companion computer software stack  called MAVROS. This is a ROS (Robot \nOperating System) node that acts as a bridge between the Jetson Nano and the Pixhawk. \n3.3.3 Obstacle Detection and Depth Sensing \n  To achieve precise obstacle avoidance, the drone is equipped with an Intel RealSense D435 \ndepth camera. The camera captures high -resolution depth maps and RGB images, allowing the \nsystem to detect obstacles in the drone's path and calculate alternative paths. Its lightweight design \nand compatibility with the Jetson Nano make it an ideal choice for this application. \n3.3.4 GPS Navigation \n  The drone is equipped with a GPS module to support autonomous waypoint navigation and \ngeofencing. The GPS module provides accurate location data, enabling the Pixhawk 6X to execute \npre-programmed flight missions with precision. The GPS data is also utilized to maintain the \ndrone's position during hover and adjust its route dynamically in response to obstacles. \n3.3.5 Manual Control \n  Manual control is  done by a FlySky transmitter and receiver , providing  direct control \nduring testing phases or emergency operations. Th is step is crucial as it enables reliable \ncommunication between the operator and the drone for takeoff, landing, and overrides. The \ntransmitter operates on a stable frequency range to minimize interference and maintain consistent \nconnectivity. \n \n56 \n \nFig 3.4. Hardware Setup for Autonomous Flight \n \n  \n \nFig 3.5. Pixhawk 6X Flight Controller (left), Jetson Nano (top middle), Flysky Transmitter \n(right), Intel RealSense D435 Depth Camera (bottom) \n\n \n57 \n3.4 Implementation Details \n3.4.1 Preprocessing \nI preprocessed the video data by sampling frames at 16 frames per second (fps) and \nresizing them to 112×112  pixels. I normalized the pixel values to zero mean and unit \nvariance for consistent input representation. \n3.4.2 Training \nThe 3D ResNet component was initialized using weights pre -trained on the Kinetics-400 \ndataset. Categorical cross-entropy loss has been used to train the model [2]: \n                      𝑳 = −\n𝟏\n𝑵 ∑ ∑ 𝒚𝒊,𝒌\n𝑲\n𝒌=𝟏 𝐥𝐨𝐠(𝒚𝒊,𝒌̂ )𝑵\n𝒊=𝟏                                                       3.5 \nHere, 𝑦𝑖,𝑘 is the true label, and (𝑦𝑖,𝑘̂ ) is the predicted probability for class k. \nI optimized the model using stochastic gradient descent (SGD) with momentum  μ=0.93 and a \nlearning rate schedule of 10(−2). \n3.4.3 Hyperparameter Optimization & Performance \nHyperparameter tuning is crucial for enhancing the efficiency of the 3D ResNet -\nTransformer model. There were several hyperparameters considered during training sessions and \neach affected them differently.  \nLearning Rate: \nThe learning rate controls the step size during gradient descent. I experimented with a range \nof initial learning rates, including 10(−1), 10(−2), 10(−3).  \no Impact: \n▪ A high initial learning rate 10(−1) caused instability, leading to fluctuating \nloss values during training. \n \n58 \n▪ A moderate rate 10(−2) provided smooth convergence and yielded the best \nresults. \n▪ A low rate 10(−3) resulted in slower convergence and suboptimal \nperformance due to insufficient exploration of the parameter space. \nBatch Size: \nThe batch sizes of 16, 32 and 64 were tested to balance computational efficiency and \ngradient stability. Larger batch sizes improved gradient estimation accuracy but required more \nmemory, which limited their practicality. \no Impact: \n▪ A batch size of 32 provided the best trade -off between computational \nefficiency and performance. Larger batches led to marginally better \naccuracy but increased memory usage significantly, while smaller batches \ncaused noisier gradient updates. \nDropout Rate: \nThe Dropout was used to regularize the transformer encoding layer, with rates of 0.1 , 0.2 \nand 0.5 being tested. \no Impact: \n▪ A dropout rate of 0.1 balanced regularization and information retention, \nachieving the best results.  \n▪ Higher dropout rates caused underfitting, as too many features were \nrandomly zeroed during training, reducing the model’s capacity. \nWeight Initialization: \nThe 3D ResNet backbone was initiated with weights pre -trained on the Kinerics-400 \ndataset, while the transformer layers were initialized using pretrained Vision Transformer model \non ImageNet dataset. \n \n59 \no Impact: \n▪ Using pre -trained weights significantly accelerated convergence and \nimproved accuracy on datasets compared to training from scratch. \nOptimizer: \nThe optimizer used was Stochastic Gradient Descent (SGD) with momentum  0.93. \nAlternative optimizers such as Adam and RMSprop were also tested but performed slightly worse.  \no Impact: \n▪ SGD with momentum provided smoother convergence and better \ngeneralization, as it efficiently traversed the loss landscape while avoiding \nlocal minima. \nNumber of Transformer Attention Heads: \nThe number of heads in the transformer’s multi -headed attention mechanism determines \nhow many parallel attention distributions are computed. The graph shows the diminishing results \nof increasing attention heads. \n \nFig 3.6. Attention heads Performance on Each Dataset \n \n\n \n60 \nSummary of Optimized Hyperparameters: \nThe following configuration provided the best performance: \n• Learning Rate: 10(−2) \n• Batch Size: 32 \n• Attention Heads: 8 \n• Dropout Rate: 0.1 \n• Weight Initialization: Pre -trained Kinetics-400; Vision Transformer on ImageNet for \nTransformer \n• Optimizer: SGD with momentum. \n3.5 Evaluation \nI evaluated the model  using the datasets mentioned above using accuracy as the primary \nmetric: Accuracy =\n𝑻𝒄\n𝑻 × 𝟏𝟎𝟎%. Where 𝑇𝑐 is the number of correctly classified samples and T is \nthe total number of test samples. The model achieved  mAP of 47.1 on COCO dataset, 67.2 on \nWaymo Open and 61.5 on UCF101 dataset. \n  \n \n61 \n4. RESULTS & DISCUSSION \nIn this section of the study a detailed assessment is given of the 3D ResNet-transformer \nmodel, for detecting objects effectively. The model’s performance is examined using datasets and \nits outcomes are contrasted with current techniques. Significant discoveries emphasize the model’s \ncapacity to manage spatiotemporal connections optimize computational effectiveness and adapt \nacross various tasks.  \n \nIn the domain of  object detection, the model displayed accuracy levels when tested on the  \ndatasets COCO, Waymo Open and UCF101. Further insights into these outcomes are elaborated \nin the sections with thorough analysis complemented by tables, graphs and detailed conversations. \n4.1.1 Dataset Performance \nThe 3D ResNet-Transformer underwent Testing using the same datasets known as common \nbenchmarks, in  object detection . The assessment criteria applied were the accuracy metric \ntypically used for evaluating classification models. The outcomes of the suggested model were \ncontrasted with baseline models. \nTable 4.1. Accuracy Comparison on UCF101 \nModel UCF101 Accuracy \n(mAP) \nWaymo Open \n(mAP) \nCOCO (mAP) \nTwo-Stream CNN 44.0  [2] 32.4 23.5 \nRNN-FV+iDT 56.5  [2] 42.7 30.8 \nSTC-ResNext101 67.6  [2] 58.5 39.7 \nD3D 70.1  [2] 64.9 46.8 \nVidTr 69.2  [2] 66.0 47.5 \n3D ResNet-\nTransformer (with \nfine-tuning) \n61.5 67.2 47.1 \n \nThe findings show that fine-tuned model performs similar to the best model D3D. This fine-tuned \nmodel is trained using 3 datasets. D3D integrates dynamic convolutions to optimize both spatial \nand temporal feature representations efficiently.  This maybe the reason for better ac curacy, 3D \nResNet-Transformer model benefits from training on three datasets, which enhances \n \n62 \ngeneralization but may introduce some domain mismatch if the categories differ significantly from \nUCF101. \n4.1.2 Ablation Study \nTo quantify the contributions of the 3D ResNet and Transformer components, I conducted \nan ablation study. The results, presented in Table 4.2, highlight the necessity of integrating  the \nmodules. \nTable 4.2. Impact of 3D ResNet and Transformer Components \nComponent COCO (mAP) UCF101 (%) Waymo Open (mAP) \nTransformer Only 43.5 95.0 64.2 \n3D ResNet -\nTransformer \n47.1 96.3 67.2 \n \nThe research findings indicate that although the Transformer encoding layer performs well \non its own, incorporating the 3D ResNet substantially improves accuracy when analyzing several \ndatasets. This confirms the impact of blending spatiotemporal feature extraction, with long range \ndependency modeling. \n4.1.3 Object Detection Results \nTo test the model’s capability in object detection, I fine -tuned it on the COCO dataset, \nwhich is a benchmark for evaluating object detection models. The evaluation metric used was \nMean Average Precision (mAP), calculated over 80 object classes.  Some example figures are \nshown below. All these were captured using the proposed model. \n \n63 \n \nFig 4.1. mAP (mean Average Precision) Metrics for Different Models across the COCO, \nWaymo Open, and UCF101 datasets \n \n \nFigure 4.2. Trail Detection and Follow Captured form the Camera on a Drone \n \nTable 4.2. mAP Metrics of Some Datasets Over 3D ResNet-Transformer Model and Others \nModel mAP (%) \nFaster R-CNN 42.3 \nYOLOv7 45.1 \n3D ResNet-Transformer 47.1 \n\n \n64 \n \nThe results indicate that the proposed model outperforms popular object detection architectures \nlike Faster R -CNN and YOLOv 7. The improvement in mAP is attributed to the ability of the \nTransformer encoding layer to capture contextual relationships between objects. \n \n \nFigure 4.3. Image of the Drone with Intel RealSense Camera mounted \n \nFigure 4.4. Semi-dense SLAM Maps for Obstacle Avoidance \n \n\n \n65 \n \nFigure 4.5. Information Monitoring from Jetson Device \n4.1.4 Performance Summary \nThe results confirm the effectiveness of the 3D ResNet -Transformer in handling object \ndetection tasks as well as the ability of autonomous flying of drone: \n• State-of-the-Art Results: The model achieves the better consistent accuracy on various \ndatasets and competitive mAP on object detection benchmarks. \n• Versatility: The hybrid architecture adapts seamlessly to diverse tasks, highlighting its \nflexibility. \n4.1.5 Challenges and Limitations \nDespite its strengths, the model has certain limitations: \n• Computational Complexity: The hybrid design increases the computational overhead, \nwhich may hinder real-time applications. \n• Dataset Dependency: Performance depends heavily on the quality and diversity of \ntraining datasets. \n\n \n66 \nThe outcomes underscore the performance of the fine-tuned 3D ResNet-Transformer Model and \nits room for improvement in the future showcasing that the integration of 3 D ResNet and \nTransformer creates a sturdy structure suitable, for tasks dealing with intricate visual information. \n  \n \n67 \n5. CONCLUSION & FUTURE WORK \nThe main goal of this research was to create a hybrid model called 3D ResNet-Transformer \nfor detecting objects in videos more efficiently and use the same model for autonomous systems. \nBy merging the ability of 3D ResNet to extract features with the self -focus mechanism of \nTransformers this model effectively tackled the issues related to intricate temporal connections \nand spatial feature associations, in video datasets. This project al so integrated the NVIDIA \nDeepStream SDK for implementation in real world situations like advanced video analysis and \nedge computing applications. This chapter outlines the discoveries and impacts of this study. \n5.1 Achievements in Object Detection \nThe 3 D ResNet-Transformer model underwent  testing on UCF101, COCO and Waymo \nOpen and demonstrated leading edge performance levels by attaining Map scores of 61.5, 47.1 and \n67.2 respectively. This model proved it’s consistency over other established methods such, as the \nTwo Stream CNNs, RNN FVs+iDTs and STCs ResNext101 versions showcasing notable \nadvancements. Results indicate the following achievements: \n• The combination of 3D convolutional layers and Transformer encoding allowed the system \nto effectively grasp both nearby spatial and temporal patterns as well, as distant \nconnections demonstrating higher levels of precision. \n• The collaborative impact of merging two frameworks has contributed to addressing the \nissues posed by occlusions and simultaneous actions often encountered in tasks related to \nrecognizing actions. \n5.2 Object Detection Capabilities and Deployment \nThe suggested framework was customized for object detection assignments utilizing the \nCOCO dataset. Yielded a Mean Average Precision (mAP) of 47.1 over several established models \nsuch as Faster R-CNN and YOLO v 7. This model could be deployed in real-world scenarios with \nthe following outcomes:  \n \n68 \n• The integration of NVIDIA TensorRT to optimize the model for inference led to substantial \nimprovements in speed, enabling real-time detection. \n• Applying the DeepStream SDK to implement the model ’s deployment offered benefits \nsuch as boosting the frame rate to 30 FPS and cutting inference latency. These \nenhancements make the model well suited for performing real time analysis in fields such \nas video monitoring and self-driving cars and autonomous flying drones. \n• The model was used on Jetson edge devices, it showed scalability in its design that allowed \nit to be versatile, in low power settings while still maintaining detection capabilities. \n• By employing DeepStream SDK and TensorRT, the model was able to meet real -time \nrequirements, thereby making it suitable for practical deployment in edge devices and \nlarge-scale systems. \n5.3 Future Work \nThe model showed promising performance however there are multiple ways for additional \ninvestigation and improvement to be explored further in depth below are some exciting paths \nfor future research that could greatly enhance the potential of this model and overcome its \nexisting constraints. \nEnhancing Transformer Architectures: \nThe self-focus mechanism in the Transformer is computationally intensive when dealing \nwith lengthy video sequences; henceforth there could be potential, for investigating alternative \nTransformer structures that offer greater efficiency in such scenarios. \nReal-Time and Low Latency Optimizations: \nAlthough the current model has achieved frame rates there is room for additional \nimprovements to ensure that the model is optimized for quick and real-time deployment. \n• Implementing pruning techniques to remove redundant weights and quantization to INT8 \nprecision can lead to significant speed improvements without major losses in accuracy. \n \n69 \n• Adaptive inference mechanisms, where not all frames are fully processed, could reduce the \ncomputational load further while retaining detection accuracy. Dynamic computation \ngraphs and adaptive frame skipping could be employed to achieve this. \nApplication to Multi-Modal Learning: \nThe current model is focused only on visual data , future extensions could involve the \naddition of multimodal inputs. \n• Including audio signals along with videos can enhance detection. \n• Adding text inputs, such as captions or context descriptions, to enhance scene \nunderstanding. \nImproving Spatiotemporal Consistency in Object Detection: \nTo further improve the spatiotemporal consistency of the detected actions: \n• Adding memory modules could enable the model to remember crucial previous features. \n• Adding Long Short-Term Memory (LSTM) layers with the Transformer encoder may \noffer enhanced temporal modeling by explicitly modeling time-based dependencies. \nBroader Applications in Real-World Scenarios: \nAdapting the model for pedestrian detection  in autonomous vehicles may improve safety \nby accurately predicting actions and movement. U sing the model for monitoring patients and \nrecognizing activities, such as falls or abnormal behaviors, would help healthcare professionals \nin providing timely interventions. Applying the model to sports action recognition and tracking \ncould provide insights into player movements,  strategies, and statistics  that can be used for \nfurther modifications and help in better gameplay. \n  \n \n70 \n5.4 Concluding Reflections \nDeveloping and perfecting the proposed 3D ResNet model combined with Transformer has \nbeen a demanding yet enlightening journey that provided valuable insights into video analysis \nand object detection fields. The unique fusion of convolutional neural networks and \nTransformers attention mechanisms in this proposed model has led to an effective solution for \nunderstanding intricate relationships within video data. \n \nOne important achievement of this study was addressing the difficulties  in videos – like \nocclusions and different camera perspectives as well as changes in the background – by using \na mixed method approach. The integration of 3D ResNet and Transformer layers enabled the \nextraction of spatiotemporal features and modeling long range temporal patterns to create a \nstrong framework that performed exceptionally well considering the consistent accuracy over \n3 datasets. \n \nThe DeepStream SDK was crucial in getting the system ready for use and improving its \nscalability and hardware efficiency. It demonstrates how sophisticated AI models can be \nseamlessly incorporated into applications in the real world to show the blend of innovative \nresearch and real world implementation.  \n \nThe study also pointed out how flexible the 3D ResNet. Transformer model is in adjusting \nto types of data and real-world situations it faces. When used on Jetson edge devices it showed \nits ability to work well in environments with limited resources which means it can be used for \ndistributing complex video analysis tasks to the edges. Its ability to handle challenges and \nadapt well makes it a great asset for fields like healthcare monitoring and analyzing sports and \nretail behavior.  \n \nThe future presents chances to improve the existing solution further by exploring \nalternative Transformer architectures that could enhance efficiency and reduce resource \nrequirements in edge computing environments. Ensuring real time performance and reduci ng \nlatency through techniques like pruning and quantization are measures to enhance the model’s \npreparedness, for widespread implementation. Overall, the 3D ResNet-Transformer model is \n \n71 \na step forward in object detection. By integrating spatial and temporal feature extraction with \nlong range attention mechanisms fine tuning it with TensorRT and implementing it using \nDeepStream SDK the model successfully meets the demands for both performance and real \ntime processing. \n  \n \n72 \nREFERENCES \n1. K. Simonyan and A. Zisserman, \"Two -Stream Convolutional Networks for Action \nRecognition in Videos,\" in Advances in Neural Information Processing Systems \n(NeurIPS), Montreal, QC, Canada, Dec. 2014. \n2. Y. Cheng and L. Wan, \"Integrating 3D Convolutional Neural Networks and \nTransformer for Video Action Recognition,\" College of Computer Information \nEngineering, Jiangxi Normal University, Nanchang, China, 2023. \n3. D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, \"Learning Spatiotemporal \nFeatures with 3D Convolutional Networks,\" in Proceedings of the IEEE International \nConference on Computer Vision (ICCV), Santiago, Chile, Dec. 2015. \n4. J. Carreira and A. Zisserman, \"Quo Vadis, Action Recognition? A New Model and the \nKinetics Dataset,\" in Proceedings of the IEEE Conference on Computer Vision and \nPattern Recognition (CVPR), Honolulu, HI, USA, Jul. 2017. \n5. C. Feichtenhofer, H. Fan, J. Malik, and K. He, \"SlowFast Networks for Video \nRecognition,\" in Proceedings of the IEEE International Conference on Computer \nVision (ICCV), Seoul, South Korea, Oct. 2019. \n6. A. Dosovitskiy et al., \"An Image is Worth 16x16 Words: Transformers for Image \nRecognition at Scale,\" in International Conference on Learning Representations \n(ICLR), Virtual Event, May 2021. \n7. Z. Liu et al., \"Swin Transformer: Hierarchical Vision Transformer Using Shifted \nWindows,\" in Proceedings of the IEEE International Conference on Computer Vision \n(ICCV), Montreal, QC, Canada, Oct. 2021. \n8. Y. Li et al., \"VidTr: Video Transformer Without Convolutions,\" in Proceedings of the \nIEEE International Conference on Computer Vision Workshops (ICCVW), Montreal, \nQC, Canada, Oct. 2021. \n \n73 \n9. K. He, X. Zhang, S. Ren, and J. Sun, \"Deep Residual Learning for Image Recognition,\" \nin Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition \n(CVPR), Las Vegas, NV, USA, Jun. 2016. \n10. H. Wang et al., \"Non-Local Neural Networks,\" in Proceedings of the IEEE Conference \non Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, Jun. \n2018. \n11. A. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, \"HMDB: A Large Video \nDatabase for Human Motion Recognition,\" in Proceedings of the IEEE International \nConference on Computer Vision (ICCV), Barcelona, Spain, Nov. 2011. \n12. K. Soomro, A. R. Zamir, and M. Shah, \"UCF101: A Dataset of 101 Human Actions \nClasses from Videos in the Wild,\" arXiv preprint arXiv:1212.0402, Dec. 2012. \n13. J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \"You Only Look Once: Unified, \nReal-Time Object Detection,\" in Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition (CVPR), Las Vegas, NV, USA, Jun. 2016. \n14. W. Liu et al., \"SSD: Single Shot MultiBox Detector,\" in European Conference on \nComputer Vision (ECCV), Amsterdam, Netherlands, Oct. 2016. \n15. Y. Zhu et al., \"Hidden Two-Stream Convolutional Networks for Action Recognition,\" \nin Proceedings of the British Machine Vision Conference (BMVC), Swansea, UK, Sep. \n2016. \n16. P. Zhou et al., \"Temporal Relational Reasoning in Videos,\" in Proceedings of the \nEuropean Conference on Computer Vision (ECCV), Munich, Germany, Sep. 2018. \n17. R. Goyal et al., \"The Kinetics Human Action Video Dataset,\" arXiv preprint \narXiv:1705.06950, May 2017. \n18. X. Wang et al., \"Temporal Context Networks for Action Recognition in Videos,\" in \nProceedings of the IEEE International Conference on Computer Vision (ICCV), \nVenice, Italy, Oct. 2017. \n \n74 \n19. F. Caron et al., \"Emerging Properties in Self -Supervised Vision Transformers,\" in \nProceedings of the IEEE International Conference on Computer Vision (ICCV), \nMontreal, QC, Canada, Oct. 2021. \n20. K. Simonyan and A. Zisserman, \"Two -Stream Convolutional Networks for Action \nRecognition in Videos,\" in Advances in Neural Information Processing Systems \n(NeurIPS), Montreal, QC, Canada, Dec. 2014. \n21. D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, \"Learning Spatiotemporal \nFeatures with 3D Convolutional Networks,\" in Proceedings of the IEEE International \nConference on Computer Vision (ICCV), Santiago, Chile, Dec. 2015. \n22. J. Carreira and A. Zisserman, \"Quo Vadis, Action Recognition? A New Model and the \nKinetics Dataset,\" in Proceedings of the IEEE Conference on Computer Vision and \nPattern Recognition (CVPR), Honolulu, HI, USA, Jul. 2017. \n23. C. Feichtenhofer, H. Fan, J. Malik, and K. He, \"SlowFast Networks for Video \nRecognition,\" in Proceedings of the IEEE International Conference on Computer \nVision (ICCV), Seoul, South Korea, Oct. 2019. \n24. A. Dosovitskiy et al., \"An Image is Worth 16x16 Words: Transformers for Image \nRecognition at Scale,\" in International Conference on Learning Representations \n(ICLR), Virtual Event, May 2021. \n25. Z. Liu et al., \"Swin Transformer: Hierarchical Vision Transformer Using Shifted \nWindows,\" in Proceedings of the IEEE International Conference on Computer Vision \n(ICCV), Montreal, QC, Canada, Oct. 2021. \n26. Y. Li et al., \"VidTr: Video Transformer Without Convolutions,\" in Proceedings of the \nIEEE International Conference on Computer Vision Workshops (ICCVW), Montreal, \nQC, Canada, Oct. 2021. \n27. K. He, X. Zhang, S. Ren, and J. Sun, \"Deep Residual Learning for Image Recognition,\" \nin Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition \n(CVPR), Las Vegas, NV, USA, Jun. 2016. \n \n75 \n28. H. Wang et al., \"Non-Local Neural Networks,\" in Proceedings of the IEEE Conference \non Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, Jun. \n2018. \n29. A. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, \"HMDB: A Large Video \nDatabase for Human Motion Recognition,\" in Proceedings of the IEEE International \nConference on Computer Vision (ICCV), Barcelona, Spain, Nov. 2011. \n30. K. Soomro, A. R. Zamir, and M. Shah, \"UCF101: A Dataset of 101 Human Actions \nClasses from Videos in the Wild,\" arXiv preprint arXiv:1212.0402, Dec. 2012. \n31. J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \"You Only Look Once: Unified, \nReal-Time Object Detection,\" in Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition (CVPR), Las Vegas, NV, USA, Jun. 2016. \n32. W. Liu et al., \"SSD: Single Shot MultiBox Detector,\" in European Conference on \nComputer Vision (ECCV), Amsterdam, Netherlands, Oct. 2016. \n33. Y. Zhu et al., \"Hidden Two-Stream Convolutional Networks for Action Recognition,\" \nin Proceedings of the British Machine Vision Conference (BMVC), Swansea, UK, Sep. \n2016. \n34. P. Zhou et al., \"Temporal Relational Reasoning in Videos,\" in Proceedings of the \nEuropean Conference on Computer Vision (ECCV), Munich, Germany, Sep. 2018. \n35. R. Goyal et al., \"The Kinetics Human Action Video Dataset,\" arXiv preprint \narXiv:1705.06950, May 2017. \n36. Y. Wang et al., \"TACT: Temporal Attention and Contextual Transformer for Video \nObject Detection,\" in Proceedings of the IEEE/CVF Winter Conference on \nApplications of Computer Vision (WACV), Waikoloa, HI, USA, Jan. 2021. \n37. D. Bertasius, H. Wang, and L. Torresani, \"Is Space -Time Attention All You Need for \nVideo Understanding?\" in Proceedings of the International Conference on Machine \nLearning (ICML), Virtual Event, Jul. 2021. \n \n76 \n38. X. Wang et al., \"Temporal Context Networks for Action Recognition in Videos,\" in \nProceedings of the IEEE International Conference on Computer Vision (ICCV), \nVenice, Italy, Oct. 2017. "
  },
  {
    "source": "2401.03407v6.pdf",
    "content": "Bilateral Reference for High-Resolution Dichotomous Image Segmentation\nPeng Zheng1,4,5,6† Dehong Gao2 Deng-Ping Fan1∗ Li Liu3\nJorma Laaksonen4 Wanli Ouyang5 Nicu Sebe6\n1 Nankai University, 2 Northwest Polytechnical University,\n3 National University of Defense Technology,4 Aalto University,\n5 Shanghai AI Laboratory, 6 University of Trento\nzhengpeng0108@gmail.com\nGT Ours\nIS-Net UDUN\nGT Ours\nIS-Net UDUN\nFigure 1. Visual comparison between the results of our proposed BiRefNet and the latest state-of-the-art methods (e.g., IS-Net [37]\nand UDUN [34]) for high-resolution dichotomous image segmentation (DIS). Details of segmentation are zoomed in for better display.\nAbstract\nWe introduce a novel bilateral reference framework\n(BiRefNet) for high-resolution dichotomous image segmen-\ntation (DIS). It comprises two essential components: the\nlocalization module (LM) and the reconstruction module\n(RM) with our proposed bilateral reference (BiRef). LM\naids in object localization using global semantic informa-\ntion. Within the RM, we utilize BiRef for the reconstruc-\ntion process, where hierarchical patches of images pro-\nvide the source reference, and gradient maps serve as the\ntarget reference. These components collaborate to gen-\nerate the final predicted maps. We also introduce auxil-\niary gradient supervision to enhance the focus on regions\nwith finer details. In addition, we outline practical train-\ning strategies tailored for DIS to improve map quality and\nthe training process. To validate the general applicability\nof our approach, we conduct extensive experiments on four\ntasks to evince that BiRefNet exhibits remarkable perfor-\nmance, outperforming task-specific cutting-edge methods\nacross all benchmarks. Our codes are publicly available at\nhttps://github.com/ZhengPeng7/BiRefNet.\n† Peng finished the majority of this work when he was a visiting\nscholar at Nankai University.\n∗ Corresponding author (dengpfan@gmail.com).\n1. Introduction\nWith the advancement in high-resolution image acquisi-\ntion, image segmentation technology has evolved from tra-\nditional coarse localization to achieving high-precision ob-\nject segmentation. This task, whether it involves salient [12]\nor concealed object detection [10, 11], is referred to as high-\nresolution dichotomous image segmentation (DIS) [37] and\nhas attracted widespread attention and use in the industry,\ne.g., by Samsung, Adobe, and Disney.\nFor the new DIS task, recent works have considered\nstrategies such as intermediate supervision [37], frequency\nprior [58], and unite-divide-unite [34], and have achieved\nfavorable results. Essentially, they either split the supervi-\nsion [34, 37] at the feature-level or introduce an additional\nprior [58] to enhance feature extraction. These strategies\nare, however, still insufficient to capture very fine features\n(see Fig. 1). Based on our observations, we found that fine\nand non-salient features in image objects can be well re-\nflected by obtaining gradient features through derivative op-\nerations on the original image. In addition, when certain\npositions exhibit high similarity in color and texture to the\nbackground, the gradient features are probably too weak.\nFor such cases, we further introduce ground-truth (GT) fea-\ntures for side supervision, allowing the framework to learn\nthe characteristics of these positions. We name the incorpo-\n1\narXiv:2401.03407v6  [cs.CV]  24 Jul 2024\n…\n…\nSupervisionon grad-maps\n…\n…\n… …\n(a) (b) (c) (d)\nEnc Dec Enc Dec Enc Dec Enc Dec\nFigure 2. Comparison between our proposed BiRefNet and other existing methods for HR segmentation tasks. (a) Common frame-\nwork [38]; (b) Image pyramid as input [13, 56]; (c) Scaled images as inward reference [21, 27]; (d) BiRefNet: patches of original images\nat original scales as inward reference and gradient priors as outward reference. Enc = encoder, Dec = decoder.\nration of the image reference and the introduction of both\nthe gradient and GT references as bilateral reference.\nWe propose a novel progressive bilateral reference net-\nwork BiRefNet to handle the high-resolution DIS task with\nseparate localization and reconstruction modules. For the\nlocalization module, we extract hierarchical features from\nvision transformer backbone, which are combined and\nsqueezed to obtain corase predictions in low resolution in\ndeep layers. For the reconstruction module, we further de-\nsign the inward and outward references as bilateral refer-\nences (BiRef), in which the source image and the gradi-\nent map are fed into the decoder at different stages. In-\nstead of resizing the original images to lower-resolution\nversions to ensure consistency with decoding features at\neach stage [21, 27], we keep the original resolution for in-\ntact detail features in inward reference and adaptively crop\nthem into patches for compatibility with decoding features.\nIn addition, we investigate and summarize practical strate-\ngies for the training on high-resolution (HR) data, including\nlong training and region-level loss for better segmentation\nin parts of fine details, and multi-stage supervision to accel-\nerate learning of them.\nOur main contributions are summarized as follows:\n1. We present a bilateral reference network (BiRefNet),\nwhich is a simple yet strong baseline to perform high-\nquality dichotomous image segmentation.\n2. We propose a bilateral reference module , which con-\nsists of an inward reference with source image guidance\nand an outward reference with gradient supervision. It\nshows great efficacy in the reconstruction of the pre-\ndicted HR results.\n3. We explore and summarize various practical strategies\ntailored for DIS to easily improve performance, predic-\ntion quality, and convergence acceleration.\n4. The proposed BiRefNet shows its excellent performance\nand strong generalization capabilities to achieve state-\nof-the-art performance on not only the DIS5K task but\nalso on HRSOD and COD with 6.8%, 2.0%, and 5.6%\naverage Sm [7] improvements, respectively.\n2. Related Works\n2.1. High-Resolution Class-agnostic Segmentation\nHigh-resolution class-agnostic segmentation has been a typ-\nical computer vision objective for decades, and many re-\nlated tasks have been proposed and attracted much atten-\ntion, such as dichotomous image segmentation (DIS) [37],\nhigh-resolution salient object detection (HRSOD) [53], and\nconcealed object detection (COD) [10]. To provide stan-\ndard HRSOD benchmarks, several typical HRSOD datasets\n(e.g., HRSOD [53], UHRSD [46], HRS10K [6]) and nu-\nmerous approaches [21, 42, 46, 53] have been proposed.\nZeng et al. [53] employed a global-local fusion of the multi-\nscale input in their network. Xie et al. [46] used cross-\nmodel grafting modules to process images at different scales\nfrom multiple backbones (lightweight [16] and heavy [30]).\nPyramid blending was also used in [21] for a lower compu-\ntational cost. Concealed objects are difficult to locate due\nto similar-looking surrounding distractors [9]. Therefore,\nimage priors, such as frequency [57], boundary [40], gradi-\nent [19], etc, are used as auxiliary guidance to train COD\nmodels. Furthermore, a higher resolution has been found\nbeneficial for detecting targets [17–19]. To produce more\nprecise and fine-detail segmentation results, Yin et al. [50]\nemployed progressive refinement with masked separable at-\ntention. Li et al. [27] incorporated the original images at\ndifferent scales to aid in the refining process.\nHigh-resolution DIS is a newly proposed task that fo-\ncuses more on the complex slender structure of target ob-\njects in high-resolution images, making it even more chal-\nlenging. Qin et al. [37] proposed the DIS5K dataset and\nIS-Net with intermediate supervision to alleviate the loss of\nfine areas. In addition, Zhou et al. [58] embedded a fre-\nquency prior to their DIS network to capture more details.\nPei et al. [34] applied a label decoupling strategy [45] to\nthe DIS task and achieved competitive segmentation per-\nformance in the boundary areas of objects. Yu et al. [52]\nused patches of HR images to accelerate the training in a\nmore memory-efficient way. Unlike previous models that\nused compressed/resized images to enhance HR segmenta-\ntion, we utilized intact HR images as supplementary infor-\n2\nBiRef Block\nBiRef Block\nLocalization ModuleReconstruction ModulePredictor\nTransformerBlocks\nTransformerBlocks\nTransformerBlocks\nTransformerBlocks\n1x11x11x1\nBiRef Block\nReconstruction Block\nInward ReferenceFeature\nFeature\nS·\n·ASPP\n1x1\nOutward Reference\n1x1 Conv\nSupervisionon Grad-Maps\nSupervisionon Seg-Maps\nAuxiliaryClassification\nSupervisionon Class\nCls\nCls1x1\nS·Element-wiseMultiplicationSigmoid\nMulti-scale Supervision\nI M\nFe\n1\nFe\n2\nFe\n3\nFd\n1\nFd\n2\nFd\n3\nFe Fd\nFd+\n1\nFd+\n2\nFd+\n3\nFl\n1\nFl\n2\nFl\n3\nFd\n2\nFd′\n3\nFd+\n3\n{PN\nk=1}\nAG\n3\nM2\nˆG3\nLCE\nLBCE, IoU\nLBCE\nLIoU\nLBCE\nFigure 3. Pipeline of the proposed bilateral reference Network (BiRefNet). BiRefNet mainly consists of the localization module (LM)\nand the reconstruction module (RM) with bilateral reference (BiRef) blocks. Please refer to Sec. 3.1 for details.\nmation for better predictions in high resolution.\n2.2. Progressive Refinement in Segmentation\nIn the image matting task, trimaps have been used as a\npre-positioning technique for more precise segmentation re-\nsults [26, 47]. In Fig. 2, we illustrate different approaches\nof relevant networks and compare the differences [34, 37,\n38, 55, 58]. Many approaches have been proposed based\non the progressive refinement strategy. Yu et al. [51] used\nthe predicted LR alpha matte as a guide for refined HR\nmaps. In BASNet [35], the initial results are revised with\nan additional refiner network. The CRM [39] continuously\naligns the feature map with the refinement target to aggre-\ngate detailed features. In ICNet [56], the original images\nare also downscaled and added to the decoder output at\ndifferent stages for refinement. In ICEG [15], the genera-\ntor and the detector iteratively evolve through interaction to\nobtain better COD segmentation results. In addition to im-\nages and GT, auxiliary information is also used in existing\nmethods. For example, Tang et al. [41] cropped patches on\nthe boundary to further refine them. In the LapSRN net-\nwork [23, 24] for image super-resolution, Laplacian pyra-\nmids are also generated to help with image reconstruction\nat higher resolution. Although these methods successfully\nemployed refinement to achieve better results, models are\nnot guided to focus on certain areas, which is a problem\nin DIS. Therefore, we introduce gradient supervision in our\noutward reference to guide features sensitive to areas with\nricher fine details.\n3. Methodology\n3.1. Overview\nAs shown in Fig. 2(d), our proposed BiRefNet is differ-\nent from the previous DIS methods. On the one hand,\nour BiRefNet explicitly decomposes the DIS task on HR\ndata into two modules, i.e., a localization module (LM) and\na reconstruction module (RM). On the other hand, instead\nof directly adding the source images [13] or the priors [58]\nto the input, BiRefNet employs our proposed bilateral ref-\nerence in the RM, making full use of the source images at\nthe original scales and the gradient priors. The complete\nframework of our BiRefNet is illustrated in Fig. 3.\n3.2. Localization Module\nFor a batch of HR images I ∈ RN×3×H×W as in-\nput, the transformer encoder [30] extracts features at\ndifferent stages, i.e., Fe\n1 , Fe\n2 , Fe\n3 , Fe with resolutions as\n{[H\nk , W\nk ], k= 4, 8, 16, 32}. The features of the first four\nstages {Fe\ni }3\ni=1 are transferred to the corresponding de-\ncoder stages with lateral connections (1×1 convolution lay-\ners). Meanwhile, they are stacked and concatenated in the\nlast encoder block to generate Fe.\nThe encoder output feature Fe is then fed into a classifi-\ncation module, where Fe is led into a global average pool-\ning layer and a fully connected layer for classification with\nthe category C to obtain a better semantic representation for\nlocalization. HR features are squeezed in the bottleneck. To\nenlarge the receptive fields to cover features of large objects\nand focus on local features for high precision simultane-\n3\nC\n C\nS·AvgPool\nDFC1x1 Conv Dilate\nDFC7x7DFC3x3BRBRBR\nDFCDeformableConvolutionBRBN+ReLU\nConv1x1BR\nConv1x1BR ·…\n Supervisionon Grad-MapsCConcatenation\nDilateDilation Operation\nI\n1024×1024\nAdaptatively Crop\n{PN\nk=1}\nFd+\ni\nFθ\ni Fd′\ni\nMi\nGm\ni\nGgt\ni\nˆGiFG\ni\nMiAG\ni\nFd\ni−1\nInward Reference Reconstruction Block Outward Reference\nFigure 4. Pipeline of the proposed bilateral reference blocks. The source images at the original scale are combined with decoder\nfeatures as the inward reference and fed into the reconstruction block, where deformable convolutions with hierarchical receptive fields are\nemployed. The aggregated features are then used to predict the gradient maps in the outward reference. Gradient-aware features are then\nturned into the attention map to act on the original features.\nously [49], which is important for HR tasks involved, we\nemploy ASPP modules [3] here for multi-context fusion.\nFe is squeezed toFd for transfer to the reconstruction mod-\nule.\n3.3. Reconstruction Module\nThe setting of the receptive field (RF) has been a challenge\nof HR segmentation. Small RFs lead to inadequate context\ninformation to locate the right target on a large background,\nwhereas large RFs often result in insufficient feature extrac-\ntion in detailed areas. To achieve balance, we propose the\nreconstruction block (RB) in each BiRef block as a replace-\nment for the vanilla residual blocks. In RB, we employ de-\nformable convolutions [4] with hierarchical receptive fields\n(i.e., 1×1, 3×3, 7×7) and an adaptive average pooling layer\nto extract features with RFs of various scales. These fea-\ntures extracted by different RFs are then concatenated as\nFθ\ni , followed by a 1 ×1 convolution layer and a batch nor-\nmalization layer to generate the output feature of RM Fd′\ni .\nIn the reconstruction module, the squeezed featureFd is fed\ninto the BiRef block for the feature Fd\n3 . With Fl\n3, the first\nBiRef block predicts coarse maps, which are then recon-\nstructed into higher-resolution versions through the follow-\ning BiRef blocks. Following [29], the output feature of each\nBiRef block Fd\ni is added with its lateral feature Fl\ni of the\nLM at each stage,i.e., {Fd+\ni = Upsample ↑ (Fd\ni +Fl\ni ), i=\n, 3, 2, 1}. Meanwhile, all BiRef blocks generate intermedi-\nate predictions {Mi}1\ni=3 by multi-stage supervision, with\nresolutions in ascending order. Finally, the last decoding\nfeature Fd+\n1 is passed through an 1×1 convolution layer to\nobtain the final predicted maps M ∈RN×1×H×W .\n3.4. Bilateral Reference\nIn DIS, HR training images are very important for deep\nmodels to learn details and perform highly accurate seg-\nmentation. However, most segmentation models follow pre-\nvious works [29, 38] to design the network architecture in\nan encoder-decoder structure with down-sampling and up-\nsampling, respectively. Besides, due to the large size of\nthe input, concentrating on the target objects becomes more\nchallenging. To deal with these two main problems, we pro-\npose bilateral reference, consisting of an inward reference\n(InRef) and an outward reference (OutRef), which is illus-\ntrated in Fig. 4. Inward reference and outward reference\nplay the roles of supplementing HR information and draw-\ning attention to areas with dense details, respectively.\nIn InRef, images I with original high resolution are\ncropped to patches {PN\nk=1} of consistent size with the out-\nput features of the corresponding decoder stage. These\npatches are stacked with the original feature Fd+\ni to be fed\ninto the RM. Existing methods with similar techniques ei-\nther add I only at the last decoding stage [34] or resizeI to\nmake it applicable with original features in low resolution.\nOur inward reference avoids these two problems through\nadaptive cropping and supplies the necessary HR informa-\ntion at every stage.\nIn OutRef, we use gradient labels to draw more attention\nto areas of richer gradient information, which is essential\nfor the segmentation of fine structures. First, we extract the\ngradient maps of the input images as Ggt\ni . Meanwhile, Fθ\ni\nis used to generate the feature FG\ni to produce the predicted\ngradient maps ˆGi. With this gradient supervision, FG\ni is\nsensitive to the gradient. It passes through a conv and a\nsigmoid layer and is used to generate the gradient referring\nattention AG\ni , which is then multiplied by Fd′\ni to generate\noutput of the BiRef block as Fd\ni−1.\nConsidering that the background may have non-target\nnoise with a lot of gradient information, we apply a masking\nstrategy to alleviate the influence of non-target areas. We\nperform morphological operations on intermediate predic-\ntions Mi and use dilated Mi as a mask. The mask is used\nto multiply the gradient mapGgt\ni to generate Gm\ni , where the\ngradients outside the mask area are removed.\n3.5. Objective Function\nIn HR segmentation tasks, using only pixel-level supervi-\nsion (BCE loss) usually results in the deterioration of de-\ntailed structural information in HR data. Inspired by the\ngreat results in [35] which used a hybrid loss, we use BCE,\nIoU, SSIM, and CE losses together to collaborate for the\n4\nsupervision on the levels of pixel, region, boundary, and\nsemantic, respectively. The final objective function is a\nweighted combination of the above losses and can be for-\nmulated as:\nL = Lpixel + Lregion + Lboundary + Lsemantic\n= λ1LBCE + λ2LIoU + λ3LSSIM + λ4LCE , (1)\nwhere λ1, λ2, λ3, and λ4 are respectively set to 30, 0.5, 10,\nand 5 to keep all the losses on the same quantitative level at\nthe beginning of the training. The final objective function\nconsists of binary cross-entropy (BCE) loss, intersection\nover union (IoU) loss, structural similarity index measure\n(SSIM) loss, and cross-entropy (CE) loss. The complete\ndefinition of losses can be found below.\n• BCE loss: pixel-aware supervision, which is used for\npixel-level supervision for the generation of binary maps:\nLBCE =− P\n(i,j)\n[G(i,j) log(M(i,j))+(1−G(i,j)) log(1−M(i,j))],\n(2)\nwhere G(i, j) and M(i, j) denote the value of the GT and\nbinarized predicted maps, respectively, at pixel (i, j).\n• IoU loss: region-aware supervision for the enhancement\nof binary map predictions:\nLIoU = 1−\nHP\nr=1\nWP\nc=1\nM(i,j)G(i,j)\nHP\nr=1\nWP\nc=1\n[M(i,j)+G(i,j)−M(i,j)G(i,j)]\n. (3)\n• SSIM loss: boundary-aware supervision to improve the\naccuracy in boundary parts. Given GT maps G and\npredicted maps M, y = {yj : j = 1, ..., N2} and\nx = {xj : j = 1, ..., N2} represent the pixel values of\ntwo corresponding N × N patches derived from G and\nM, respectively. SSIM (x, y) is defined as:\nLSSIM = 1− (2µxµy + C1)(2σxy + C2)\n(µ2x + µ2y + C1)(σ2x + σ2y + C2), (4)\nwhere µx, µy and σx, σy are the means and standard de-\nviations of x and y, respectively, σxy is their covariance.\nC is used to avoid division by zero.\n• CE loss: semantic-aware supervision, which is used to\nlearn better semantic representation:\nLCE = −\nNX\nc=1\nyo,c log(po,c), (5)\nwhere N is the number of classes, yo,c states whether\nclass label c is the correction classification for observa-\ntion o, and po,c denotes the predicted probability that o is\nof class c.\n3.6. Training Strategies Tailored for DIS\nDue to the high cost of training models on HR data, we\nhave explored training tricks for HR segmentation tasks to\nimprove performance and reduce training costs.\nFirst, we found that our model converges relatively\nquickly in the localization of targets and the segmenta-\ntion of rough structures (measured by F-measure [1], S-\nmeasure [7]) on DIS5K ( e.g., 200 epochs). However, the\nperformance in segmenting fine parts is still increasing af-\nter very long training ( e.g., 400 epochs), which is reflected\nin metrics such as Fω\nβ and HCE γ. Second, though long\ntraining can easily achieve great results in terms of both\nstructure and edges, it consumes too much computation; we\nfound that multi-stage supervision can dramatically accel-\nerate the learning on segmenting fine details and make the\nmodel achieve similar performance as before but with only\n30% training epochs. Third, we also found that fine-tuning\nwith only region-level losses can easily improve the bina-\nrization of predicted results and those metric scores ( e.g.,\nFω\nβ , Em\nϕ , HCE ) that are closer to practical use. Finally, we\nused context feature fusion and image pyramid inputs on the\nbackbone, which are commonly used tricks to process HR\nimages with deep models. In experiments, these two mod-\nifications to the backbone achieved a general improvement\nin DIS and similar HR segmentation tasks.\nAs shown in Tab. 1, we show the effectiveness of training\nepochs and the multi-stage supervision. As the results show,\nour BiRefNet can achieve relatively good results after 200\nepochs of training. Continuous training in 400 epochs can\nincrease a small portion of metrics measuring structural in-\nformation (e.g., Fx\nβ , Sm), while bringing a larger improve-\nment in metrics measuring fine details (e.g., HCE γ).\nAlthough simple long training can achieve better results,\nthe improvement is relatively small, concerning its high\ncomputational cost on HR data. We investigated the multi-\nstage supervision (MSS), which is a widely used training\nstrategy used in binary segmentation works [37, 54]. Differ-\nent from MSS in these works for higher precision, it plays\na role in accelerating the training convergence. As the re-\nsults in Tab. 1 show, our BiRefNet trained for 200 epochs\nwith MSS can achieve similar performance with it trained\nfor 400 epochs. MSS successfully cut training time in half\nand can be used for further HR segmentation tasks for more\nefficient training.\n4. Experiments\n4.1. Datasets\nTraining Sets. For DIS, we follow [34, 37, 58] to use\nDIS5K-TR as our training set in experiments. For HRSOD,\nwe follow [46] to set different combinations of HRSOD,\nUHRSD, and DUTS as the training set. For COD, we fol-\nlow [10, 18] to use the concealed samples in CAMO-TR\n5\nTable 1. Quantitative ablation studies of the proposed multi-\nstage supervision for acceleration and training epochs.\nSettings DIS-VD\nMSS Epoch Fx\nβ ↑ Fω\nβ ↑ M ↓ Sm ↑ Em\nϕ ↑ HCEγ ↓\n200 .875 .848 .041 .886 .914 1207\n400 .897 .863 .036 .905 .937 1039\n✓ 200 .892 .858 .037 .901 .932 1043\nand COD10K-TR as the training set.\nTest Sets. To obtain a complete evaluation of\nour BiRefNet, we tested it on all test sets in DIS5K (DIS-\nTE1, DIS-TE2, DIS-TE3, and DIS-TE4). We also con-\nducted an evaluation of BiRefNet on the HRSOD test sets\n(DA VIS-S [53], HRSOD-TE [53], and UHRSD-TE [46])\nand the COD test sets (CAMO-TE [25], COD10K-TE [9],\nand NC4K [31]). Low-resolution SOD test sets (DUTS-\nTE [44] and DUT-OMRON [48]) are additionally used for\nsupplementary experiments.\n4.2. Evaluation Protocol\nFor a comprehensive evaluation, we employ the widely\nused metrics, i.e., S-measure [7] (Sm), max/mean/weighted\nF-measure [1] ( Fx\nβ /Fm\nβ /Fω\nβ ), max/mean E-measure [8]\n(Ex\nξ /Em\nξ ), mean absolute error (MAE), and relax HCE [37]\n(HCE γ) to evaluate performance. Detailed descriptions of\nthese metrics can be found as follows.\n• S-measure [7] (structure measure,Sα) is a structural sim-\nilarity measurement between a saliency map and its cor-\nresponding GT map. Evaluation with Sα can be obtained\nat high speed without binarization. The Sα-measure is\ncomputed as:\nSα = α · So + (1− α) · Sr, (6)\nwhere So and Sr denote object-aware and region-aware\nstructural similarity, and α is set to 0.5 by default, as sug-\ngested by Fan et al. in [7].\n• F-measure [1] (Fβ) is designed to evaluate the weighted\nharmonic mean value of precision and recall. The output\nof the saliency map is binarized with different thresholds\nto obtain a set of binary saliency predictions. The pre-\ndicted saliency maps and GT maps are compared to obtain\nprecision and recall values. F-measure can be computed\nas:\nFβ = (1 +β2) · P recision· Recall\nβ2 · P recision+ Recall , (7)\nwhere β2 is set to 0.3 to emphasize precision over recall,\nfollowing [2]. The maximum F-measure score obtained\nwith the best threshold for the entire dataset is used and\ndenoted as Fx\nβ .\n• E-measure [8] (enhanced-alignment measure, Eξ) is de-\nsigned as a perceptual metric to evaluate the similarity be-\ntween the predicted maps and the GT maps both locally\nTable 2. Quantitative ablation studies of the proposed com-\nponents in the proposed BiRefNet. The ablation studies are con-\nducted on the effectiveness of the proposed components, including\nreconstruction module (RM), inward reference (InRef), outward\nreference (OutRef), and their combinations.\nModules DIS-VD\nRM InRef OutRef Fx\nβ ↑ Fω\nβ ↑ M ↓Sm ↑ Em\nϕ ↑ HCEγ ↓\n.837 .785 .056 .845 .887 1204\n✓ .855 .831 .048 .865 .895 1167\n✓ .848 .825 .050 .857 .903 1152\n✓ ✓ .869 .834 .041 .886 .912 1093\n✓ ✓ .863 .831 .042 .891 .918 1106\n✓ ✓ .861 .839 .044 .881 .911 1114\n✓ ✓ ✓ .889 .851 .038 .900 .924 1065\nand globally. E-measure is defined as:\nEξ = 1\nW H\nWX\nx=1\nHX\ny=1\nϕξ(x, y), (8)\nwhere ϕξ indicates the enhanced alignment matrix. Sim-\nilarly to the F-measure, we also adopt the maximum E-\nmeasure (Ex\nξ ) and also the mean E-measure ( Em\nξ ) as our\nevaluation metrics.\n• MAE (mean absolute error,ϵ) is a simple pixel-level eval-\nuation metric that measures the absolute difference be-\ntween non-binarized predicted results M and GT maps\nG. It is defined as:\nϵ = 1\nW H\nWX\nx=1\nHX\ny=1\n|ˆY (x, y) − G(x, y)|. (9)\n• HCEγ [37] (human correction efforts, HCE ) is a newly\nproposed metric aimed at evaluating the human efforts re-\nquired to correct faulty predictions to satisfy specific ac-\ncuracy requirements in real-world applications. Specifi-\ncally, HCE is quantified by the approximate number of\nmouse clicks. In practical applications, minor prediction\nerrors can be tolerated. Therefore, relax HCE (HCE γ)\nis introduced, where γ denotes tolerance. In experiments,\nwe use HCE 5 (relax HCE with γ = 5) to stay consis-\ntent with that of the original paper [37], where detailed\ndescriptions of HCE γ are provided.\n4.3. Implementation Details\nAll images are resized to 1024 ×1024 for training and test-\ning. The generated segmentation maps are resized ( i.e., bi-\nlinear interpolation) to the original size of the correspond-\ning GT maps for evaluation. Horizontal flip is the only data\naugmentation used in the training process. The number of\ncategories C is set to 219, as given in DIS-TR. The pro-\nposed BiRefNet is trained with Adam optimizer [22] for\n6\nTable 3. Effectiveness of practical strategies for training high-\nresolution segmentation. The experimental comparison of the\nproposed several tricks for HR segmentation tasks is provided\nhere, including context feature fusion (CFF), image pyramids in-\nput (IPT), regional loss fine-tuning (RLFT), and their combina-\ntions. The results are obtained by our final model.\nModules DIS-VD\nCFF IPT RLFT Fx\nβ ↑ Fω\nβ ↑ M ↓Sm ↑ Em\nϕ ↑ HCEγ ↓\n.889 .851 .038 .900 .924 1065\n✓ .893 .856 .038 .904 .928 1054\n✓ .895 .857 .037 .904 .927 1051\n✓ .890 .861 .036 .899 .932 1043\n✓ ✓ ✓ .897 .863 .036 .905 .937 1039\nFigure 5. Quantitative comparisons of the pro-\nposed BiRefNet and the best task-specific models. S-\nmeasure [7] is used for the comparison here. UDUN [34],\nFSPNet [18], PGNet-UH [46], and PGNet-DH [46] are currently\nthe best models for the DIS, COD, HRSOD, and SOD tasks,\nrespectively.\nDIS/HRSOD/COD tasks for 600/150/150 epochs, respec-\ntively. The model is fine-tuned with the IoU loss for the last\n20 epochs. The initial learning rate is set to 10−4 and 10−5\nfor DIS and others, respectively. Models are trained with\nPyTorch [33] on eight NVIDIA A100 GPUs. The batch size\nis set to N=4 for each GPU during training.\n4.4. Ablation Study\nWe study the effectiveness of each component (i.e., RM and\nBiRef) and practical strategies ( i.e., CFF, IPT, and RLFT)\nintroduced for our BiRefNet and conduct an investigation\nabout their contributions to improved DIS results. Quanti-\ntative results regarding each module and strategy are shown\nin Tab. 2 and 3, respectively.\nBaseline. We provide a simple but strong encoder-\ndecoder network as the baseline for the DIS task. To capture\nbetter hierarchical features on various scales, we chose the\nSwin transformer large [30] as our default backbone net-\nwork. Then, to obtain a better semantic representation in\nthe DIS task, we divided the images in DIS-TR into 219\nclasses according to their label names and added an auxil-\niary classification head at the end of the encoder. In the de-\ncoder of the baseline network, each decoder block is made\nup of two residual blocks [16]. All stages of the encoder\nand decoder are connected with an 1 ×1 convolution, ex-\ncept the deepest stage, where an ASPP [3] block is used for\nconnectivity. With this setup, our baseline network has out-\nperformed existing DIS models in most metrics, as shown\nin Tab. 2 and Tab. 4.\nReconstruction Module. As shown in Tab. 2, our model\ngains an overall improvement with the proposed RM. The\nRM provides multi-scale receptive fields on the HR features\nfor local details and overall semantics. It brings ∼2.2% Fx\nβ\nrelative improvement with little extra computational cost.\nBilateral Reference. We separately investigate the ef-\nfectiveness of the inward reference (InRef, with source im-\nages) and the outward reference (OutRef, with gradient la-\nbels) in BiRef. InRef supplemented lossless HR informa-\ntion globally, while OutRef drew more attention to the fine-\ndetail parts to achieve higher precision in those areas. As\nshown in Tab. 2, they work jointly to bring 2.9% Fx\nβ rela-\ntive improvement toBiRefNet. RM and BiRef are combined\nto achieve 6.2% Fx\nβ relative improvement.\nTraining Strategies. As shown in Tab. 3, the proposed\nstrategies improve performance from different perspectives.\nCCF and IPT improve overall performance, while RLFT\nspecifically improves precision in edge details, which is re-\nflected in metrics such as Fω\nβ and HCE γ.\n4.5. State-of-the-Art Comparison\nTo validate the general applicability of our method, we\nconduct extensive experiments on four tasks, i.e., high-\nresolution dichotomous image segmentation (DIS), high-\nresolution salient object detection (HRSOD), concealed ob-\nject detection (COD), and salient object detection (SOD).\nWe compare our proposed BiRefNet with all the latest task-\nspecific models on existing benchmarks [10, 25, 31, 37, 44,\n46, 48, 53].\nQuantitative Results. Tab. 4 shows a quantitative com-\nparison between the proposed BiRefNet and previous state-\nof-the-art methods. Our BiRefNet outperforms all previ-\nous methods in widely used metrics. The complexities of\nDIS-TE1∼DIS-TE4 are in ascending order. The metrics for\nstructure similarity (e.g., Sα, Ex\nϕ) focus more on global in-\nformation. Pixel-level metrics, such as MAE ( M), empha-\nsize the precision of details. Metrics based on mean values\n(e.g., Em\nϕ , Fm\nϕ ) better match the requirements of practical\napplications where maps are thresholded. As seen in Tab. 4,\nour BiRefNet outperforms previous methods not only on the\naccuracy of the global shape but also in the details of the\npixels. It is noteworthy that the results are better, especially\nin metrics that cater more to practical applications.\nAdditionally, our BiRefNet outperforms existing task-\n7\nImage GT Ours UDUN [34] IS-Net [37] U 2Net [36] HRNet [43]\nDIS-TE1DIS-TE2DIS-TE3DIS-TE4DIS-VD\nFigure 6. Qualitative comparisons of the proposed BiRefNet and previous methods on the DIS5K benchmark. The results of the\nprevious methods are from [34], where all models are trained with images in 1024×1024. Zoom in for a better view.\nTinySlimOccludedMultiple\nInput GT Ours HitNet FSPNet ZoomNet SINetv2\nFigure 7. Visual comparisons of the proposed BiRefNet and other competitors on COD10K benchmark. Samples with different\nchallenges are provided here to show the superiority of BiRefNet from different perspectives.\nspecific models on the HRSOD and COD tasks. As\nshown in Tab. 5, BiRefNet achieved much higher accuracy\non both high-resolution and low-resolution SOD bench-\nmarks. Compared with the previous SOTA method [46],\n8\nTable 4. Quantitative comparisons between our BiRefNet and the state-of-the-art methods on DIS5K.“↑” (“↓”) means that the higher\n(lower) is better. We use the results from [34], where all methods take 1024×1024 input.\nMethods DIS-TE1 (500) DIS-TE2 (500) DIS-TE3 (500)\nFx\nβ ↑ Fω\nβ ↑ M ↓Sm ↑ Em\nϕ ↑ HCEγ ↓ Fx\nβ ↑ Fω\nβ ↑ M ↓Sm ↑ Em\nϕ ↑ HCEγ ↓ Fx\nβ ↑ Fω\nβ ↑ M ↓Sm ↑ Em\nϕ ↑ HCEγ ↓\nBASNet19 [35] .663 .577 .105 .741 .756 155 .738 .653 .096 .781 .808 341 .790 .714 .080 .816 .848 681\nU2Net20 [36] .701 .601 .085 .762 .783 165 .768 .676 .083 .798 .825 367 .813 .721 .073 .823 .856 738\nHRNet20 [43] .668 .579 .088 .742 .797 262 .747 .664 .087 .784 .840 555 .784 .700 .080 .805 .869 1049\nPGNet22 [46] .754 .680 .067 .800 .848 162 .807 .743 .065 .833 .880 375 .843 .785 .056 .844 .911 797\nIS-Net22 [37] .740 .662 .074 .787 .820 149 .799 .728 .070 .823 .858 340 .830 .758 .064 .836 .883 687\nFP-DIS23 [58] .784 .713 .060 .821 .860 160 .827 .767 .059 .845 .893 373 .868 .811 .049 .871 .922 780\nUDUN23 [34] .784 .720 .059 .817 .864 140 .829 .768 .058 .843 .886 325 .865 .809 .050 .865 .917 658\nBiRefNet .860 .819 .037 .885 .911 106 .894 .857 .036 .900 .930 266 .925 .893 .028 .919 .955 569\nBiRefNetSwinB .857 .819 .038 .884 .912 110 .890 .854 .037 .898 .930 275 .919 .886 .030 .915 .953 597\nBiRefNetSwinT .823 .774 .048 .855 .887 117 .862 .821 .046 .877 .912 290 .899 .860 .036 .897 .942 627\nBiRefNetPV Tv2b2 .839 .796 .042 .870 .903 111 .881 .842 .040 .888 .925 280 .903 .866 .036 .901 .941 614\nMethods DIS-TE4 (500) DIS-TE (1-4) (2,000) DIS-VD (470)\nFx\nβ ↑ Fω\nβ ↑ M ↓Sm ↑ Em\nϕ ↑ HCEγ ↓ Fx\nβ ↑ Fω\nβ ↑ M ↓Sm ↑ Em\nϕ ↑ HCEγ ↓ Fx\nβ ↑ Fω\nβ ↑ M ↓Sm ↑ Em\nϕ ↑ HCEγ ↓\nBASNet19 [35] .785 .713 .087 .806 .844 2852 .744 .664 .092 .786 .814 1007 .737 .656 .094 .781 .809 1132\nU2Net20 [36] .800 .707 .085 .814 .837 2898 .771 .676 .082 .799 .825 1042 .753 .656 .089 .785 .809 1139\nHRNet20 [43] .772 .687 .092 .792 .854 3864 .743 .658 .087 .781 .840 1432 .726 .641 .095 .767 .824 1560\nPGNet22 [46] .831 .774 .065 .841 .899 3361 .809 .746 .063 .830 .885 1173 .798 .733 .067 .824 .879 1326\nIS-Net22 [37] .827 .753 .072 .830 .870 2888 .799 .726 .070 .819 .858 1016 .791 .717 .074 .813 .856 1116\nFP-DIS23 [58] .846 .788 .061 .852 .906 3347 .831 .770 .047 .847 .895 1165 .823 .763 .062 .843 .891 1309\nUDUN23 [34] .846 .792 .059 .849 .901 2785 .831 .772 .057 .844 .892 977 .823 .763 .059 .838 .892 1097\nBiRefNet .904 .864 .039 .900 .939 2723 .896 .858 .035 .901 .934 916 .891 .854 .038 .898 .931 989\nBiRefNetSwinB .899 .860 .040 .895 .938 2836 .891 .855 .036 .898 .933 954 .881 .844 .039 .890 .925 1029\nBiRefNetSwinT .880 .834 .049 .878 .925 2888 .866 .822 .045 .877 .916 980 .862 .819 .045 .874 .917 1070\nBiRefNetPV Tv2b2 .890 .846 .045 .886 .929 2871 .878 .838 .041 .886 .925 969 .868 .827 .044 .880 .919 1073\nTable 5. Quantitative comparisons between our BiRefNet and the state-of-the-art methods in high-resolution and low-resolution\nSOD datasets. TR denotes the training set. To provide a fair comparison, we train our BiRefNet with different combinations of training\nsets, where 1, 2, and 3 represent DUTS [44], HRSOD [53], and UHRSD [46], respectively.\nTest Sets\nHigh-Resolution Benchmarks Low-Resolution Benchmarks\nMethods TR\nDA VIS-S (92) HRSOD-TE (400) UHRSD-TE (988) DUTS-TE (5,019) DUT-OMRON(5,168)\nSm ↑ Fx\nβ ↑ Em\nϕ ↑ M ↓Sm ↑ Fx\nβ ↑ Em\nϕ ↑ M ↓Sm ↑ Fx\nβ ↑ Em\nϕ ↑ M ↓Sm ↑ Fx\nβ ↑ Em\nϕ ↑ M ↓Sm ↑ Fx\nβ ↑ Em\nϕ ↑ M ↓\nLDF20 [45] 1 .922 .911 .947 .019 .904 .904 .919 .032 .888 .913 .891 .047 .892 .898 .910 .034 .838 .820 .873 .051\nHRSOD19 [53] 1,2 .876 .899 .955 .026 .896 .905 .934 .030 - - - - .824 .835 .885 .050 .762 .743 .831 .065\nDHQ21 [42] 1,2 .920 .938 .947 .012 .920 .922 .947 .022 .900 .911 .905 .039 .894 .900 .919 .031 .836 .820 .873 .045\nPGNet22 [46] 1 .935 .936 .947 .015 .930 .931 .944 .021 .912 .931 .904 .037 .911 .917 .922 .027 .855 .835 .887 .045\nPGNet22 [46] 1,2 .948 .950 .975 .012 .935 .937 .946 .020 .912 .935 .905 .036 .912 .919 .925 .028 .858 .835 .887 .046\nPGNet22 [46] 2,3 .954 .957 .979 .010 .938 .945 .946 .020 .935 .949 .916 .026 .859 .871 .897 .038 .786 .772 .884 .058\nBiRefNet 1 .967 .966 .984 .008 .957 .958 .972 .014 .931 .933 .943 .030 .939 .937 .958 .019 .868 .813 .878 .040\nBiRefNet 1,2 .973 .976 .990 .006 .962 .963 .976 .011 .937 .942 .951 .024 .938 .935 .960 .018 .868 .818 .882 .040\nBiRefNet 1,3 .975 .977 .989 .006 .959 .958 .972 .014 .952 .960 .965 .019 .942 .942 .961 .018 .881 .837 .896 .036\nBiRefNet 2,3 .976 .980 .990 .006 .956 .953 .967 .016 .952 .958 .964 .019 .933 .928 .954 .020 .864 .810 .879 .040\nBiRefNet 1,2,3 .975 .979 .989 .006 .962 .961 .973 .013 .957 .963 .969 .016 .944 .943 .962 .018 .882 .839 .896 .038\nour BiRefNet achieved an average improvement of 2.0%\nSm. Furthermore, as shown in Tab. 6, in the COD task,\nBiRefNet also shows a much better performance compared\nto the previous SOTA models, with an average improvement\nof 5.6% Sm on the three widely used COD benchmarks.\nThese results show the remarkable generalization ability of\nour BiRefNet to similar HR tasks.\nFor a clearer illustration of the generalizability and pow-\nerful performance of BiRefNet, we provide a radar picture\nshown in Fig. 5, where we run our model and the best task-\nspecific models on DIS/HRSOD/COD/SOD tasks. As the\nresults show, our BiRefNet achieves leading results in all\nfour tasks. The other task-specific models show their weak-\nness in similar HR segmentation tasks. For example, FSP-\nNet [18] ranks second in COD benchmarks, while it ranks\nfourth/third/third in DIS/HRSOD/SOD tasks, respectively.\nQualitative Results. Fig. 6 shows segmentation maps\nproduced by the most competitive existing DIS models and\n9\nTable 6. Comparison of BiRefNet with recent methods. As seen, BiRefNet performs much better than previous methods.\nMethods CAMO (250) COD10K (2,026) NC4K (4,121)\nSm ↑ Fω\nβ ↑ Fm\nβ ↑ Em\nϕ ↑ Ex\nϕ ↑ M ↓Sm ↑ Fω\nβ ↑ Fm\nβ ↑ Em\nϕ ↑ Ex\nϕ ↑ M ↓Sm ↑ Fω\nβ ↑ Fm\nβ ↑ Em\nϕ ↑ Ex\nϕ ↑ M ↓\nSINet20 [9] .751 .606 .675 .771 .831 .100 .771 .551 .634 .806 .868 .051 .808 .723 .769 .871 .883 .058\nBGNet22 [40] .812 .749 .789 .870 .882 .073 .831 .722 .753 .901 .911 .033 .851 .788 .820 .907 .916 .044\nSegMaR22 [20] .815 .753 .795 .874 .884 .071 .833 .724 .757 .899 .906 .034 .841 .781 .820 .896 .907 .046\nZoomNet22 [32] .820 .752 .794 .878 .892 .066 .838 .729 .766 .888 .911 .029 .853 .784 .818 .896 .912 .043\nSINetv222 [10] .820 .743 .782 .882 .895 .070 .815 .680 .718 .887 .906 .037 .847 .770 .805 .903 .914 .048\nFEDER23 [14] .802 .738 .781 .867 .873 .071 .822 .716 .751 .900 .905 .032 .847 .789 .824 .907 .915 .044\nHitNet23 [17] .849 .809 .831 .906 .910 .055 .871 .806 .823 .935 .938 .023 .875 .834 .853 .926 .929 .037\nFSPNet23 [18] .856 .799 .830 .899 .928 .050 .851 .735 .769 .895 .930 .026 .879 .816 .843 .915 .937 .035\nBiRefNet .904 .890 .904 .954 .959 .030 .913 .874 .888 .960 .967 .014 .914 .894 .909 .953 .960 .023\nTable 7. Comparison of different DIS methods on the performance, efficiency, and model complexity. Full details can be referred\nto https://drive.google.com/drive/u/0/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM.\nModel Runtime (ms) #Params (MB) MACs (G) DIS-TEs (HCE, Fω\nβ )\nBiRefNetSwinL 83.3 215 1143 916, .858\nBiRefNetSwinL cp 78.3 215 1143 916, .858\nBiRefNetSwinB 61.4 101 561 954, .855\nBiRefNetSwinT 40.9 39 231 980, .822\nBiRefNetPV Tv2b2 47.8 35 195 969, .838\nBiRefNetPV Tv2b1 36.6 23 147 978, .817\nBiRefNetPV Tv2b0 32.9 11 89 1013, .806\nIS-Net 16.0 44 160 1016, .726\nUDUNRes50 33.5 25 142 977, .772\n20 30 40 50 60 70 80\nRuntime (ms) ↓\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86\nFω\nβ ↑\nBiRefNet-SwinL\nBiRefNet-SwinB\nBiRefNet-SwinT\nBiRefNet-PVT_v2_b2\nBiRefNet-PVT_v2_b1\nBiRefNet-PVT_v2_b0\nIS-Net\nUDUN\nFigure 8. Comparison of the efficiency, size, complexity, and per-\nformance of BiRefNet and existing DIS methods.\nthe proposed BiRefNet. As the results show, we provide\nsamples of all test sets and one validation set. BiRefNet\noutperforms the previous DIS methods from two perspec-\ntives, i.e., the location of target objects and the more accu-\nrate segmentation of the details of the objects. For example,\nin the samples of DIS-TE4 and DIS-TE2, there are neigh-\nboring distractors that attract the attention of other models\nto produce false positives. On the contrary, our BiRefNet\neliminates the distractors and accurately segments the tar-\nget. In samples of DIS-TE3 and DIS-VD, BiRefNet shows\nmuch greater performance in precisely segmenting areas\nwhere fine details are rich. Compared with previous meth-\nods, our BiRefNet can clearly segment slim shapes and\ncurved edges.\nWe also provide a qualitative comparison on the COD\ntask. Fig. 7 shows hard samples with different challenges.\nFor example, in the row of the occluded frog, the area\nof the frog is divided by the branch that covers it, while\nour BiRefNet can accurately segment the scattered frag-\nments almost the same as the GT map. In contrast, in the\nresults of the other methods, fragments are difficult to find\nall, let alone to provide precise segmentation maps. For tiny\nand slim objects, BiRefNet shows a better ability to find the\nright target. Our BiRefNet also shows superiority in finding\nmultiple concealed objects.\nEfficiency and Complexity Comparison. We equip\nour BiRefNet with different backbones to obtain models in\ndifferent sizes. Runtime, number of parameters, MACs, and\nperformance of them are further tested to provide a com-\nprehensive comparison between them and other methods.\nFirst, we provide a quantitative comparison in Tab. 7. The\nFPS of the largest BiRefNet can be more than 10, which is\n10\nA\nB\nD\nE\nF\nG\nC\nFigure 9. Potenial applications and selected existing third-party applications based on BiRefNet, and visual comparisons on social\nmedia. (A) Potential application #1. Building crack detection for the maintenance of architecture health. (B) Potential application #2.\nHighly accurate object extraction in high-resolution natural images. (C) A project by viperyl first packs our BiRefNet as a ComfyUI node\nand makes this SOTA model easier to use for everyone. (D) ZHO also provides a ComfyUI-based project to further improve the UI for\nour BiRefNet, especially for video data. (E) Fal.AI encapsulates our BiRefNet online with more useful options in UI and API to call the\nmodel. (F) ZHO provides a visual comparison between our BiRefNet and previous SOTA method BRIA RMGB v1.4 which has extra\ntraining on their private training dataset. (G) Toyxyz conducted a comparison between our BiRefNet and previous competitive human\nmatting methods (e.g., BRIAAI, RemBG, Robust Video Matting, and Person YOLOv8s) with both videos and images.\nacceptable in most practical applications. We also used the\ncompiled version ( BiRefNetSwinL cp) by PyTorch 2.0 [33]\non BiRefNetSwinL to accelerate its inference by 13%. In ad-\ndition, we draw the performance and runtime of each model\nin Fig. 8 for a clearer display.BiRefNet with different back-\nbones are evaluated and compared with existing DIS meth-\nods on DIS-TEs and DIS-VD. Different methods are drawn\nin different colors and markers. All tests are conducted on a\nsingle NVIDIA A100 GPU and an AMD EPYC 7J13 CPU.\n5. Potential Applications\nWe envisage that generated fine segmentation maps have the\npotential to be utilized in various practical applications.\nPotential Application #1 Crack Detection. The qual-\nity of the walls is important for the health of the architec-\nture [59]. However, segmentation models trained on com-\nmonly used datasets ( e.g., COCO [28]) can only segment\nregular foreground objects. The proposed BiRefNet trained\non the DIS5K dataset is more aware of the fine details and\ncan also segment targets with higher shape complexities. As\nshown in Fig. 9 (A), ourBiRefNet can accurately find cracks\nin the walls and help maintain when to repair them.\nPotential Application #2 Highly Accurate Object Ex-\ntraction. Foreground object extraction and background re-\nmoval have been popular applications in recent years. How-\never, commonly seen methods fail to generate high-quality\nresults when target objects have too high shape complexi-\nties [35, 36] or need manual guidance (e.g., scribble, point,\nand coarse mask) for more accurate segmentation [5, 26].\nThe proposed BiRefNet trained on DIS5K can generate re-\nsults with much higher resolution and segment thin threads\nat the hair level without a mask, as shown in Fig. 9 (B).\nOn the basis of such refined results, there may be numerous\nsuccessful downstream applications in the future.\n6. Third-Party Creations\nSince the release of our project on Mar 7, 2024, it has at-\ntracted much attention from many researchers and develop-\ners in the community to promote it spontaneously. Further-\nmore, great third-party applications have also been made\nbased on our BiRefNet. Due to the rapid growth of relevant\nworks, we only list some typical ones.\n#1 Practical Applications. Because of the excellent per-\nformance of our BiRefNet, more and more third-party ap-\nplications have been created by developers in the commu-\nnity12 . As shown in Fig. 9 (C and D), some developers have\nintegrated our BiRefNet into the ComfyUI as a node, which\nhelps a lot in matting foreground segmentation to better pro-\ncessing in the subsequent stable diffusion models. For bet-\n1https://github.com/comfyanonymous/ComfyUI\n2https : / / github . com / ZHO - ZHO - ZHO / ComfyUI -\nBiRefNet-ZHO\n11\nter online access, Fal.AI has established an online demo\nof our BiRefNet running on an A6000 GPU 34 , as shown\nin Fig. 9 (E). In addition to the common prediction of re-\nsults, this online application also provides an API service\nfor easy use with HTTP requests.\n#2 Social Media. In recent days, ourBiRefNet has drawn\nattention from the community. Many tweets have been\nposted on the X platform (formerly Twitter) 5. ZHO pro-\nvides a visual comparison between our BiRefNet and other\nmethods, as given in Fig. 9 (F). BiRefNet achieves compet-\nitive results with the previous SOTA method BRIA RMGB\nv1.4 in their tests 6. It should be noted that our BiRefNet\nwas trained on the training set of the open-source dataset\nDIS5K [37] under MIT license, while the other one was\ntrained on their carefully selected private data and cannot\nbe used for commercial use. As shown in Fig. 9 (G), more\ncomparisons on both video and image data have been pro-\nvided by Toyxyz on X between our BiRefNet and previous\ngreat foreground human matting methods 7. In addition to\nthese posts, PurzBeats has also made an animation with\nour BiRefNet and uploaded relevant videos8. A video tuto-\nrial can also be found on YouTube by ‘AI is in wonderland’\nin Japanese about how to use our BiRefNet in ComfyUI9.\n7. Conclusions\nThis work proposes a BiRefNet framework equipped with\na bilateral reference, which can perform dichotomous im-\nage segmentation, high-resolution (HR) salient object de-\ntection, and concealed object detection in the same frame-\nwork. With the comprehensive experiments conducted, we\nfind that unscaled source images and a focus on regions of\nrich information are vital to generating fine and detailed ar-\neas in HR images. To this end, we propose the bilateral\nreference to fill in the missing information in the fine parts\n(inward reference) and guide the model to focus more on\nregions with richer details (outward reference). This signif-\nicantly improves the model’s ability to capture tiny-pixel\nfeatures. To alleviate the high training cost of HR data\ntraining, we also provide various practical tricks to deliver\nhigher-quality prediction and faster convergence. Competi-\ntive results on 13 benchmarks demonstrate outstanding per-\nformance and strong generalization ability of our BiRefNet.\n3https://fal.ai/models/birefnet\n4Thanks to FAL.AI for providing us with additional computation re-\nsources for further explorations on more practical applications.\n5https : / / twitter . com / search ? q = birefnet & src =\ntyped_query\n6https : / / twitter . com / ZHOZHO672070 / status /\n1771026516388041038\n7https : / / twitter . com / toyxyz3 / status /\n1771413245267746952\n8https : / / twitter . com / i / status /\n1772323682934775896\n9https://www.youtube.com/watch?v=o2_nMDUYk6s\nWe also show that the techniques of BiRefNet can be trans-\nferred and used in many practical applications. We hope\nthat the proposed framework can encourage the develop-\nment of unified models for various tasks in the academic\ncommunity and that our model can empower and inspire\nthe developer community to create more great works.\nReferences\n[1] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada,\nand Sabine Susstrunk. Frequency-tuned salient region detec-\ntion. In IEEE / CVF Computer Vision and Pattern Recogni-\ntion Conference, 2009. 5, 6\n[2] Ali Borji, Ming-Ming Cheng, Huaizu Jiang, and Jia Li.\nSalient object detection: A benchmark. IEEE Transactions\non Image Process., 24:5706–5722, 2015. 6\n[3] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nEuropean Conference on Computer Vision Workshop, 2018.\n4, 7\n[4] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In IEEE / CVF International Conference on Com-\nputer Vision, 2017. 4\n[5] Linhui Dai, Xiang Song, Xiaohong Liu, Chengqi Li, Zhi-\nhao Shi, Martin Brooks, and Jun Chen. Enabling trimap-free\nimage matting with a frequency-guided saliency-aware net-\nwork via joint learning. IEEE Transactions on Multimedia,\n25:4868–4879, 2022. 11\n[6] Xinhao Deng, Pingping Zhang, Wei Liu, and Huchuan Lu.\nRecurrent multi-scale transformer for high-resolution salient\nobject detection. In ACM International Conference on Mul-\ntimedia, 2023. 2\n[7] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and Ali\nBorji. Structure-measure: A new way to evaluate foreground\nmaps. In IEEE / CVF International Conference on Computer\nVision, 2017. 2, 5, 6, 7\n[8] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-\nMing Cheng, and Ali Borji. Enhanced-alignment measure\nfor binary foreground map evaluation. In International Joint\nConference on Artificial Intelligence, 2018. 6\n[9] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng,\nJianbing Shen, and Ling Shao. Camouflaged object detec-\ntion. In IEEE / CVF Computer Vision and Pattern Recogni-\ntion Conference, 2020. 2, 6, 10\n[10] Deng-Ping Fan, Ge-Peng Ji, Ming-Ming Cheng, and Ling\nShao. Concealed object detection. IEEE Transactions on\nPattern Analysis and Machine Intelligence , 44(10):6024–\n6042, 2022. 1, 2, 5, 7, 10\n[11] Deng-Ping Fan, Ge-Peng Ji, Peng Xu, Ming-Ming Cheng,\nChristos Sakaridis, and Luc Van Gool. Advances in deep\nconcealed scene understanding. Visual Intelligence, 1(1):16,\n2023. 1\n[12] Deng-Ping Fan, Jing Zhang, Gang Xu, Ming-Ming Cheng,\nand Ling Shao. Salient objects in clutter. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 45(2):2344–\n2366, 2023. 1\n12\n[13] William I Grosky and Ramesh Jain. A pyramid-based ap-\nproach to segmentation applied to region matching. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n8(5):639–650, 1986. 2, 3\n[14] Chunming He, Kai Li, Yachao Zhang, Longxiang Tang, Yu-\nlun Zhang, Zhenhua Guo, and Xiu Li. Camouflaged object\ndetection with feature decomposition and edge reconstruc-\ntion. In IEEE / CVF Computer Vision and Pattern Recogni-\ntion Conference, 2023. 10\n[15] Chunming He, Kai Li, Yachao Zhang, Yulun Zhang, Chenyu\nYou, Zhenhua Guo, Xiu Li, Martin Danelljan, and Fisher\nYu. Strategic preys make acute predators: Enhancing cam-\nouflaged object detectors by generating camouflaged objects.\nIn International Conference on Learning Representations ,\n2023. 3\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. InIEEE / CVF\nComputer Vision and Pattern Recognition Conference, 2016.\n2, 7\n[17] Xiaobin Hu, Shuo Wang, Xuebin Qin, Hang Dai, Wenqi Ren,\nDonghao Luo, Ying Tai, and Ling Shao. High-resolution\niterative feedback network for camouflaged object detection.\nIn AAAI Conference on Artificial Intelligence, 2023. 2, 10\n[18] Zhou Huang, Hang Dai, Tian-Zhu Xiang, Shuo Wang, Huai-\nXin Chen, Jie Qin, and Huan Xiong. Feature shrinkage pyra-\nmid for camouflaged object detection with transformers. In\nIEEE / CVF Computer Vision and Pattern Recognition Con-\nference, 2023. 5, 7, 9, 10\n[19] Ge-Peng Ji, Deng-Ping Fan, Yu-Cheng Chou, Dengxin Dai,\nAlexander Liniger, and Luc Van Gool. Deep gradient learn-\ning for efficient camouflaged object detection. Machine In-\ntelligence Research, 20(1):92–108, 2023. 2\n[20] Qi Jia, Shuilian Yao, Yu Liu, Xin Fan, Risheng Liu, and\nZhongxuan Luo. Segment, magnify and reiterate: Detecting\ncamouflaged objects the hard way. In IEEE / CVF Computer\nVision and Pattern Recognition Conference, 2022. 10\n[21] Taehun Kim, Kunhee Kim, Joonyeong Lee, Dongmin Cha,\nJiho Lee, and Daijin Kim. Revisiting image pyramid struc-\nture for high resolution salient object detection. In Asian\nConference on Computer Vision, 2022. 2\n[22] Diederik P. Kingma and Jimmy Ba. Adam: A method\nfor stochastic optimization. In International Conference on\nLearning Representations, 2015. 6\n[23] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-\nHsuan Yang. Deep laplacian pyramid networks for fast and\naccurate super-resolution. In IEEE / CVF Computer Vision\nand Pattern Recognition Conference, 2017. 3\n[24] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-\nHsuan Yang. Fast and accurate image super-resolution with\ndeep laplacian pyramid networks.IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence , 41(11):2599–2613,\n2018. 3\n[25] Trung-Nghia Le, Tam V Nguyen, Zhongliang Nie, Minh-\nTriet Tran, and Akihiro Sugimoto. Anabranch network for\ncamouflaged object segmentation. Computer Vision and Im-\nage Understanding, 184:45–56, 2019. 6, 7\n[26] Jizhizi Li, Jing Zhang, Stephen J Maybank, and Dacheng\nTao. Bridging composite and real: towards end-to-end deep\nimage matting. International Journal of Computer Vision ,\n130(2):246–266, 2022. 3, 11\n[27] Xiaofei Li, Jiaxin Yang, Shuohao Li, Jun Lei, Jun Zhang, and\nDong Chen. Locate, refine and restore: A progressive en-\nhancement network for camouflaged object detection. In In-\nternational Joint Conference on Artificial Intelligence, 2023.\n2\n[28] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC. Lawrence Zitnick. Microsoft coco: Common objects in\ncontext. In European Conference on Computer Vision Work-\nshop, 2014. 11\n[29] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In IEEE / CVF Computer Vi-\nsion and Pattern Recognition Conference, 2017. 4\n[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nIEEE / CVF International Conference on Computer Vision ,\n2021. 2, 3, 7\n[31] Yunqiu Lv, Jing Zhang, Yuchao Dai, Aixuan Li, Bowen Liu,\nNick Barnes, and Deng-Ping Fan. Simultaneously localize,\nsegment and rank the camouflaged objects. In IEEE / CVF\nComputer Vision and Pattern Recognition Conference, 2021.\n6, 7\n[32] Youwei Pang, Xiaoqi Zhao, Tian-Zhu Xiang, Lihe Zhang,\nand Huchuan Lu. Zoom in and out: A mixed-scale triplet\nnetwork for camouflaged object detection. In IEEE / CVF\nComputer Vision and Pattern Recognition Conference, 2022.\n10\n[33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zem-\ning Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch:\nAn imperative style, high-performance deep learning library.\nAdvances in Neural Information Processing Systems , 2019.\n7, 11\n[34] Jialun Pei, Zhangjun Zhou, Yueming Jin, He Tang, and Heng\nPheng-Ann. Unite-divide-unite: Joint boosting trunk and\nstructure for high-accuracy dichotomous image segmenta-\ntion. In ACM International Conference on Multimedia, 2023.\n1, 2, 3, 4, 5, 7, 8, 9\n[35] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao,\nMasood Dehghan, and Martin Jagersand. Basnet: Boundary-\naware salient object detection. In IEEE / CVF Computer\nVision and Pattern Recognition Conference, 2019. 3, 4, 9, 11\n[36] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood De-\nhghan, Osmar R Zaiane, and Martin Jagersand. U2-net: Go-\ning deeper with nested u-structure for salient object detec-\ntion. Pattern Recognition, 106:107404, 2020. 8, 9, 11\n[37] Xuebin Qin, Hang Dai, Xiaobin Hu, Deng-Ping Fan, Ling\nShao, et al. Highly accurate dichotomous image segmen-\ntation. In European Conference on Computer Vision Work-\nshop, 2022. 1, 2, 3, 5, 6, 7, 8, 9, 12\n[38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn Medical Image Computing and Computer Assisted Inter-\nventions, 2015. 2, 3, 4\n13\n[39] Tiancheng Shen, Yuechen Zhang, Lu Qi, Jason Kuen,\nXingyu Xie, Jianlong Wu, Zhe Lin, and Jiaya Jia. High qual-\nity segmentation for ultra high-resolution images. In IEEE /\nCVF Computer Vision and Pattern Recognition Conference,\n2022. 3\n[40] Yujia Sun, Shuo Wang, Chenglizhao Chen, and Tian-Zhu Xi-\nang. Boundary-guided camouflaged object detection. In In-\nternational Joint Conference on Artificial Intelligence, 2022.\n2, 10\n[41] Chufeng Tang, Hang Chen, Xiao Li, Jianmin Li, Zhaoxi-\nang Zhang, and Xiaolin Hu. Look closer to segment bet-\nter: Boundary patch refinement for instance segmentation. In\nIEEE / CVF Computer Vision and Pattern Recognition Con-\nference, 2021. 3\n[42] Lv Tang, Bo Li, Yijie Zhong, Shouhong Ding, and Mofei\nSong. Disentangled high quality salient object detection. In\nIEEE / CVF Computer Vision and Pattern Recognition Con-\nference, 2021. 2, 9\n[43] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,\nChaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui\nTan, Xinggang Wang, et al. Deep high-resolution represen-\ntation learning for visual recognition. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 43(10):3349–\n3364, 2020. 8, 9\n[44] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng,\nDong Wang, Baocai Yin, and Xiang Ruan. Learning to de-\ntect salient objects with image-level supervision. In IEEE /\nCVF Computer Vision and Pattern Recognition Conference,\n2017. 6, 7, 9\n[45] Jun Wei, Shuhui Wang, Zhe Wu, Chi Su, Qingming Huang,\nand Qi Tian. Label decoupling framework for salient ob-\nject detection. In IEEE / CVF Computer Vision and Pattern\nRecognition Conference, 2020. 2, 9\n[46] Chenxi Xie, Changqun Xia, Mingcan Ma, Zhirui Zhao, Xi-\naowu Chen, and Jia Li. Pyramid grafting network for one-\nstage high resolution saliency detection. In IEEE / CVF\nComputer Vision and Pattern Recognition Conference, 2022.\n2, 5, 6, 7, 8, 9\n[47] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang.\nDeep image matting. In IEEE / CVF Computer Vision and\nPattern Recognition Conference, 2017. 3\n[48] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and\nMing-Hsuan Yang. Saliency detection via graph-based man-\nifold ranking. In IEEE / CVF Computer Vision and Pattern\nRecognition Conference, pages 3166–3173, 2013. 6, 7\n[49] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan\nYang. Denseaspp for semantic segmentation in street scenes.\nIn IEEE / CVF Computer Vision and Pattern Recognition\nConference, pages 3684–3692, 2018. 4\n[50] Bowen Yin, Xuying Zhang, Qibin Hou, Bo-Yuan Sun, Deng-\nPing Fan, and Luc Van Gool. Camoformer: Masked sep-\narable attention for camouflaged object detection. arXiv\npreprint arXiv:2212.06570, 2022. 2\n[51] Qihang Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe\nLin, Ning Xu, Yutong Bai, and Alan Yuille. Mask guided\nmatting via progressive refinement network. In IEEE / CVF\nComputer Vision and Pattern Recognition Conference, 2021.\n3\n[52] Qian Yu, Xiaoqi Zhao, Youwei Pang, Lihe Zhang,\nand Huchuan Lu. Multi-view aggregation network\nfor dichotomous image segmentation. arXiv preprint\narXiv:2404.07445, 2024. 2\n[53] Yi Zeng, Pingping Zhang, Jianming Zhang, Zhe Lin, and\nHuchuan Lu. Towards high-resolution salient object detec-\ntion. In IEEE / CVF Computer Vision and Pattern Recogni-\ntion Conference, 2019. 2, 6, 7, 9\n[54] Zhao Zhang, Wenda Jin, Jun Xu, and Ming-Ming Cheng.\nGradient-induced co-saliency detection. In European Con-\nference on Computer Vision Workshop, 2020. 5\n[55] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nIEEE / CVF Computer Vision and Pattern Recognition Con-\nference, 2017. 3\n[56] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping\nShi, and Jiaya Jia. Icnet for real-time semantic segmenta-\ntion on high-resolution images. In European Conference on\nComputer Vision Workshop, 2018. 2, 3\n[57] Yijie Zhong, Bo Li, Lv Tang, Senyun Kuang, Shuang Wu,\nand Shouhong Ding. Detecting camouflaged object in fre-\nquency domain. In IEEE / CVF Computer Vision and Pattern\nRecognition Conference, 2022. 2\n[58] Yan Zhou, Bo Dong, Yuanfeng Wu, Wentao Zhu, Geng\nChen, and Yanning Zhang. Dichotomous image segmenta-\ntion with frequency priors. InInternational Joint Conference\non Artificial Intelligence, 2023. 1, 2, 3, 5, 9\n[59] Qin Zou, Zheng Zhang, Qingquan Li, Xianbiao Qi, Qian\nWang, and Song Wang. Deepcrack: Learning hierarchical\nconvolutional features for crack detection. IEEE Transac-\ntions on Image Process., 28(3):1498–1512, 2018. 11\n14"
  },
  {
    "source": "2407.13078v1 (1).pdf",
    "content": "Enhancing Temporal Action Localization:\nAdvanced S6 Modeling with Recurrent Mechanism\nSangyoun Lee1, Juho Jung2, Changdae Oh3, Sunghee Yun4\nSogang University1, Sungkyunkwan University2, University of Wisconsin–Madison3, Erudio Bio4\nleesy0882@sogang.ac.kr1, jhjeon9@g.skku.edu2, changdae.oh@wisc.edu3, sunghee.yun@erudio.bio4\nAbstract\nTemporal Action Localization (TAL) is a critical task in\nvideo analysis, identifying precise start and end times of\nactions. Existing methods like CNNs, RNNs, GCNs, and\nTransformers have limitations in capturing long-range de-\npendencies and temporal causality. To address these chal-\nlenges, we propose a novel TAL architecture leveraging\nthe Selective State Space Model (S6). Our approach in-\ntegrates the Feature Aggregated Bi-S6 block, Dual Bi-S6\nstructure, and a recurrent mechanism to enhance tempo-\nral and channel-wise dependency modeling without increas-\ning parameter complexity. Extensive experiments on bench-\nmark datasets demonstrate state-of-the-art results with mAP\nscores of 74.2% on THUMOS-14, 42.9% on ActivityNet,\n29.6% on FineAction, and 45.8% on HACS. Ablation stud-\nies validate our method’s effectiveness, showing that the\nDual structure in the Stem module and the recurrent mech-\nanism outperform traditional approaches. Our findings\ndemonstrate the potential of S6-based models in TAL tasks,\npaving the way for future research. Our code is available at\nhttps://github.com/lsy0882/RDFA-S6.\n1. Introduction\nTemporal Action Localization (TAL) is a crucial video\nanalysis task that identifies the precise start and end times\nof actions in videos. As video content becomes increasingly\ncomplex and abundant, accurate TAL methods are essential\nfor effectively capturing and analyzing meaningful actions\nin applications like sports analytics, surveillance, and inter-\nactive media [4, 15, 23, 43]. However, significant challenges\nremain in TAL, particularly in effectively capturing long-\nrange dependencies and temporal causality in video data.\nTraditional approaches to TAL, including CNNs, RNNs,\nGCNs, and Transformers, each bring unique strengths but\nalso have inherent limitations. CNNs are effective at captur-\ning spatial features but struggle with long-range dependen-\ncies due to limited receptive fields [31]. RNNs can model\ntemporal sequences but face challenges such as vanishing\ngradients, which hinder their ability to capture long-term\ndependencies [10, 26]. GCNs are powerful for relational\ndata but are not inherently designed for sequential temporal\ndata [17]. Transformers have revolutionized TAL with their\nability to model global context using self-attention mecha-\nnisms [2, 32]. However, their reliance on attention scores\nto capture relationships within a sequence does not inher-\nently account for the temporal causality and history of visual\nelements over time. This limitation makes Transformers\nless optimal for tasks requiring precise temporal causality,\nsuch as TAL, where understanding the sequential nature and\ndependencies of actions is crucial [11, 13].\nThe State Space Model (SSM) [11, 13] has emerged as a\npromising alternative for sequence modeling by addressing\nthe limitations of traditional methods, especially in capturing\ntemporal causality. Within the SSM framework, the Selec-\ntive State Space Model (S6) [11] stands out for TAL tasks\ndue to its ability to maintain and leverage historical context\nthrough its selection mechanism and gating operation. These\nproperties enable S6 to dynamically adjust the influence\nof new inputs—specifically, the spatiotemporal feature vec-\ntors extracted from the current video clip—ensuring that the\nmodel retains and utilizes critical temporal information while\nintegrating new data. This dynamic adjustment and selec-\ntive retention enable S6 to capture long-range dependencies\nand temporal causality effectively, providing understanding\nof action sequences essential for accurately pinpointing the\nstart and end times of actions in TAL.\nActionMamba [7], an S6-based TAL method, has demon-\nstrated that S6-based method can surpass Transformers\nin sequence modeling by replacing Transformer blocks\nwith S6 blocks. ActionMamba simply substitutes the\nTransformer-based blocks for sequence modeling in the Ac-\ntionFormer [42] architecture with S6-based blocks. The S6\nblocks use a bi-directional processing approach [45] and\nincorporate weight sharing between networks operating in\neach direction. However, this study lacks a thoughtful de-\nsign focused on effective TAL methods, instead offering\na straightforward replacement of Transformer blocks with\n1\narXiv:2407.13078v1  [cs.CV]  18 Jul 2024\nslightly enhanced S6 ones. While ActionMamba highlights\nthe potential for S6-based sequence modeling to outperform\nTransformer-based approaches, it falls short of fully explor-\ning this potential or providing clear guidelines for leveraging\nS6 effectively in TAL tasks.\nOur research aims to explore the potential of S6-based\nTAL methods by building on insights from previous studies\non CNNs, RNNs, GCNs, and Transformers [10,17,26,31,32].\nWe propose a novel architecture that leverages the strengths\nof these traditional models while capitalizing on the unique\ncapabilities of S6.\nThis paper makes the following contributions to the field\nof TAL:\n1. Advanced Dependency Modeling with S6: We con-\nduct a pioneering exploration of S6’s potential in TAL\ntasks, particularly focusing on its dependency modeling\ncapabilities. By introducing an advanced dependency\nmodeling technique based on the Feature Aggregated\nBi-S6 (FA-Bi-S6) block design and the Dual Bi-S6\nstructure, we enable robust and effective modeling of\ndependencies within video sequences. The FA-Bi-S6\nblock employs multiple Conv1D layers with different\nkernel sizes to capture various granularities of tempo-\nral and channel-wise features, while the Dual Bi-S6\nstructure processes features along both the temporal\nand channel dimensions to enhance the integration of\nspatiotemporal dependencies. This approach provides\ndirection for TAL modeling, enabling more effective\nutilization of S6 in this domain.\n2. Efficiency through Recurrent Mechanism: Our study\nreveals that using a recurrent mechanism to repeatedly\napply a single S6-based model outperforms the tradi-\ntional approach of stacking multiple blocks. This re-\ncurrent application enhances the model’s performance\nwithout increasing the number of parameters, providing\nan effective solution for improving TAL models.\n3. State-of-the-Art Performance : We achieve state-\nof-the-art (SOTA) results across multiple benchmark\ndatasets, including THUMOS-14 [15], ActivityNet [4],\nFineAction [23], and HACS [43]. Our ablation stud-\nies analyze the effectiveness of each component of our\nproposed architecture, confirming the performance im-\nprovements brought by our method.\n2. Related works\nConvolutional Neural Networks (CNNs) Early TAL re-\nsearch used 2D CNNs to process spatial information, with\ninitial attempts like FV-DTF [25] combining spatial and\ntemporal data but handling them separately. The introduc-\ntion of 3D CNNs, as seen in CDC [29], marked a signifi-\ncant advancement by capturing spatiotemporal features with\nthree-dimensional convolution kernels. However, temporal\nresolution loss inherent in traditional 3D CNNs was still\na challenge to conquer. To cope with this, methods such\nas TPC [37] and FSN [38] aimed to balance spatial and\ntemporal feature processing. GTAN [24] and PBRNet [20]\nfurther optimized temporal intervals and hierarchical fea-\nture extraction. TPC maintained temporal receptive fields\nwhile downsampling spatial fields, and FSN captured finer-\ngrained dependencies by sequentially processing spatial and\ntemporal features.\nOur FA-Bi-S6 block builds on these advances by incorpo-\nrating multiple Conv1D layers with varying kernel sizes in\nparallel to capture a wide range of local contexts. The result-\ning feature map is processed bi-directionally by the Bi-S6\nnetwork, enhancing the model’s ability to capture complex\ndynamics effectively.\nRecurrent Neural Networks (RNNs) To address the tem-\nporal challenges that CNNs alone couldn’t solve, RNNs\nwere integrated into TAL frameworks. Early efforts like\nPSDF [40] and advancements such as AS [1] used RNNs to\nenhance temporal context modeling from dense trajectories\nand refine spatial features for detailed analysis. More sophis-\nticated integrations followed, such as GRU-Split [16], which\nemployed GRUs to refine action boundaries and probabili-\nties. However, RNNs introduced challenges like managing\nlong sequences and vanishing gradients. RCL [34] addressed\nthese issues by using a recurrent module to dynamically ad-\njust action segment predictions.\nOur research transcends the limitations of CNNs and\nRNNs by incorporating a recurrent mechanism within our\nS6-based architecture. This mechanism, integrated with\nour Backbone’s Stem module, enhances temporal context\nmodeling using the efficiency and precision of state space\nmodels.\nGraph Convolutional Networks (GCNs) The limitations\nof RNNs led to the exploration of GCNs in the TAL domain.\nGCNs structure video data as graphs, with nodes represent-\ning spatiotemporal features and edges defining their relation-\nships, allowing for more comprehensive modeling of tem-\nporal dependencies. A notable advancement, P-GCN [41],\nexpanded the range of dependencies that could be modeled\nbut faced challenges in scalability and efficiency due to com-\nputational overhead. G-TAD [36] addressed these issues\nwith a dual-stream graph convolution framework, efficiently\ncapturing both fixed and adaptive temporal dependencies.\nBuilding on GCN insights, we developed the Dual Bi-S6\nstructure, integrating the TFA-Bi-S6 and CFA-Bi-S6 blocks.\nTFA-Bi-S6 captures temporal dependencies, while CFA-Bi-\nS6 handles dependencies between spatiotemporal features by\nfocusing on the channel dimension. This combined approach\nenhances the robustness and accuracy of TAL by effectively\nmodeling both temporal and channel-wise contexts.\nTransformers The limitations of GCNs in handling ex-\ntensive temporal dependencies led to the adoption of\n2\nNeckd\nNeck2\nNeck1\nHeadsd\nHeads2\nHeads1\n𝑶𝒖𝒕𝒑𝒖𝒕𝒅\n𝑩𝒓𝒂𝒏\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝑶𝒖𝒕𝒑𝒖𝒕𝒅\n𝑵𝒆𝒄𝒌\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\nPretrained\nvideo \nencoder\nBackbone\n𝑿\nሾ 𝐵,𝐶௜௡,𝐿ሿ\n𝑪𝒍𝒊𝒑𝒔\nሾ 𝐿,𝑇,𝐻,𝑊ሿ\nFreeze\nInternVideo2\n6B / 1B\n𝑺𝒄𝒐𝒓𝒆𝒔 𝒄𝒍𝒂𝒔𝒔\n  𝒑𝒓𝒆𝒅\n㏝𝑻 𝒔𝒕𝒂𝒓𝒕,𝑻 𝒆𝒏𝒅㏞\n𝑺𝒄𝒐𝒓𝒆𝒔 𝒄𝒍𝒂𝒔𝒔\n  𝒑𝒓𝒆𝒅\n㏝𝑻 𝒔𝒕𝒂𝒓𝒕,𝑻 𝒆𝒏𝒅㏞\n𝑺𝒄𝒐𝒓𝒆𝒔 𝒄𝒍𝒂𝒔𝒔\n  𝒑𝒓𝒆𝒅\n㏝𝑻 𝒔𝒕𝒂𝒓𝒕,𝑻 𝒆𝒏𝒅㏞\nEmbedding\nmodule\nStem\nmodule\nBranch\nmodule\nLayerNorm\n㏙C㏚\nConv1D\n(𝑪𝒆𝒎𝒃,𝑪𝒄𝒍𝒂𝒔𝒔㏚\nConv1D\n(𝑪𝒆𝒎𝒃,𝟐㏚\nConv1D\n(𝑪𝒆𝒎𝒃,𝑪𝒆𝒎𝒃㏚\nConv1D \n(𝑪𝒆𝒎𝒃,𝑪𝒆𝒎𝒃㏚\nx2\nx2\nScale \nReLU\nLN㏙C㏚ \nReLU\nLN㏙C㏚ \nReLU\nNM\nS\n(a)\n𝑪𝒍𝒊𝒑𝟏\nሾ 𝑇,𝐻,𝑊ሿ\n𝑻\n𝑪𝒍𝒊𝒑𝟐\nሾ 𝑇,𝐻,𝑊ሿ\n𝑪𝒍𝒊𝒑𝑳\nሾ 𝑇,𝐻,𝑊ሿ\nPretrained\nvideo \nencoder\nPretrained\nvideo \nencoder\nPretrained\nvideo \nencoder\n𝑪𝒊𝒏 𝑪𝒆𝒎𝒃\nSpatio㎿tem\nporal\nfeature extract\n㏙ 𝑪𝒊𝒏\n㏚\nEm\nbedding\nm\nodule\n𝑪𝒆𝒎𝒃 𝑪𝒆𝒎𝒃\n𝑪𝒆𝒎𝒃\nFeature\nAggregation\nDual㎿path\nProcessing\nVideo\nSequence\nTemporal\nBi㎿S6\n㏙Inter clip㏚\n𝑪𝒆𝒎𝒃\n𝑪𝒆𝒎𝒃\n𝑪𝒆𝒎𝒃\nChannel\nBi㎿S6\n㏙Intra clip㏚\nShared\nShared\n𝑪𝒆𝒎𝒃\nAdd\nFeature\nAggregation\nRepeat 㐱 𝒓㐲\nStem module\n(b)\nFigure 1. Illustration of the proposed architecture and its components. (a) The architecture overview, which consists of four main\nparts: Pretrained video encoder, Backbone, Neck, and Heads (Action classification head and Temporal boundary regression head). (b) The\noverview of the proposed methods, highlighting the Stem module with an orange shaded area. The Stem module consists of three parts:\nDual-path processing (Dual Bi-S6 Structure), Feature Aggregation & Temporal/Channel Bi-S6 (Feature Aggregated Bi-S6 Block Design),\nand the repeat processing with shared networks (Recurrent Mechanism).\nTransformer-based models in TAL. Transformers use self-\nattention to extend temporal dependencies beyond GCN\nconstraints. TRA [44] used variable temporal boundary pro-\nposals with multi-head self-attention for flexible temporal\nmodeling, though it faced challenges in maintaining tempo-\nral causality over long sequences. ActionFormer [42] im-\nproved on this by using local self-attention and a multiscale\nfeature pyramid to capture various temporal resolutions, but\nit still struggled with capturing long-range dependencies and\nmaintaining precise temporal causality.\nTo address these issues, we introduced the S6 network\ninto our TAL system. The S6 network uses selective mecha-\nnisms and gating functions to modulate the impact of each\ntime step’s spatiotemporal features. This approach allows\nS6 to preserve critical historical information while integrat-\ning new spatiotemporal features, effectively capturing long-\nrange dependencies and temporal causality. By leveraging\nthese capabilities, S6 enhances the accuracy of feature ex-\ntraction and action localization, addressing the limitations of\nTransformer-based models in TAL.\n3. Proposed Methods\nWe introduce our approach, emphasizing advanced de-\npendency modeling for TAL by integrating the S6 model to\nimprove long-range dependency handling. Our key compo-\nnents include the Feature Aggregated Bi-S6 Block Design,\nDual Bi-S6 Structure, and Recurrent Mechanism.\n3.1. Preliminary: Selective Space State Model (S6)\nOur architecture uses the S6 model with selective mech-\nanisms and gating operations to capture complex temporal\ndynamics and capture long-range dependencies effectively.\nThe S6 model operates with parameters (∆t, A, B, C),\n3\ndiscretized to manage sequence transformations:\nht = Aht−1 + Bxt, y t = Cht\nHere, xt represents the input at time step t, which, in the\ncase of TAL, is the spatiotemporal feature vector extracted\nfrom single clip. The hidden state at time step t, ht, captures\nthe temporal context of the sequence. The output at time\nstep t, yt, represents the processed feature. The state matrix\nA determines how the previous hidden state ht−1 and the\nhistorical information from all previous steps influence the\ncurrent hidden state ht [12], contributing to precise action\nlocalization. The input matrix B defines how the input xt\naffects the hidden state ht. Finally, the output matrix C\ntranslates the hidden state ht into the output yt.\nThe process starts with the input xt being projected to\nderive B, C, and ∆t. This step transforms raw input fea-\ntures into suitable representations for state-space modeling.\nSpecifically, the projection functions apply linear transfor-\nmations to the input xt:\nB = Linear(xt), C = Linear(xt)\nTo dynamically manage information flow, the S6 model\nemploys selection mechanism and gating function. The dy-\nnamically adjusted parameter ∆t controls the discretization\nof the state-space model based on the relevance of the input\nxt, functioning similarly to a gating mechanism in RNNs.\nThe projection function s∆(xt), which includes learnable\nparameters, projects the input xt to one dimension before\nbroadcasting it across channels:\n∆t = softplus(s∆(xt))\nNext, the discretization step adjusts the parameters A and\nB for the current time step t, ensuring that the parameters\nare appropriately scaled for discrete-time processing:\nAt = exp(∆tA)\nBt = (∆tA)−1(exp(∆tA) − I) · ∆tB\nThe hidden state ht is updated using At and Bt, and the\noutput yt is generated using Ct = C:\nht = Atht−1 + Btxt, y t = Ctht\nThe selective update of the hidden state can be understood\nas:\nht = (1 − ∆t)ht−1 + ∆txt\nwhere ∆t functions similarly to the gating function gt in\nRNNs, determining the influence of the input xt on the\nhidden state ht. This dynamic adjustment helps the model\nfocus on relevant portions of the input, ensuring effective\nhandling of long-range dependencies.\nS6 is particularly effective in TAL tasks due to its abil-\nity to maintain and refine temporal context over extended\nsequences. By dynamically adjusting ∆t, the model can\nselectively retain important temporal features.\n3.2. Overview\nOur architecture, inspired by ActionFormer [42] and Ac-\ntionMamba [7], consists of four primary components: a\nPretrained video encoder, a Backbone, a Neck, and Heads.\nThe overview of architecture is depicted in Figure 1a.\nPretrained Video Encoder The Pretrained video encoder\nextracts spatiotemporal attributes from video clips. Trained\non diverse datasets such as UCF, Kinetics, Something-\nSomething, and vision-language multi-modal datasets like\nWebVid and InternVid, it leverages the vast training data\nfrom InterVideo2-6B/1B [35]. The pretrained video en-\ncoder’s example of receiving each clip and extracting spa-\ntiotemporal features is shown in Appendix A.\nBackbone The Backbone captures dependencies and ex-\ntracts features at various temporal resolutions from the se-\nquence data. As illustrated in Figure 1a, it consists of three\nmain modules:\n• Embedding Module: This module captures the coarse\nlocal context of spatiotemporal features. As shown\nin Figure 2a, the sequence is first passed through a\nConv1D to increase the dimensionality from Cin to\nCemb, followed by Layer Normalization (LN) and\nReLU activation. This process is followed by Be se-\nquential Conv1D with dimensions Cemb to Cemb, each\nfollowed by LN and ReLU activation, resulting in an\nembedded sequence of shape [B, Cemb, L].\n• Stem Module: This core component processes the em-\nbedded sequences to capture long-range dependency\nusing the Dual Bi-S6 Structure. As shown in Figure\n2b, it applies two main blocks in parallel: the Temporal\nFeature Aggregated Bi-S6 (TFA-Bi-S6) block and the\nChannel Feature Aggregated Bi-S6 (CFA-Bi-S6) block,\nwhich focus on capturing temporal and channel-wise\ndependencies, respectively. Each of these blocks is\nstacked Bs times. The TFA-Bi-S6 block handles input\nsequences reshaped from [B, Cemb, L] to [B, L, Cemb]\nand outputs back to [B, Cemb, L]. The CFA-Bi-S6\nblock processes the temporal-pooled output of TFA-\nBi-S6 with shape [B, Cemb, 1] and scales it using a\nsigmoid activation. The outputs from these blocks are\ncombined through point-wise multiplication with the\nTFA-Bi-S6 output. This combined output then goes\nthrough an affine transformation with a drop path and\nskip connection, followed by LN to enhance capacity.\nThis process uses a Recurrent Mechanism, repeating\nr times, with a weight-shared network applied at each\nrepetition to refine temporal dependency modeling.\n• Branch Module: This module handles temporal multi-\nscale dependencies. As shown in Figure 2c, each branch\napplies the Temporal Bi-S6 (T-Bi-S6) block, which is\n4\n· σ\nTFA㎿Bi㎿S6\n CFA㎿Bi㎿S6\n𝐈𝐧𝐩𝐮𝐭𝒓\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒓\n𝒓: Weight shared\nrecurrent index\nൈ𝑩𝒔 ൈ𝑩𝒔\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕  ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ Avg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nDrop path ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nConv1D㏙𝐶௘௠௕,𝐶௘௠௕㏚\nConv1D㏙𝐶௜௡,𝐶௘௠௕㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௜௡,𝐿 ሿ\nൈ𝑩𝒆\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\n T㎿Bi㎿S6\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒅\n𝒅: Weight unshared\ndownsample index\n𝐈𝐧𝐩𝐮𝐭𝒅\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nൈ𝑩𝒃\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nDrop path\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nMax pool\nLN㏙C㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗ⁄  ሿ\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nc\nS6\n S6\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nSiLUSiLU\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n· ·\nSum\nSum\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nLinear㏙𝐿௔, 2𝐿௔㏚\nc· ·\nS6\n S6\nLinear㏙2𝐿௔, 𝐿௔㏚\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,1  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nSiLUSiLU\nAvg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nSum\n Sum\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nc· ·\nS6\n S6\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗିଵሿ\n: Shared\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ Chunk\nSiLUSiLU\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n(a) Embedding module\n· σ\nTFA㎿Bi㎿S6\n CFA㎿Bi㎿S6\n𝐈𝐧𝐩𝐮𝐭𝒓\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒓\n𝒓: Weight shared\nrecurrent index\nൈ𝑩𝒔 ൈ𝑩𝒔\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕  ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ Avg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nDrop path ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nConv1D㏙𝐶௘௠௕,𝐶௘௠௕㏚\nConv1D㏙𝐶௜௡,𝐶௘௠௕㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௜௡,𝐿 ሿ\nൈ𝑩𝒆\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\n T㎿Bi㎿S6\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒅\n𝒅: Weight unshared\ndownsample index\n𝐈𝐧𝐩𝐮𝐭𝒅\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nൈ𝑩𝒃\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nDrop path\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nMax pool\nLN㏙C㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗ⁄  ሿ\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nc\nS6\n S6\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nSiLUSiLU\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n· ·\nSum\nSum\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nLinear㏙𝐿௔, 2𝐿௔㏚\nc· ·\nS6\n S6\nLinear㏙2𝐿௔, 𝐿௔㏚\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,1  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nSiLUSiLU\nAvg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nSum\n Sum\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nc· ·\nS6\n S6\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗିଵሿ\n: Shared\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ Chunk\nSiLUSiLU\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ (b) Stem module\n· σ\nTFA㎿Bi㎿S6\n CFA㎿Bi㎿S6\n𝐈𝐧𝐩𝐮𝐭𝒓\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒓\n𝒓: Weight shared\nrecurrent index\nൈ𝑩𝒔 ൈ𝑩𝒔\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕  ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ Avg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nDrop path ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nConv1D㏙𝐶௘௠௕,𝐶௘௠௕㏚\nConv1D㏙𝐶௜௡,𝐶௘௠௕㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௜௡,𝐿 ሿ\nൈ𝑩𝒆\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\n T㎿Bi㎿S6\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒅\n𝒅: Weight unshared\ndownsample index\n𝐈𝐧𝐩𝐮𝐭𝒅\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nൈ𝑩𝒃\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nDrop path\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nMax pool\nLN㏙C㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗ⁄  ሿ\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nc\nS6\n S6\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nSiLUSiLU\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n· ·\nSum\nSum\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nLinear㏙𝐿௔, 2𝐿௔㏚\nc· ·\nS6\n S6\nLinear㏙2𝐿௔, 𝐿௔㏚\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,1  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nSiLUSiLU\nAvg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nSum\n Sum\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nc· ·\nS6\n S6\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗିଵሿ\n: Shared\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ Chunk\nSiLUSiLU\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ (c) Branch module\nFigure 2. Diagrams of the Embedding, Stem, and Branch modules. (a) Embedding module. (b) Stem module. (c) Branch module.\na modified version of the Bi-S6 block used in Action-\nMamba [7], followed by an affine drop path and resid-\nual connection. After this, the output undergoes LN\nand max pooling along the temporal dimension, effec-\ntively obtaining various temporal resolutions. The T-Bi-\nS6 block processes the input sequence reshaped from\n[B, Cemb, L/2d−1] to [B, L/2d−1, Cemb] and outputs\nback to [B, Cemb, L/2d−1]. This process is repeated\nfor each downsampling index (d = 1, 2, ...,5), where\nthe output shape becomes [B, Cemb, L/2d].\nNeck and Heads The Neck is designed with simplicity and\nefficiency in mind, utilizing layer normalization for channel-\nwise normalization, which is the same as the LN used in the\nBranch module. This step ensures that the temporal multi-\nscale sequences reflecting precise temporal dependencies\nprocessed by the Backbone are normalized and ready for\nsubsequent processing.\nThe Heads leverage the normalized features from the\nNeck to carry out two primary tasks: action classification and\ntemporal boundary regression. The action classification head\ngenerates channels equal to the number of action categories,\npredicting class scores for each category. Simultaneously,\nthe temporal boundary regression head outputs two channels\nto predict the frame indices marking the start and end of\nan action. This dual-head design ensures that the model\ncan accurately classify actions and determine their temporal\nboundaries within the video segments.\n3.3. Advanced Dependency Modeling for TAL\nFeature Aggregated Bi-S6 (FA-Bi-S6) Block Design The\nFA-Bi-S6 block design is one of our contributions, enabling\nrobust and effective modeling of dependencies within video\nsequences. This block design incorporates multiple Conv1D\nlayers, each with different kernel sizes, operating sequen-\ntially within two main blocks: the TFA-Bi-S6 block and the\nCFA-Bi-S6 block, as shown in Figure 3a and 3b.\nIn the TFA-Bi-S6 block, the input sequence of shape\n[B, Cemb, L] is first passed through a linear layer that ad-\njusts the dimensions from [B, Cemb, L] to [B, L,2Cemb].\nThe sequence is then divided into two chunks, and one of\nthese chunks is flipped. These chunks are processed through\nmultiple Conv1D layers with varying kernel sizes (2, 3, 4),\neach capturing different granularities of temporal features.\nThe outputs from these Conv1D layers are summed to create\nan aggregated feature map, which is then processed through a\nS6 network focusing on temporal dependencies. The output\nof the S6 blocks is then multiplied pointwise with the orig-\ninal chunked input processed through the SiLU activation.\nThe results from each chunk are concatenated, which handle\nbi-directional temporal dependencies. The final output is\nobtained by combining the results, which are then processed\nthrough a linear layer and reshaped back to [B, Cemb, L].\nIn the CFA-Bi-S6 block, the process is similar to the TFA-\nBi-S6 block with adaptations for channel-wise dependency\nmodeling. The input sequence is first adaptively pooled\nto [B, Cemb, La] before the linear layer processing. The\nConv1D layers in this block have varying kernel sizes (2, 4,\n8) to capture different scales of channel-wise dependencies.\nAfter processing through the S6 blocks and linear layer, the\nfinal output is average pooled to [B, Cemb, 1]. These adjust-\nments enable the CFA-Bi-S6 block to focus on capturing\ndiverse channel-wise dependencies and enhance the over-\nall capacity to model complex spatiotemporal interactions\nwithin video sequences.\nBy integrating the Bi-S6 block with the aggregated fea-\nture map, our design leverages the strengths of both multi-\nscale feature extraction and bi-directional processing. The\ncombined architecture allows the model to effectively cap-\n5\n· σ\nTFA㎿Bi㎿S6\n CFA㎿Bi㎿S6\n𝐈𝐧𝐩𝐮𝐭𝒓\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒓\n𝒓: Weight shared\nrecurrent index\nൈ𝑩𝒔 ൈ𝑩𝒔\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕  ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ Avg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nDrop path ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nConv1D㏙𝐶௘௠௕,𝐶௘௠௕㏚\nConv1D㏙𝐶௜௡,𝐶௘௠௕㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௜௡,𝐿 ሿ\nൈ𝑩𝒆\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\n T㎿Bi㎿S6\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒅\n𝒅: Weight unshared\ndownsample index\n𝐈𝐧𝐩𝐮𝐭𝒅\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nൈ𝑩𝒃\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nDrop path\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nMax pool\nLN㏙C㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗ⁄  ሿ\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nc\nS6\n S6\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nSiLUSiLU\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n· ·\nSum\nSum\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nLinear㏙𝐿௔, 2𝐿௔㏚\nc· ·\nS6\n S6\nLinear㏙2𝐿௔, 𝐿௔㏚\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,1  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nSiLUSiLU\nAvg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nSum\n Sum\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nc· ·\nS6\n S6\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗିଵሿ\n: Shared\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ Chunk\nSiLUSiLU\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n(a) TFA-Bi-S6\n· σ\nTFA㎿Bi㎿S6\n CFA㎿Bi㎿S6\n𝐈𝐧𝐩𝐮𝐭𝒓\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒓\n𝒓: Weight shared\nrecurrent index\nൈ𝑩𝒔 ൈ𝑩𝒔\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕  ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ Avg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nDrop path ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nConv1D㏙𝐶௘௠௕,𝐶௘௠௕㏚\nConv1D㏙𝐶௜௡,𝐶௘௠௕㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௜௡,𝐿 ሿ\nൈ𝑩𝒆\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\n T㎿Bi㎿S6\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒅\n𝒅: Weight unshared\ndownsample index\n𝐈𝐧𝐩𝐮𝐭𝒅\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nൈ𝑩𝒃\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nDrop path\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nMax pool\nLN㏙C㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗ⁄  ሿ\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nc\nS6\n S6\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nSiLUSiLU\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n· ·\nSum\nSum\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nLinear㏙𝐿௔, 2𝐿௔㏚\nc· ·\nS6\n S6\nLinear㏙2𝐿௔, 𝐿௔㏚\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,1  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nSiLUSiLU\nAvg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nSum\n Sum\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nc· ·\nS6\n S6\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗିଵሿ\n: Shared\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ Chunk\nSiLUSiLU\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ (b) CFA-Bi-S6\n· σ\nTFA㎿Bi㎿S6\n CFA㎿Bi㎿S6\n𝐈𝐧𝐩𝐮𝐭𝒓\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒓\n𝒓: Weight shared\nrecurrent index\nൈ𝑩𝒔 ൈ𝑩𝒔\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕  ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ Avg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nDrop path ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nConv1D㏙𝐶௘௠௕,𝐶௘௠௕㏚\nConv1D㏙𝐶௜௡,𝐶௘௠௕㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௜௡,𝐿 ሿ\nൈ𝑩𝒆\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\n T㎿Bi㎿S6\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒅\n𝒅: Weight unshared\ndownsample index\n𝐈𝐧𝐩𝐮𝐭𝒅\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nൈ𝑩𝒃\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nDrop path\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nMax pool\nLN㏙C㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗ⁄  ሿ\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nc\nS6\n S6\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nSiLUSiLU\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n· ·\nSum\nSum\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nLinear㏙𝐿௔, 2𝐿௔㏚\nc· ·\nS6\n S6\nLinear㏙2𝐿௔, 𝐿௔㏚\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,1  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nSiLUSiLU\nAvg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nSum\n Sum\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nc· ·\nS6\n S6\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗିଵሿ\n: Shared\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ Chunk\nSiLUSiLU\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ (c) T-Bi-S6\nFigure 3. Diagrams of the Feature aggregated Bi-S6 block design. (a) TFA-Bi-S6 model. (b) CFA-Bi-S6 model. (c) T-Bi-S6 model.\nture and utilize spatiotemporal features across a wide range\nof context, addressing the limitations of traditional single\nconvolutional approaches. This design is particularly ad-\nvantageous for TAL tasks, where actions may occur over\nvarying temporal spans, and the local context provided by\nsurrounding frames is crucial for accurate localization.\nDual Bi-S6 Structure The Dual Bi-S6 structure is a novel\ncomponent of our proposed architecture, designed to en-\nhance the modeling of spatiotemporal dependencies by pro-\ncessing features along both the temporal and channel dimen-\nsions. This dual-path approach ensures that the model can\ncapture and integrate the rich contextual information present\nin video sequences, thereby improving the accuracy of TAL.\nAs shown in Figure 2b, the Dual Bi-S6 structure consists\nof two parallel paths: the TFA-Bi-S6 and the CFA-Bi-S6.\nEach path processes the input sequence differently to ex-\ntract complementary information. The TFA-Bi-S6 reflects\ntemporal dynamics within the video sequence, providing\na detailed temporal analysis of the input. Simultaneously,\nthe CFA-Bi-S6 captures the interactions between different\nspatiotemporal features, and its output is then scaled using a\nsigmoid function to transform the values into a range suitable\nfor modulation.\nAfter processing the input through both paths, the out-\nputs of the TFA-Bi-S6 and CFA-Bi-S6 are combined using\npoint-wise multiplication. This fusion step integrates the\ntemporal dependencies captured by the TFA-Bi-S6 with the\nchannel-wise dependencies modeled by the CFA-Bi-S6. The\npoint-wise multiplication ensures that the combined features\nreflect both types of dependencies, with the TFA-Bi-S6 han-\ndling global dependencies between clips and the CFA-Bi-S6\naddressing local dependencies between spatiotemporal fea-\ntures within clips. The design intention behind this structure\nis to leverage the strengths of both paths: the TFA-Bi-S6\ncaptures temporal dependencies and dynamics, while the\nCFA-Bi-S6 emphasizes the relationships between spatiotem-\nporal features. By scaling the output of the CFA-Bi-S6 and\nmultiplying it with the TFA-Bi-S6 output, the model effec-\ntively combines temporal analysis with channel-wise context,\nleading to a more comprehensive understanding of the video.\nRecurrent Mechanism This mechanism, integrated with\nour Stem module in the Backbone, enhances the accuracy\nof temporal context modeling by leveraging the efficiency\nand precision of state space models. As shown in Figure 2b,\nthe process begins by passing the input sequence through\nthe Stem module to capture initial temporal dependencies.\nThe output is combined with the original input sequence\nand reprocessed by the Stem module, repeating this process\nr times. Each iteration refines the temporal dependencies\nfurther, enhancing the model’s ability to capture long-range\ndependencies and intricate temporal patterns. This recurrent\nmechanism provides a robust framework for refining tempo-\nral context, allowing the model to improve its understanding\nof temporal dependencies dynamically.\nThe effectiveness of this recurrent mechanism in speech\nseparation tasks highlights its potential for TAL tasks as well.\nIn speech separation, recurrent mechanisms have proven to\nexcel in capturing long-range dependencies and intricate\ntemporal patterns [6, 14]. This iterative refinement process,\nwhich involves passing the input sequence through a module\nmultiple times to capture and refine temporal dependencies,\nallows models to handle complex long-range dependencies\nwith greater precision. Such capabilities are directly applica-\nble to TAL tasks, where identifying precise segments within\na video also requires understanding temporal dependencies\nover extended periods.\n6\nBase System\nmAP (%)\n@.3 @.4 @.5 @.6 @.7 Avg\nCNN CDC [29] 40.1 29.4 23.3 13.1 7.9 22.8\nTAL-Net [5] 53.2 48.5 42.8 33.8 20.8 39.8\nPBRNet [20] 58.5 54.6 51.3 41.8 29.5 47.1\nRNN AS [1] 51.8 42.4 30.8 20.2 11.1 31.3\nRCL [34] 70.1 62.3 52.9 42.7 30.7 51.7\nGCN G-TAD [36] 66.4 60.4 51.6 37.6 22.9 47.8\nTransformer TallFormer [9] 76.0 71.5 63.2 50.9 34.5 59.2\nActionFormer [42] 82.1 77.8 71.0 59.4 43.9 66.8\nTriDet [28] 83.6 80.1 72.9 62.4 47.4 69.3\nS6 ActionMamba [7] 86.9 83.1 76.9 65.1 50.8 72.7\nOurs 88.7 84.6 78.2 66.6 51.9 74.2\n(a)\nBase System\nmAP (%)\n@.5 @.75 @.95 Avg\nCNN BSN [19] 46.5 30.0 8.0 30.0\nDCAN [8] 51.8 36.0 9.5 35.4\nRNN DeepAct [30] 37.8 24.8 10.0 24.0\nGCN G-TAD [36] 50.4 34.6 9.0 34.1\nA VFusion [3] 54.3 37.7 8.9 36.8\nTransformer ActionFormer [42] 54.7 37.8 8.4 36.6\nTriDet [28] 54.7 38.0 8.4 36.8\nTCANet [27] 54.3 39.1 8.4 37.6\nAdaTAD [21] 61.7 43.4 10.9 41.9\nS6 ActionMamba [7] 62.4 43.5 10.2 42.0\nOurs 64.1 44.0 10.6 42.9\n(b)\nBase System\nmAP (%)\n@.5 @.75 @.95 Avg\nCNN DBG [18] 10.7 6.4 2.5 6.8\nGCN G-TAD [36] 13.7 8.8 3.1 9.1\nTransformer VideoMAE-v2 [33] 29.1 17.7 5.1 18.2\nS6 ActionMamba [7] 45.4 28.8 6.8 29.0\nOurs 46.4 29.5 7.6 29.6\n(c)\nBase System\nmAP (%)\n@.5 @.75 @.95 Avg\nCNN DyFADet [39] 64.0 44.8 14.1 44.3\nTransformer TadTR [22] 47.1 32.1 10.9 32.1\nTriDet [28] 62.4 44.1 13.1 43.1\nS6 ActionMamba [7] 64.0 45.7 13.3 44.6\nOurs 66.4 47.2 14.3 45.8\n(d)\nTable 1. Results of temporal action localization on benchmark datasets. (a) THUMOS-14 [15], (b) ActivityNet [4], (c) FineAction [23],\n(d) HACS [43]. The metric used is mean Average Precision (mAP) evaluated at multiple tIoU thresholds.\n4. Experiments\nWe provide a comprehensive evaluation of our TAL\nmethod through extensive experiments. We demonstrate its\neffectiveness using various benchmark datasets and conduct\nablation studies to assess the impact of various components\nof our proposed approach.\n4.1. Evaluation on Benchmarks\nTo evaluate the effectiveness of the proposed method for\nTAL, we utilized the benchmark datasets THUMOS-14 [15],\nActivityNet [4], FineAction [23], and HACS [43]. Detailed\ndescriptions of each benchmark can be found in Appendix B.\nTable 1a presents experimental results on THUMOS-14.\nWe compared our method with various approaches, includ-\ning CNNs, RNNs, GCNs, Transformers-based, and the latest\nSOTA S6-based model. Our method achieved an average\nmAP of 74.2%, surpassing the previous SOTA by 1.5%. In\nTable 1b, we summarize our performance on ActivityNet.\nDespite its larger scale and variety of classes, which gener-\nally result in lower scores, our method achieved an average\nmAP of 42.9%, surpassing the previous SOTA by 0.9%.\nThe outcomes on FineAction are presented in Table 1c.\nThis benchmark, being relatively new, lacked RNN-based\nstudies for comparison. Therefore, we included studies uti-\nlizing CNN, GCN, Transformer, and S6 models. FineAc-\ntion’s high class variety relative to its size makes it par-\nticularly challenging, generally resulting in lower mAP\nscores. Nonetheless, our approach achieved an average mAP\nof 29.6%, which is 0.6% higher than the previous SOTA.\nFinally, Table 1d displays our experimental performance\non HACS. Most studies focused on Transformer-based ap-\nproaches due to the dataset’s large scale. Despite this, our\nproposed method achieved an average mAP of 45.8%, ex-\nceeding the previous SOTA by 1.2%.\n4.2. Ablation Studies\nStem module structure and Block quantities We investi-\ngated the impact of varying the structure of the Stem mod-\nule and the number of blocksin the Embedding, Stem, and\nBranch modules to understand their effect on performance.\nThe results, presented in Table 2a, demonstrate the su-\nperiority of the Dual structure in the Stem module, which\nutilizes both temporal and channel blocks, consistently out-\nperforming the Single structure that only uses the temporal\nblock. This finding suggests that addressing both temporal\nand channel-wise dependencies provides a more compre-\nhensive understanding for TAL. Additionally, using a single\nblock in each module often yielded better performance than\nmultiple blocks, indicating that simpler, less complex model\nstructures help prevent overfitting and effectively capture es-\nsential spatiotemporal features. Notably, omitting the Stem\nmodule (Bs = 0) results in a significant performance drop,\nhighlighting its importance in sequence interpretation.\nKernel sizes and Aggregation methods We evaluated the\nperformance impact of differentkernel sizecombinations for\nTFA-Bi-S6 and CFA-Bi-S6 blocks and variousaggregation\nmethods using the Dual structure. This analysis, detailed in\nTable 2b, explores how different configurations influence the\nmodel’s ability to capture temporal and channel-wise local\ncontext.\n7\nStructure (Be,Bs,Bb) Params Avg mAP\n(M) (%)\nSingle (1,0,1) 16.0 69.4\n(1,1,1) 18.8 72.2\n(2,1,1) 19.6 72.0\n(1,2,1) 21.6 71.7\n(1,1,2) 33.0 71.0\n(2,2,1) 22.5 71.8\n(2,1,2) 33.8 71.1\n(1,2,2) 35.9 71.3\n(2,2,2) 36.6 71.5\n(1,4,1) 27.3 71.3\n(1,8,1) 38.6 70.7\nDual (1,1,1) 21.7 72.8\n(1,2,1) 28.5 72.5\n(a)\nKTFA KCFA Aggregate Params Avg mAP\n(M) (%)\nX X Sum 20.5 72.1\n(4) X Sum 21.6 72.6\nX (4) Sum 20.6 72.3\n(4) (4) Sum 21.7 72.8\n(2,4) (4) Sum 22.1 73.0\n(4) (2,4) Sum 21.7 72.9\n(2,4) (2,4) Sum 22.2 73.1\n(2,3,4) (2,3,4) Sum 23.0 73.4\n(2,3,4,8) (2,3,4,8) Sum 25.2 73.2\n(2,4,8) (2,3,4) Sum 24.3 73.4\n(2,3,4) (2,4,8) Sum 23.1 73.5\n(2,3,4) (2,4,8) Concat 31.6 72.8\n(2,4,8) (2,4,8) Sum 24.5 73.4\n(b)\nBs r Params Avg mAP\n(M) (%)\n1 1 23.1 73.5\n2 23.1 73.6\n4 23.1 73.7\n8 23.1 73.9\n16 23.1 74.2\n32 23.1 74.0\n2 1 31.3 73.1\n16 31.3 73.4\n32 31.3 73.2\n4 1 47.8 72.8\n16 47.8 72.6\n32 47.8 72.1\n8 1 80.9 72.3\n(c)\nTable 2. Ablation studies on the proposed methods. (a) Performance comparison with varying numbers of blocks in the Embedding, Stem,\nand Branch modules (Be, Bs, Bb) and different structures (Structure) using only single Conv1D layer without Feature Aggregation. In this\ncontext, “Single” refers to using only the temporal block in the Stem module, while “Dual” refers to using both the temporal and channel\nblocks in the Stem module. (b) Performance comparison with different kernel size combinations for TFA-Bi-S6 and CFA-Bi-S6 blocks\n(KTFA and KCFA ) and different aggregation methods (Aggregate) using the Dual structure. (c) Performance comparison with varying\niterations (r) of applying residual connections in the recurrent Dual S6 structure in the Stem module and different numbers of blocks in the\nStem module (Bs), with both Dual structure and Feature Aggregation applied. All results are from the THUMOS-14 dataset.\nThe results show that using multiple kernel sizes for\nConv1D layers in both TFA-Bi-S6 and CFA-Bi-S6 blocks\nimproves performance, demonstrating the benefit of captur-\ning a diverse range of local contexts at multiple scales for\nTAL. However, configurations with four or more kernel sizes\nper block resulted in decreased performance, likely due to\noverfitting, as the increased model complexity led to learning\nnoise and less relevant patterns.\nThe absence of Conv1D layers led to reduced perfor-\nmance, underscoring the importance of capturing temporal\nand channel-wise local context through these layers. Further-\nmore, the Sum aggregation method outperformed the Concat\nmethod, indicating that summing feature maps effectively\nintegrates information across different scales without adding\nexcessive complexity.\nRecurrent mechanism iterations We examined the im-\npact of varying the number of iterations r in the recurrent\nmechanism, along with the Dual structure and Feature Ag-\ngregation. This analysis, detailed in Table 2c, assesses how\niterative refinement of temporal dependencies affects model\nperformance compared to increasing the number of Stem\nblocks (Bs).\nThe results show that increasing the number of recur-\nrent iterations r generally improves performance up to a\ncertain point. Beyond this point, however, additional itera-\ntions resulted in a slight performance drop, likely due to an\nimbalance in temporal dependency. This suggests that there\nis an optimal number of iterations after which the benefits\nbegin to diminish. In contrast, increasing the number of\nStem blocks (Bs) while keeping r fixed at 1 led to a decrease\nin performance, indicating that simply adding more Stem\nblocks is not effective for improving TAL.\nThis comparison shows that adopting a recurrent ap-\nproach, with Bs set to 1 and increasing r, is more efficient\nand effective than stacking additional blocks. The recurrent\nmechanism improves temporal precision and long-range de-\npendency modeling while optimizing memory usage, crucial\nfor accurately understanding extended actions in video se-\nquences and boosting performance, making it a practical\nstrategy for TAL tasks using the S6-based model.\n5. Conclusion\nIn this paper, we introduced a novel architecture leverag-\ning S6 to provide effective solutions for TAL tasks based\non insights from previous studies. By integrating the Fea-\nture Aggregated Bi-S6 block and the Dual Bi-S6 structure,\nour approach captures multi-scale temporal and channel-\nwise dependencies. The recurrent mechanism further refines\ntemporal context modeling, enhancing performance with-\nout increasing parameter complexity. Consequently, our ap-\nproach achieves state-of-the-art results on various benchmark\ndatasets, with average mAP scores of 74.2% on THUMOS-\n14, 42.9% on ActivityNet, 29.6% on FineAction, and 45.8%\non HACS. Additionally, ablation studies confirm the advan-\ntages of our design, demonstrating that the Dual structure\nin the Stem module outperforms the Single structure, the\nrecurrent mechanism is more effective than merely stacking\nadditional blocks, and Temporal Aggregation further boosts\nperformance. These findings pave the way for future re-\nsearch to further explore the potential of state space models\nin TAL tasks.\n8\nReferences\n[1] Humam Alwassel, Fabian Caba Heilbron, and Bernard\nGhanem. Action search: Spotting actions in videos and its\napplication to temporal action localization. In Proceedings\nof the European Conference on Computer Vision (ECCV) ,\nSeptember 2018. 2, 7\n[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun,\nMario Luˇci´c, and Cordelia Schmid. Vivit: A video vision\ntransformer. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pages 6836–6846,\nOctober 2021. 1\n[3] Anurag Bagchi, Jazib Mahmood, Dolton Fernandes, and\nRavi Kiran Sarvadevabhatla. Hear me out: Fusional ap-\nproaches for audio augmented temporal action localization.\narXiv preprint arXiv:2106.14118, 2021. 7\n[4] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and\nJuan Carlos Niebles. Activitynet: A large-scale video bench-\nmark for human activity understanding. In Proceedings of the\nieee conference on computer vision and pattern recognition,\npages 961–970, 2015. 1, 2, 7\n[5] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold,\nDavid A Ross, Jia Deng, and Rahul Sukthankar. Rethinking\nthe faster r-cnn architecture for temporal action localization.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 1130–1139, 2018. 7\n[6] Chen Chen, Chao-Han Huck Yang, Kai Li, Yuchen Hu,\nPin-Jui Ku, and Eng Siong Chng. A neural state-space\nmodel approach to efficient speech separation. arXiv preprint\narXiv:2305.16932, 2023. 6\n[7] Guo Chen, Yifei Huang, Jilan Xu, Baoqi Pei, Zhe Chen,\nZhiqi Li, Jiahao Wang, Kunchang Li, Tong Lu, and Limin\nWang. Video mamba suite: State space model as a ver-\nsatile alternative for video understanding. arXiv preprint\narXiv:2403.09626, 2024. 1, 4, 5, 7\n[8] Guo Chen, Yin-Dong Zheng, Limin Wang, and Tong Lu.\nDcan: improving temporal action detection via dual context\naggregation. In Proceedings of the AAAI conference on artifi-\ncial intelligence, volume 36, pages 248–257, 2022. 7\n[9] Feng Cheng and Gedas Bertasius. Tallformer: Temporal\naction localization with a long-memory transformer. In\nEuropean Conference on Computer Vision, pages 503–521.\nSpringer, 2022. 7\n[10] Felix A Gers, J ¨urgen Schmidhuber, and Fred Cummins.\nLearning to forget: Continual prediction with lstm. Neural\ncomputation, 12(10):2451–2471, 2000. 1, 2\n[11] Albert Gu and Tri Dao. Mamba: Linear-time sequence\nmodeling with selective state spaces. arXiv preprint\narXiv:2312.00752, 2023. 1\n[12] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christo-\npher R´e. Hippo: Recurrent memory with optimal polynomial\nprojections. Advances in neural information processing sys-\ntems, 33:1474–1487, 2020. 4\n[13] Albert Gu, Karan Goel, and Christopher R´e. Efficiently mod-\neling long sequences with structured state spaces. arXiv\npreprint arXiv:2111.00396, 2021. 1\n[14] Xiaolin Hu, Kai Li, Weiyi Zhang, Yi Luo, Jean-Marie\nLemercier, and Timo Gerkmann. Speech separation using\nan asynchronous fully recurrent convolutional neural net-\nwork. Advances in Neural Information Processing Systems,\n34:22509–22522, 2021. 6\n[15] Haroon Idrees, Amir R Zamir, Yu-Gang Jiang, Alex Gorban,\nIvan Laptev, Rahul Sukthankar, and Mubarak Shah. The\nthumos challenge on action recognition for videos “in the\nwild”. Computer Vision and Image Understanding, 155:1–23,\n2017. 1, 2, 7\n[16] Hassan Keshvarikhojasteh, Hoda Mohammadzade, and\nHamid Behroozi. Temporal action localization using gated re-\ncurrent units. The Visual Computer, 39(7):2823–2834, 2023.\n2\n[17] Thomas N Kipf and Max Welling. Semi-supervised classi-\nfication with graph convolutional networks. arXiv preprint\narXiv:1609.02907, 2016. 1, 2\n[18] Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao\nLuo, Zhipeng Cui, Chengjie Wang, Jilin Li, Feiyue Huang,\nand Rongrong Ji. Fast learning of temporal action proposal\nvia dense boundary generator. In Proceedings of the AAAI\nconference on artificial intelligence, volume 34, pages 11499–\n11506, 2020. 7\n[19] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and\nMing Yang. Bsn: Boundary sensitive network for temporal\naction proposal generation. In Proceedings of the European\nconference on computer vision (ECCV), pages 3–19, 2018. 7\n[20] Qinying Liu and Zilei Wang. Progressive boundary refine-\nment network for temporal action detection. In Proceedings\nof the AAAI conference on artificial intelligence, volume 34,\npages 11612–11619, 2020. 2, 7\n[21] Shuming Liu, Chen-Lin Zhang, Chen Zhao, and Bernard\nGhanem. End-to-end temporal action detection with 1b\nparameters across 1000 frames. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18591–18601, 2024. 7\n[22] Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Shiwei\nZhang, Song Bai, and Xiang Bai. End-to-end temporal action\ndetection with transformer. IEEE Transactions on Image\nProcessing, 31:5427–5441, 2022. 7\n[23] Yi Liu, Limin Wang, Yali Wang, Xiao Ma, and Yu Qiao.\nFineaction: A fine-grained video dataset for temporal ac-\ntion localization. IEEE transactions on image processing ,\n31:6937–6950, 2022. 1, 2, 7\n[24] Fuchen Long, Ting Yao, Zhaofan Qiu, Xinmei Tian, Jiebo\nLuo, and Tao Mei. Gaussian temporal awareness networks\nfor action localization. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages\n344–353, 2019. 2\n[25] Dan Oneata, Jakob Verbeek, and Cordelia Schmid. The lear\nsubmission at thumos 2014. 2014. 2\n[26] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On\nthe difficulty of training recurrent neural networks. In Inter-\nnational conference on machine learning, pages 1310–1318.\nPmlr, 2013. 1, 2\n[27] Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang,\nWei Wu, Xiang Wang, Yu Qiao, Junjie Yan, Changxin Gao,\nand Nong Sang. Temporal context aggregation network for\ntemporal action proposal refinement. In Proceedings of the\n9\nIEEE/CVF conference on computer vision and pattern recog-\nnition, pages 485–494, 2021. 7\n[28] Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, and\nDacheng Tao. Tridet: Temporal action detection with relative\nboundary modeling. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18857–18866, 2023. 7\n[29] Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki\nMiyazawa, and Shih-Fu Chang. Cdc: Convolutional-de-\nconvolutional networks for precise temporal action local-\nization in untrimmed videos. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n5734–5743, 2017. 2, 7\n[30] Yeongtaek Song and Incheol Kim. Deepact: a deep neural\nnetwork model for activity detection in untrimmed videos.\nJournal of Information Processing Systems, 14(1):150–161,\n2018. 7\n[31] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,\nand Manohar Paluri. Learning spatiotemporal features with\n3d convolutional networks. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 4489–4497,\n2015. 1, 2\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 1, 2\n[33] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan\nHe, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling\nvideo masked autoencoders with dual masking. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 14549–14560, 2023. 7\n[34] Qiang Wang, Yanhao Zhang, Yun Zheng, and Pan Pan. Rcl:\nRecurrent continuous localization for temporal action detec-\ntion. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition , pages 13566–13575,\n2022. 2, 7\n[35] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He,\nGuo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang,\net al. Internvideo2: Scaling video foundation models for multi-\nmodal video understanding. arXiv preprint arXiv:2403.15377,\n2024. 4\n[36] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and\nBernard Ghanem. G-tad: Sub-graph localization for temporal\naction detection. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 10156–\n10165, 2020. 2, 7\n[37] Ke Yang, Peng Qiao, Dongsheng Li, Shaohe Lv, and Yong\nDou. Exploring temporal preservation networks for precise\ntemporal action localization. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 32, 2018. 2\n[38] Ke Yang, Xiaolong Shen, Peng Qiao, Shijie Li, Dongsheng\nLi, and Yong Dou. Exploring frame segmentation networks\nfor temporal action localization. Journal of Visual Communi-\ncation and Image Representation, 61:296–302, 2019. 2\n[39] Le Yang, Ziwei Zheng, Yizeng Han, Hao Cheng, Shiji Song,\nGao Huang, and Fan Li. Dyfadet: Dynamic feature ag-\ngregation for temporal action detection. arXiv preprint\narXiv:2407.03197, 2024. 7\n[40] Jun Yuan, Bingbing Ni, Xiaokang Yang, and Ashraf A. Kas-\nsim. Temporal action localization with pyramid of score\ndistribution features. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June\n2016. 2\n[41] Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin\nZhao, Junzhou Huang, and Chuang Gan. Graph convolutional\nnetworks for temporal action localization. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 7094–7103, 2019. 2\n[42] Chen-Lin Zhang, Jianxin Wu, and Yin Li. Actionformer: Lo-\ncalizing moments of actions with transformers. In European\nConference on Computer Vision, pages 492–510. Springer,\n2022. 1, 3, 4, 7\n[43] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and\nZhicheng Yan. Hacs: Human action clips and segments\ndataset for recognition and temporal localization. In Proceed-\nings of the IEEE/CVF International Conference on Computer\nVision, pages 8668–8678, 2019. 1, 2, 7\n[44] Yibo Zhao, Hua Zhang, Zan Gao, Weili Guan, Jie Nie, Anan\nLiu, Meng Wang, and Shengyong Chen. A temporal-aware\nrelation and attention network for temporal action localiza-\ntion. IEEE Transactions on Image Processing, 31:4746–4760,\n2022. 3\n[45] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,\nWenyu Liu, and Xinggang Wang. Vision mamba: Efficient\nvisual representation learning with bidirectional state space\nmodel. arXiv preprint arXiv:2401.09417, 2024. 1\n10\nSampled video\n㏝3, 16, 224, 224㏞\nW㚍224\nH㚍224\nPer frames\n㏙x16㏚\nProjection\n㏙588,3200㏚\nT㚍256\nC㚍588\nPatchify\n㏙1,14,14㏚\nFlatten\n㏙3,14,14㏚\n14\n14\nPatch embed.\n㏝16, 256,3200㏞\nT㚍256\nC㚍3200\nPosition embed.\n㏝4096, 3200㏞\nT㚍4096\nC㚍3200\nFlatten\n㏙16,256㏚\n3D sine㎿\ncosine init\nMHSA㏙3200㏚\nMLP㏙12800㏚\nMasking\n㏙1㚊ρ㏚\n㏙          ㏚\nx48\nContextual embed.\n㏝4096㏙1㎿ρ㏚, 3200㏞\nT㚍4096㏙1㚊ρ㏚\nC㚍3200\nEncoded feat.\n㏝1, 3200㏞\nL㚍1\nC㚍3200\nMHSA\n㏙3200㏚\nMean pool\n㏙4096㏙1㚊ρ㏚㏚\nColor mapping: ㏙Temporal, Token,Channel, Spatial㏚\nFigure 4. Process of extracting spatiotemporal features using the pretrained video encoder. The encoder processes 30fps RGB video\nframes, groups them into 16-frame clips, and applies patchification, positional embedding, and multi-head self-attention to produce encoded\nfeature vectors.\nAppendix / supplemental material\nThis appendix is organized as follows:\n• Appendix A provides a detailed explanation of the process in-\nvolved in extracting spatiotemporal features from video clips\nusing a pretrained video encoder. It covers the technical steps\nand methodology used, including patchification, positional\nembedding, and multi-head self-attention mechanisms.\n• Appendix B describes the benchmark datasets used for eval-\nuating Temporal Action Localization (TAL) methods. It in-\ncludes detailed descriptions of datasets such as THUMOS-14,\nActivityNet, FineAction, and HACS, along with their key\ncharacteristics and evaluation metrics.\nA. Example of Pretrained Video Encoder\nExtracting Spatiotemporal Features from\nEach Clip\nTo understand the design intention of the Dual Bi-S6\nstructure, it is crucial to explain how the Pretrained video\nencoder extracts spatiotemporal features from each clip, clar-\nifying the information contained in the sequence.\nFor instance, when processing the THUMOS dataset us-\ning the same Pretrained video encoder as ActionMamba,\nwe start with RGB videos at 30 fps and a spatial resolu-\ntion of 224x224. We segment 16 frames into a single clip,\nsetting a frame interval of 4 (stride=4) between clips, yield-\ning multiple clips from each video, each clip measuring\n[3, 16, 224, 224]. Within each frame, patches of size 14x14\nare generated, producing 256 patch tokens per frame. Each\npatch token, representing spatial information and RGB chan-\nnels, is flattened to a dimension of [256, 588]. These spatial\ntokens are projected to a channel size of 3200, forming\npatch embedding tokens with dimensions [16, 256, 3200].\nAdding 3D sine-cosine positional embeddings to both the\npatch and frame dimensions, and then flattening these dimen-\nsions, results in position-embedded tokens with dimensions\n[4096, 3200]. Next, a proportion ρ of tokens is masked,\nand the channels are projected to 3200, followed by multi-\nhead self-attention and a feedforward layer with a hidden\nchannel size of 12800, repeated 48 times to incorporate spa-\ntiotemporal context, resulting in contextual embedded tokens\nwith dimensions [4096(1 − ρ), 3200]. Finally, multi-head\nself-attention and mean pooling are applied along the token\ndimension to produce an encoded feature vector with dimen-\nsions [1, 3200] for each clip. This process is repeated for all\nclips, stacking the encoded feature vectors sequentially over\ntime to generate the sequence data, excluding the first and\nlast two clips, which may lack video information, as shown\nin Figure 4.\nB. Benchmark Datasets for Temporal Action\nLocalization\nTo provide a comprehensive evaluation of TAL methods,\nwe employ several benchmark datasets that vary in size, com-\nplexity, and focus. Here, we describe the key characteristics\nand evaluation metrics of the datasets utilized in this study:\nTHUMOS-14: This large-scale dataset is specifically\ndesigned for video action recognition and includes detailed\ntemporal frame index annotations for 20 action classes. The\nprimary evaluation metric for THUMOS-14 is mean Average\nPrecision (mAP), which is calculated at various temporal\nIntersection over Union (tIoU) thresholds of 0.3, 0.4, 0.5,\n0.6, and 0.7. This allows for a thorough assessment of the\nmodel’s performance across different levels of temporal pre-\ncision.\nActivityNet: Significantly larger and more complex than\nTHUMOS-14, ActivityNet comprises approximately 20,000\nvideos spanning 200 action classes. The diverse range of\nclasses in ActivityNet presents a more challenging scenario\nfor TAL models. The mAP evaluation metric is also em-\nployed here, with tIoU thresholds set at 0.5, 0.75, and 0.95,\nproviding a stringent test for action localization performance.\nFineAction: Consisting of around 16,000 videos fea-\nturing 106 action classes, FineAction emphasizes everyday\nactivities and sports. The high variety of classes relative to\n11\nits size makes it a particularly challenging dataset. Evalua-\ntion methods are akin to those used for ActivityNet, utilizing\nmAP scores at multiple tIoU thresholds.\nHACS (Human Action Clips and Segments): This ex-\ntensive dataset includes approximately 50,000 videos cover-\ning 200 action classes, primarily capturing various actions\nfrom everyday life. Evaluation of the HACS dataset is con-\nducted using the same methodology as ActivityNet, ensuring\na consistent benchmark for comparing TAL model perfor-\nmance across different datasets.\nThese detailed descriptions of the datasets underscore the\ndiverse and comprehensive nature of the benchmarks used\nin this study, providing a robust framework for evaluating\nthe effectiveness of TAL methods.\n12"
  },
  {
    "source": "2307.08629v1.pdf",
    "content": "Deficiency-Aware Masked Transformer for Video Inpainting\nYongsheng Yu1 Heng Fan2 Libo Zhang1†\n1Institute of Software, Chinese Academy of Sciences\n2Department of Computer Science and Engineering, University of North Texas\nyonsheng.yu@gmail.com; heng.fan@unt.edu; libo@iscas.ac.cn\n(c) Adapt to one-shot object removal\nGive a text ‘the person’\n(b)  In-the-wild video inpainting(a) Deficiency case with large masks\nFGT\nE2FGVI\n Ours\nFigure 1: (a) Video inpainting result and a comparison with state-of-the-art models for handling large mask. (b) Generalizing\nto anime video clips that is outside the training dataset. (c) The proposed model easily adapts to one-shot object removal by\naccepting text or stroke input only. Best viewed in color and by zooming in for all figures throughout the paper.\nAbstract\nRecent video inpainting methods have made remarkable\nprogress by utilizing explicit guidance, such as optical flow,\nto propagate cross-frame pixels. However, there are cases\nwhere cross-frame recurrence of the masked video is not\navailable, resulting in a deficiency. In such situation, in-\nstead of borrowing pixels from other frames, the focus of the\nmodel shifts towards addressing the inverse problem. In this\npaper, we introduce a dual-modality-compatible inpainting\nframework called Deficiency-aware Masked Transformer\n(DMT), which offers three key advantages. Firstly, we pre-\ntrain a image inpainting model DMT img serve as a prior\nfor distilling the video model DMT vid, thereby benefiting\nthe hallucination of deficiency cases. Secondly, the self-\nattention module selectively incorporates spatiotemporal\ntokens to accelerate inference and remove noise signals.\nThirdly, a simple yet effective Receptive Field Contextu-\nalizer is integrated into DMT, further improving perfor-\nmance. Extensive experiments conducted on YouTube-VOS\nand DAVIS datasets demonstrate that DMT vid significantly\noutperforms previous solutions. The code and video demon-\nstrations can be found at github.com/yeates/DMT.\n1. Introduction\nVideo inpainting, aiming to reconstruct the corrupted re-\ngions with coherent and consistent contents in videos, is an\nimportant problem in computer vision and has many appli-\ncations, including object removal [18, 7, 23], video restora-\ntion [20], etc. Despite considerable progress in recent years,\naccurate video inpainting remains an open problem because\nof the complex and challenging scenarios in videos.\nOne major challenge in video inpainting is the deficiency\ncase [28], where the masked content is absent through-\nout the whole video. In such a case, the inpainting mod-\nels often degrade into an inverse problem. This issue be-\ncomes more severe when dealing with large mask, as ex-\nisting video inpainting models may fail and produce arti-\nfacts (see Fig. 1(a)). In contrast, generative image inpaint-\ning [56, 34, 51, 16] offers a high-fidelity solution to address\nthe deficiency case by leveraging the context of unmasked\nimage regions. Given the close interrelation between im-\nage and video inpainting, a natural question arises: Can we\nleverage image inpainting techniques to enhance video in-\npainting in deficiency cases? In simpler terms, Can we pre-\ntrain image inpainting models to improve video inpainting?\nWe answer yes. However, it is non-trivial. The domain\ngap between video and image inpainting tasks arises from\ntheir different objectives. Video models attempt to borrow\naccurate pixel information across frames, whereas image\nmodels are learnt to fill holes based on the unmasked re-\ngions. Additionally, extending a 2D module to handle video\ninpainting in a simplistic manner, such as utilizing 3D con-\nvolutions [7, 38] and vanilla self-attention [53, 25] in the\ngenerator, lacks task-specific exploration.\nOur solution. In order to conquer this barrier and wipe out\nthe design and domain gap between the image and the video\narXiv:2307.08629v1  [cs.CV]  17 Jul 2023\ninpainting, we introduce a novel dual-modality-compatible\ninpainting framework that is flawlessly compatible with im-\nage and video inputs. To leverage the hallucination ability\nof image inpainting model, we empirically use pretrained\nDMTimg as a prior to preserves knowledge in dealing with\ndeficiency problems. Additionally, we train DMT vid from\nscratch and apply a continual learning loss, which transfers\nknowledge from the DMT img prior and trades off the plas-\nticity and stability [27] when learning from new video data.\nFig. 2 shows our idea.\nAt the core of our framework is the Deficiency-aware\nMasked Transformer, which models long-range dependen-\ncies across the spatial or temporal axis for images se-\nquences. Unlike the vanilla transformer, DMT incorporates\na Token Selection mechanism to drop invalid tokens where\nall internal pixels fall within the mask region. To iteratively\nactivate invalid tokens, we introduce a heuristic Mask Ac-\ntivation algorithm for self-attention and convolution oper-\nators. Furthermore, to benefit from a high receptive field\nfor inpainting, we integrate a simple yet effective network\ncalled Receptive Field Contextualizer (RFC) into DMT.\nOur approach offers several advantages compared to pre-\nvious video inpainting models. First, it seamlessly inherits\nthe hallucination ability from the pre-trained image inpaint-\ning model, DMTimg, enabling it to generate content for defi-\ncienct region. Second, the Token Selection mechanism and\nMask Activation algorithm reduce computational cost, as\nthe complexity of the transformer is proportional to the in-\nput mask ratio, while slightly improving performance by re-\nmoving noise tokens. Third, the RFC expands the receptive\nfield of DMT and learns high-frequency signals [30], com-\nbining the strengths of both transformers and convolutional\nnetworks for better results.\nTo validate our proposed framework, we conduct exten-\nsive experiments on YouTube-VOS [43] and DA VIS [32].\nThe results demonstrate that our method outperforms state-\nof-the-art video inpainting approaches, setting new records.\nSpecifically, compared to the SOTA method [23] in terms\nof PSNR, our method achieves an increase of 0.81 dB on\nDA VIS and 0.56 dB on YouTube-VOS, respectively. More-\nover, our model generalizes well to in-the-wild video in-\npainting scenarios and can be easily adapted to a one-shot\nobject removal pipeline without frame-wise masks input\n(see Fig.1(b) and (c)).\n2. Related works\nImage Inpainting. Classical image inpainting [4] employs\nheuristic methods to search for and propagate information\nfrom reference regions to fill in missing pixels. Follow-\ning the seminal inpainting work [31] using deep learn-\ning, many extensions have been introduced for improve-\nments, such as multi-step sampling [35, 50], auxiliary prior\nguidance [49, 50], generator-discriminators [16, 51], mask-\nImage \n Video \n Video \nImage \n:Pre-Training Stage:Identical Networks:Continual learning\n(a) (b) (c) \nFigure 2: Illustration of our idea. Image (a) shows video\nand image models are independent of each other, which is\nthe most common way currently; image (b) shows an im-\nage inpainting model is used for video inpainting task [44].\nImage (c) shows the bridging of image and video inpainting\nby an efficient pre-training and continual learning strategy.\naware network designs [15, 24, 47, 48], etc. More re-\ncently, inspired by the power of Transformer [37, 11], it\nhas been leveraged for inpainting by modeling the long-\nrange dependencies in images and exhibited promising per-\nformance [21, 49, 53]. Despite this, the vanilla Transformer\nmight suffer from high computational complexity, limiting\nits efficiency. Similar to these approaches, we also utilize\nTransformer for capturing long-range dependency. But dif-\nferently, our framework employs a Token Selection mecha-\nnism and Mask Activation algorithm to drop and activate\ninvalid tokens, improving efficiency and boosting perfor-\nmance.\nVideo Inpainting. Benefiting from deep learning, video in-\npainting [18] has witnessed great progress in recent years.\nCompared with image inpainting, video inpainting needs to\ndeal with the additional time dimension, which introduces\nchallenges such as camera movement, temporal consistency\npreserving, and reference between frames. As a result, it is\nhard to apply image inpainting models for video inpainting\nin generating temporally coherent content. In order to adapt\nthe video domain, great efforts have been devoted to design-\ning 3D convolutions [7, 38], optical flow [44, 55, 17, 54]\nand temporal transformers [53, 25, 54, 23, 5], showing ex-\ncellent performance. Among these works, E 2FGVI [23],\nFGT [54], and DeViT [5] are the most recent leading mod-\nels. E 2FGVI [23] proposes end-to-end flow-guided feature\npropagation to enhance its temporal focal transformer [46].\nFGT [54] integrates the flow completion network into the\ntransformer to decouple self-attention along temporal and\nspatial perspectives. DeViT [5] designs spatial and tempo-\nral branches of transformers with patch-wise alignment and\nmatching in order to adapt various motion scenarios. Dif-\nferent than these approaches, we introduce Receptive Field\nContextualizer into DMT that enjoy strong strength of both\ntransformer and CNN.\nMasked Visual Modeling. Mask modeling [10] comes\nfrom natural language processing. The work of Masked Au-\nOverlapped Tokenization\nDeficiency-aware Masked \nTransformer (DMT)\nInput Mask\nToken to Feature map\nMask Activation\nInput Video\nInput Image\nOne-frame Video\nToken Selection\n𝑭𝒍 𝟏\n𝒎𝒍 𝟏\n𝒁𝒍 𝟏\n𝒁𝒍\n𝒎𝒍\n𝒁𝑳\n𝒎𝒍 𝟏\n: Invalid Position\n: Feature map\n: Valid Position\n: Slide window\n: SpatioTemporal Token\n: Convolutional Flow\nℒ𝑚𝑖𝑔\nFigure 3: Our proposed inpainting framework consists of the following components: DMT bottlenecks, token selection oper-\nator, mask activation strategy, migration regularization, and 2D-convolutional encoder and decoder (shown as convolutional\nflow in the figure). In each successive DMT layer, masks are dynamically hallucinated until they vanish.\ntoencoders [13] (MAEs) proposes to learn representations\nby recreating the original images from patch-form masked\nimages. Several studies [2, 40, 3] observe that this self-\nsupervised pre-training is beneficial for improving trans-\nformers. Our work shares similar spirits with these variants\nof MAEs that only encodes unmasked tokens for long-range\ndependency modeling. The benefit of employing a masked\ntransformer is to lessen the computational load, especially\nfor large masks.\nPre-training in Vision. Pre-training is popular in computer\nvision to transfer knowledge between tasks. For example,\nthe classification networks pre-trained on ImageNet [9] are\noften employed for other various tasks such as object detec-\ntion [33], segmentation [26], etc. In these tasks, the back-\nbone of the pre-trained will be borrowed for initializing the\nmodel in the new task. Our method shares a similar idea but\nis different. First, we train a new video model from scratch,\ninstead of finetune pre-trained backbone. In addition, we\nintroduce a continual learning loss to trade off the plasticity\nand stability [27].\n3. The Proposed Methodology\nGiven a masked sequence {Xt ∈ R3×H×W |t ∈ [1, T]}\nwith sequence length T and corresponding frame-wise bi-\nnary masks {Mt ∈ R1×H×W |t ∈ [1, T]}, we aim to hal-\nlucinate visually appealing and semantically plausible con-\ntents in space and time dimensions for missing regions. The\ntask degenerates to image inpainting while the time step\nof the frame sequence is set to 1. The proposed inpaint-\ning framework, which infers context features across spa-\ntial and temporal adaptively and reconstructs them to output\n{ˆYt ∈ R3×h×w|t ∈ [1, T]}. Our approach works flawlessly\nwith image and video inputs, and we ignore time step t be-\nlow for simplicity’s sake.\n3.1. Overall Architecture\nThe pipeline of our proposed approach is illustrated in\nFigure 3. It consists of a 2D-convolutional encoder-decoder\nand a Deficiency-aware Masked Transformer. The architec-\nture operates as follows: The encoder encodes the masked\ninput, reducing its size by a factor of 4 and producing\nC channel convolutional feature maps X↓ ∈ RC×H\n4 ×W\n4 .\nNext, we tokenize the X↓ feature map using a linear net-\nwork that maps the feature dimension C to d. Tokens from\nall frames are merged into the same dimension, resulting\nin spatiotemporal tokens represented by F ∈ RN×d. The\nDeficiency-aware Masked Transformer (DMT) acts as the\nbottleneck blocks with L layers to generate missing content\nusing valid-only spatiotemporal tokens Zl, l∈ [0, L− 1].\nThis enables the learning of long-range dependencies across\nboth temporal and spatial dimensions. Finally, we inverse-\ntokenize ZL to obtain the feature map and reconstruct com-\npleted video frames ˆY using the decoder.\nToken Selection. In previous transformer-based methods,\nall tokens are considered to have equal importance. How-\never, tokens extracted from the missing region provide in-\nsignificant knowledge and impede computational speed.\nConv2D 1×1Conv2D 1×1Multi-Head AttentionFeed Forward NetworkDepth-wise Conv2D K×KPoint-wise Conv2DConv2D 1×1\nLinear\nLinear\nLinearLinear\nLinear\nLinear\nLinearLinear\nLinear\nLinear\nLinearLinear\nToken-wise\n𝑁’Norm Norm Norm+NormNormGELU+NormSpatial-wise௟\n௟ିଵReceptive Field Contextualizer𝑄௟ିଵ𝐾௟ିଵ𝑉௟ିଵCUCUCUAU\nFigure 4: Illustration of Deficiency-aware Masked Trans-\nformer. Blocks ‘AU’ and ‘CU’ represent Attention mask\nUpdater and Convolution mask Updater modules of mask\nactivation, respectively.\nWe hence drop all tokens within the masked region and only\ninput the valid tokens Z0 into the DMT blocks. We define\nthe Token Selection process as ϕ(·, ·), which yields valid-\nonly tokens:\nZ = ϕ(F, m), N ′ = |ϕ(F, m)|, (1)\nHere, F ∈ RN×d and Z ∈ RN′×d represent the full-amount\ntokens and valid-only tokens, respectively. N′ denotes the\nnumber of tokens after removing the invalid ones. Since\nour method handles free-form masks rather than patch-form\nmasks, we consider an image token as valid when its corre-\nsponding pixels are not fully masked. To indicate the mask-\ning status of each pixel explicitly, the downscaled mask\nm ∈ R1×H\n4 ×W\n4 is sent to the DMT blocks along with to-\nkenized sequence Z. As a result, the DMT blocks learn\nvalid-only tokens instead of full-amount tokens. This re-\nmoval of noise signals from invalid tokens speeds up DMT.\n3.2. Deficiency-aware Masked Transformer\nIn this section, we introduce the Deficiency-aware\nMasked Transformer, which offers a more effective alter-\nnative to the commonly used vanilla vision transformer [11,\n25]. The DMT consists of a masked transformer that specif-\nically handles valid tokens, as well as novel components\nsuch as Mask Activation strategy and Receptive Field Con-\ntextualizer module.\nThe Masked Transformer is borrowed from the vanilla\ntransformer and consists of multi-head self-attention and a\nfeed-forward network. Given the input valid-only tokensZl\nat the l-th stack, where l ∈ [1, L− 1] and L is the number\nof stacked DMT blocks, the front part of a DMT block can\nbe formulated as follows:\nZ′\nl−1 = MSA(LN1(Zl−1)) +Zl−1, (2a)\nZl−1 = FFN(LN2(Z′\nl−1)) +Z′\nl−1, (2b)\nHere, MSA and LN denote the standard multi-head self-\nattention [11] and layer normalization [1], respectively. We\nuse FFN [11] with token-feature alternately warping [52,\n25] to establish connections between embedded tokens.\nMask Activation. Learning with valid-only tokens can al-\nleviate the computational strain of the vanilla transformer,\nespecially when dealing with large-scale masks. However,\nthe dropped tokens cannot be automatically reconstructed\nby the Masked Transformer. Hence, we propose a heuristic\nMask Activation strategy, which ensures the hallucination\nof all invalid tokens at the end of the DMT bottleneck. The\nMask Activation explicitly changes the validity of invalid\ntokens based on a simple rule: self-attention and convo-\nlution operators will reconstruct masked pixels. Thus, we\nsimulate these operators to activate the corresponding to-\nkens.\nSpecifically, we introduce a collection of mask updaters,\nwhich are used for the convolution and self-attention opera-\ntors, respectively. As summarized in Algorithm 1, the mask\nupdater first tokenizes the current mask using a sliding win-\ndow with the same parameters as the operator. Then, it re-\nnormalizes each token P according to the following rule:\nPx,y = 1, iff sum(P) > 0. (3)\nThe tokenized mask P is binary-valued, similar to m,\nwhere 1 represents an unmasked pixel at the location (x, y).\nFinally, the mask map validity is updated by rearranging\nthe tokens into a feature map. The mask updaters simulate\nthe dynamics of hallucination, thereby iteratively activating\ninvalid tokens in the DMT blocks.\nReceptive Field Contextualizer. Tokenizing feature maps\ninto patches can result in the loss of high-frequency details\nand spatial structure in the image. To address this issue, we\npropose a simpler and stronger Receptive Field Contextual-\nizer (RFC). The RFC module reconstructs spatial features,\nextracts high-frequency semantic details, and embeds learn-\nable positions implicitly [42].\nAs shown in Figure 4, the RFC first employs skip con-\nnections to preserve temporal correlation and low-level fea-\nture information. It then utilizes two parallel branches to\nreconstruct spatial information. Within each branch, a1×1\nconvolution integrates features across channels. One branch\napplies Gaussian Error Linear Unit (GELU) to capture fine-\ngrained local details at a smaller scale and enhance non-\nlinear representation capability. The other branch utilizes\ndepthwise separable convolution [8] with a large kernel size\nK × K. This approach enhances the convolutional recep-\ntive field without excessively increasing the number of pa-\nrameters. We integrate the RFC module into the Masked\nTransformer, and the rest of a DMT block is as follows:\nZl = RFC(Zl−1) +Zl−1. (4)\nThe token feature is reformed into a spatial size because\nit was first flattened before being fed into the RFC. Detailed\nanalysis and comparisons with similar high-receptive-field\nFFC modules [36] are provided in Table 2 and 4, highlight-\ning the effectiveness of our approach.\n3.3. Migration Regularization\nTo train a video inpainting model DMT vid, we first pre-\ntrain a DMT img on the YouTube-VOS dataset [43], which\nwe consider as an image dataset. We penalize the image\ninpainting model DMT img using adversarial loss, percep-\ntual loss, andR1 regularization loss, following the approach\nin [21], which preserves the knowledge in coping with defi-\nciency problems.\nBuilding upon the pre-trained DMT img, we proceed to\ntrain DMTvid on the video dataset YouTube-VOS [43] from\nscratch. We supervise the model DMT vid using various\nlosses, such as the reconstruction loss (L1 distance) and the\nadversarial loss, following [23]. To better exploit the knowl-\nedge from the pre-trained DMT img, we introduce a Migra-\ntion Regularization term Lmig to facilitate continual learn-\nAlgorithm 1 Mask Activation\nInput: m ∈ RC×H×W , k, s, p: mask map and sliding window\nparameters including kernel size, stride, and padding\nRequire: intermediate token sequence m ∈ RCk2×N , number of\ntokens N = ⌊H+2p−k\ns ⌋ + 1× ⌊W+2p−k\ns ⌋ + 1, and token index\nn ∈ [0, N− 1]\nOutput: ˆm ∈ RC×H×W : updated mask map\n1: Initialize padded mask map m ← pad(m, p), zero-filled\ntoken sequence m ← zeros like(Ck2, N), and ˆm ←\nzeros like(C, H+ 2p, W+ 2p)\n2: Align Mask Sequence:\n3: for (n, i, j) in enumerate(range(1, H+ 2p − k +\n1, s), range(1, W+ 2p − k + 1, s)) do\n4: Token ← m[:, i: i + k, j: j + k]\n5: m[:, n] ← reshape(Token, Ck2)\n6: end for\n7: Re-normalize Validity:\n8: m ← ones like(m) × (sum(m, axis = 0)> 0) (Eqn. 3)\n9: Update Mask Map:\n10: for (n, i, j) in enumerate(range(1, H+ 2p − k +\n1, s), range(1, W+ 2p − k + 1, s)) do\n11: ˆm[:, i: i + k, j: j + k] +=reshape(m[:, n], C, k, k)\n12: end for\n13: ˆm ← clamp(unpad( ˆm, p), 0, 1)\ning. This regularization term propagates context encoding\nfeatures from the pre-trained model to the video model, and\nit is defined as:\nLmig =\nLX\nl=1\n∥m ⊙\n\u0010\nh(l)\nθ − ReLU(ˆh(l))\n\u0011\n∥2\n2, (5)\nwhere h(l)\nθ and ˆh(l) indicates the l-th layer output fea-\ntures of video model and pre-trained model, respectively.\nAnalysis. The fundamental issue in continual learning is\nbalancing plasticity and stability [27]. Maintaining a copy\nof the pre-trained model weights can compromise the plas-\nticity of the video model, as verified in the experiments (see\nTable 2). In such cases, the model may struggle to adapt to\nthe video domain and become trapped in a local optimum.\nTherefore, we train DMT vid from scratch instead of fine-\ntuning it on DMTimg. The use of ReLU activation in Equa-\ntion 5 helps suppress negative information while retaining\npositive information. Similarly, the Hadamard product of\nthe dynamic mask m used in Equation 5 blocks invalid sig-\nnals in the masked regions. It is worth noting that the mask\nm is iteratively updated using the Mask Activation algo-\nrithm.\n3.4. One-shot Object Removal\nExisting video inpainting research [23, 54, 5] typically\nrelies on frame-wise masks to specify inpainting regions.\nHowever, creating frame-wise masks can be labor-intensive,\nespecially for long-term videos in real-world scenarios.\nSeveral prior studies have explored inpainting videos with-\nout frame-wise masks, opting instead for inpainting guided\nby a single-frame mask [29, 19] or a click point [45]. To\nimprove the ease of application interaction, we introduce a\none-shot object removal pipeline that allows users to pro-\nvide simple text or stroke inputs.\nTable 1: Quantitative comparisons with SOTAs of video in-\npainting methods on free-form masks. ↑ indicates higher\nis better, and ↓ indicates lower is better. The best and\nsecond best results are in bold and underline.\nPSNR↑/SSIM↑/VFID↓\nMethod DA VIS Youtube-VOS\nVINet [18] 28.96 / 0.941 / 0.199 29.20 / 0.943 / 0.072\nDFVI [44] 28.81 / 0.940 / 0.187 29.16 / 0.943 / 0.066\nLGTSM [6] 28.57 / 0.941 / 0.170 29.74 / 0.950 / 0.070\nCAP [20] 30.28 / 0.952 / 0.182 31.58 / 0.961 / 0.071\nFGVC [12] 30.80 / 0.950 / 0.165 29.67 / 0.940 / 0.064\nSTTN [53] 30.67 / 0.956 / 0.149 32.34 / 0.966 / 0.053\nFuseFormer [25] 32.54 / 0.970 / 0.138 33.29 / 0.968 / 0.053\nFGT [54] 33.23 / 0.966 / 0.138 32.25 / 0.960 / 0.055\nE2FGVI [23] 33.01 / 0.972 / 0.116 33.71 / 0.970 / 0.046\nOurs 33.82 / 0.976 / 0.104 34.27 / 0.973 / 0.044\nMasked Frames FuseFormer [25] E 2FGVI [23] FGT [54] Ours\nFigure 5: Qualitative video inpainting results compared with FuseFormer [25], E2FGVI [23], and FGT [54].\nBuilding upon our proposed DMT, we incorporate\nSEEM [59] to handle one-shot input and predict the seg-\nmentation of the selected object. As depicted in Figure 1\n(c), our method removes unwanted objects from the video\nby providing a text prompt that corresponds to the unwanted\nobject. Compared to using a single-frame mask or a click\npoint, providing input through text is a simpler way to se-\nlect the unwanted object. Additionally, we offer stroke-\nbased input, where users can draw a point or brush over\nthe unwanted object in the reference image. For further de-\ntails, please refer to the video demonstration or the provided\ndemo code.\n4. Experiments\nImplementation details. For training the DMT in both the\nimage and video domains, we utilize eight A100 GPUs. For\nexperiments involving quantitative and efficiency compar-\nisons, we use one RTX 3090 GPU. The DMT vid is trained\nusing the Adam optimizer with a batch size of 8 and an ini-\ntial learning rate of 0.0001. The learning rate is halved at it-\nerations 30e4, 40e4, and 45e4. The DMTimg is trained using\nthe Adam optimizer with a batch size of 32 and a learning\nrate of 0.001. It is important to note that video inpainting\nrequires maintaining temporal consistency, handling refer-\nencing between frames, and addressing viewpoint changes,\nwhile image inpainting does not. Due to this significant gap\nbetween the two tasks, it is common to approach video and\nimage tasks separately. Although some methods [44] at-\ntempt to embed image inpainting into video inpainting to\nhandle deficiency cases, the efficiency of such multi-stage\napproaches remains a concern.\nDataset. To evaluate the DMT vid for video inpainting, we\nassess its performance on two datasets: YouTube-VOS [43]\nand DA VIS [32]. YouTube-VOS contains 3,471 videos for\ntraining and 508 videos for testing. Following the approach\nin [25], we use one hundred video clips from DA VIS for\ntraining the model and report the experimental metrics on\nthe remaining 50 video clips.\nEvaluation metric. To comprehensively evaluate the per-\nformance of our model, we utilize three different metrics:\nPSNR, SSIM [41], and VFID [39]. These metrics are com-\nmonly used in previous video inpainting literature [53, 25].\nTo ensure fairness in evaluation, we employ the same pair\nof testing images and masks and use the same video sam-\npling process. In video experiments, we use DMTimg and\nDMTvid both trained on YouTube-VOS to avoid informa-\ntion leakage.\nBaselines. We present quantitative findings under a\nfree-form masks setting [23] on YouTube-VOS [43] and\nDA VIS [32] in Table 1. We compare our method to existing\nvideo inpainting methods such as VINet [18], DFVI [44],\nLGTSM [6], CAP [20], STTN [53], FGVC [12], Fuse-\nFormer [25], E 2FGVI [23], and FGT [54]. E 2FGVI and\nFGT are the SOTAs in video inpainting based on optical\nflow and transformer. For additional settings of masks,\nplease refer to the supplementary video inpainting results.\nMask Activation OutputInput\nFigure 6: Mask activation effect on qualitative results. The\ngreen-line box indicates our mask activation, while the red-\nline box denotes inference without mask activation.\n4.1. Results Analysis\nComparison with state-of-the-art methods . Our ap-\nproach outperforms all previous state-of-the-art models on\nall three quantitative metrics, as illustrated in Table 1.\nThe superior results demonstrate how our method pro-\nduces content that is more faithful to the original frames\n(PSNR, SSIM) with less distortion (VFID), showcasing the\neffectiveness of our approach. Figure 5 provides quali-\ntative comparisons against other transformer-based meth-\nods [25, 23] and flow-guided methods [23, 54]. Notably,\nFuseFormer [25] and E 2FGVI [23] fail to synthesize the\nentire body of the distant camel in the first row, while our\nmethod produces more refined outcomes with less repetitive\nmosaic in the second and third rows. The effectiveness of\nour method in propagating objects, such as the mallard and\nstroller in the second and fourth rows of Figure 8, can also\nbe observed. For additional cases, please refer to the video\ndemonstration.\nFrom image to video . To verify the hypothesis that ex-\nisting image inpainting methods struggle with the video\ndomain, we utilize the state-of-the-art image inpainting\nmethod LAMA [36], which has shown strong performance\nin object removal, to inpaint masked videos. For a fair com-\nTable 2: Comparison with the SOTA of image inpaint-\ning on DA VIS dataset. LAMA [36] indicates a pre-trained\nimage inpainting model. To activate temporal-awareness,\nLAMA¶ denotes utilizing pre-trained weights and fine tun-\ning on the video dataset with flow-guided features [23] and\n3D discriminator loss [53].\nMethod Temporal\nAwareness PSNR↑ SSIM↑ VFID↓\nLAMA [36] % 30.25 0.9560 0.181\nLAMA¶ ! 32.23 0.9672 0.137\nDMTimg % 28.97 0.9475 0.162\nDMTvid ! 33.82 0.9759 0.104\nOurs w/ FFC w/o RFCTarget Frame\nReference Frame\nFigure 7: Attention map visualization for ablation experi-\nments on RFC. “w/ FFC” represents replacing our RFC with\nfast fourier convolution in LAMA [36].\nparison, we introduce a variant called LAMA¶, which fine-\ntunes a pre-trained LAMA model on the video dataset. To\nenable temporal awareness of LAMA¶, we use flow-guided\nfeatures [23] and a 3D discriminator loss [53]. As shown\nin Table 2, LAMA [36] initially outperforms our DMT img\nin terms of PSNR and SSIM in video inpainting. How-\never, LAMA¶, which finetunes the pre-trained image model\nLAMA [36], converges to a local optimum due to weak\nmodel plasticity [27]. In contrast, our DMT vid is trained\nwith Migration Regularization and DMT img from scratch,\nstriking a good balance between plasticity and stability and\nsignificantly outperforming LAMA ¶ in all metrics. This\nverifies the effectiveness of our method in learning an in-\npainting model from image to video. Additionally, the\ncomparison between the two image models, LAMA [36]\nand DMTimg, reveals that LAMA performs better in terms\nof similarity-wise metrics (PSNR and SSIM), while our\nDMTimg performs better in terms of VFID due to its training\nas a generative model [21, 56] to handle large corruptions.\nEfficiency of Deficiency-aware Masked Transformer .\nOur masked transformer effectively reduces computational\ncomplexity by dropping invalid tokens before performing\nmulti-head self-attention. In Table 3, we compare the\ncomputational complexity (MACs), and running speed (La-\nTable 3: Efficiency analysis of unmasked transformer vs.\nmasked transformer. The variant ♠ denotes the transformer\nlayer without dropping invalid tokens. Two window strate-\ngies [46] are employed to accelerate the vanilla transformer.\nThe variant Valid-Only x% represents our masked trans-\nformer layer with an average input mask ratio of x percent.\nMACs Latency↓\nMethod (G) # input frame = 1 # input frame = 8\n♠ 6.04 0.58 ± 0.071 7.83 ± 0.09\n♠ w/ Focal [46] 6.14 1.97 ± 0.133 5.42 ± 0.199\nValid-Only 10% 5.72 0.56 ± 0.068 5.91 ± 0.236\nValid-Only 30% 4.50 0.58 ± 0.161 3.68 ± 0.193\nValid-Only 60% 2.99 0.55 ± 0.048 1.82 ± 0.104\nValid-Only 90% 1.19 0.54 ± 0.052 0.63 ± 0.093\nTable 4: Investigation on the kernel size K of RFC.\nMethod # K PSNR↑ SSIM↑ / % VFID↓\nRFC 31 29.88 94.90 0.207\nRFC 13 30.26↑.16 95.35↑.27 0.188↓.007\nRFC 7 30.06 95.03 0.197\nRFC 3 29.79 94.76 0.210\nRFC 1 29.72 94.65 0.213\nw/ FFC [36] - 30.10 95.08 0.195\nw/o RFC - 29.78 94.77 0.219\ntency) of self-attention with different schemes. We adopt\na focal window attention strategy [46] to accelerate self-\nattention. As shown in Table 3, our masked transformer\nlayer exhibits decreased computational cost and increased\ninference speed as the input mask ratio increases. Our\nproposed module still achieves comparable efficiency com-\npared to the vanilla transformer.\nEvaluation of Receptive Field Contextualizer. To evalu-\nate the effectiveness of our RFC, we conduct ablation ex-\nperiments as shown in Table 4 on the DA VIS dataset and\ntrain the model for 50,000 iterations (10% of the complete\nconfiguration) on YouTube-VOS [43]. We construct vari-\nants with different kernel sizes K and a variant that uses\nFast Fourier Convolution (FFC) [36] as a substitute. FFC is\na similar module with a high receptive field. Table 4 quan-\ntitatively demonstrates that a larger receptive field enhances\nperformance, and our RFC substantially improves PSNR,\nSSIM, and VFID scores. Figure 7 provides a quantitative\ncomparison of our RFC with FFC and no RFC, showcasing\nthe superior performance of our RFC in terms of restoring\nhigh-frequency details and enhancing spatiotemporal mod-\neling.\n4.2. Ablation Study\nIn this section, we conduct an ablation study to gauge the\ncontributions of the proposed components in our framework\nand verify our hypothesis. Table 5 provides an overview\nof the contribution of each component of DMT based on\nexperiments conducted on the DA VIS dataset. Based on the\nanalysis of the PSNR, SSIM, and VFID metrics, our RFC\nand Migration Regularization provide key improvements in\nvideo inpainting performance.\nEffectiveness of Token Selection. Our proposed Token Se-\nlection mechanism, which explicitly discards masked to-\nkens, helps reduce invalid noise and computational com-\nplexity. The results in Table 3 demonstrate the higher infer-\nence efficiency achieved by this mechanism.\nEffectiveness of Mask Activation . We find that DMT\nwithout Mask Activation leads to significant performance\ndegradation, as it becomes challenging to reconstruct in-\nvalid tokens solely relying on the convolutional Decoder.\nTable 5: Exploration of different component in DMT.\nMethod PSNR↑ SSIM↑ / % VFID↓\nOurs 33.82 97.59 0.104\nw/o token selection 33.74 ↓0.08 97.53↓0.05 0.106↑0.002\nw/o mask activation 31.98 ↓1.84 96.41↓1.18 0.142↑0.038\nw/o RFC 33.43 ↓0.39 97.39↓0.2 0.109↑0.005\nw/o Lmig 33.51↓0.31 97.47↓0.12 0.108↑0.004\nFigure 6 visually illustrates the process of mask activation,\nhighlighting the blurred results in the second and fourth\nrows that indicate the ineffectiveness of our method in\nperforming effective spatiotemporal correlation modeling\nwithout mask activation.\nEffectiveness of RFC. Omitting the Receptive Field Con-\ntextualizer from the network training results in a loss of\nhigh-frequency details and positional information, as dis-\ncussed in Section 3.2. The results in Table 4 demonstrate\nthat incorporating our RFC module significantly enhances\nall three novelty scores, indicating its ability to capture\nhigh-frequency signals and learn positions. We empirically\nchoose a large kernel size of K = 13for the RFC.\nEffectiveness of Migration Regularization . By applying\nthe Migration Regularization in Equation 5 to the network,\nwe can enhance video inpainting performance by transfer-\nring the generative prior of a pre-trained image inpainting\nmodel without requiring direct access to the image dataset.\nMoreover, this shows that our Migration Regularization fa-\ncilitates new tasks with old domain knowledge, achieving a\ngood balance between plasticity and stability. Please refer\nto the supplementary material for inpainting comparisons\nwith large corruptions.\n5. Conclusion\nIn conclusion, we have introduced a novel Deficiency-\naware Masked Transformer for video inpainting. Our\nproposed Migration Regularization effectively enables the\nmodel to handle deficiency cases. The components of To-\nken Selection, Mask Activation, and Receptive Field Con-\ntextualizer have all made significant contributions to the\noverall performance improvement. Extensive experiments\nhave demonstrated the superiority of our method over SO-\nTAs in terms of quantitative metrics and visual quality. Our\nDMT exhibits robust generalization to in-the-wild input and\neasily adapts to one-shot object removal task, showcasing\nits potential for various applications in video editing and\nrestoration. To the best of our knowledge, we are the first to\npropose leveraging pre-trained image inpainting models for\nvideo inpainting. We believe that our approach will inspire\nfurther research in connecting image inpainting with video\ninpainting and vice versa.\nReferences\n[1] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization. CoRR, abs/1607.06450, 2016. 4\n[2] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir\nZamir. Multimae: Multi-modal multi-task masked autoen-\ncoders. CoRR, abs/2204.01678, 2022. 3\n[3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBERT pre-training of image transformers. In ICLR, 2022. 3\n[4] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and\nDan B. Goldman. Patchmatch: a randomized correspon-\ndence algorithm for structural image editing. TOG, page 24,\n2009. 2\n[5] Jiayin Cai, Changlin Li, Xin Tao, Chun Yuan, and Yu-Wing\nTai. Devit: Deformed vision transformers in video inpaint-\ning. In MM, pages 779–789. ACM, 2022. 2, 5\n[6] Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Win-\nston H. Hsu. Learnable gated temporal shift module for deep\nvideo inpainting. CoRR, abs/1907.01131, 2019. 5, 6\n[7] Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Winston\nHsu. Free-form video inpainting with 3d gated convolution\nand temporal patchgan. In ICCV, October 2019. 1, 2\n[8] Franc ¸ois Chollet. Xception: Deep learning with depthwise\nseparable convolutions. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n1251–1258, 2017. 5\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 3\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. InNAACL, pages 4171–\n4186. Association for Computational Linguistics, 2019. 2\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021. 2, 4\n[12] Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf.\nFlow-edge guided video completion. In ECCV, volume\n12357, pages 713–729. Springer, 2020. 5, 6\n[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In CVPR, pages 16000–16009, June 2022. 3\n[14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In NeurIPS, pages 6626–6637, 2017. 12\n[15] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa.\nGlobally and locally consistent image completion. TOG,\n36(4):107:1–107:14, 2017. 2\n[16] Jitesh Jain, Yuqian Zhou, Ning Yu, and Humphrey Shi. Keys\nto better image inpainting: Structure and texture go hand in\nhand. CoRR, abs/2208.03382, 2022. 1, 2, 12, 13\n[17] Jaeyeon Kang, Seoung Wug Oh, and Seon Joo Kim. Error\ncompensation framework for flow-guided video inpainting.\nCoRR, abs/2207.10391, 2022. 2\n[18] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So\nKweon. Deep video inpainting. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2019,\nLong Beach, CA, USA, June 16-20, 2019, pages 5792–5801.\nComputer Vision Foundation / IEEE, 2019. 1, 2, 5, 6\n[19] Sangjin Lee, Suhwan Cho, and Sangyoun Lee. One-shot\nvideo inpainting. arXiv preprint arXiv:2302.14362 , 2023.\n5\n[20] Sungho Lee, Seoung Wug Oh, DaeYeun Won, and Seon Joo\nKim. Copy-and-paste networks for deep video inpainting. In\nICCV, pages 4412–4420. IEEE, 2019. 1, 5, 6\n[21] Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, and Jiaya\nJia. MAT: mask-aware transformer for large hole image in-\npainting. In CVPR, pages 10748–10758. IEEE, 2022. 2, 5,\n7\n[22] Xiaoguang Li, Qing Guo, Di Lin, Ping Li, Wei Feng, and\nSong Wang. MISF: multi-level interactive siamese filtering\nfor high-fidelity image inpainting. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2022,\nNew Orleans, LA, USA, June 18-24, 2022, pages 1859–1868.\nIEEE, 2022. 12, 13\n[23] Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and\nMing-Ming Cheng. Towards an end-to-end framework for\nflow-guided video inpainting. InCVPR, pages 17562–17571,\nJune 2022. 1, 2, 5, 6, 7, 11, 12, 13\n[24] Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang,\nAndrew Tao, and Bryan Catanzaro. Image inpainting for ir-\nregular holes using partial convolutions. In ECCV, volume\n11215 of Lecture Notes in Computer Science, pages 89–105.\nSpringer, 2018. 2\n[25] Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei\nLu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hong-\nsheng Li. Fuseformer: Fusing fine-grained information in\ntransformers for video inpainting. In ICCV, pages 14040–\n14049, October 2021. 1, 2, 4, 5, 6, 7, 11, 12, 13\n[26] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In\nCVPR, 2015. 3\n[27] Martial Mermillod, Aur ´elia Bugaiska, and Patrick Bonin.\nThe stability-plasticity dilemma: Investigating the contin-\nuum from catastrophic forgetting to age-limited learning ef-\nfects, 2013. 2, 3, 5, 7\n[28] Hao Ouyang, Tengfei Wang, and Qifeng Chen. Internal\nvideo inpainting by implicit long-range propagation. In2021\nIEEE/CVF International Conference on Computer Vision,\nICCV 2021, Montreal, QC, Canada, October 10-17, 2021 ,\npages 14559–14568. IEEE, 2021. 1\n[29] Hao Ouyang, Tengfei Wang, and Qifeng Chen. Internal\nvideo inpainting by implicit long-range propagation. In\nICCV, pages 14579–14588, October 2021. 5\n[30] Namuk Park and Songkuk Kim. How do vision transformers\nwork? In ICLR, 2022. 2\n[31] Deepak Pathak, Philipp Kr ¨ahenb¨uhl, Jeff Donahue, Trevor\nDarrell, and Alexei A. Efros. Context encoders: Feature\nlearning by inpainting. In CVPR, pages 2536–2544, 2016.\n2\n[32] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc\nVan Gool, Markus Gross, and Alexander Sorkine-Hornung.\nA benchmark dataset and evaluation methodology for video\nobject segmentation. In CVPR, June 2016. 2, 6\n[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. NeurIPS, 2015. 3\n[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10674–\n10685. IEEE, 2022. 1\n[35] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In ICLR, 2021. 2\n[36] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,\nAnastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,\nNaejin Kong, Harshith Goka, Kiwoong Park, and Victor\nLempitsky. Resolution-robust large mask inpainting with\nfourier convolutions. In WACV, pages 3172–3182, 2022. 5,\n7, 8, 12, 13\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NeurIPS, 2017. 2\n[38] Chuan Wang, Haibin Huang, Xiaoguang Han, and Jue Wang.\nVideo inpainting by jointly learning temporal structure and\nspatial details. In AAAI, pages 5232–5239. AAAI Press,\n2019. 1, 2\n[39] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Nikolai\nYakovenko, Andrew Tao, Jan Kautz, and Bryan Catanzaro.\nVideo-to-video synthesis. In NeurIPS, pages 1152–1164,\n2018. 6\n[40] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-\niang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-\nhammed, Saksham Singhal, Subhojit Som, and Furu Wei.\nImage as a foreign language: Beit pretraining for all vision\nand vision-language tasks. CoRR, abs/2208.10442, 2022. 3\n[41] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P.\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. TIP, pages 600–612, 2004. 6\n[42] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing\nconvolutions to vision transformers. In ICCV, pages 22–31,\n2021. 4\n[43] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang,\nDingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen,\nand Thomas Huang. Youtube-vos: Sequence-to-sequence\nvideo object segmentation. In ECCV, September 2018. 2,\n5, 6, 8\n[44] Rui Xu, Xiaoxiao Li, Bolei Zhou, and Chen Change Loy.\nDeep flow-guided video inpainting. In CVPR, June 2019. 2,\n5, 6\n[45] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing\nWang, and Feng Zheng. Track anything: Segment anything\nmeets videos. arXiv preprint arXiv:2304.11968, 2023. 5\n[46] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai,\nBin Xiao, Lu Yuan, and Jianfeng Gao. Focal attention for\nlong-range interactions in vision transformers. In NeurIPS,\npages 30008–30022, 2021. 2, 7, 8\n[47] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S. Huang. Free-form image inpainting with gated\nconvolution. In ICCV, pages 4470–4479, 2019. 2, 12, 13\n[48] Tao Yu, Zongyu Guo, Xin Jin, Shilin Wu, Zhibo Chen, Weip-\ning Li, Zhizheng Zhang, and Sen Liu. Region normalization\nfor image inpainting. In AAAI, pages 12733–12740, 2020. 2\n[49] Yongsheng Yu, Dawei Du, Libo Zhang, and Tiejian Luo.\nUnbiased multi-modality guidance for image inpainting. In\nECCV, pages 668–684. Springer, 2022. 2\n[50] Yongsheng Yu, Hao Wang, Tiejian Luo, Heng Fan, and Libo\nZhang. Magic: Multi-modality guided image completion.\narXiv preprint arXiv:2305.11818, 2023. 2\n[51] Yongsheng Yu, Libo Zhang, Heng Fan, and Tiejian Luo.\nHigh-fidelity image inpainting with gan inversion. In ECCV,\npages 242–258. Springer, 2022. 1, 2\n[52] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, Francis E. H. Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. In ICCV, pages 538–547. IEEE, 2021.\n4\n[53] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning\njoint spatial-temporal transformations for video inpainting.\nIn ECCV, pages 528–543, 2020. 1, 2, 5, 6, 7, 13\n[54] Kaidong Zhang, Jingjing Fu, and Dong Liu. Flow-guided\ntransformer for video inpainting. In ECCV, pages 74–90.\nSpringer, 2022. 2, 5, 6, 7, 11, 12\n[55] Kaidong Zhang, Jingjing Fu, and Dong Liu. Inertia-guided\nflow completion and style fusion for video inpainting. In\nCVPR, pages 5982–5991, June 2022. 2\n[56] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao\nLiang, Eric I-Chao Chang, and Yan Xu. Large scale im-\nage completion via co-modulated generative adversarial net-\nworks. In ICLR, 2021. 1, 7, 12, 13\n[57] Chuanxia Zheng, Tat-Jen Cham, Jianfei Cai, and Dinh Q.\nPhung. Bridging global context interactions for high-fidelity\nimage completion. In CVPR, pages 11502–11512. IEEE,\n2022. 12, 13\n[58] Bolei Zhou, `Agata Lapedriza, Aditya Khosla, Aude Oliva,\nand Antonio Torralba. Places: A 10 million image database\nfor scene recognition. TPAMI, pages 1452–1464, 2018. 12,\n13, 15\n[59] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,\nJianfeng Gao, and Yong Jae Lee. Segment everything every-\nwhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n6\n[60] Xueyan Zou, Linjie Yang, Ding Liu, and Yong Jae Lee. Pro-\ngressive temporal feature alignment network for video in-\npainting. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 16448–\n16457, 2021. 11, 12\nMasked Frames FuseFormer [25] E 2FGVI [23] FGT [54] Ours\nFigure 8: More qualitative results compared to the state-of-the-art video inpainting methods FuseFormer [25], E2FGVI [23],\nand FGT [54]. Best viewed in color and by zooming in for all figures throughout the supplementary.\n6. Appendix\n6.1. Token Selection Complexity Analysis\nDue to reducing the number of tokens before patch em-\nbedding and self-attention, our masked transformer lessens\nthe computational strain quadratically. Formally, the vary-\ning time complexity from the standard transformer to the\nmasked transformer is represented as O(4Nd2 + 2N2d) →\nO(4N′d2 + 2N′2d), where d indicates the feature dimen-\nsion of embedded tokens. And the number of tokens of the\nmasked transformer N′ subjects to:\nN′ < N, N ′ ∼ N · 1 (m)\n|m| , (6)\nwhere 1 (·) stands for the number of elements whose value\nis equal to 1, i.e., masked transformers keep complexity in-\nversely proportional to mask coverage.\n6.2. Evaluation on More Mask Settings\nThe performance of video inpainting varies for random\nmasks and random videos. In addition to the free-form\nmask setting [23] used in the paper, we follow an existing\nmethod [60] to evaluate the video inpainting task in a for-\nmulaic setting. To assess the performance of our approach,\nwe conduct quantitative experiments on the DA VIS dataset\nusing three types of masks - Curve, Stationary, and Object.\nWe present our results and compare them with the state-of-\nthe-art E2FGVI [23], FGT [54], and FuseFormer [25] meth-\nods in Tab. 6.\nTable 6: Comparison in the formulaic mask settings.\nPSNR↑/SSIM↑/FID↓\nCurve Mask Stationary Mask Object Mask\nOurs 35.77/0.991/0.122 37.14 /0.982/0.084 27.39 /0.942/0.252\nE2FGVI 34.58/0.988/0.154 36.52/0.979/0.089 26.97/0.934/0.259\nFuseFormer 33.34/0.984/0.201 35.85/0.977/0.102 26.04/0.920/0.308\nAs demonstrated in Table 6, our method outperforms\nFree form Segmentation Curve [60] Object [60] Stationary [60]\nFigure 9: The examples of mask settings in our experimental evaluation.\nt = 33\n t = 9 t = 69t = 45\nt = 17t = 9 t = 15t = 14\nTarget frame Inpainting results Attention map of reference frames\nFigure 10: Illustration of the cross-frame attention maps learned by DMT for missing regions of the target frame. The color\ngradient, ranging from blue to green to yellow, represents the gradual increase in attention values.\nstate-of-the-art approaches in all formulaic settings w.r.t.\nPSNR, SSIM, and VFID metrics. These results showcase\nthe superior performance of our method across various ex-\nperimental settings and demonstrate its better generalization\ncapability.\nTo illustrate the different masks used in our experiments,\nwe provide examples in Fig. 9. We present the masks from\nright to left, starting with free-form, segmentation, curve,\nobject, and stationary masks. The segmentation mask is a\nmanually crafted segmentation label from the dataset. Dur-\ning the training of our DMT vid model, we used randomly\ngenerated free-form masks. We evaluated the performance\nof our approach using both free-form masks and segmenta-\ntion masks through qualitative comparisons.\n6.3. Long-range Dependencies Modeling\nOur proposed framework leverages the power of DMT to\nmodel long-range dependencies and exploit frame-to-frame\ncorrelations. Fig. 10 shows the attention maps learned by\nDMTvid in the last layer. For instance, when a running\ndog in a video is occluded by a segmentation mask (t=9,\n15, 17), our model fills the missing regions with coherent\nbackground texture. Similarly, in a video with a man on a\nparachute partially corrupted by a random mask, our model\naccurately tracks the flying person across frames.\n6.4. More Qualitative Results\nFigure 8 shows additional qualitative results of our video\ninpainting method and compares it with three state-of-the-\nart methods: E2FGVI [23], FGT [54], and FuseFormer [25].\nOur method outperforms them in several aspects: it pro-\nduces less ghosting artifacts (see row four), less mosaic or\nrepeated patterns (see rows one and six), more complete\nforeground objects (see row two), and can handle object\nmasks of various scales (see rows one, three, five and six).\nOur model demonstrates strong performance in various\nvideo inpainting tasks, as shown in Figure 11 for long-term\nvideo inpainting and Figure 12 for text-guided object re-\nmoval. Notably, our method achieves effective inpainting of\nunwanted objects without visible artifacts and generalizes to\nin-the-wild and one-shot video inpainting tasks without the\nneed for frame-wise masks.\n6.5. By-product Image Inpainting\nIn addition to the video inpainting model, we obtain an\nimage inpainting model DMT img as a by-product. We em-\nploy Places2 [58] as train and test dataset for the image\ninpainting experiments. For evaluating recent image in-\npainting methods, we employ perceptual metrics including\nFrechet Inception Distance (FID) [14], Paired/Unpaired In-\nception Discriminative Score (P/U-IDS) [56].\nWe compare our model with state-of-the-art image in-\npainting methods, including DeepFill [47], Lama [36], Co-\nModGAN [56], MISF [22], TFill [57], and FcF [16]. The\nextensive image completion comparisons are carried out on\nPlaces2 dataset in terms of random masks with small and\nlarge coverage ratios. Mask ratios in the Small and Large\nsettings, respectively, roughly range from 10% to 30% and\nFigure 11: Long-term video inpainting result comprising 1358 frames, inferring with the text prompt ”The female dancer\nwearing a yellow outfit” without using frame-wise masks. For a video demonstration, please refer to our code repository.\n40% to 90%. From Table 7, our approach shows the best\nperformance with respect to FID, P-IDS, and U-IDS quan-\ntitative metrics on Places2 dataset.\nTo further study our DMT img, we provide qualitative\ncomparison results on the DA VIS dataset. The reconstruc-\ntion results of Lama [36], CoModGAN [56], TFill [57], and\nFcF [16] are presented for comparisons. Compared to these\nadvanced image inpainting approaches, our DMT img can\nproduce more accurate textural and structural data and more\nconsistent content in masked areas, as shown in Fig. 13.\nMoreover, in cases of large corruptions, our DMTimg can re-\nconstruct the image content with realistic semantics instead\nof repeating patterns caused by inherent ambiguity.\n6.6. Limit Dicussion\nWe have introduced a novel method with promising re-\nsults for video inpainting. However, our approach does have\ntwo main limitations. Firstly, while our method exhibits ro-\nbustness to random video shapes, one common drawback of\nTransformers-based methods [25, 53, 23] is their high mem-\nory requirements when processing high-resolution content,\nsuch as 1080p videos. The attention matrix product neces-\nsitates significant memory resources, particularly for videos\nTable 7: Quantitative evaluation of image inpainting meth-\nods on Places dataset [58].\nFID↓ P-IDS↑ U-IDS↑\nMethod Small Large Small Large Small Large\nDeepFill [47] 2.598 21.403 0.055 0.006 0.304 0.098\nCoModGAN [56] 1.177 6.680 0.200 0.087 0.396 0.251\nMISF [22] 4.458 17.451 0.003 0.001 0.126 0.064\nLama [36] 0.910 8.331 0.193 0.045 0.402 0.219\nTFill [57] 1.417 14.537 0.126 0.021 0.361 0.159\nFcF [16] 0.910 5.550 0.285 0.132 0.426 0.289\nDMTimg 0.766 4.208 0.300 0.156 0.434 0.314\nwith large temporal scales or resolutions. Secondly, we\nobserved through empirical studies that utilizing a unified\nmodel trained on both image and video datasets hinders\nachieving state-of-the-art performance in both tasks. This\ndiscrepancy arises from the substantial gap between image\nand video inpainting objectives: image inpainting focuses\non generatively filling missing regions, while video inpaint-\ning requires referencing cross-frame information and main-\ntaining temporal coherence. This highlights the ongoing\nchallenge in addressing both image and video inpainting in\nsuch scenarios.\n(JWF\u0001B\u0001UFYU\u0001\u00035IF\u0001QFSTPO\u0001TUBOEJOH\u0001PO\u0001UIF\u0001MFGU\u0001XIP\u0001TVEEFOMZ\u0001TUBSUFE\u0001SVOOJOH\u000f\u0003\n(JWF\u0001B\u0001UFYU\u0001\u0003'FNBMF\u0001EBODFS\u000f\u0003\n(JWF\u0001B\u0001UFYU\u0001\u00031BSBDIVUF\u000f\u0003\n(JWF\u0001B\u0001UFYU\u0001\u00035IF\u0001NBO\u0001PO\u0001UIF\u0001GBS\u0001SJHIU\u000f\u0003\nFigure 12: Results of the text-guided object removal task. For a complete video demonstration, please visit our code reposi-\ntory.\nLama CoModGAN TFill FcF Ours Masked\nFigure 13: Qualitative comparison with the state-of-the-art image inpainting methods on Places2 [58] dataset. Zoom in for a\nbetter view."
  },
  {
    "source": "flash3.pdf",
    "content": "FlashAttention-3:\nFast and Accurate Attention with Asynchrony and Low-precision\nJay Shah∗1\n, Ganesh Bikshandi∗1\n, Ying Zhang\n2\n, Vijay Thakkar\n34\n, Pradeep Ramani\n3\n, and Tri Dao\n56\n1\nColfax Research\n2\nMeta\n3\nNVIDIA\n4\nGeorgia Tech\n5\nPrinceton University\n6\nTogether AI\n{jayhshah,ganesh}@colfax-intl.com, yingz@meta.com, {vithakkar,prraman}@nvidia.com, tri@tridao.me\nJuly 12, 2024\nAbstract\nAttention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language\nmodels and long-context applications.FlashAttention elaborated an approach to speed up attention on GPUs\nthrough minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in\nrecent hardware, withFlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three\nmain techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA\nto (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise\nmatmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware\nsupport for FP8 low-precision. We demonstrate that our method,FlashAttention-3, achieves speedup on\nH100 GPUs by 1.5-2.0\u0002with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching\nclose to 1.2 PFLOPs/s. We validate that FP8FlashAttention-3 achieves 2.6\u0002lower numerical error than a\nbaseline FP8 attention.\n1 Introduction\nFor the Transformer architecture [59], the attention mechanism constitutes the primary computational bottleneck,\nsince computing the self-attention scores of queries and keys has quadratic scaling in the sequence length. Scaling\nattention to longer context will unlock new capabilities (modeling and reasoning over multiple long documents [24,\n43, 50] and ﬁles in large codebases [30, 48]), new modalities (high-resolution images [11], audio [23], video [25]), and\nnew applications (user interaction with long history [53], agent workﬂow with long horizon [62]). This has generated\nsigniﬁcant interest in making attention faster in the long-context regime, including by approximation [14, 27, 56]\nand software optimization ([17, 29, 45]), or even alternative architectures [22, 42, 55].\nIn this work, we build on the work of Dao et al.[17] on developing exact-attention algorithms that integrate\nknowledge of the GPU’s execution model and hardware characteristics into their high-level design. In [17], Dao et\nal. introduced FlashAttention, a novel tiling strategy for parallelizing attention that eliminates intermediate\nreads/writes to slow global memory through fusing all of the attention operations into a single GPU kernel. Dao\n[15] restructured the algorithm asFlashAttention-2 to also parallelize over the sequence length dimension and\nperform the inner loop of the forward pass over blocks of the key and value matrices, thus improving the occupancy\nand distribution of work on the GPU. However, we observe thatFlashAttention-2 nonetheless achieves poor\nutilization on newer GPUs relative to optimized matrix-multiplication (GEMM) kernels, such as 35% vs. 80-90% on\nthe Hopper H100 GPU. Partially, this may be attributed to implementation-level diﬀerences, such as not using\nHopper-speciﬁc instructions in place of Ampere ones when targeting the Tensor Cores. Several work such as\nThunkerKitten [52] and cuDNN 9 [39] has shown that with Hopper-speciﬁc instructions and tile-based abstractions,\none can speedup attention computation and simplify the implementation.\n∗Equal contribution\n1\nMore fundamentally,FlashAttention-2’s algorithm adheres to a simpliﬁed synchronous model and makes\nno explicit use of asynchrony and low-precision in its design. Asynchrony is a result of hardware specialization to\naccelerate the most important operations in a ML workload: speciﬁc hardware units performing matrix multiplication\n(Tensor Cores) or memory loading (Tensor Memory Accelerator – TMA), separate from the rest of the CUDA\ncores performing logic, integer, and ﬂoating point computation. Low precision such as FP8 in Hopper and FP4\nin Blackwell, continuing the trend of FP16 (Pascal in 2017) and BF16 (Ampere in 2020), is a proven technique\nto get double or quadruple throughput for the same power and chip area. We review the capabilities aﬀorded by\nHopper in these directions in §2.2. The technical challenge is to redesignFlashAttention-2 to make use of these\nhardware features: asynchrony requires overlapping computation between matmul and softmax even though one\ndepends on the output of the other, and low-precision requires care to minimize quantization error, especially in the\ncase of outlier features in LLMs [20, 54].\nTo this end, we proposeFlashAttention-3, which contributes and synthesizes three new ideas to further\nimprove performance on newer GPU architectures:1\n1. Producer-Consumer asynchrony:We deﬁne a warp-specialized software pipelining scheme that exploits\nthe asynchronous execution of data movement and Tensor Cores by splitting producers and consumers of data\ninto separate warps, thereby extending the algorithm’s ability to hide memory and instruction issue latencies.\n2. Hiding softmax under asynchronous block-wise GEMMs:We overlap the comparatively low-throughput\nnon-GEMM operations involved in softmax, such as ﬂoating point multiply-add and exponential, with the\nasynchronous WGMMA instructions for GEMM. As part of this, we rework theFlashAttention-2 algorithm\nto circumvent certain sequential dependencies between softmax and the GEMMs. For example, in the 2-stage\nversion of our algorithm, while softmax executes on one block of the scores matrix, WGMMA executes in the\nasynchronous proxy to compute the next block.\n3. Hardware-accelerated low-precision GEMM:We adapt the forward pass algorithm to allow for targeting\nthe FP8 Tensor Cores for GEMM, nearly doubling the measured TFLOPs/s. This requires bridging the\ndiﬀerent layout conformance requirements of WGMMA in terms of how blocks of FP32 accumulator and FP8\noperand matrices are assumed to be laid out in memory. We use the techniques of block quantization and\nincoherent processing to mitigate the loss of accuracy that results from moving to FP8 precision.\nTo validate our method empirically, we benchmarkFlashAttention-3 on the H100 SXM5 GPU over a range\nof parameters and show that (1) FP16 achieves 1.5-2.0\u0002speedup overFlashAttention-2 in the forward pass\n(reaching up to 740 TFLOPs/s) and 1.5-1.75\u0002in the backward pass, (2) FP8 achieves close to 1.2 PFLOPs/s, and\n(3) for large sequence length, FP16 outperforms and FP8 is competitive2 with a state-of-the-art implementation\nof attention from NVIDIA’s cuDNN library. We also validate that FP16FlashAttention-3 yields the same\nnumerical error asFlashAttention-2 and is better than the standard attention implementation as intermediate\nresults (e.g., softmax rescaling) are kept in FP32. Moreover, FP8FlashAttention-3 with block quantization and\nincoherent processing is 2.6\u0002more accurate than standard attention with per-tensor quantization in cases with\noutlier features.\nWe open-sourceFlashAttention-3 with a permissive license3 and plan to integrate it with PyTorch and\nHugging Face libraries to beneﬁt the largest number of researchers and developers.\n2 Background: Multi-Head Attention and GPU Characteristics\n2.1 Multi-Head Attention\nLet QKV 2R𝑁\u0002𝑑 be the query, key and value input sequences associated to a single head, where𝑁 is the sequence\nlength and𝑑 is the head dimension. Then the attention outputO is computed as:\nS = 𝛼QK>2R𝑁\u0002𝑁 P = softmax¹Sº2 R𝑁\u0002𝑁 O = PV 2R𝑁\u0002𝑑\n1We describe our results in the context of NVIDIA’s Hopper architecture. However, our algorithm is operative for any GPU\narchitecture with suﬃciently robust asynchronous execution and low-precision capabilities.\n2More precisely, for head dimension 64FlashAttention-3 FP8 is ahead, while for head dimensions 128 and 256 it is at par for\nthose cases without causal masking and behind with causal masking.\n3FlashAttention-3 is available athttps://github.com/Dao-AILab/flash-attention\n2\nwhere softmax is applied row-wise and one typically sets𝛼= 1\np\n𝑑 as the scaling factor. In practice, we subtract\nrowmax¹Sºfrom S to prevent numerical instability with the exponential function. For multi-head attention (MHA),\neach head has its own set of query, key and value projections, and this computation parallelizes across multiple\nheads and batches to produce the full output tensor.\nNow let 𝜙 be a scalar loss function and letd¹\u0000º= 𝜕𝜙𝜕¹\u0000ºbe notation for the gradient. Given the output\ngradient dO 2R𝑁\u0002𝑑, we computedQ, dK, anddV according to the chain rule as follows:\ndV = P>dO 2R𝑁\u0002𝑑\ndP = dOV>2R𝑁\u0002𝑁\ndS = dsoftmax¹dPº2 R𝑁\u0002𝑁\ndQ = 𝛼dSK 2R𝑁\u0002𝑑\ndK = 𝛼dS>Q 2R𝑁\u0002𝑑\nHere, we have thatd𝑠= ¹diag¹𝑝º\u0000𝑝𝑝>ºd𝑝for 𝑝= softmax¹𝑠ºas a function of a vector𝑠, and we writedsoftmax¹dPº\nfor this formula applied row-wise. Finally, this computation again parallelizes across the number of heads and\nbatches for the backward pass of MHA.\n2.2 GPU hardware characteristics and execution model\nWe describe the aspects of the GPU’s execution model relevant forFlashAttention-3, with a focus on the\nNVIDIA Hopper architecture as a concrete instantiation of this model.\nMemory hierarchy: The GPU’s memories are organized as a hierarchy of data locales, with capacity inversely\nrelated to bandwidth (Table 1)4. Global memory (GMEM), also known as HBM, is the oﬀ-chip DRAM accessible\nto all streaming multiprocessors (SMs). Data from GMEM gets transparently cached into an on-chip L2 cache.\nNext, each SM contains a small on-chip, programmer-managed highly banked cache called shared memory (SMEM).\nLastly, there is the register ﬁle within each SM.\nThread hierarchy: The GPU’s programming model is organized around logical groupings of execution units\ncalled threads. From the ﬁnest to coarsest level, the thread hierarchy is comprised of threads, warps (32 threads),\nwarpgroups (4 contiguous warps), threadblocks (i.e., cooperative thread arrays or CTAs), threadblock clusters (in\nHopper), and grids.\nThese two hierarchies are closely interlinked. Threads in the same CTA are co-scheduled on the same SM, and\nCTAs in the same cluster are co-scheduled on the same GPC. SMEM is directly addressable by all threads within a\nCTA, whereas each thread has at most 256 registers (RMEM) private to itself.\nTable 1: Thread-Memory hierarchy for the NVIDIA Hopper H100 SXM5 GPU.\nHardware Level Parallel Agent Data Locale Capacity @ Bandwidth\nChip Grid GMEM 80 GiB @ 3.35 TB/s\nGPC Threadblock Clusters L2 50 MiB @ 12 TB/s\nSM Threadblock (CTA) SMEM 228 KiB per SM, 31TB/s per GPU\nThread Thread RMEM 256 KiB per SM\nAsynchronyandwarp-specialization: GPUsarethroughputprocessorsthatrelyonconcurrencyandasynchrony\nto hide memory and execution latencies. For async memory copy between GMEM and SMEM, Hopper has the\nTensor Memory Accelerator (TMA) as a dedicated hardware unit [38, §7.29]. Furthermore, unlike prior architectures\nsuch as Ampere, the Tensor Core of Hopper, exposed via the warpgroup-wide WGMMA instruction [40, §9.7.14], is\nalso asynchronous and can source its inputs directly from shared memory.\n4Luo et al.[34] reports shared memory bandwidth of 128 bytes per clock cycle per SM, and we multiply that by 132 SMs and the\nboost clock of 1830 MHz.\n3\nHardware support for asynchrony allows for warp-specialized kernels, where the warps of a CTA are divided into\nproducer or consumer roles that only ever issue either data movement or computation. Generically, this improves\nthe compiler’s ability to generate optimal instruction schedules [4]. In addition, Hopper supports the dynamic\nreallocation of registers between warpgroups viasetmaxnreg [40, §9.7.17.1], so those warps doing MMAs can obtain\na larger share of RMEM than those just issuing TMA (for which only a single thread is needed).\nLow-precision number formats: Modern GPUs have specialized hardware units for accelerating low-precision\ncomputation. For example, the WGMMA instruction can target the FP8 Tensor Cores on Hopper to deliver 2x the\nthroughput per SM when compared to FP16 or BF16.\nHowever, correctly invoking FP8 WGMMA entails understanding the layout constraints on its operands. Given\na GEMM call to multiply𝐴\u0002𝐵> for an𝑀\u0002𝐾-matrix 𝐴 and an𝑁\u0002𝐾-matrix 𝐵, we say that the𝐴 or 𝐵 operand\nis mn-major if it is contiguous in the outer𝑀 or 𝑁 dimension, andk-major if is instead contiguous in the inner\n𝐾-dimension. Then for FP16 WGMMA, both mn-major and k-major input operands are accepted for operands in\nSMEM, but for FP8 WGMMA, only the k-major format is supported. Moreover, in situations such as attention\nwhere one wants to fuse back-to-back GEMMs in a single kernel, clashing FP32 accumulator and FP8 operand\nlayouts pose an obstacle to invoking dependent FP8 WGMMAs.\nIn the context of attention, these layout restrictions entail certain modiﬁcations to the design of an FP8 algorithm,\nwhich we describe in §3.3.\n2.3 Standard Attention and Flash Attention\nFollowing Dao et al.[17], we letstandard attentiondenote an implementation of attention on the GPU that\nmaterializes the intermediate matricesS and P to HBM. The main idea ofFlashAttention was to leverage a\nlocal version of the softmax reduction to avoid these expensive intermediate reads/writes and fuse attention into a\nsingle kernel. Local softmax corresponds to lines 18-19 of the consumer mainloop in Algorithm 1 together with the\nrescalings of blocks ofO. The simple derivation that this procedure indeed computesO can be found in [15, §2.3.1].\n3 FlashAttention-3: Algorithm\nIn this section, we describe theFlashAttention-3 algorithm. For simplicity, we focus on the forward pass, with\nthe backward pass algorithm described in Appendix B.1. We ﬁrst indicate how to integrate warp-specialization with\na circular SMEM buﬀer into the base algorithm ofFlashAttention-2. We then explain how to exploit asynchrony\nof WGMMA to deﬁne an overlapped GEMM-softmax 2-stage pipeline. Finally, we describe the modiﬁcations needed\nfor FP8, both in terms of layout conformance and accuracy via block quantization and incoherent processing.\n3.1 Producer-Consumer asynchrony through warp-specialization and pingpong schedul-\ning\nWarp-specialization As withFlashAttention-2, the forward pass ofFlashAttention-3 is embarrassingly\nparallel in the batch size, number of heads, and query sequence length. Thus, it will suﬃce to give a CTA-level\nview of the algorithm, which operates on a tileQ𝑖 of the query matrix to compute the corresponding tileO𝑖 of the\noutput. To simplify the description, we ﬁrst give the warp-specialization scheme with a circular SMEM buﬀer that\ndoes not have in addition the GEMM-softmax overlapping. Let𝑑 be the head dimension,𝑁 the sequence length,\nand ﬁx a query block size𝐵𝑟 to divideQ into 𝑇𝑟 = d𝑁\n𝐵𝑟\neblocks Q1 Q𝑇𝑟 .\n4\nAlgorithm 1FlashAttention-3 forward passwithout intra-consumer overlapping – CTA view\nRequire: Matrices Q𝑖 2R𝐵𝑟 \u0002𝑑 and KV 2R𝑁\u0002𝑑 in HBM, key block size𝐵𝑐 with 𝑇𝑐 = d𝑁\n𝐵𝑐\ne.\n1: Initialize pipeline object to manage barrier synchronization with𝑠-stage circular SMEM buﬀer.\n2: if in producer warpgroupthen\n3: Deallocate predetermined number of registers.\n4: Issue loadQ𝑖 from HBM to shared memory.\n5: Upon completion, commit to notify consumer of the load ofQ𝑖.\n6: for 0 \u0014𝑗 𝑇𝑐 do\n7: Wait for the¹𝑗% 𝑠ºth stage of the buﬀer to be consumed.\n8: Issue loads ofK𝑗V𝑗 from HBM to shared memory at the¹𝑗% 𝑠ºth stage of the buﬀer.\n9: Upon completion, commit to notify consumers of the loads ofK𝑗V𝑗.\n10: end for\n11: else\n12: Reallocate predetermined number of registers as function of number of consumer warps.\n13: On-chip, initializeO𝑖 = ¹0º2 R𝐵𝑟 \u0002𝑑 and ℓ𝑖𝑚𝑖 = ¹0º¹\u00001º2 R𝐵𝑟 .\n14: Wait forQ𝑖 to be loaded in shared memory.\n15: for 0 \u0014𝑗 𝑇𝑐 do\n16: Wait forK𝑗 to be loaded in shared memory.\n17: Compute S¹𝑗º\n𝑖 = Q𝑖K𝑇\n𝑗 (SS-GEMM). Commit and wait.\n18: Store 𝑚old\n𝑖 = 𝑚𝑖 and compute𝑚𝑖 = max¹𝑚old\n𝑖 rowmax¹S¹𝑗º\n𝑖 ºº.\n19: Compute eP¹𝑗º\n𝑖 = exp¹S¹𝑗º\n𝑖 \u0000𝑚𝑖ºand ℓ𝑖 = exp¹𝑚old\n𝑖 \u0000𝑚𝑖ºℓ𝑖 ¸rowsum¹eP¹𝑗º\n𝑖 º.\n20: Wait forV𝑗 to be loaded in shared memory.\n21: Compute O𝑖 = diag¹exp¹𝑚old\n𝑖 \u0000𝑚𝑖ºº\u00001O𝑖 ¸eP¹𝑗º\n𝑖 V𝑗 (RS-GEMM). Commit and wait.\n22: Release the¹𝑗% 𝑠ºth stage of the buﬀer for the producer.\n23: end for\n24: Compute O𝑖 = diag¹ℓ𝑖º\u00001O𝑖 and 𝐿𝑖 = 𝑚𝑖 ¸log¹ℓ𝑖º.\n25: Write O𝑖 and 𝐿𝑖 to HBM as the𝑖th block ofO and 𝐿.\n26: end if\nFor our implementation of Algorithm 1 on Hopper, we usesetmaxnreg for (de)allocations, TMA for loads ofQ𝑖\nand fK𝑗V𝑗g0\u0014𝑗𝑇𝑐 , and WGMMA to execute the GEMMs in the consumer mainloop, with the SS or RS preﬁx\nindicating whether the ﬁrst operand is sourced from shared memory or register ﬁle. For interpreting the execution\nﬂow of Algorithm 1, note that issuing TMA loads does not stall on the completion of other loads due to asynchrony.\nMoreover, in the producer mainloop, no waits will be issued for the ﬁrst𝑠 iterations as the buﬀer gets ﬁlled.\nPingpong scheduling The asynchronous nature of WGMMA and TMA, along with warp-specialization, opens\nup the opportunity to overlap the softmax computation of one warpgroup with the GEMM of another warpgroup.\nTo motivate this, notice that non-matmul operations have much lower throughput than matmul operations on\nmodern hardware accelerators. As an example, the H100 SXM5 GPU has 989 TFLOPS of FP16 matmul but only\n3.9 TFLOPS of special functions such as exponential5 (necessary for softmax). For the attention forward pass in\nFP16 with head dimension 128, there are 512x more matmul FLOPS compared to exponential operations, but\nthe exponential has 256x lower throughput, so exponential can take 50% of the cycle compared to matmul. The\nsituation is even worse with FP8, where the matmul throughput doubles but the exponential throughput stays the\nsame.\nSince the exponential is performed by a separate hardware unit (the multi-function unit), ideally we’d want\nthe exponential calculation to be scheduled when the Tensor Cores are performing the matmul. To do so, we use\nsynchronization barriers (bar.sync instructions) to force the GEMMs (GEMM1 –PV of one iteration, and GEMM0\n– QK> of the next iteration) of warpgroup 1 to be scheduled before the GEMMs of warpgroup 2. As a result, the\nsoftmax of warpgroup 1 will be scheduled while warpgroup 2 is performing its GEMMs. Then the roles swap, with\nwarpgroup 2 doing softmax while warpgroup 1 doing GEMMs (hence, “pingpong” scheduling). This is illustrated\n5The CUDA programming guide speciﬁes that 16 operations of special functions can be performed per streaming multiprocessor\n(SM) per clock cycle. We multiply 16 by 132 SMs and 1830 MHz clock speed to get 3.9 TFLOPS of special functions.\n5\nin Fig. 1. Though in practice the pingpong scheduling is not as clean as depicted in the ﬁgure, we generally ﬁnd this\nto improve performance (e.g., from 570 TFLOPS to 620-640 TFLOPS for FP16 forward with head dimension 128\nand sequence length 8192).\nFigure 1: Pingpong scheduling for 2 warpgroups to overlap softmax and GEMMs: the softmax of one warpgroup should be\nscheduled when the GEMMs of another warpgroup are running. The same color denotes the same iteration.\nAttention variants For multi-query attention [51] and grouped query attention [3], we follow the approach in\nFlashAttention-2 and adjust the tensor indexing to avoid duplicatingK and V in HBM.\n3.2 Intra-warpgroup overlapping GEMMs and softmax\nEven within one warpgroup, we can overlap some instructions in the softmax with some instructions in the GEMMs.\nWe describe one technique to do so.\nIn the attention algorithm, operations within the inner loop (main loop) have sequential dependencies that\nimpede parallelization within a single iteration. For example, (local) softmax (lines 18 to 19) relies on the output\nS¹𝑗º\n𝑖 of the ﬁrst GEMM, while the second GEMM takes its resulteP¹𝑗º\n𝑖 as an operand. Indeed, the wait statements\nin lines 17 and 21 of Algorithm 1 serialize the execution of softmax and GEMMs. However, we can break these\ndependencies by pipelining across iterations through additional buﬀers in registers. Pursuing this idea, we propose\nthe following two-stage6 GEMM-softmax pipelining algorithm:\nFigure 2: 2-stage WGMMA-softmax pipelining\n6Note that the number of stages of the overlapping scheme is bounded by, but need not equal, the number𝑠 of stages in the circular\nSMEM buﬀer.\n6\nAlgorithm 2FlashAttention-3 consumer warpgroup forward pass\nRequire: Matrices Q𝑖 2R𝐵𝑟 \u0002𝑑 and KV 2R𝑁\u0002𝑑 in HBM, key block size𝐵𝑐 with 𝑇𝑐 = d𝑁\n𝐵𝑐\ne.\n1: Reallocate predetermined number of registers as function of number of consumer warps.\n2: On-chip, initializeO𝑖 = ¹0º2 R𝐵𝑟 \u0002𝑑 and ℓ𝑖𝑚𝑖 = ¹0º¹\u00001º2 R𝐵𝑟 .\n3: Wait forQ𝑖 and K0 to be loaded in shared memory.\n4: Compute Scur = Q𝑖K𝑇\n0 using WGMMA. Commit and wait.\n5: Release the0th stage of the buﬀer forK.\n6: Compute 𝑚𝑖, ~Pcur and ℓ𝑖 based onScur, and rescaleO𝑖.\n7: for 1 \u0014𝑗 𝑇𝑐 \u00001 do\n8: Wait forK𝑗 to be loaded in shared memory.\n9: Compute Snext = Q𝑖K𝑇\n𝑗 using WGMMA. Commit but do not wait.\n10: Wait forV𝑗\u00001 to be loaded in shared memory.\n11: Compute O𝑖 = O𝑖 ¸~PcurV𝑗\u00001 using WGMMA. Commit but do not wait.\n12: Wait for the WGMMAQ𝑖K𝑇\n𝑗 .\n13: Compute 𝑚𝑖, ~Pnext and ℓ𝑖 based onSnext.\n14: Wait for the WGMMA~PcurV𝑗\u00001 and then rescaleO𝑖\n15: Release the¹𝑗% 𝑠ºth, resp. ¹𝑗\u00001 %𝑠ºth stage of the buﬀer forK, resp. V.\n16: Copy Snext to Scur.\n17: end for\n18: Wait forV𝑇𝑐\u00001 to be loaded in shared memory.\n19: Compute O𝑖 = O𝑖 ¸~PlastV𝑇𝑐\u00001 using WGMMA. Commit and wait.\n20: Epilogue: Rescale O𝑖 based on𝑚𝑖. Compute 𝐿𝑖 based on𝑚𝑖 and ℓ𝑖. WriteO𝑖 and 𝐿𝑖 to HBM as the𝑖-th block\nof O and 𝐿.\nAlgorithm 2 functions as a replacement for the consumer path of Algorithm 1 to comprise the complete\nFlashAttention-3 algorithm for FP16 precision. At a high-level, we use WGMMA as a metonym for asynchronous\nGEMM. Within the mainloop (lines 8 to 16), the second WGMMA operation of iteration𝑗 (line 11) is overlapped\nwith softmax operations from iteration𝑗¸1 (line 13).\nWhile the pipelined structure illustrated above oﬀers theoretical performance gains, there are several practical\naspects to consider:\nCompiler reordering The pseudocode represents an idealized execution order but the compiler (NVCC) often\nrearranges instructions for optimization. This can disrupt the carefully crafted WGMMA and non-WGMMA\noperation pipelining sequence, potentially leading to unexpected behavior or diminished performance gains. An\nanalysis of the SASS code shows that the compiler generates overlapped code as expected (Section B.2).\nRegister pressure To maintain optimal performance, register spilling should be minimized. However, the 2-stage\npipeline requires additional registers to store intermediate results and maintain context between stages. Speciﬁcally,\nan extraSnext must be kept in registers, leading to extra register usage of size𝐵𝑟 \u0002𝐵𝑐\u0002sizeof¹ﬂoatºper threadblock.\nThis increased register demand may conﬂict with using larger block sizes (another common optimization), which is\nalso register-hungry. In practice, trade-oﬀs should be made based on proﬁling results.\n3-stage pipelining Extending the 2-stage algorithm described above, we propose a 3-stage variant that would\nfurther overlap the second WGMMA with softmax. While this approach oﬀers the potential for even higher Tensor\nCore utilization, it requires even more registers due to an additional stage in the pipeline, making the trade-oﬀ\nbetween tile size and pipeline depth more diﬃcult to balance. A detailed description of the 3-stage algorithm and\nits evaluation results can be found in Appendix B.3.\n3.3 Low-precision with FP8\nEﬃciency: layout transformations. Computing the forward pass ofFlashAttention-3 in FP8 precision poses\nadditional challenges not encountered for FP16 in terms of layout conformance.\n7\nT0{d0,d1} T1{d0,d1} T0{d4,d5} T1{d4,d5}T2{d0,d1} T3{d0,d1} T2{d4,d5} T3{d4,d5}\nT0{d2,d3} T1{d2,d3} T0{d6,d7} T1{d6,d7}T2{d2,d3} T3{d2,d3} T2{d6,d7} T3{d6,d7}\nFigure 3: FP32 accumulator register WGMMA layout – rows 0 and 8, threads 0-3, entries 0-7.\nT0{a0,a1} T0{a2,a3} T1{a0,a1} T1{a2,a3} T2{a0,a1} T2{a2,a3} T3{a0,a1} T3{a2,a3}\nT0{a4,a5} T0{a6,a7} T1{a4,a5} T1{a6,a7} T2{a4,a5} T2{a6,a7} T3{a4,a5} T3{a6,a7}\nFigure 4: FP8 operand A register WGMMA layout – rows 0 and 8, threads 0-3, entries 0-7.\nFirst, we note that the input tensorsQ, K, andV are typically given as contiguous in the head dimension, while\nto satisfy the k-major constraint on FP8 WGMMA for the second GEMM we needV, or rather the tiles ofV\nloaded into SMEM, to be contiguous in the sequence length dimension. Since the TMA load itself cannot change\nthe contiguous dimension, we then need to either (1) transposeV in GMEM as a pre-processing step, or (2) do an\nin-kernel transpose of tiles ofV after loading them into SMEM. To implement option (1), we can either (1a) fuse the\ntranspose to the epilogue of a preceding step such as the rotary embedding, or (1b) call a standalone pre-processing\ntranspose kernel7 to exchange the strides of the sequence length and head dimensions. However, (1a) is diﬃcult to\nintegrate into a standard library, and (1b) is too wasteful in a memory-bound situation such as inference.\nInstead, for FP8FlashAttention-3 we opt for option (2). For the in-kernel transpose, we take advantage of\nthe LDSM (ldmatrix) and STSM (stmatrix) instructions, which involve a warp of threads collectively loading\nSMEM to RMEM and storing RMEM to SMEM at a granularity of 128 bytes.8 The LDSM/STSM instructions are\nboth register eﬃcient, allowing us to execute them in the producer warpgroup, and capable of transposing layouts\nwhen doing memory copy. Moreover, after the ﬁrst iteration we can arrange for the transpose of the nextV tile to\nbe executed in the shadow of the two WGMMAs that involve the precedingV and currentK tile.\nSecond, we observe that unlike with FP16, the memory layout of the FP32 accumulator of an FP8 WGMMA is\ndiﬀerent from that assumed for its operand A when held in registers. We depict fragments of these two layouts\nin Fig. 3 and Fig. 4, where the entries are held in registers per thread in the listed order. By using byte permute\ninstructions, we can then transform the ﬁrst WGMMA’s accumulator into a format suitable for the second WGMMA,\nand compatibly with the layout of theV tile produced by the in-kernel transpose. Speciﬁcally, with reference to\nFig. 3, we change the order in sequence to\nfd0 d1 d4 d5 d2 d3 d6 d7g\nand this register permutation is then replicated over every 8 bytes. In terms of the logical shape of theP tile, this\nmanuever permutes its columns (e.g., columns0189 now become the ﬁrst four columns). For WGMMA to then\ncompute the correct output tile, we can correspondingly arrange for the in-kernel transpose to write out a matching\nrow permutation of theV tile.9\nAccuracy: block quantization and incoherent processing.With FP8 (e4m3) format, one only uses 3\nbits to store the mantissa and 4 bits for the exponent. This results in higher numerical error than FP16/BF16.\nMoreover, large models typically have outlier values [20, 54] that are much larger in magnitude than most other\nvalues, making quantization diﬃcult. One typically use per-tensor scaling [37] by keeping one scalar per tensor (e.g.,\none forQ, forK, and forV). To reduce the numerical error of attention in FP8, we employ two techniques:\n1. Block quantization: we keep one scalar per block, so that for each ofQ, K, V we split the tensor into blocks\nof size 𝐵𝑟 \u0002𝑑 or 𝐵𝑐 \u0002𝑑 and quantize them separately. This quantization can be fused with an operation\nright before attention (e.g., rotary embedding) with no additional slow down (since rotary embedding is\nmemory-bandwidth bound). As theFlashAttention-3 algorithm naturally operates on blocks, we can scale\neach block ofS to account for this block quantization at no computation cost.\n7An optimized transpose kernel will achieve speed near the bandwidth of the device [46].\n8In the PTX documentation, LDSM/STSM are described as copying8 \u00028 matrices with 16-bit entries [40, §9.7.13.4.15-16], but we can\npack 8-bit entries two at a time to use LDSM/STSM in the context of FP8 precision. However, the transpose versions of LDSM/STSM\ncannot split packed 8-bit entries, which necessitates certain register movements in between LDSM and STSM to actually perform a\ntile-wise transpose; we omit the details.\n9This additional freedom aﬀorded by doing the in-kernel transpose eliminates having to use shuﬄe instructions to change register\nownership across threads, which we previously described in [7].\n8\n2. Incoherent processing: to even out outliers, we multiplyQ and K with a random orthogonal matrixM\nbefore quantizing to FP8. SinceM is orthogonal,MM>= 𝐼 and so¹QMº¹KMº>= QK>, i.e., multiplying both\nQ and K with M does not change the attention output. This serves to “spread out” the outliers since each\nentry ofQM or KM is a random sum of entries ofQ or K, thus reducing quantization error. In practice, we\nfollow Chee et al.[9] and Tseng et al.[58] and chooseM to be the product of random diagonal matrices of\u00061\nand a Hadamard matrix, which can be multiplied in𝑂¹𝑑log 𝑑ºinstead of𝑂¹𝑑2º, and can also be fused with\nthe rotary embedding at no extra computation cost.\nWe validate that these two techniques reduces numerical error by up to 2.6\u0002in §4.3.\n4 Empirical Validation\nWe use the primitives from CUTLASS [57] such as WGMMA and TMA abstractions to implementFlashAttention-\n3 and evaluate its eﬃciency and accuracy.\n• Benchmarking attention.We measure the runtime ofFlashAttention-3 across diﬀerent sequence lengths\nand compare it to a standard implementation in PyTorch,FlashAttention-2, FlashAttention-2 in\nTriton (which uses H100-speciﬁc instructions), as well as a vendor’s implementation ofFlashAttention-2\noptimized for H100 GPUs from cuDNN. We conﬁrm thatFlashAttention-3 is up to 2.0\u0002faster than\nFlashAttention-2 and 1.5\u0002faster thanFlashAttention-2 in Triton.FlashAttention-3 reaches up to\n740 TFLOPs/s, 75% of the theoretical maximum TFLOPs/s on H100 GPUs.\n• Ablation study.We conﬁrm that our algorithmic improvements with warp-specialization and GEMM-softmax\npipelining contribute to the speedup ofFlashAttention-3.\n• Accuracy of FP8 attention.We validate that block quantization and incoherent processing reduces the\nnumerical error of FP8FlashAttention-3 by 2.6\u0002.\n4.1 Benchmarking Attention\nWe measure the runtime of diﬀerent attention methods on an H100 80GB SXM5 GPU for diﬀerent settings (without\n/ with causal mask, head dimension 64 or 128) for FP16 inputs. We report the results in Fig. 5 and Fig. 6, showing\nthat FlashAttention-3 is around 1.5-2.0\u0002faster thanFlashAttention-2 in the forward pass and 1.5-1.75\u0002\nfaster in the backward pass. Compared to a standard attention implementation,FlashAttention-3 can be up to\n3-16\u0002faster. For medium and long sequences (1k and above),FlashAttention-3 even surpasses the speed of a\nvendor’s library (cuDNN – closed source) that has been optimized for H100 GPUs.\nBenchmark settings: We vary the sequence length as 512, 1k, ..., 16k, and set batch size so that the total\nnumber of tokens is 16k. We set the hidden dimension to 2048, and head dimension to be either 64, 128, or 256 (i.e.,\n32 heads, 16 heads, or 8 heads). To calculate the FLOPs of the forward pass, we use:\n4 \u0001seqlen2 \u0001head dimension\u0001number of heads\nWith causal masking, we divide this number by 2 to account for the fact that approximately only half of the entries\nare calculated. To get the FLOPs of the backward pass, we multiply the forward pass FLOPs by 2.5 (since there\nare 2 matmuls in the forward pass and 5 matmuls in the backward pass, due to recomputation).\nWe also measure the runtime for FP8 for the forward pass under similar settings. We report the results for\nheaddim 256 in Fig. 7 and give the full results in Appendix C.2.\n4.2 Ablation Study: 2-Stage Pipelining Experiments\nWeablateboththe2-stageWGMMA-softmaxpipeliningandwarp-specializationfornon-causalFP16 FlashAttention-\n3 with ﬁxed parametersfbatchseqlennheadshdimg= f4844816128g. The result in Table 2 conﬁrms that our\nalgorithmic improvements (asynchrony with warp-specialization and overlapping between GEMM and softmax) lead\nto signiﬁcant speedup, from 570 to 661 TFLOPs.\n9\n512 1k 2k 4k 8k 16k\nSequence length\n200\n400\n600Speed (TFLOPs/s)\n52 63 67 72 73\nOOM\n282\n306 318 321 322 324340\n382 396 400 401 403\n335\n373\n395 408 412 413\n333\n392\n460 476\n496 497\nAttention forward speed, head dim 64 (H100 80GB SXM5)\nStandard attention\nFlashAttention-2\nTriton\ncuDNN\nFlashAttention-3\n(a) Forward, without causal mask, head dim 64\n512 1k 2k 4k 8k 16k\nSequence length\n200\n400\n600Speed (TFLOPs/s)\n16 18 18 18 18\nOOM\n180\n229\n262\n284 295 299\n152\n291\n342\n363 376 363\n225\n288\n334\n363 379 388\n197\n265\n371\n420\n460 473\nAttention forward speed, head dim 64 (H100 80GB SXM5)\nStandard attention\nFlashAttention-2\nTriton\ncuDNN\nFlashAttention-3 (b) Forward, with causal mask, head dim 64\n512 1k 2k 4k 8k 16k\nSequence length\n200\n400\n600Speed (TFLOPs/s)\n74\n100\n119 133 139\nOOM\n309\n350 362 368 370 370\n323\n372\n389 389 392 395\n497\n574\n617 609 600 595\n467\n565\n625 638 646 648\nAttention forward speed, head dim 128 (H100 80GB SXM5)\nStandard attention\nFlashAttention-2\nTriton\ncuDNN\nFlashAttention-3\n(c) Forward, without causal mask, head dim 128\n512 1k 2k 4k 8k 16k\nSequence length\n200\n400\n600Speed (TFLOPs/s)\n26 31 34 35 35\nOOM\n191\n260\n298\n319 333 335\n146\n273\n323\n353 369 378\n315\n410\n484\n518 529 539\n292\n423\n521\n579\n602 616\nAttention forward speed, head dim 128 (H100 80GB SXM5)\nStandard attention\nFlashAttention-2\nTriton\ncuDNN\nFlashAttention-3 (d) Forward, with causal mask, head dim 128\n512 1k 2k 4k 8k 16k\nSequence length\n200\n400\n600Speed (TFLOPs/s)\n275\n313 321 323 324 326\n470\n546\n580 581 580 581\n482\n627\n707\n736 746 756\nAttention forward speed, head dim 256 (H100 80GB SXM5)\nFlashAttention-2\ncuDNN\nFlashAttention-3\n(e) Forward, without causal mask, head dim 256\n512 1k 2k 4k 8k 16k\nSequence length\n200\n400\n600Speed (TFLOPs/s)\n208\n251\n278 293 297 298308\n391\n450\n483 497 509\n286\n427\n537\n612 628 642\nAttention forward speed, head dim 256 (H100 80GB SXM5)\nFlashAttention-2\ncuDNN\nFlashAttention-3 (f) Forward, with causal mask, head dim 256\nFigure 5: Attention forward speed (FP16/BF16) on H100 GPU\n4.3 Numerical Error Validation\nAs there has been interest in the numerical error [21] of FlashAttention, we compareFlashAttention-2,\nFlashAttention-3, and a standard implementation of attention against a reference implementation in FP64. To\nsimulate outlier features and activations in LLMs [20, 54], we generate the entries ofQKV with the following\n10\n512 1k 2k 4k 8k 16k\nSequence length\n200\n400\n600Speed (TFLOPs/s)\n68 76 88 92 95\nOOM\n198\n238\n264 279 287 291\n266\n348\n395\n417\n432 433\n272\n363\n422\n453\n472 474\nAttention backward speed, head dim 64 (H100 80GB SXM5)\nStandard attention\nFlashAttention-2\ncuDNN\nFlashAttention-3\n(a) Backward, without causal mask, head dim 64\n512 1k 2k 4k 8k 16k\nSequence length\n200\n400\n600Speed (TFLOPs/s)\n104\n131\n159 174 181\nOOM\n214\n260\n291\n310 318 322\n305\n408\n465\n499\n518 516\n316\n424\n501\n542\n559 561\nAttention backward speed, head dim 128 (H100 80GB SXM5)\nStandard attention\nFlashAttention-2\ncuDNN\nFlashAttention-3 (b) Backward, without causal mask, head dim 128\nFigure 6: Attention backward speed (FP16/BF16) on H100 GPU\n512 1k 2k 4k 8k 16k\nSequence length\n400\n800\n1200Speed (TFLOPs/s)\n529\n664\n766\n854\n897 903\n686\n878\n1001\n1087\n1122 1139\n510\n744\n931\n966\n1151 1171\nAttention forward speed, head dim 256 (H100 80GB SXM5)\nTriton\ncuDNN\nFlashAttention-3\n(a) Forward, without causal mask, head dim 256\n512 1k 2k 4k 8k 16k\nSequence length\n400\n800\n1200Speed (TFLOPs/s)\n299\n425\n520\n591\n628\n663\n304\n449\n768\n1015\n1056\n1099\n329\n521\n703\n856\n960\n1024\nAttention forward speed, head dim 256 (H100 80GB SXM5)\nTriton\ncuDNN\nFlashAttention-3 (b) Forward, with causal mask, head dim 256\nFigure 7: Attention forward speed (FP8) on H100 GPU\nTable 2: Pipelining ablation measurements\nConﬁguration Time TFLOPs/s\nFlashAttention-3 3.538 ms 661\nNo GEMM-Softmax Pipelining, Warp-Specialization 4.021 ms 582\nGEMM-Softmax Pipelining, No Warp-Specialization 4.105 ms 570\ndistribution:\nN¹01º¸N¹ 0100º\u0001 Bernoulli¹0001º\nThat is, each entry is normally distributed with zero mean and standard deviation 1, but for 0.1% of entries we\nadd an independent term that’s normally distributed with standard deviation 10. We then measure the root mean\nsquared error (RMSE) in Table 3. In FP16, bothFlashAttention-2 and FlashAttention-3 achieves 1.7\u0002\nlower RMSE compared to the standard implementation since intermediate results (softmax) are kept in FP32. The\nbaseline attention in FP8 uses per-tensor scaling, with matmul accumulator in FP32 and intermediate softmax\nresults kept in FP16. Thanks to block quantization and incoherent processing,FlashAttention-3 in FP8 is 2.6\u0002\nmore accurate than this baseline.\n11\nTable 3: Numerical error comparisons in FP16 and FP8 (e4m3).\nMethod Baseline FP16 FlashAttention-2 FP16 FlashAttention-3 FP16\nRMSE 3.2e-4 1.9e-4 1.9e-4\nMethod Baseline FP8 FlashAttention-3 FP8 No block quant No incoherent processing\nRMSE 2.4e-2 9.1e-3 9.3e-3 2.4e-2\n5 Dicussion, Limitations, Conclusion\nWith FlashAttention-3, we have demonstrated that new programming techniques and hardware features such\nas asynchrony and low-precision can have a dramatic impact on the eﬃciency and accuracy of attention. We are\nable to speed up attention by 1.5-2.0\u0002times compared toFlashAttention-2, and reduce FP8 numerical error\nby 2.6\u0002compared to standard per-tensor quantization. Some limitations of our work that we hope to address in\nthe future include: optimizing for LLM inference, integrating a persistent kernel design into the FP8 kernel,10 and\nunderstanding the eﬀects of low-precision attention in large-scale training. Though we have focused on Hopper\nGPUs in this work, we expect that the techniques developed here will apply to other hardware accelerators. We\nhope that a faster and more accurate primitive such as attention will unlock new applications in long-context tasks.\nAcknowledgments\nWe are grateful to the NVIDIA CUTLASS team (especially Haicheng Wu, Aniket Shivam, and Cris Cecka) for\nhelping us understand Hopper’s programming model and for their library, which provides clean and powerful building\nblocks for the implementation ofFlashAttention-3. We thank the cuDNN team for the idea of in-kernel transpose\nfor FP8. The idea of overlapping GEMMs and softmax was inspired by insightful conversations with Christopher\nRé, Benjamin Spector, Aniket Shivam, and Markus Hoehnerbach. The pingpong scheduling is adapted from the\nwarp-specialized pingpong GEMM implementation in CUTLASS. We appreciate Driss Guessous for integrating\nFlashAttention to PyTorch. FlashAttention-3 has beneﬁted from helpful discussions with Horace He on\ndiﬀerent attention variants, with Hao Liu and Phil Wang on distributed attention, and with Daniel Haziza and\nChris De Sa on quantization. We thank Meta, Together AI, and Princeton Language and Intelligence (PLI) for\ncompute support.\nReferences\n[1] Ahmad Abdelfattah, Azzam Haidar, Stanimire Tomov, and Jack Dongarra. Performance, design, and autotuning\nof batched gemm for gpus. pages 21–38, 06 2016. ISBN 978-3-319-41320-4. doi: 10.1007/978-3-319-41321-1_2.\n[2] AI21. Introducing jamba: Ai21’s groundbreaking ssm-transformer model.AI21 blog, 2024.\n[3] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.\nGqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint\narXiv:2305.13245, 2023.\n[4] Michael Bauer, Henry Cook, and Brucek Khailany. CudaDMA: Optimizing GPU Memory Bandwidth via\nWarp Specialization. In Proceedings of 2011 International Conference for High Performance Computing,\nNetworking, Storage and Analysis, SC ’11, New York, NY, USA, 2011. Association for Computing Machinery.\nISBN 9781450307710. doi: 10.1145/2063384.2063400. URLhttps://doi.org/10.1145/2063384.2063400.\n[5] Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp,\nGünter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory.\narXiv preprint arXiv:2405.04517, 2024.\n10For our benchmarks, FP16FlashAttention-3 has a persistent kernel and load balancing strategy, while FP8FlashAttention-3\ndoes not. This partly explains why FP8FlashAttention-3 does not perform as well for small sequence length and causal masking\ncompared to the FP8 cuDNN kernels.\n12\n[6] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.arXiv preprint\narXiv:2004.05150, 2020.\n[7] Ganesh Bikshandi and Jay Shah. Delivering 1 PFLOP/s of Performance with FP8 FlashAttention-2, 2024.\nURL https://research.colfax-intl.com/adding-fp8-to-flashattention/ .\n[8] William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, and Jonathan\nRagan-Kelley. Striped attention: Faster ring attention for causal transformers.arXiv preprint arXiv:2311.09431,\n2023.\n[9] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip: 2-bit quantization of large\nlanguage models with guarantees.Advances in Neural Information Processing Systems, 36, 2024.\n[10] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying sparse\nand low-rank attention. InAdvances in Neural Information Processing Systems (NeurIPS), 2021.\n[11] Richard J Chen, Chengkuan Chen, Yicong Li, Tiﬀany Y Chen, Andrew D Trister, Rahul G Krishnan, and\nFaisal Mahmood. Scaling vision transformers to gigapixel images via hierarchical self-supervised learning. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16144–16155,\n2022.\n[12] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.\narXiv preprint arXiv:1904.10509, 2019.\n[13] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,\nPeter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In\nThe International Conference on Learning Representations (ICLR), 2021.\n[14] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with\nperformers. In International Conference on Learning Representations (ICLR), 2020.\n[15] Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, 2023. URL\nhttps://arxiv.org/abs/2307.08691.\n[16] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and eﬃcient algorithms with structured\nstate space duality. InInternational Conference on Machine Learning (ICML), 2024.\n[17] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-\neﬃcient exact attention with IO-awareness. InAdvances in Neural Information Processing Systems, 2022.\n[18] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré. Hungry hungry\nhippos: Towards language modeling with state space models. InThe International Conference on Learning\nRepresentations (ICLR), 2023.\n[19] DeepSeek-AI. Deepseek-v2: A strong, economical, and eﬃcient mixture-of-experts language model.arXiv\npreprint arXiv:2405.04434, 2024.\n[20] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication\nfor transformers at scale.CoRR abs/2208.07339, 2022.\n[21] Alicia Golden, Samuel Hsia, Fei Sun, Bilge Acun, Basil Hosmer, Yejin Lee, Zachary DeVito, Jeﬀ Johnson,\nGu-Yeon Wei, David Brooks, et al. Is ﬂash attention stable?arXiv preprint arXiv:2405.02803, 2024.\n[22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. 2023.\n[23] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,\nZhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech recognition.\narXiv preprint arXiv:2005.08100, 2020.\n13\n[24] Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.\nLongt5: Eﬃcient text-to-text transformer for long sequences.arXiv preprint arXiv:2112.07916, 2021.\n[25] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video\ndiﬀusion models. Advances in Neural Information Processing Systems, 35:8633–8646, 2022.\n[26] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer,\nand Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization.\narXiv preprint arXiv:2401.18079, 2024.\n[27] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs: Fast\nautoregressive transformers with linear attention. InInternational Conference on Machine Learning, pages\n5156–5165. PMLR, 2020.\n[28] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The eﬃcient transformer. InThe International\nConference on Machine Learning (ICML), 2020.\n[29] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao\nZhang, and Ion Stoica. Eﬃcient memory management for large language model serving with pagedattention.\nIn Proceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, 2023.\n[30] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoﬀ, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you!arXiv preprint\narXiv:2305.06161, 2023.\n[31] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-inﬁnite context.\narXiv preprint arXiv:2310.01889, 2023.\n[32] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language\nwith ringattention.arXiv preprint arXiv:2402.08268, 2024.\n[33] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia\nHu. Kivi: A tuning-free asymmetric 2bit quantization for kv cache.arXiv preprint arXiv:2402.02750, 2024.\n[34] Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Qiang Wang, and Xiaowen Chu. Benchmarking and Dissecting the\nNvidia Hopper GPU Architecture, 2024. URLhttps://arxiv.org/abs/2402.13499.\n[35] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke\nZettlemoyer. Mega: Moving average equipped gated attention. InThe International Conference on Learning\nRepresentations (ICLR), 2023.\n[36] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer,\nOmer Levy, and Chunting Zhou. Megalodon: Eﬃcient llm pretraining and inference with unlimited context\nlength. arXiv preprint arXiv:2404.08801, 2024.\n[37] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite,\nSangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning.arXiv\npreprint arXiv:2209.05433, 2022.\n[38] NVIDIA. CUDA Programming Guide Version 12.4, 2024. URL https://docs.nvidia.com/cuda/\ncuda-c-programming-guide/index.html .\n[39] Nvidia. Accelerating transformers with nvidia cudnn 9.Nvidia blog, 2024. URLhttps://developer.nvidia.\ncom/blog/accelerating-transformers-with-nvidia-cudnn-9/ .\n[40] NVIDIA. Parallel Thread Execution ISA Version 8.4, 2024. URLhttps://docs.nvidia.com/cuda/pdf/ptx_\nisa_8.4.pdf.\n14\n[41] Muhammad Osama, Duane Merrill, Cris Cecka, Michael Garland, and John D. Owens. Stream-k: Work-\ncentric parallel decomposition for dense matrix-matrix multiplication on the gpu. InProceedings of the 28th\nACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, PPoPP ’23, pages\n429–431, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400700156. doi:\n10.1145/3572848.3577479. URL https://doi.org/10.1145/3572848.3577479.\n[42] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael\nChung, Matteo Grella, Kranthi Kiran GV, et al. RWKV: Reinventing RNNs for the Transformer era.arXiv\npreprint arXiv:2305.13048, 2023.\n[43] Bowen Peng, Jeﬀrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Eﬃcient context window extension of\nlarge language models.arXiv preprint arXiv:2309.00071, 2023.\n[44] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. Random\nfeature attention. InThe International Conference on Learning Representations (ICLR), 2021.\n[45] Markus N Rabe and Charles Staats. Self-attention does not need 𝑂¹𝑛2º memory. arXiv preprint\narXiv:2112.05682, 2021.\n[46] Colfax Research. Tutorial: Matrix Transpose in CUTLASS, 2024. URLhttps://research.colfax-intl.\ncom/tutorial-matrix-transpose-in-cutlass/ .\n[47] Aurko Roy, Mohammad Saﬀar, Ashish Vaswani, and David Grangier. Eﬃcient content-based sparse attention\nwith routing Transformers.arXiv preprint arXiv:2003.05997, 2020.\n[48] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code.arXiv preprint\narXiv:2308.12950, 2023.\n[49] Rya Sanovar, Srikant Bharadwaj, Renee St. Amant, Victor Rühle, and Saravan Rajmohan. Lean attention:\nHardware-aware scalable attention mechanism for the decode-phase of transformers. 2024.\n[50] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor\nGeva, Jonathan Berant, et al. Scrolls: Standardized comparison over long language sequences.arXiv preprint\narXiv:2201.03533, 2022.\n[51] Noam Shazeer. Fast transformer decoding: One write-head is all you need.arXiv preprint arXiv:1911.02150,\n2019.\n[52] Benjamin Spector, Aaryan Singhal, Simran Arora, and Christopher Ré, 2024. URLhttps://github.com/\nHazyResearch/ThunderKittens.\n[53] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. Bert4rec: Sequential\nrecommendation with bidirectional encoder representations from transformer. InProceedings of the 28th ACM\ninternational conference on information and knowledge management, pages 1441–1450, 2019.\n[54] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive activations in large language models.arXiv\npreprint arXiv:2402.17762, 2024.\n[55] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.\nRetentive network: A successor to transformer for large language models.arXiv preprint arXiv:2307.08621,\n2023.\n[56] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Eﬃcient transformers: A survey.arXiv preprint\narXiv:2009.06732, 2020.\n[57] Vijay Thakkar, Pradeep Ramani, Cris Cecka, Aniket Shivam, Honghao Lu, Ethan Yan, Jack Kosaian, Mark\nHoemmen, Haicheng Wu, Andrew Kerr, Matt Nicely, Duane Merrill, Dustyn Blasig, Fengqi Qiao, Piotr\nMajcher, Paul Springer, Markus Hohnerbach, Jin Wang, and Manish Gupta. CUTLASS, January 2023. URL\nhttps://github.com/NVIDIA/cutlass.\n15\n[58] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better\nllm quantization with hadamard incoherence and lattice codebooks.arXiv preprint arXiv:2402.04396, 2024.\n[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017.\n[60] Roger Waleﬀe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali\nHatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mamba-based language models.\narXiv preprint arXiv:2406.07887, 2024.\n[61] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh.\nNyströmformer: A nystöm-based algorithm for approximating self-attention. InProceedings of the AAAI\nConference on Artiﬁcial Intelligence. AAAI Conference on Artiﬁcial Intelligence, volume 35, page 14138, 2021.\n[62] Shunyu Yao, Jeﬀrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React:\nSynergizing reasoning and acting in language models.arXiv preprint arXiv:2210.03629, 2022.\n[63] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences.\nAdvances in Neural Information Processing Systems, 33, 2020.\n[64] Zyphra. Zyphra unveils zamba: A compact 7b ssm hybrid model.Zyphra blog, 2024.\n16\nA Related Work\nAttention variants and distributed attentionEver since attention became popular with the Transformer\narchitecture [59], there has been a large body of work on approximating attention to scale it to longer sequences.\nThese approximation methods can generally be categorized into two classes: sparse and low-rank. Sparse attention\nonly computes some entries of the attention matrix (softmax¹QK𝑇º) and assumes that other entries are zero.\nDiﬀerent methods have diﬀerent ways of choosing which entries should be zero, either with a ﬁxed pattern [12],\nwith a sliding window [6], or with a dynamic pattern through hashing [28] or routing [47]. The low-rank approach\ninstead assumes that the attention matrix has a low-rank structure, and apply a pointwise nonlinearity to the query\nand key [27] with random projection [13, 44, 61]. One can also combine the sparse and low-rank approximation for\nbetter quality [10, 63]. However, these approximation methods typically do not oﬀer the same model quality as\nstandard attention [56], and so most large-scale models do not employ these techniques.\nThere are other variants of attention aimed at reducing the size of the KV cache to improve inference eﬃciency.\nMulti-query attention [51] and grouped query attention [3] tie diﬀerent heads ofK and V, and multiple query heads\ninteract with the same key and value head. Multi-head latent attention [19] parameterizes theK and V as low-rank\nprojections of a shared matrix to further reduce the KV cache size. However, all of these approaches do not change\nthe core computationsoftmax¹QK𝑇ºV during training and simply change howQKV are obtained. As a result,\nany eﬃciency or accuracy improvement to the standard attention computation beneﬁts these methods.\nTo extend to even longer context, attention computation can be distributed across multiple GPUs. Methods such\nas Ring attention [31, 32] and variants [8] can reach a context length of up to 1 million. They useFlashAttention\n(or FlashAttention-2) as a primitive, and so the improvement fromFlashAttention-3 would beneﬁt these\ndistributed attention methods as well.\nAlternative architectures Motivated by the limitations of attention, a variety of alternative architectures have\nbeen proposed. They build on the connection between linear attention [27] and recurrent neural networks (RNNs).\nRWKV [42], H3 [18], MEGA [35], Retnet [55] enhance the expressivity of the simple cumulative sum in linear\nattention with more sophisticated recurrences. Mamba [22] and xLSTM [5] use learnable weighting for the recurrence\nand can match the quality of Transformers in language modeling at small or medium scale. These approaches can\nbe connected to generalizations of linear attention through the lens of the structure of the token-mixing matrix [16].\nThese models have started to see some traction, seeing usage in some medium to large-scale models such as Jamba [2],\nZamba [64], Megalodon [36], and Mamba2-hybrid [60]. For the highest quality, these SSM- and RNN-based models\nstill employ many layers of attention. We expect that techniques to speed up attention presented in this work will\nbe useful to speedup these alternative architectures.\nLow-precision attention Quantization is a promising approach to speed up attention, but they have mostly\nfocused on reducing the space for KV cache for inference eﬃciency. QuIP [9] and QuIP#[58] use incoherent\nprocessing to reduce the quantization, and we adapted this technique for FP8FlashAttention-3. Recent work\nsuggests that for inference the KV cache is highly compressible down to 4-, 3-, or even 2-bits [26, 33]. However,\nquantization during training is still challenging as higher precision is typically required for stable training.\nHardware-aware Algorithms Our work presented in this paper focuses on the micro-architecture speciﬁc tuning\nto leverage new instruction sets and adopt a natively asynchronous programming model. There are other orthogonal\naxes for hardware-aware algorithm co-design being explored. A recent example of this is LeanAttention [49],\nwhich recognizes the poor GPU occupancy and high memory bandwidth requirements of the sequential token\ngeneration phase as primary bottlenecks for inference and optimizes it via a smarter load balancing strategy similar\nto Stream-K load balancing [41] to achieve nearly peak occupancy. There is a large literature on optimizing GEMM\nfor speciﬁc hardware that employs many of the same techniques. As an example, Abdelfattah et al.[1] presents a\nhigh performance batched GEMM kernel on K40c Graphics Processing Units (GPU) for both ﬁxed and variable\nsizes, proposing specialized GEMM designs and a comprehensive autotuning process to deliver state-of-the-art\nperformance.\n17\nB Addition Details on Algorithms\nB.1 Asynchrony Through Warp Specialization for the Backward Pass\nSimilar to the forward pass § 3.1, we use warp specialization to handle asynchrony. Instead of just a simple\nproducer-consumer pattern in the forward pass, we add one extra role of adQ writer, since we need to accumulate\nthe value ofdQ produced by each thread block to the global value ofdQ. This dQ accumulation introduces memory\ncontention (many thread blocks writing to the same location) so having a separate warp to handle this (along with\nasynchrony) will avoid blocking the rest of the warps in the thread block to perform the next computation (matmul).\nWe include the backward pass with warp specialization in Algorithm 3.\nAlgorithm 3FlashAttention-3 backward pass with warp specialization\nRequire: Matrices QKVOdO 2R𝑁\u0002𝑑 in HBM, logsumexp vector𝐿 2R𝑁 in HBM, block sizes𝐵𝑐, 𝐵𝑟.\n1: In a preprocessing kernel, compute𝐷 = rowsum¹dO \u000eOº2 R𝑑 (pointwise multiply), write𝐷 to HBM and divide\nit into𝑇𝑟 blocks 𝐷1𝐷 𝑇𝑟 of size𝐵𝑟 each.\n2: Divide Q into𝑇𝑟 =\nl\n𝑁\n𝐵𝑟\nm\nblocks Q1 Q𝑇𝑟 of size𝐵𝑟\u0002𝑑 each, and divideKV in to𝑇𝑐 =\nl\n𝑁\n𝐵𝑐\nm\nblocks K1 K𝑇𝑐\nand V1 V𝑇𝑐 , of size𝐵𝑐 \u0002𝑑 each.\n3: Divide dO into 𝑇𝑟 blocks dO𝑖 dO𝑇𝑟 of size𝐵𝑟 \u0002𝑑 each, and divide𝐿 into 𝑇𝑟 blocks 𝐿𝑖𝐿 𝑇𝑟 of size𝐵𝑟\neach.\n4: Initialize pipeline object to manage barrier synchronization with𝑠-stage circular SMEM buﬀer.\n5: if in producer warpgroupthen\n6: Deallocate predetermined number of registers.\n7: Issue loadK𝑗 and V𝑗 from HBM to shared memory.\n8: Upon completion, commit to notify consumer of the load ofK𝑗 and V𝑗.\n9: for 1 \u0014𝑖 \u0014𝑇𝑟 do\n10: Wait for the¹𝑖% 𝑠ºth stage of the buﬀer to be consumed.\n11: Issue loads ofQ𝑖dO𝑖 from HBM to shared memory at the¹𝑖% 𝑠ºth stage of the buﬀer.\n12: Upon completion, commit to notify consumers of the loads ofQ𝑖dO𝑖.\n13: end for\n14: else ifin consumer warpgroupsthen\n15: Reallocate predetermined number of registers as function of number of consumer warps.\n16: On-chip, InitializedK𝑗 = ¹0º𝐵𝑐\u0002𝑑dV𝑗 = ¹0º𝐵𝑐\u0002𝑑 .\n17: Wait forK𝑗 and V𝑗 to be loaded in shared memory.\n18: for 1 \u0014𝑖 \u0014𝑇𝑟 do\n19: Wait forQ𝑖 to be loaded in shared memory.\n20: Load 𝐿𝑖𝐷𝑖 from HBM to on-chip SRAM.\n21: On chip, computeS¹𝑗º\n𝑖 = Q𝑖K𝑇\n𝑗 2R𝐵𝑟 \u0002𝐵𝑐 (SS-GEMM). Commit.\n22: Wait fordO𝑖 to be loaded in shared memory.\n23: On chip, computedP¹𝑗º\n𝑖 = dO𝑖V>\n𝑗 2R𝐵𝑟 \u0002𝐵𝑐 (SS-GEMM). Commit.\n24: On chip, wait forS¹𝑗º\n𝑖 , then computeP¹𝑗º\n𝑖 = exp¹S𝑖𝑗 \u0000𝐿𝑖º2 R𝐵𝑟 \u0002𝐵𝑐 .\n25: On chip, wait fordP¹𝑗º\n𝑖 , then computedS¹𝑗º\n𝑖 = P¹𝑗º\n𝑖 \u000e¹dP¹𝑗º\n𝑖 \u0000𝐷𝑖º2 R𝐵𝑟 \u0002𝐵𝑐 .\n26: On chip, computedV𝑗  dV𝑗 ¸¹P¹𝑗º\n𝑖 º>dO𝑖 2R𝐵𝑐\u0002𝑑 (RS-GEMM). Commit.\n27: On chip, computedK𝑗  dK𝑗 ¸dS¹𝑗º\n𝑖\n>\nQ𝑖 2R𝐵𝑐\u0002𝑑 (RS-GEMM). Commit and wait for bothdV𝑗 and dK𝑗.\n28: On chip, computedQ¹localº\n𝑖 = dS¹𝑗º\n𝑖 K𝑗 2R𝐵𝑟 \u0002𝑑 (SS-GEMM), and writedQ¹localº\n𝑖 to smem. Notify the\ndQ-writer.\n29: end for\n30: else ifin dQ-writer warpthen\n31: for 1 \u0014𝑖 \u0014𝑇𝑟 do\n32: Wait fordQ¹localº\n𝑖 to be ready in smem.\n33: Using a semaphore, atomically adddQ¹localº\n𝑖 to dQ𝑖 in global memory.\n34: end for\n35: end if\n18\nB.2 2-Stage Pipelining SASS Analysis\nWe give simpliﬁed SASS code for the inside of the consumer warpgroup mainloop.\n// Compute row_max\nFMNMX.FTZ R0, R24, R6, !PT ;\nSHFL.BFLY PT, R185, R2, 0x2, 0x1f ;\n... FMNMX and SHFL.BFLY ...\n// Apply exp2 and row_sum. Rescale O.\nFMUL.FTZ R2, R4, UR9 ;\nMUFU.EX2 R185, R184 ;\nFFMA.FTZ R24, R24, UR9, -R6.reuse ;\nFADD.FTZ R24, R211, R24 ;\n... FMUL, FFMA, FMUL, MUFU.EX2, FADD ...\n// FP32 -> FP16 conversion are interleaved with exp2, row_sum and O rescaling.\nF2FP.F16.F32.PACK_AB R231, R25, R231 ;\n... F2FP, FMUL, MUFU, FFMA, FADD ...\n// Start the first WGMMA. Broken down into 8 HGMMAs.\n// The first 7 HGMMAs are packed together.\nWARPGROUP.ARRIVE ;\nHGMMA.64x192x16.F32 R24, gdesc[UR44], RZ, !UPT ;\n... HGMMA x 6 ...\n// FP32->FP16, exp2, row_sum, O rescaling are interleaved with HGMMA.\nF2FP.F16.F32.PACK_AB R214, R214, R187 ;\nMUFU.EX2 R234, R5 ;\nFADD.FTZ R237, R187, R2 ;\n... F2FP, MUFU, FADD ...\n// The last HGMMA is issued here. No need to wait.\nHGMMA.64x192x16.F32 R24, gdesc[UR44], R24, gsb0 ;\n// Start the second WGMMA. Broken down into 12 HGMMAs.\n// All 12 HGMMAs are packed together. Not interleaved with other instructions.\nWARPGROUP.ARRIVE ;\nHGMMA.64x128x16.F32 R120, R228, gdesc[UR8].tnspB, R120 ;\n... HGMMA x 10 ...\nHGMMA.64x128x16.F32 R120, R184, gdesc[UR8].tnspB, R120, gsb0 ;\n// wgmma.wait_group at the end.\nWARPGROUP.DEPBAR.LE gsb0, 0x0 ;\nWe make the following observations:\n1. Softmax is reordered to the very beginning, even before the ﬁrst WGMMA.\n2. The ﬁrst WGMMA is interleaved with softmax and FP32!FP16 datatype conversion ofS. This indicates\nthat WGMMA and non-WGMMAs are executed in parallel.\n3. exp2, row\\_sum, O rescaling and FP32!FP16 conversions are interleaved together.\n4. The second WGMMA is not overlapped with other instructions, as expected.\nOverall, SASS shows that the 2-stage pipelining idea works as expected.\n19\nB.3 3-Stage Pipelining Algorithm\nWe experiment with a 3-stage pipelining algorithm to parallelize the ﬁrst WGMMA from iteration𝑗¸2, softmax\nfrom iteration 𝑗¸1, and the second WGMMA from iteration𝑗. We describe this algorithm in Algorithm 4. This\nalgorithm behaves worse than the 2-stage pipelining algorithm due to the reasons below:\nFigure 8: 3-Stage Pipelining\nAlgorithm 4FlashAttention 3-stage pipelining consumer warpgroup forward pass\nRequire: Matrices QKV 2R𝑁\u0002𝑑 in HBM, block sizes𝐵𝑐, 𝐵𝑟. Each warpgroup reads 1 block Qi of size𝐵𝑟 \u0002𝑑,\n𝑇𝑐 =\nl\n𝑁\n𝐵𝑐\nm\nblocks K1 K𝑇𝑐 and V1 V𝑇𝑐 of size𝐵𝑐 \u0002𝑑. Each warpgroup writes 1 output blockO𝑖 of size\n𝐵𝑟 \u0002𝑑, and 1 logsumexp block𝐿𝑖 of size𝐵𝑟.\n1: Initialization. Load Q𝑖 from HBM to on-chip SRAM. InitializeO𝑖ℓ𝑖𝑚𝑖𝑠𝑐𝑎𝑙𝑒 _𝑜.\n2: Wait for the producer warpgroup loadingK0 from HBM to on-chip SRAM.\n3: Compute S = Q𝑖K𝑇\n0 using WGMMA. Commit and wait.\n4: Compute 𝑚𝑖, ~P𝑖, ℓ𝑖, 𝑠𝑐𝑎𝑙𝑒_𝑜 based onS.\n5: Wait for the producer warpgroup loadingK1 from HBM to on-chip SRAM.\n6: Compute S = Q𝑖K𝑇\n1 using WGMMA. Commit and wait.\n7: for 2 \u0014𝑗 𝑇𝑐 \u00002 do\n8: Wait for the producer warpgroup loadingK𝑗 from HBM to on-chip SRAM.\n9: Compute S_𝑛𝑒𝑥𝑡 = Q𝑖K𝑇\n𝑗 using WGMMA. Commit but do not wait.\n10: Wait for the producer warpgroup loadingV𝑗\u00002 from HBM to on-chip SRAM.\n11: Rescale O𝑖 based on𝑠𝑐𝑎𝑙𝑒_𝑜.\n12: Compute O𝑖 = O𝑖 ¸~P𝑖V𝑗\u00002 using WGMMA. Commit but do not wait.\n13: Compute 𝑚𝑖, ~P𝑖_𝑛𝑒𝑥𝑡, ℓ𝑖, 𝑠𝑐𝑎𝑙𝑒_𝑜 based onS.\n14: Wait for all previous WGMMAs.\n15: Copy S_𝑛𝑒𝑥𝑡 to S.\n16: Copy ~P𝑖_𝑛𝑒𝑥𝑡 to ~P𝑖.\n17: end for\n18: Wait for the producer warpgroup loadingV𝑇𝑐\u00002 from HBM to on-chip SRAM.\n19: Rescale O𝑖 based on𝑠𝑐𝑎𝑙𝑒_𝑜.\n20: Compute O𝑖 = O𝑖 ¸~P𝑖V𝑇𝑐\u00002 using WGMMA. Commit and wait.\n21: Compute 𝑚𝑖, ~P𝑖, ℓ𝑖, 𝑠𝑐𝑎𝑙𝑒_𝑜 based onS.\n22: Wait for the producer warpgroup loadingV𝑇𝑐\u00001 from HBM to on-chip SRAM.\n23: Rescale O𝑖 based on𝑠𝑐𝑎𝑙𝑒_𝑜.\n24: Compute O𝑖 = O𝑖 ¸~P𝑖V𝑇𝑐\u00001 using WGMMA. Commit and wait.\n25: Epilogue. Rescale O𝑖 based onℓ𝑖. Compute 𝐿𝑖 based onℓ𝑖 and 𝑚𝑖. WriteO𝑖 and 𝐿𝑖 to HBM as the𝑖-th block\nof O and 𝐿.\nOverlapping. We expected that softmax can be overlapped with (the ﬁrst WGMMA + the second WGMMA).\nHowever, the compiler doesn’t cooperate in this way. SASS code shows that only the ﬁrst WGMMA is overlapped\nwith softmax, while the second WGMMA is not. It’s not clear why the compiler chooses to reorder instructions in\nthis way.\n20\nRegister pressure. This algorithm requires more registers compared to the 2-stage pipelining algorithm. In theory,\nit needs to store an extra~P𝑖 and 𝑠𝑐𝑎𝑙𝑒_𝑜, which is of size𝐵𝑟 \u0002𝐵𝑐 \u0002sizeof¹input_data_typeº¸ 𝐵𝑟 \u0002sizeof¹ﬂoatº.\nAs a result, a smaller block size needs to be chosen.\nC Addition Details on Experiments and Benchmarking\nC.1 System and libraries\nWe benchmark the speed on an H100 80GB SXM5 (700W). We generally use the latest versions of the libraries, at\nthe time of writing (May 2024). Speciﬁcally, we use:\n• CUDA 12.3\n• cuDNN 9.1.1.17\n• CUTLASS 3.5\n• FlashAttention 2.5.8\n• Triton nightly 3.0.0.post20240424212437\n• PyTorch 2.3.0\nTo reduce variability, we ﬁx the GPU clock speed to 1830MHz (clock speed used to calculate the 989 TFLOPS\nFP16 theoretical max throughput). We repeat the benchmarks 100 times and take the average timing.\nC.2 FP8 Attention Full Results\nWe use following sequence lengths: 512, 1024, 2048, 4224, 8448, 16896. When sequence length\u00154k, we make it also\ndivisible by 132 (number of SMs in H100 SXM5) to avoid wave quantization.\n21\n512 1k 2k 4k 8k 16k\nSequence length\n400\n800\n1200Speed (TFLOPs/s)\n392\n444\n473 499 506 511\n344\n398\n447\n413 431 438\n240\n396\n462\n568 596 613\nAttention forward speed, head dim 64 (H100 80GB SXM5)\nTriton\ncuDNN\nFlashAttention-3\n(a) Forward, without causal mask, head dim 64\n512 1k 2k 4k 8k 16k\nSequence length\n400\n800\n1200Speed (TFLOPs/s)\n234\n325\n393\n440 459 481\n194\n258\n317 324\n464 483\n164\n244\n369\n475\n533\n572\nAttention forward speed, head dim 64 (H100 80GB SXM5)\nTriton\ncuDNN\nFlashAttention-3 (b) Forward, with causal mask, head dim 64\n512 1k 2k 4k 8k 16k\nSequence length\n400\n800\n1200Speed (TFLOPs/s)\n408\n502\n563\n605 630 635617\n751\n886 864\n971\n1003\n348\n596\n733\n918\n974\n1008\nAttention forward speed, head dim 128 (H100 80GB SXM5)\nTriton\ncuDNN\nFlashAttention-3\n(c) Forward, without causal mask, head dim 128\n512 1k 2k 4k 8k 16k\nSequence length\n400\n800\n1200Speed (TFLOPs/s)241\n340\n413\n464 483 510\n253\n384\n528\n719\n883\n922\n241\n367\n553\n716\n815\n881\nAttention forward speed, head dim 128 (H100 80GB SXM5)\nTriton\ncuDNN\nFlashAttention-3 (d) Forward, with causal mask, head dim 128\n512 1k 2k 4k 8k 16k\nSequence length\n400\n800\n1200Speed (TFLOPs/s)\n529\n664\n766\n854\n897 903\n686\n878\n1001\n1087\n1122 1139\n510\n744\n931\n966\n1151 1171\nAttention forward speed, head dim 256 (H100 80GB SXM5)\nTriton\ncuDNN\nFlashAttention-3\n(e) Forward, without causal mask, head dim 256\n512 1k 2k 4k 8k 16k\nSequence length\n400\n800\n1200Speed (TFLOPs/s)\n299\n425\n520\n591\n628\n663\n304\n449\n768\n1015\n1056\n1099\n329\n521\n703\n856\n960\n1024\nAttention forward speed, head dim 256 (H100 80GB SXM5)\nTriton\ncuDNN\nFlashAttention-3 (f) Forward, with causal mask, head dim 256\nFigure 9: Attention forward speed (FP8) on H100 GPU\n22"
  },
  {
    "source": "2406.07551v1.pdf",
    "content": "Blur-aware Spatio-temporal Sparse Transformer for Video Deblurring\nHuicong Zhang1, Haozhe Xie2, Hongxun Yao 1 \u0000\n1 Harbin Institute of Technology 2 S-Lab, Nanyang Technological University\nhttps://vilab.hit.edu.cn/projects/bsstnet\nInput\n VRT\n RVRT\n Shift- Net+\n BSSTNet (Ours)\n GT\n(a) Blur Map Generation\nBlur frame Blur map\nOptical flows\n(b) FLOPs Comparison (d) Blur- aware Spatio-temporal Sparse Transformer\nAttention\nK/V\nQ\nK/V/Q\n(c) Standard Spatio-temporal Transformer\nAttention\n(e) Standard F low-guided Feature  Alignment\nFlow-guided \nDCN\n(f) Blur- aware Feature Alignment\n…\n…\n(g) Visual C omparisons on the GoPro dataset\nFlow-guided \nDCN\n10 0\n15 0\n20 0\n25 0\n30 0\n35 0\n40 0\n12 24 36 48 60\nFLOPs\nTemporal Window Length\n…\n…\n…\n…\nStd. Transformer\nOurs\nOrdered by blur levels\nTop 50%\nBtm . 50%\n…\n…\nFigure 1. (a) Large motions in optical flows are highlighted in blur maps. (b) Comparison of FLOPs between the standard spatio-temporal\ntransformer and the blur-aware spatio-temporal transformer. (c-d) Summary of the standard spatio-temporal transformer and the blur-aware\nspatio-temporal transformer. (e-f) Summary of the standard flow-guided feature alignment and blur-aware feature alignment. (g) In the\nvisual comparisons on the GoPro dataset, the proposed BSSTNet restores the sharpest frame.\nAbstract\nVideo deblurring relies on leveraging information from\nother frames in the video sequence to restore the blurred re-\ngions in the current frame. Mainstream approaches employ\nbidirectional feature propagation, spatio-temporal trans-\nformers, or a combination of both to extract information\nfrom the video sequence. However, limitations in mem-\nory and computational resources constraints the temporal\nwindow length of the spatio-temporal transformer, prevent-\ning the extraction of longer temporal contextual informa-\ntion from the video sequence. Additionally, bidirectional\nfeature propagation is highly sensitive to inaccurate op-\ntical flow in blurry frames, leading to error accumula-\n\u0000 Corresponding author: h.yao@hit.edu.cn\ntion during the propagation process. To address these is-\nsues, we propose BSSTNet, Blur-aware Spatio-temporal\nSparse Transformer Network. It introduces the blur map,\nwhich converts the originally dense attention into a sparse\nform, enabling a more extensive utilization of information\nthroughout the entire video sequence. Specifically, BSSTNet\n(1) uses a longer temporal window in the transformer, lever-\naging information from more distant frames to restore the\nblurry pixels in the current frame. (2) introduces bidirec-\ntional feature propagation guided by blur maps, which re-\nduces error accumulation caused by the blur frame. The ex-\nperimental results demonstrate the proposed BSSTNet out-\nperforms the state-of-the-art methods on the GoPro and\nDVD datasets.\narXiv:2406.07551v1  [cs.CV]  11 Jun 2024\n1. Introduction\nVideo deblurring aims to recover clear videos from blurry\ninputs, and it finds wide applications in many subsequent\nvision tasks, including tracking [5, 16], video stabiliza-\ntion [15], and SLAM [8]. Therefore, it is of great interest\nto develop an effective algorithm to deblur videos for above\nmentioned high-level vision tasks.\nVideo deblurring presents a significant challenge, as it\nnecessitates the extraction of pertinent information from\nother frames within the video sequence to restore the blurry\nframe. In recent years, there have been noteworthy ad-\nvancements [1, 4, 11–13, 24] in addressing this challenge.\nFlow-guided bidirectional propagation methods [1, 4, 12,\n13, 24] employ flow-guided deformable convolution and\nflow-guided attention for feature alignment. However, inac-\ncurate optical flow in blurry frames causes the introduction\nof blurry pixels during bidirectional propagation. VRT and\nRVRT [11, 12] use spatio-temporal self-attention with tem-\nporal window to fuse the information from video sequence.\nDue to the high memory demand of self-attention, these\napproaches frequently feature restricted temporal windows,\nlimiting their ability to incorporate information from distant\nsections of the video.\nAnalyzing videos afflicted by motion blur reveals a cor-\nrespondence between the blurry regions in the video and ar-\neas with pixel displacement, where the degree of blurriness\nis directly associated with the magnitude of pixel displace-\nment. Moreover, The blurry regions are typically less fre-\nquent in both the temporal and spatial aspects of the blurry\nvideos. By leveraging the sparsity of blurry regions, the\ncomputation of the spatio-temporal transformer can focus\nsolely on these areas, thereby extending the temporal win-\ndow to encompass longer video clips. Moreover, bidirec-\ntional feature propagation based on blurry regions enables\nthe minimization of error accumulation. As shown in Fig-\nure 1a, the green box area represents the blurry region in\nthe frame. Similarly, both the forward and backward optical\nflows in the same location are also maximized, indicating a\ncorrelation between the motion and blurry regions.\nBy introducing blur maps, we propose BSSTNet, Blur-\naware Spatio-temporal Sparse Transformer Network. Com-\npared to methods based on spatio-temporal transformer,\nBSSTNet introduces Blur-aware Spatio-temporal Sparse\nTransformer (BSST) and Blur-aware Bidirectional Feature\nPropagation (BBFP). The proposed BSST efficiently uti-\nlizes a long temporal window by applying sparsity on input\ntokens in the spatio-temporal domain based on blur maps.\nThis enables the incorporation of distant information in the\nvideo sequence while still maintaining computational ef-\nficiency. BBFP introduces guidance from blur maps and\nchecks for flow consistency beforehand. This aids in mini-\nmizing the introduction of blurry pixels during bidirectional\npropagation, ultimately enhancing the ability to gather in-\nformation from the adjacent frames.\nThe contributions are summarized as follows.\n• We propose a non-learnable, parameter-free method for\nestimating the blur map of video frames. The blur map\nprovides crucial prior information on motion-blurry re-\ngions in the video, enabling sparsity in the transformer\nand error correction during bidirectional propagation.\n• We propose BSSTNet, comprising two major compo-\nnents: BSST and BBFP. BSST incorporates spatio-\ntemporal sparse attention to leverage distant information\nin the video sequence while still achieving high perfor-\nmance. BBFP corrects errors in the propagation process\nand boosts its capability to aggregate information from\nthe video sequence.\n• We quantitatively and qualitatively evaluate BSSTNet on\nthe DVD and GoPro datasets. The experimental results\nindicate that BSSTNet performs favorably against state-\nof-the-art methods.\n2. Related Work\nMany methods in video deblurring have achieved impres-\nsive performances. The video deblur methods can be cate-\ngorized into two categories:\nRNN-based Methods. On the other hand, some re-\nsearchers [6, 18, 20, 23, 25, 26] are focusing on the\nRNN-base methods. STRCNN [6] adopts a recurrent neu-\nral network to fuse the concatenation of multi-frame fea-\ntures. RDN [23] develops a recurrent network to recurrently\nuse features from the previous frame at multiple scales.\nIFRNN [18] adopts an iterative recurrent neural network\n(RNN) for video deblurring. STFAN [26] uses dynamic\nfilters to align consecutive frames. PVDNet [20] contains\na pre-trained blur-invariant flow estimator and a pixel vol-\nume module. To aggregate video frame information, ES-\nTRNN [25] employs a GSA module in the recurrent net-\nwork. Recently, the BiRNN-based method [1, 4, 13, 24, 28]\nhas achieved impressive deblur results through aggressive\nbidirectional propagation. BasicVSR ++[1] adopts aggres-\nsive bidirectional propagation. Based on BasicVSR ++,\nRNN-MBP[28] introduces the multi-scale bidirectional re-\ncurrent neural network for video deblurring. STDANet [24]\nand FGST [13] employ the flow-guided attention to align\nand fuse the information of adjacent frames. However, due\nto error accumulation, these methods do not effectively fuse\nthe information from long-term frames. Ji and Yao [4] de-\nvelop a Memory-Based network, which contains a multi-\nscale bidirectional recurrent neural network and a memory\nbranch. However, the memory branch introduces a large\nsearch space of global attention and ineffective alignment.\nTransformer-based Methods.The Spatio-temporal trans-\nformer is widely used in video deblurring [11, 12].\nVRT [11] utilizes spatio-temporal self-attention mechanism\nto integrate information across video frames. Due to the\nBBFP (Sec 3.3)\n… 𝑁× BSST (Sec 3.4)\n……\nInput FramesBlur Map  Estimator (Sec 3.2)\nBFA\nBFA\nBFA\nBFA\nBFA\nBFA\n……\nOutput Frames\nBack. FlowsFwd. Flows\nBlur Maps\n…\n… …\n………\n……… …\nBSST\nBSST…Encoder Decoder\nImage FeaturesForward FlowBackward FlowBlur Maps\nSkip Connections\nFlowEstimator\nFigure 2. Overview of the proposed BSSTNet.BSSTNet consists of three major components: Blur Map Estimation, Blur-aware Bidirec-\ntional Feature Propagation (BBFP), and Blur-aware Spatio-temporal Sparse Transformer (BSST).\ncomputational complexity of self-attention, VRT employs a\n2-frame temporal window size and utilizes a shifted window\nmechanism for cross-window connections. However, the\nindirect connection approach with a small window size fails\nto fully exploit long-range information within the video se-\nquence. RVRT [12] divides the video sequence into 2-\nframe clips, employing small-window spatio-temporal self-\nattention within each clip and Flow-guided biderectional\npropagation and alignment between clips. However, due\nto the small window constraint of spatio-temporal self-\nattention and the error accumulation caused by the optical\nflow of blurred frames in Flow-guided biderectional propa-\ngation, RVRT still falls short of fully utilizing the informa-\ntion from the entire video sequence.\n3. Our Approach\n3.1. Overview\nAs shown in Figure 2, the BSSTNet contains three key\ncomponents: Blur Map Estimation, Blur-aware Bidirec-\ntional Feature Propagation (BBFP), and Blur-aware Spatio-\ntemporal Sparse Transformer (BSST). First, the forward\nand backward optical flows, denoted as {Ot+1→t}T−1\nt=1 and\n{Ot→t+1}T−1\nt=1 , are estimated from the downsampled video\nsequence ˆX = { ˆXt}T\nt=1. Then, Blur Map Estimation gen-\nerates the blur maps B = {Bt}T\nt=1 for each frame are gen-\nerated based on {Ot+1→t}T−1\nt=1 and {Ot→t+1}T−1\nt=1 . Next,\nBBFP produces the aggregated features ˆF using Blur-aware\nFeature Alignment (BFA). After that, BSST generates the\nrefined features F from ˆF with the Blur-aware Sparse\nSpatio-temporal Attention (BSSA) layers. Finally, the de-\ncoder reconstructs the sharp video sequence R = {Rt}T\nt=1.\n3.2. Blur Map Estimation\nGiven the optical flows {Ot+1→t}T−1\nt=1 and {Ot→t+1}T−1\nt=1 ,\nthe unnormalized blur maps ˆB = {ˆBt}T\nt=1 can be obtained\nas follows\nˆBt =\n2X\ni=1\n((Ot→t+1)2\ni + (Ot→t−1)2\ni ) (1)\nSpecially, we define O1→0 = 0 and OT→T+1 = 0. The\nblur map B and sharp map A can be generated as follows\nBt =\nˆBt − min( ˆB)\nmax( ˆB) − min( ˆB)\nAt = 1− Bt (2)\nwhere i and t index the channel of optical flows and the time\nsteps, respectively.\n3.3. Blur-aware Bidirectional Feature Propagation\nWithin BBFP, bidirectional feature propagation propagates\nthe aggregated features ˆF in both the forward and back-\nward directions, incorporating Blur-aware Feature Align-\nment (BFA). BFA is designed to align features from neigh-\nboring frames to reconstruct the current frame. As shown\nin Figure 1e and Figure 1f, the standard flow-guided fea-\nture alignment aligns all pixels in the neighboring frames,\nwhereas BFA selectively integrates information from sharp\npixels guided by blur maps. This prevents the propagation\nof blurry regions from the features of neighboring frames\nduring bidirectional feature propagation.\nBidirectional Feature Propagation.Assuming the current\ntime step is thet-th step, and the corresponding propagation\nDCN Conditions\n𝐎!→!#$\n𝐎!→!#%\n𝐀!#$\n𝐀!#%\nDCN Masks\n!𝐅!\"#$ !𝐅!$\"#\n!𝐅!\"%$ W((𝐅!\"#$)\nWW((𝐅!\"%$)\nW\nConv. Layers\nDCN Offsets!𝐅!$ Conv. Layers\nC\nC\nDeformable Conv. Layers\nC C C\nFigure 3. The details of BFA.Note that W⃝, C⃝, and Ldenotes the\n“Warp”, “Concatenation”, and “Element-wise Add” operations,\nrespectively.\nbranch is the j-th branch, the generation of the current time\nstep aggregated feature ˆFj\nt can be obtained as\nˆFj\nt = BFA(ˆFj−1\nt , ˆFj\nt−1, ˆFj\nt−2,\nW(ˆFj\nt−1, Ot→t−1), W(ˆFj\nt−2, Ot→t−2),\nOt→t−1, Ot→t−2, At−1, At−2) (3)\nwhere BFA and W denote the “BFA” and “Backward Warp”\noperations, resepectively. ˆFj−1\nt represents the feature ag-\ngregated from the t-th time step in the (j − 1)-th branch.\nˆFj\nt−1 and ˆFj\nt−2 are the features generated from the previ-\nous and the second previous time step. The aforementioned\nprocess progresses forward through the time steps until it\nreaches t = T. The backward propagation process mirrors\nthe forward propagation process.\nBlur-aware Feature Alignment.Different from the stan-\ndard flow-guided feature alignment [1] that aligns all pix-\nels in neighboring frames, BFA introduces sharp maps to\nprevent the introduction of blurry pixels in the neighboring\nframes. As illustrated in Figure 3, along with features ˆFj\nt−1\nand ˆFj\nt−2 from previous time steps, the corresponding op-\ntical flows Ot→t−1 and Ot→t−2, and the warped features\nW(ˆFj\nt−1) and W(ˆFj\nt−2), sharp maps At−1 and At−2 are\nadditionally introduced. These sharp maps serve as addi-\ntional conditions to generate the offsets and masks of the de-\nformable convolution layers [3]. Moreover, the sharp map\nacts as a base mask for DCN by being added to the DCN\nmask. This ensures that only sharp regions of features are\npropagated.\nSpatial SparseinQuerySpace\nTemporal SparseinQuerySpace\nTemporal Sparse in Key/Value Space\nSpatial SparseinKey/ValueSpace\n𝐠!,𝐠\"\nOrder by Blur LevelsOrderbyBlurLevels\n…\nMulti-head Self Attention\nK/V TokensQuery Tokens… … …\nFeatures 𝓕\"\nBlur Maps 𝓑\n……\n…\n…Global FeaturesLocal Features\nFlatten by WindowFlatten by WindowSelect Top 50%Select Bottom 50%\n……\n𝐘%!, 𝐘%\"𝐘%#\n𝐄$ 𝐄𝒌, 𝐄𝒗\n𝐈$ 𝐈𝒌, 𝐈𝒗,𝐠𝒌, 𝐠𝒗𝐄!𝐄\"\n𝐄$, 𝐄𝒌, 𝐄𝒗\n𝐄#\nPP\n𝐔𝐔 𝐔\n𝐔 𝐔\nFigure 4. The details of BSST.Note that P⃝ denotes the “Window\nPartition” operation. “Flatten by window” indicates that query to-\nkens are flattened for each query window, and K/V tokens are gen-\nerated in a similar manner. Multi-head Self Attention is also com-\nputed on the query and K/V tokens generated for each window.\n3.4. Blur-aware Spatio-temporal Sparse Trans-\nformer\nThe spatio-temporal attention is commonly employed in\nvideo deblurring and demonstrates remarkable perfor-\nmance, as shown in Figure 1c. However, the standard\nspatio-temporal attention method often restricts its temporal\nwindow size due to computational complexity, thereby con-\nstraining its capability to capture information from distant\nparts of the video sequence. To overcome this limitation,\nwe introduce the Blur-aware Spatio-temporal Sparse Trans-\nformer (BSST). As illustrated in Figure 1d, BSST filters out\nunnecessary and redundant tokens in the spatio and tempo-\nral domain according to blur maps B. As shown in Fig-\nure 1b, allowing BSST to include a larger temporal window\nwhile maintaining computational efficiency. The detailed\nimplementation of BSST is illustrated in Figure 4.\nGiven the aggregated features ˆF = {ˆFt ∈\nRH/4×W/4×C}T\nt=1 from the last branch of BBFP,\nwe employ a soft split operation [14] to divide each ag-\ngregated feature into overlapping patches of sizep × p with\na stride of s. The split features are then concatenated, gen-\nerating the patch embeddings z ∈ RT×M×N×p2C. Next,\nthe blur map B are downsampled by average pooling with\na kernel size of p × p and a stride of s, resulting in B↓ ∈\nRT×M×N×p2C. For simplicity,p2C is denoted asCz. After\nthat, z is fed to three separate linear layer transformations,\nresulting in ˜zq ∈ RT×M×N×Cz , ˜zk ∈ RT×M×N×Cz , and\n˜zv ∈ RT×M×N×Cz , where M, N, and Cz respectively de-\nnote the number of patches in the height and width domains,\nand the number of channels. Subsequently, ˜zq, ˜zk, ˜zv are\npartitioned into m × n non-overlapping windows, generat-\ning partitioned features Eq, Gk, Gv ∈ RT×m×n×h×w×Cz ,\nwhere m×n and h×w are the number and size of the win-\ndows, respectively. Utilizing the embedding z and incor-\nporating depth-wise convolution, the generation of pooled\nglobal tokens gk and gv takes place as follows\ngk = lk(DC(z))\ngv = lv(DC(z)) (4)\nwhere DC represents depth-wise convolution, and gk, gv ∈\nRT×hp×wp×Cz . Following that, we repeat and concate-\nnate gk with Gk and gv with Gv, resulting in Ek, Ev ∈\nRT×m×n×(h+hp)×(w+wp)×Cz . Note that for the key/value\nwindows, we enlarge its window size to enhance the recep-\ntive field of key/value [14, 27]. For simplicity, we ignore it\nin the following discussion.\nSpatial Sparse in Query/Key/Value Spaces.We observe\nthat the blurry regions are typically less frequent in both\nthe temporal and spatial aspects of the blurred videos. Mo-\ntivated by this observation, we only choose the tokens of\nblurry windows in Eq and tokens of sharp windows in\nEk, Ev to participate in the computation of spatio-temporal\nattention. This ensures that the spatio-temporal attention\nmechanism focuses solely on restoring the blurred regions\nby the utilization of sharp regions in video sequences. First,\nthe blur maps of windows U ∈ RT×m×n are generated by\ndownsampling B↓ ∈ RT×M×N using max pooling. Next\n, the spatial sparse mask of windows is obtained as follows\nQt,i,j =\n\n\n\n1, if Ut,i,j ≥ θ,\n∀t ∈ [1, T], i∈ [1, m], j∈ [1, n]\n0, otherwise\nS = Clip\n\u0010XT\nt=1\nQt, 1\n\u0011\n(5)\nwhere θ, Clip, S ∈ Rm×n are the threshold for considering\nrelated windows as blurry windows, a clipping function that\nset S to 1 if PT\nt=1 Qt > 0, and the spatial sparse mask\nfor Eq, Ek and Ev, respectively. Then, the spatial sparse\nembedding features Iq, Ik, and Iv are generated using the\nfollowing equations\nIq = Concat({Eq\nt,i,j | Si,j = 1, i∈ [1, m], j∈ [1, n]}T\nt=1)\nIk = Concat({Ek\nt,i,j | Si,j = 1, i∈ [1, m], j∈ [1, n]}T\nt=1)\nIv = Concat({Ev\nt,i,j | Si,j = 1, i∈ [1, m], j∈ [1, n]}T\nt=1)\n(6)\nwhere Concat denotes the “Concatenation” operation. If\nSi,j = 0, it indicates that the window’s position indexed\nby (i, j) in the video sequence does not encompass blurry\ntokens. This allows us to exclude the tokens within those\nwindows from the spatio-temporal attention mechanism.\nIq ∈ RT×msns×hw×Cz , while both Ik and Iv share the\nsize of RT×msns×(h+hp)(w+wp)×Cz , where ms and ns rep-\nresenting the number of selected windows in m and n do-\nmains, respectively.\nTemporal Sparse in Query Space.Along the temporal do-\nmain, we choose the windows of the blurry region for query\nspace, ensuring that the spatio-temporal attention mecha-\nnism is dedicated to restoring only the blurry regions of the\nvideo sequence. Given the spatial sparse embedding fea-\ntures Iq, the spatio-temporal sparse embedding yq is gener-\nated as follows\nHq = {Iq\nt,i,j | Ut,i,j ≥ Top(Kq, Ui,j),\ni ∈ [1, ms], j∈ [1, ns]}T\nt=1\nYq = Concat(Hq) (7)\nwhere Yq ∈ RKq×msns×hw×Cz . Top(Kq, ·) represents the\noperation of finding the Kq-th largest element in a vector.\nFor each window located at position (i, j) in Iq, within the\ntemporal domain, we selectively chose the topKq windows\nwith the highest blur levels for deblurring.\nTemporal Sparse in Key/Value Spaces. In contrast to\nthe query space, we select the sharp regions in Ik, Iv for\nkey/value spaces. Due to the high similarity in textures\nbetween adjacent frames, we alternately choose temporal\nframes with a stride of 2 in each BSST. In BSSTNet, con-\nsisting of multiple BSSTs, odd-numbered BSSTs select\nframes with odd numbers, while even-numbered BSSTs\nchoose frames with even numbers, resulting in a 50% re-\nduction in the size of the key/value space. Given the spatial\nsparse embedding features Ik and Iv, the spatio-temporal\nsparse embedding features yk and yv are generated as fol-\nlows\nHk = {Ik\nt,i,j | Ut,i,j ≥ Top(Kkv, 1 − Ui,j),\nt mod 2 = 0, i∈ [1, ms], j∈ [1, ns]}T\nt=1\nyk = Concat(Hk)\nHv = {Iv\nt,i,j | Ut,i,j ≥ Top(Kkv, 1 − Ui,j),\nt mod 2 = 0, i∈ [1, ms], j∈ [1, ns]}T\nt=1\nyv = Concat(Hv) (8)\nwhere yk, yv ∈ RKkv×msns×(h+hp)(w+wp)×Cz .\nSpatio-temporal Sparse Attention. The spatio-temporal\nsparse query embedding yq is reshaped into ˆYq ∈\nRmsns×Kqhw×Cz . Similarly, The spatio-temporal sparse\nkey/value embedding yk and yv are each reshaped\ninto ˆYk ∈ Rmsns×Kkv(h+hp)(w+wp)×Cz and ˆYv ∈\nTable 1. Quantitative comparisons on the GoPro dataset.The best results are highlighted in bold.\nMethod STFAN [26] STDAN [24] RNN-MBP [28] NAFNet [2] VRT [11] RVRT [12] Shift-Net+ [10] BSSTNet\nPSNR 28.69 32.62 33.32 33.69 34.81 34.92 35.88 35.98\nSSIM 0.8610 0.9375 0.9627 0.9670 0.9724 0.9738 0.9790 0.9792\nTable 2. Quantitative comparisons on the DVD dataset.The best results are highlighted in bold.\nMethod STFAN [26] ARV o [9] RNN-MBP [28] STDAN [24] VRT [11] RVRT [12] Shift-Net+ [10] BSSTNet\nPSNR 31.24 32.80 32.49 33.05 34.27 34.30 34.69 34.95\nSSIM 0.9340 0.9352 0.9568 0.9374 0.9651 0.9655 0.9690 0.9703\nRmsns×Kkv(h+hp)(w+wp)×Cz , respectively. For each win-\ndow in msns, the self-attention is calculated as follows:\nAttention( ˆYq, ˆYk, ˆYv) = Softmax\n ˆYq ˆYT\nk√Cz\n!\nˆYv (9)\nIn BSST, the multi-head self-attention is introduced to ob-\ntain the output embedding zs ∈ Rms×ns×Kqhw×Cz .\nzs = MSA(ˆYq, ˆYk, ˆYv) (10)\nwhere MSA is the “Multi head Self-Attention” function.\nAfter applying our sparse strategy to eliminate unnecessary\nand redundant windows, we use self-attention following\nEq. 9 on the remaining windows to extract fused features.\nSpecially, standard window spatio-temporal attention is ap-\nplied to unselected (less blurry) windows, allowing features\nto be restored to their original size. Subsequently, these fea-\ntures are gathered through a soft composition operation [14]\nto serve as the input for the next BSST. The output of the\nfinal BSST is denoted as F.\n4. Experiments\n4.1. Datasets\nDVD. The DVD dataset [21] comprises 71 videos, consist-\ning of 6,708 blurry-sharp pairs. These are divided into 61\ntraining videos, amounting to 5,708 pairs, and 10 testing\nvideos with 1,000 pairs.\nGoPro. The GoPro dataset [17] consists of 3,214 pairs\nof blurry and sharp images at a resolution of 1280 ×720.\nSpecifically, 2,103 pairs are allocated for training, while\n1,111 pairs are designated for testing.\n4.2. Implementation Details\nTraining Details The network is implemented with Py-\nTorch [19] . The training is conducted with a batch size of 8\non 8 NVIDIA A100 GPUs, and the initial learning rate is set\nThe source code is available at https : / / github . com /\nhuicongzhang/BSSTNet\nTable 3. The comparison of FLOPs and runtime on the DVD\ndataset. The top two results are marked in bold and underlined.\nNote that FLOPs and runtime are computed for a single frame with\na resolution of 256 × 256.\nMethod RVRT [12] Shift-Net+ [10] BSSTNet\nPSNR 34.30 34.69 34.95\nSSIM 0.9655 0.9690 0.9703\nGFLOPs 88.8 146 133\nRuntime (ms) 23 45 28\nto 4 × 10−4. The network is optimized with L1 loss using\nAdam optimizer [7], where β1 = 0.9 and β2 = 0.999. The\nflow estimator in BSSTNet uses pre-trained weights from\nthe official RAFT [22] release and remains fixed during\ntraining. During testing, T, Kq, and Kkv are set to 48, 24,\nand 24, respectively. During training, they are 24, 12, and\n12, respectively. In the training phase, input images are ran-\ndomly cropped into patches with resolutions of 256 × 256,\nalong with the application of random flipping and rotation.\nHyperparameters To strike a better balance between video\ndeblurring quality and computational efficiency, the value\nof θ is set to 0.3. The patch size p and stride z are set to 4\nand 2, respectively.\n4.3. Main Results\nDVD. The quantitative results on the DVD dataset are\nshown in Table 2. The proposed method demonstrates su-\nperior performance in terms of both PSNR and SSIM com-\npared to existing state-of-the-art methods. Specifically, in\ncomparison to the best-performing state-of-the-art method,\nShift-Net+, the proposed BSSTNet achieves an improve-\nment of 0.26 dBin PSNR and 0.0013 in SSIM. Examples\nfrom the DVD dataset are presented in Figure 5a, demon-\nstrating that the proposed method generates images with in-\ncreased sharpness and richer visual details. This highlights\nthe robustness of the method in eliminating large blur in dy-\nnamic scenes.\nGoPro. In Table 1, the proposed BSSTNet shows favor-\nable performance in terms of both PSNR and SSIM when\nInput\n VRT\n RVRT\nShift-Net+\n BSSTNet\nGT\n(a) Qualitative comparison on the DVD dataset\nInput VRT RVRT\nGTBSSTNetShift-Net+\n(b) Qualitative comparison on the GoPro dataset\nFigure 5. Qualitative comparison on the GoPro and DVD datasets.Note that “GT” stands for “Ground Truth”. The proposed BSSTNet\nproduces images with enhanced sharpness and more detailed visuals compared to competing methods.\ncompared to state-of-the-art methods on the GoPro dataset.\nBSSTNet achieves higher PSNR and SSIM values com-\npared to Shift-Net+. The visual results in Figure 5b further\nillustrate that the proposed method restores finer image de-\ntails and structures.\nFLOPs and Runtime.We conducted a comparison of the\ncomputational complexity (FLOPs) and runtime between\nour method, RVRT, and Shift-Net+, as presented in Ta-\nble 3. In contrast to the state-of-the-art Shift-Net+, our ap-\nproach demonstrates a13 GFLOPsreduction in FLOPs and\nachieves a speedup of 1.6 times.\n4.4. Ablation Study\nEffectiveness of BBFP.To evaluate the effectiveness of\nBBFP, we conduct an experiment by excluding BBFP from\nBSSTNet. As illustrated in Table 5, the omission of BBFP\nin Exp. (b) results in a reduction of 0.21 dB in PSNR\nand 0.0011 in SSIM. BFA plays an important role in pre-\nventing the introduction of blurry pixels from neighboring\nframes. As shown in Table 6, replacing BFA with Stan-\ndard Flow-guided Feature Alignment results in a decline in\nperformance. To highlight the improved feature alignment\ncapability of BBFP, we visualize the aligned features in Fig-\nure 6, comparing them with the standard feature bidirec-\nCurrent Frame\nFeatures from SFBPFeatures from BBFP\nPrevious Frame\nFigure 6. Comparison of feature alignment between BBFP\nand Standard Flow-guided Bidirectional Propagation (SFBP).\nCompared to SFBP, BBFP prevents the propagation of blurry re-\ngions from the features of neighboring frames during propagation.\nTable 4. Comparison of different temporal lengths in terms of PSNR, SSIM, Runtime, Memory, and GFLOPs between the Standard\nSpatio-temporal Transformer (SST) and BSST.. The results are evaluated on the DVD dataset. Note that “TL.” and “Mem.” denote\n“Temporal Length” and the used memory on GPU, respectively. SST runs out of memory for a temporal length of 60.\nTL. SST BSST\nPSNR SSIM Time (ms) Mem. (GB) GFLOPs PSNR SSIM Time (ms) Mem. (GB) GFLOPs\n12 34.59 0.9684 470 6.35 171 34.52 0.9681 336 2.40 122\n24 34.83 0.9696 925 13.10 251 34.74 0.9692 684 4.79 127\n36 34.92 0.9702 1332 20.40 277 34.85 0.9697 1026 7.30 130\n48 34.97 0.9704 1776 28.27 329 34.95 0.9703 1368 9.97 133\n60 - - - - - 35.01 0.9706 1712 12.78 137\nTable 5. Effectiveness of BBFP and BSST.The best results are\nhighlighted in bold. The results are evaluated on the DVD dataset.\nExp. (a) (b) (c) (d)\nBBFP ✓ ✓\nBSST ✓ ✓\nPSNR 33.78 34.74 34.10 34.95\nSSIM 0.9645 0.9692 0.9661 0.9703\nTable 6. Comparison between BFA and Standard Flow-guided\nFeature Alignment (SFFA).The best results are highlighted in\nbold. The results are evaluated on the DVD dataset.\nPSNR SSIM\nSFFA 34.82 0.9696\nBFA 34.95 0.9703\nTable 7. Comparison of various token sparsity strategies.The\nbest results are highlighted in bold. The results are evaluated on\nthe DVD dataset.\nPSNR SSIM GFLOPs\nRandom 50% 33.92 0.9651 133\n100% 34.98 0.9704 329\nTop 25% 34.78 0.9694 127\nTop 50% (Ours) 34.95 0.9703 133\ntional propagation (SFBP). Benefiting from the incorpora-\ntion of blur maps, BBFP prevents the propagation of blurry\nregions from the features of neighboring frames during the\npropagation process, resulting in sharper features.\nEffectiveness of BSST. To evaluate the effectiveness of\nBSST, we conduct an experiment by excluding BSST from\nBSSTNet. As shown in Table 5, the omission of BSST in\nExp. (c) results in a notable degradation of0.85 dB in PSNR\nand 0.0042 in SSIM. To further evaluate the effectiveness\nand efficiency of BSST, we compare different token spar-\nsity strategies. Table 7 demonstrates that using fewer token\nnumbers or randomly selecting tokens will result in a sig-\nnificant decline in performance. This result suggests that\nwithout guidance from the blur map, discarding tokens in\nthe spatio-temporal domain results in the loss of valuable\ninformation in the video sequence. Moreover, our spar-\nsity strategy, which involves using the top 25% of tokens,\nachieves performance comparable to using all tokens while\nutilizing only approximately 43% of the FLOPs. This indi-\ncates that our sparsity strategy effectively leverages tokens\nin sharp regions within the video sequence.\nComparison of Different Temporal Length.In Table 4,\nwe present a comparison of the Standard Spatio-temporal\nTransformer (SST) under different sequence lengths in\nterms of PSNR, SSIM, Runtime, Memory, and GFLOPs.\nAs the sequence length increases, the computational com-\nplexity of SST grows rapidly. In contrast, BSST’s computa-\ntional complexity is less affected by the sequence length, al-\nlowing BSST to utilize longer sequences and boost deblur-\nring performance. Specifically, when the sequence length is\n60, BSST shows a modest gain in PSNR and SSIM. Consid-\nering the balance between performance and computational\nload, we ultimately choose 48 as the length for the input\nvideo sequence.\n5. Conclusion\nIn this paper, we present a novel approach for video de-\nblurring, named BSSTNet. Utilizing an understanding of\nthe connection between pixel displacement and blurred re-\ngions in dynamic scenes, we introduce a non-learnable,\nparameter-free technique to estimate the blur map of video\nframes by employing optical flows. By introducing Blur-\naware Spatio-temporal Sparse Transformer (BSST) and\nBlur-aware Bidirectional Feature Propagation (BBFP), the\nproposed BSSTNet can leverage distant information from\nthe video sequence and minimize the introduction of blurry\npixels during bidirectional propagation. Experimental re-\nsults indicate that the proposed BSSTNet performs favor-\nably against state-of-the-art methods on the GoPro and\nDVD datasets, while maintaining computational efficiency.\nAcknowledgments This research was funded by\nthe National Science and Technology Major Project\n(2021ZD0110901).\nReferences\n[1] Kelvin C. K. Chan, Shangchen Zhou, Xiangyu Xu, and\nChen Change Loy. Basicvsr++: Improving video super-\nresolution with enhanced propagation and alignment. In\nCVPR, 2022. 2, 4\n[2] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun.\nSimple baselines for image restoration. In ECCV, 2022. 6\n[3] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In ICCV, 2017. 4\n[4] Bo Ji and Angela Yao. Multi-scale memory-based video de-\nblurring. In CVPR, 2022. 2\n[5] Hailin Jin, Paolo Favaro, and Roberto Cipolla. Visual track-\ning in the presence of motion blur. In CVPR, 2005. 2\n[6] Tae Hyun Kim, Kyoung Mu Lee, Bernhard Sch ¨olkopf, and\nMichael Hirsch. Online video deblurring via dynamic tem-\nporal blending network. In In: ICCV, 2017. 2\n[7] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015. 6\n[8] Hee Seok Lee, Junghyun Kwon, and Kyoung Mu Lee. Si-\nmultaneous localization, mapping and deblurring. In ICCV,\n2011. 2\n[9] Dongxu Li, Chenchen Xu, Kaihao Zhang, Xin Yu, Yiran\nZhong, Wenqi Ren, Hanna Suominen, and Hongdong Li.\nArvo: Learning all-range volumetric correspondence for\nvideo deblurring. In In: CVPR, 2021. 6\n[10] Dasong Li, Xiaoyu Shi, Yi Zhang, Ka Chun Cheung, Simon\nSee, Xiaogang Wang, Hongwei Qin, and Hongsheng Li. A\nsimple baseline for video restoration with grouped spatial-\ntemporal shift. In CVPR, 2023. 6\n[11] Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang,\nRakesh Ranjan, Yawei Li, Radu Timofte, and Luc Van Gool.\nVRT: A video restoration transformer. arXiv: 2201.12288,\n2022. 2, 6\n[12] Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan,\nEddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu\nTimofte, and Luc Van Gool. Recurrent video restoration\ntransformer with guided deformable attention. In NeurIPS,\n2022. 2, 3, 6\n[13] Jing Lin, Yuanhao Cai, Xiaowan Hu, Haoqian Wang, You-\nliang Yan, Xueyi Zou, Henghui Ding, Yulun Zhang, Radu\nTimofte, and Luc Van Gool. Flow-guided sparse transformer\nfor video deblurring. In ICML, 2022. 2\n[14] Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei\nLu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hong-\nsheng Li. Fuseformer: Fusing fine-grained information in\ntransformers for video inpainting. In ICCV, 2021. 4, 5, 6\n[15] Yasuyuki Matsushita, Eyal Ofek, Weina Ge, Xiaoou Tang,\nand Heung-Yeung Shum. Full-frame video stabilization with\nmotion inpainting. TPAMI, 28(7):1150–1163, 2006. 2\n[16] Christopher Mei and Ian D. Reid. Modeling and generating\ncomplex motion blur for real-time tracking. In CVPR, 2008.\n2\n[17] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep\nmulti-scale convolutional neural network for dynamic scene\ndeblurring. In CVPR, 2017. 6\n[18] Seungjun Nah, Sanghyun Son, and Kyoung Mu Lee. Re-\ncurrent neural networks with intra-frame iterations for video\ndeblurring. In In: CVPR, 2019. 2\n[19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas K ¨opf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An\nimperative style, high-performance deep learning library. In\nNeurIPS, 2019. 6\n[20] Hyeongseok Son, Junyong Lee, Jonghyeop Lee, Sunghyun\nCho, and Seungyong Lee. Recurrent video deblurring with\nblur-invariant motion estimation and pixel volumes. TIP, 40\n(5):185:1–185:18, 2021. 2\n[21] Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo\nSapiro, Wolfgang Heidrich, and Oliver Wang. Deep video\ndeblurring for hand-held cameras. In CVPR, 2017. 6\n[22] Zachary Teed and Jia Deng. RAFT: recurrent all-pairs field\ntransforms for optical flow. In ECCV, 2020. 6\n[23] Patrick Wieschollek, Michael Hirsch, Bernhard Sch ¨olkopf,\nand Hendrik P. A. Lensch. Learning blind motion deblurring.\nIn In: ICCV, 2017. 2\n[24] Huicong Zhang, Haozhe Xie, and Hongxun Yao. Spatio-\ntemporal deformable attention network for video deblurring.\nIn ECCV, 2022. 2, 6\n[25] Zhihang Zhong, Ye Gao, Yinqiang Zheng, Bo Zheng, and\nImari Sato. Real-world video deblurring: A benchmark\ndataset and an efficient recurrent neural network. IJCV, 131\n(1):284–301, 2023. 2\n[26] Shangchen Zhou, Jiawei Zhang, Jinshan Pan, Wangmeng\nZuo, Haozhe Xie, and Jimmy S. J. Ren. Spatio-temporal\nfilter adaptive network for video deblurring. In In: ICCV,\n2019. 2, 6\n[27] Shangchen Zhou, Chongyi Li, Kelvin C. K. Chan, and\nChen Change Loy. Propainter: Improving propagation and\ntransformer for video inpainting. In ICCV, 2023. 5\n[28] Chao Zhu, Hang Dong, Jinshan Pan, Boyang Liang, Yuhao\nHuang, Lean Fu, and Fei Wang. Deep recurrent neural net-\nwork with multi-scale bi-directional propagation for video\ndeblurring. In In: AAAI, 2022. 2, 6"
  },
  {
    "source": "An Effiecient Video Classification Deep Learning Algorithm for Object Detection and Autonomous Drones.pdf",
    "content": " \n \nAN EFFICIENT VIDEO CLASSIFICATION DEEP LEARNING \nALGORITHM FOR OBJECT DETECTION AND AUTONOMOUS \nDRONES \nby \nRiyaz Ahamed Shaik \n \nA Thesis \nSubmitted to the Faculty of Purdue University \nIn Partial Fulfillment of the Requirements for the degree of \n \nMaster of Science in Engineering \n \n \nDepartment of Electrical and Computer Engineering \nFort Wayne, Indiana \nDecember 2024 \n \n  \n \n2 \nTHE PURDUE UNIVERSITY GRADUATE SCHOOL \nSTATEMENT OF COMMITTEE APPROVAL \nDr. Guoping Wang, Chair \nDepartment of Electrical and Computer Engineering \nDr. Chao Chen, Committee Member \nDepartment of Electrical and Computer Engineering \nDr. Claudio Freitas, Committee Member \nDepartment of Electrical and Computer Engineering \n \nApproved by: \nDr. Chao Chen \n \n3 \nTABLE OF CONTENTS \nABSTRACT .................................................................................................................................... 7 \n1. INTRODUCTION ...................................................................................................................... 8 \n1.1 Background ....................................................................................................................... 9 \n1.2 Problem Statement .......................................................................................................... 10 \n1.3 Research Motivation ....................................................................................................... 11 \n1.4 Objectives ....................................................................................................................... 12 \n1.5 Research Contributions .................................................................................................. 13 \n2. LITERATURE REVIEW ......................................................................................................... 14 \n2.1 Evolution of Object Detection Techniques .................................................................... 15 \n2.1.1 Handcrafted Feature-Based Methods ...................................................................... 15 \nHistogram of Oriented Gradients (HOG) ............................................................................ 15 \nSupport Vector Machines (SVM) ....................................................................................... 16 \n2.1.2 The Emergence of Deep Learning in Object Detection .......................................... 17 \nIntroduction of CNNs for Object Detection ........................................................................ 17 \n2.1.3 Real-Time Object Detectors .................................................................................... 20 \nYOLO (You Only Look Once) Framework ........................................................................ 20 \nSSD (Single Shot Multibox Detector) ................................................................................. 22 \nComparison of YOLO and SSD .......................................................................................... 23 \nAdvances in Real-Time Object Detection ........................................................................... 24 \nChallenges in Real-Time Object Detection ......................................................................... 24 \nConclusion ........................................................................................................................... 25 \n2.2 CNN’s and Its Role in Object Detection ........................................................................ 26 \n2.2.1 Two-Stream Convolutional Neural Networks (CNNs) ........................................... 26 \n2.2.2 3D Convolutional Neural Networks (3D CNNs) .................................................... 27 \nC3D (Convolutional 3D Networks) .................................................................................... 27 \nI3D (Inflated 3D Networks) ................................................................................................ 27 \n2.2.3 SlowFast Networks ................................................................................................. 28 \n2.2.4 Integration of CNN in Object Detection ................................................................. 29 \n2.3 Advancements in Transformers for Video Analysis ...................................................... 31 \n \n4 \n2.3.1 Video Transformers (ViTs) ..................................................................................... 31 \nViTs Role in Video Analysis .............................................................................................. 31 \nApplications in Video Tasks ............................................................................................... 32 \n2.3.2 Video Swin Transformers ....................................................................................... 33 \nApplications ........................................................................................................................ 33 \n2.3.3 Video Transformers Without Convolutions ............................................................ 34 \n2.3.4 Other Transformer-Based Models ........................................................................... 35 \n2.3.5 Conclusion ............................................................................................................... 35 \n2.4 Applications and Challenges in Video Based Object Detection .................................... 36 \n2.4.1 Applications ............................................................................................................ 36 \nAutonomous Driving ........................................................................................................... 36 \nSurveillance Systems ........................................................................................................... 37 \nRobotics ............................................................................................................................... 38 \n2.4.2 Challenges in Video-Based Object Detection ......................................................... 39 \nOcclusions and Motion Blur ............................................................................................... 39 \nReal-Time Constraints and Computational Costs ............................................................... 40 \nDataset Limitations and Annotation Challenges ................................................................. 41 \nConclusion ........................................................................................................................... 42 \n2.5 Gaps in Existing Research .............................................................................................. 42 \n2.6 Conclusion ...................................................................................................................... 44 \n3. METHODOLOGY ................................................................................................................... 46 \n3.1 Dataset Description ........................................................................................................ 46 \n3.1.1 UCF101 Dataset ...................................................................................................... 46 \n3.1.2 COCO-VID Dataset ................................................................................................ 47 \n3.1.3 Waymo Open Dataset .............................................................................................. 47 \n3.2 Model Framework .......................................................................................................... 48 \n3.2.1 Task Description ..................................................................................................... 48 \n3.2.2 3D ResNet Backbone .............................................................................................. 48 \n3.2.3 Transformer Encoding Layers ................................................................................. 49 \n3.2.4 DeepStream SDK for Object Detection .................................................................. 50 \nModel Preparation for DeepStream ..................................................................................... 51 \n \n5 \nPipeline Development using DeepStream ........................................................................... 51 \nAdvantages of Using DeepStream SDK ............................................................................. 53 \n3.3 Autonomous Drone Setup .............................................................................................. 54 \n3.3.1 Drone Frame and Propulsion System ...................................................................... 54 \n3.3.2 Computing Unit ....................................................................................................... 55 \n3.3.3 Obstacle Detection and Depth Sensing ................................................................... 55 \n3.3.4 GPS Navigation ....................................................................................................... 55 \n3.3.5 Manual Control ....................................................................................................... 55 \n3.4 Implementation Details .................................................................................................. 57 \n3.4.1 Preprocessing .......................................................................................................... 57 \nI preprocessed the video data by sampling frames at 16 frames per second (fps) and resizing \nthem to 112×112 pixels. I normalized the pixel values to zero mean and unit variance for \nconsistent input representation. ............................................................................................. 57 \n3.4.2 Training ................................................................................................................... 57 \n3.4.3 Hyperparameter Optimization & Performance ....................................................... 57 \nLearning Rate: ..................................................................................................................... 57 \nBatch Size: ........................................................................................................................... 58 \nDropout Rate: ...................................................................................................................... 58 \nWeight Initialization: ........................................................................................................... 58 \nOptimizer: ............................................................................................................................ 59 \nNumber of Transformer Attention Heads: .......................................................................... 59 \nSummary of Optimized Hyperparameters:.......................................................................... 60 \n3.5 Evaluation ....................................................................................................................... 60 \n4. RESULTS & DISCUSSION..................................................................................................... 61 \n4.1.1 Dataset Performance ............................................................................................... 61 \n4.1.2 Ablation Study ......................................................................................................... 62 \n4.1.3 Object Detection Results ......................................................................................... 62 \n4.1.4 Performance Summary ............................................................................................ 65 \n4.1.5 Challenges and Limitations ..................................................................................... 65 \n5. CONCLUSION & FUTURE WORK ....................................................................................... 67 \n5.1 Achievements in Object Detection ................................................................................. 67 \n \n6 \n5.2 Object Detection Capabilities and Deployment ............................................................. 67 \n5.3 Future Work .................................................................................................................... 68 \nEnhancing Transformer Architectures: ............................................................................... 68 \nReal-Time and Low Latency Optimizations: ...................................................................... 68 \nApplication to Multi-Modal Learning: ................................................................................ 69 \nImproving Spatiotemporal Consistency in Object Detection: ............................................. 69 \nBroader Applications in Real-World Scenarios: ................................................................. 69 \n5.4 Concluding Reflections .................................................................................................. 70 \nREFERENCES ............................................................................................................................. 72 \n   \n \n7 \nABSTRACT \nAdvancements in deep learning and computer vision have enabled  the use of video \nclassification and object detection,  in autonomous systems. This thesis explores the use of a 3D \nResNet-Transformer model to achieve real -time object detection and navigation for autonomous \ndrones. The model combines the spatial feature extraction s of 3D convolutional neural networks \nwith the temporal modeling  capabilities of transformers, effective for video -fed applications. It \nperforms across multiple datasets, achieving 47.1 mAP on the COCO dataset, 67.2 mAP on the \nWaymo Open dataset, and 61.5 mAP on the UCF101 dataset. The model is deployed on NVIDIA \nJetson Nano, using the GPU-accelerated capabilities of NVIDIA’s DeepStream SDK for real-time \nprocessing. This integration enables the drones to detect, classify, and track objects in dynamic \nenvironments while autonomously navigating and avoiding obstacles.  The model can be in \napplications such as surveillance, search -and-rescue, and autonomous transportation. The model \nis generalized to detect objects and classify various categories as it’s been trained on three different \ndatasets at each layer.   \n \n8 \n1. INTRODUCTION \nObject detection plays an important role in computer vision and has advanced rapidly \nthanks to the increasing use of deep learning algorithms. Whether it's self-driving cars recognizing \npedestrians or surveillance systems monitoring safety in real time vide o footage object detection \nis essential for various groundbreaking applications. Although there have been improvements, in \ndetecting objects in still images , applying these techniques to video sequences presents distinct \nchallenges. Machines need to grasp the nature of video data by identifying objects in frames and \nunderstanding how they relate over time in sequences—a crucial yet challenging aspect of video-\nbased object detection research due to the need for spatial and temporal comprehension \nsimultaneously. The conventional methods for detecting objects in videos face difficulties \nbalancing accuracy, with efficiency and scalability. Crafting features manually has its restrictions \nas it struggles to adapt to situations whereas initial deep learning models show potential but \nstruggle to combine spatial and temporal data effectively enough leading to the necessity, for \ninnovative structures capable of handling video data efficiently while being computationally adept.  \n \nThis dissertation tackles these obstacles by  using a video classification method based on \ndeep learning that focuses on identifying objects efficiently. Using a combination of 3D \nConvolutional Neural Networks and Transformer structures enables the proposed method to merge \nspatial feature extraction and temporal analysis for top notch results. The mixed approach is tested \non demanding video datasets to showcase its ability to connect video categorization, with object \nidentification effectively.  \n \nIn the following chapters we will delve into the context needed for the study by first \npresenting the background information and then addressing the issue at hand as well as outlining \nthe research goals and impacts of this project. \n  \n \n9 \n1.1 Background \nDetecting objects in videos is a major task in computer vision and supports various useful \napplications in different fields compared to pictures that don't move much; videos offer a lot of \ntime-based contexts that helps systems study how objects move and interact as situations change \nover time. This function plays a role in tasks like smart surveillance where spotting and following \npotential dangers in real time boosts safety measures significantly. Likewise i n self -driving \nvehicles, video-based object detection ensures driving by always recognizing moving elements \nsuch as people  and traffic signs. Detecting objects in videos not  only aids human computer \ninteraction but also supports gesture recognition and improves augmented reality experiences and \nadaptive interfaces. Showcasing the significance of video object detection in both technical \nintricacy and its impact on decision making, within real world settings[1, 5, 13]. \n \nOver the twenty years or so there have been big changes in the field of video -based \ncomputer vision thanks to better technology and new ways of doing things with algorithms. In the \nbeginning they used methods based on features that were made by hand , like optical flow and \nmotion detection but they didn't work great in all situations. As technology got better at processing \nvideos, more things became possible like tracking movements to recognize actions and events as \nwell as breaking down videos into different sections based on actions. Video detection has evolved \nfrom identifying irregularities to comprehending intricate scene details and actions over time. In \nthe healthcare field as well as entertainment industry video analysis is employed for live tracking \nof patient movements and conditions and, for optimizing personalized suggestions and content \norganization. In robotics and autonomous systems , video analysis adoption has revolutionized \nvarious sectors by empowering machines to sense and engage with consta ntly changing \nenvironments proficiently [11, 12 ]. The development indicates an increasing need for video \nanalysis systems that are precise and responsive to context changes.  \n \nThe advancement of learning has transformed object detection by introducing \ncomprehensive frameworks that can learn complex features directly from data sources instead of \nrelying on preset patterns like traditional approaches did in the past. Deep learning  models have \nthe ability to dynamically extract representations from data which makes them extremely reliable \nin various situations and environments. Convolutional Neural Networks (CNNs) in particular has \n \n10 \nplayed a role in this evolution by demonstrating exceptional performance in capturing spatial \nfeatures, for detecting objects in still images. New standards in precision and efficiency for image \nrelated tasks have been achieved through advancements like R-CNN and YOLO (You Only Look \nOnce) as well as SSD (Single Shot Multibox Detector) [13, 14]. \n \nThe introduction of 3D Convolutional Neural Networks represented a breakthrough by \nfacilitating the combined modeling of spatial and temporal aspects simultaneously in video \nanalysis tasks —an advancement that heightened the precision of object detection in video \nsequences significantly under circumstances necessitating insight into motion patterns [9, 13, 14]. \nMoreover, Transformers have garnered attention as an innovation in sequence modeling due to \ntheir inclusion of self-focusing mechanisms that excel at capturing extensive connections over long \ndistances—a feature that has revolutionized the scope of video analysis, for a more comprehensive \nperspective [6, 7]. \n1.2 Problem Statement \nDetecting and analyzing objects in videos face a demanding task in computer vision as \nvideo data is constantly changing and dynamic in nature; this presents a challenge for object \ndetection techniques that are primarily tailored for static images and strug gle to handle the time \nelement of videos effectively as a result of this limitation models often fail to capture motion \npatterns and long term relationships between frames leading to less than optimal performance, in \ndynamic settings.  \n \nCurrent methods that try to tackle these issues often must balance between precision, \ncomputational speediness and scalability concerns. Convolutional Neural Networks (CNN), \nthough excellent for capturing characteristics, struggle with handling sequential patterns \neffectively. On the other hand, methods for temporal modeling such as Recurrent Neural Networks \n(RNN) or Long Short Term Memory (LSTM) networks add extra computational load and may not \nbe practical for extensive video datasets. Success in achieving efficient object detection in videos \nis hindered by the challenge of integrating spatial and temporal features seamlessly [3, 5].  \n \n \n11 \nFurthermore, videos frequently contain situations like objects blocking each other’s view \nas well as motion blurriness and changes, in the sizes of objects present; all of which make \ndetection tasks more difficult to perform accurately and efficiently. To handle these obstacles \neffectively requires a framework that can meticulously capture specific spatial characteristics \nwhile also understanding how elements change over time across different frames [6, 8].  \n \nTo tackle these concerns and challenges effectively, the presented dissertation in this \nresearch titled \"An Efficient Video Classification Deep Learning Algorithm for Object Detection  \nand Autonomous Drones\" a combination design that merges 3D ResNet for extracting features and \nTransformers for modeling temporal aspects to improve the classification of videos over time more \nefficiently. This strategy is designed to address current obstacles by managing spati otemporal \nconnections and extensive associations effectively while offering a reliable and precise method for \ndetecting objects, in video content. \n1.3 Research Motivation \nThe increasing need for effective identification of objects in videos for various purposes \nlike self -driving cars and security systems highlights the importance of using advanced \ncomputational techniques; however, the changing nature of videos presents challenges, in object \ndetection tasks . Although deep learning has greatly enhanced object detection in still images \nexisting methods struggle to perform well with video data as it necessitates a comprehensive \nunderstanding of spatial and temporal relationships \n \nRecent advancements in detecting objects in videos have shown the promise of combining \ntypes of architectures to fill this void effectively observed in the field of video analysis and \nunderstanding spatiotemporal elements better; for instance, incorporating 3D CNNs and \nTransformers has proven to be quite powerful in this regard. However, the exploration of these \nmodels in detecting objects within videos is still limited. This scenario offers a chance to adjust \nand enhance these models to tackle the difficulties posed by dynamic video environments like \nobstruction, by other objects or peop le (referred to as occlusions) different movement patterns \ndisplayed by objects captured on video footage and time constraints that require real time \nprocessing capabilities [8, 10].  \n \n12 \n \nThis study is driven by the necessity to formulate a solution that does not attain  high \nprecision but also maintains computational effectiveness for extensive practical use cases. This \nresearch endeavors to establish a framework by utilizing areas of expertise from 3D ResNet and \nTransformer structures to enhance video object detection capabilities significantly. The method \naims to present an efficient approach to bridge crucial deficiencies in current systems and pave the \nway for future advancements in this field. \n1.4 Objectives \nThe main objective of this research is to create and execute a n efficient deep learning \nalgorithm for detecting objects in videos using the advantages of video classification methods \neffectively. The main emphasis is placed on  dealing with the obstacles related to combining \ntemporal features and surpassing the shortcomings of current approaches, in dynamic video \nsettings. The detailed aims are as follows.  \n1. Design and Implementation of a 3D ResNet-Transformer for Object Detection: \nDevelop a hybrid framework that integrates 3D ResNet, for capturing spatial features and \nTransformer self-awareness mechanisms for understanding temporal patterns in videos with the \ngoal of offering a comprehensive solution to address the intricacies found in video datasets.  \n \n2. Enhancing Detection Accuracy Through Hybrid Feature Extraction: \nEnhance the accuracy of detecting objects in videos by tuning the balance between spatial \nand temporal features in the model design emphasizing the effective capture of dynamic object \ninteractions while also tackling issues, like occlusions and motion blur effectively.  \n \n3. Application to Complex and Dynamic Video Datasets: \nValidate the suggested framework on a variety of challenging benchmark datasets, like \nUCF101, COCO and Waymo Open to showcase how well the model can adapt and perform in life \ndynamic settings.  \n \n \n13 \n1.5 Research Contributions \nThis thesis makes the following significant contributions to the field of video-based object \ndetection: \n1. Integration of 3D ResNet for Spatial Feature Extraction: \nThis research presents the use of 3 D ResNet as a strong foundation for extracting spatial \nfeatures in video data effectively. With the incorporation of network architecture and 3D CNN \ntechniques, the suggested framework adeptly captures intricate spatial details in each frame \ntackling issues, like different object sizes and motion blurring. \n \n2. Adapting Transformer Self-Attention for Temporal -Spatial Correlations: \nThe research applies Transformer self-focus mechanisms to improve the understanding of \nspatial relationships in video sequences by including multi head attention and positional encoding, \nin the new framework to capture extended dependencies and changing ob ject interactions \neffectively while addressing the shortcomings of conventional temporal modeling techniques \neffectively.  \n \n3. Experimental Validation on Benchmark Datasets for Object Detection: \nThE model undergoes thorough assessment using known video datasets  that cover a wide \narray of intricate scenarios. The outcomes showcase detection precision and resilience revealing \nthe effectiveness of the combined 3D ResNet Transformer framework, in video object detection. \nThe study also offers an examination of fine tuning hyperparameters and subtractive experiments \nto confirm the significance of each element’s contributions.\n \n14 \n2. LITERATURE REVIEW \nThe realm of computer vision has seen progress in the past few years as object detection \nhas become a key focus of study due to its application in various fields like surveillance systems \nand human computer interactions along with autonomous technologies. Although conventional \ntechniques have played a role, in this field’s development so far; the introduction of deep learning \nhas transformed object detection by allowing models to grasp complex features hierarchically and \nattain remarkable levels of precision. The inclusion of video categorization methods within object \ndetection frameworks has broadened the scope of study more and opened up avenues to tackle the \ndistinctive hurdles presented by video data [9, 13].  \n \nVideos present a challenge for object detection due to their dynamic nature compared to \nstatic images; they require models that can analyze both spatial and temporal characteristics \neffectively without sacrificing computational efficiency. Many current met hods excel in either \ntemporal analysis but face difficulties seamlessly combining the two aspects; this limitation often \nresults in performance issues. To address this challenge and improve detection accuracy in videos \nresearchers are increasingly explorin g architectures that combine the strengths of different \napproaches, such as utilizing Convolutional Neural Networks (CNNs) for spatial understanding \nand Transformers, for capturing long range temporal relationships [6, 7, 8].  \n \nThis chapter offers an overview of the research already existing related to the thesis. It delves \ninto the progress of techniques for detecting objects and the importance of video classification in \npushing this field as well as the rise of combined frameworks that use both CNNs and Transformers \ntechnologies. The goal is to pinpoint areas wher e current research falls short and lay down the \ngroundwork for the proposed 3D ResNet Transformer framework designed to achieve efficient \nobject detection, in dynamic video settings. This review also addresses the difficulties and \npotentials of incorporati ng temporal characteristics while providing a context for the findings \npresented in this thesis. \n \n15 \n2.1 Evolution of Object Detection Techniques \nFor years now, in the world of computer vision research community , object detection has \nbeen a key focus area continuously upgraded to recognize and pinpoint objects within images or \nvideo frames effectively with time the techniques have advanced from traditional manual feature-\nbased methods to more recent deep learning strategies marking significant progress and boosting \nthe performance reliability and versatility of object detection systems along the way. In this \nanalysis, below we delve into the evolution of this field by examining  methods and models that \nhave influenced its development. numbered. \n2.1.1 Handcrafted Feature-Based Methods \nDuring the phases of object detection, development strategies heavily leaned on manually \ncrafted features. These methods required intervention to create algorithms that could extract \nrelevant details from images before employing machine learning classifiers, for identification \npurposes. Two notable techniques that characterized this period were the Histogram of Oriented \nGradients (HOG) and Support Vector Machines (SVM) [15]. \nHistogram of Oriented Gradients (HOG) \nDalal and Triggs introduced HOG in 2005 as a feature descriptor for detecting objects in \nimages by calculating gradient orientations within specific regions to capture shape and edge \ndetails essential for recognizing entities such as pedestrians. HOG becam e popular for its ability \nto adapt to changes in lighting and poses; however its dependence on predetermined features \nhindered its applicability to real world situations with a variety of object appearances [15]. \n \n16 \n \nFigure 2.1. Working of HOG [15] \nSupport Vector Machines (SVM) \nFrequently used alongside HOG techniques to detect objects were Support Vector \nMachines (SVMs). SVMs proved to be classifiers by identifying a hyperplane that separates \ndifferent classes in a feature space with many dimensions to achieve dependable results  for tasks \ninvolving binary detection. While SVMs demonstrated success in their performance for object \ndetection tasks they faced challenges when it came to scalability, in situations involving class \ndetection and were unable to capture hierarchical features effectively [15]. \nAlthough these techniques set the groundwork for object detection systems to emerge and \nevolve further in the field of AI development their progress was hindered by the need for \nhandcrafted features and inefficiencies in processing power. This led to a transition towards \nmethods that rely more on data analysis and machine learning algorithms [15]. \n\n \n17 \n \nFigure 2.2. Working of SVMs [15] \n \n2.1.2 The Emergence of Deep Learning in Object Detection \nThe rise of deep learning brought about a significant change in object detection techniques \nby leveraging improvements in technology capabilities and the abundance of extensive data sets \nalongside innovations in neural network designs. The prominence of Convolutional Neural \nNetworks (CNNs) played a role in this transformation by offering an efficient and structured \nmethod, for extracting features automatically [9, 13]. \nIntroduction of CNNs for Object Detection \nCNN architectures were initially introduced by LeCun and colleagues in the 1990S for \nrecognizing digits and later gained traction in object detection following the breakthrough of \nAlexNet in 2012 . AlexNet showcased the capacity of learning in categorizing images and \nmotivated researchers to customize CNN models, for object detection purposes unlike traditional \nmanual methods, CNN architectures are able to extract features directly from data which enhances \ntheir resilience and versatility [9]. \nEarly CNN-based object detection models, such as the Region-based Convolutional Neural \nNetwork (R-CNN) series, played a pivotal role: \n\n \n18 \no R-CNN (2014): R-CNN introduced a two-stage approach, where region proposals \nwere generated first, followed by CNN-based feature extraction and classification. \nDespite its high accuracy, the method was computationally expensive due to \nredundant feature extraction for overlapping regions [13]. \no Fast R -CNN (2015): Fast R -CNN addressed this inefficiency by sharing \nconvolutional features across regions, significantly improving speed while \nmaintaining accuracy [13].  \no Faster R -CNN (2015): Faster R -CNN introduced a Region Proposal Network \n(RPN), enabling end-to-end training and making region proposal generation more \nefficient. It set the benchmark for high-performance object detection models [13]. \nWhile the R -CNN series demonstrated the potential of CNNs, their computational demands \nhighlighted the need for real-time detection solutions [14]. \n \nFigure 2.3. Difference Between Traditional and Deep Learning Process [14] \n \n \nFigure 2.4. Architecture of R-CNN [14] \n\n \n19 \n \nFigure2.5. Architecture of Fast R-CNN [14] \n \n \nFigure 2.6. Architecture of Faster R-CNN [14] \n \n \n\n \n20 \n2.1.3 Real-Time Object Detectors \nReal time object detection is now a focus of study because its crucial in situations where \nquick and precise decision making  matters most. Like in autonomous driving systems , video \nsurveillance setups, robotics projects and augmented reality applications. Real time detectors are \ndifferent, from the two stage methods that aim for accuracy even if it means sacrificing speed; they \nprioritize efficiency without majorly affecting accuracy too much. The change has mostly been \ninfluenced by the advancement of single stage detection designs, like Yolo (You Look Once) and \nSSD (Single Shot Multibox Detector) which have reshaped the field of object detection [13, 14].  \nYOLO (You Only Look Once) Framework \nJoseph Redmon and his team introduced Yolo in 2016 as a method that changed the way \nobject detection is approached by treating it as a single regression problem that predicts bounding \nboxes and class probabilities simultaneously in one go. The design of th e architecture prioritizes \nspeed and accuracy making it ideal, for real time applications. \no Key innovations in YOLO: \n▪ Single Neural Network Architecture: YOLO processes the entire image \nwith a single neural network, dividing it into an S×SS \\times SS×S grid. \nEach grid cell predicts a fixed number of bounding boxes, confidence scores, \nand class probabilities, eliminating the need for region proposal generation. \n▪ Unified Detection Pipeline: By combining region proposal, feature \nextraction, and classification into a single pipeline, YOLO significantly \nreduces computation time. \n▪ Trade-offs Between Speed and Accuracy: While YOLO prioritizes speed, \nits initial version had difficulty detecting small objects or objects that \nappeared in clusters. However, subsequent versions addressed these \nlimitations [13]. \no Evolution in YOLO: \n• YOLOv2 (2017): Introduced anchor boxes for improved localization and \nused batch normalization for faster convergence. It also incorporated a \n \n21 \ncustom dataset called YOLO9000, enabling detection across 9,000 object \nclasses. \n• YOLOv3 (2018): Enhanced feature extraction with a deeper network and \nmulti-scale predictions, allowing better handling of objects at different sizes \n[13]. \n• YOLOv4 and Beyond (2020 –): Integrated cutting -edge advancements \nsuch as CSPDarknet as the backbone, spatial pyramid pooling, and \nimproved activation functions, achieving state -of-the-art accuracy and \nefficiency [14]. \nYOLOs flexibility and ability to scale have positioned it as a popular framework for real time \nobject detection in real world scenarios by effectively managing the balance, between precision, \ntime efficiency and computational demands. \n \nFigure 2.7. Base Architecture of YOLO Framework [14] \n\n \n22 \nSSD (Single Shot Multibox Detector) \nIn 2016 Liu et al introduced the SSD model to overcome shortcomings of Yolo focusing \non detecting small objects and those with different aspect ratios more effectively than before . \nThanks to its innovative design allowing for multi scale predictions, in diverse scenarios [14]. \no Key features of SSD: \n▪ Multi-Scale Predictions: SSDs utilize feature maps at resolutions to identify \nobjects of different sizes; lower resolution maps are for detecting larger \nobjects and higher resolution maps are, for smaller objects. \n▪ Default Anchor Boxes: Taking cues from the anchor idea in R-CNN model \nand applying it to SSD model involves setting up default anchor boxes with \ndifferent aspect ratios and scales, at different points on the feature map to \ncapture a wider array of object shapes and sizes and minimize the chances \nof overlooking detections. \n▪ Single Shot Architecture: In contrast to two step processes SSD foresees the \nlikelihoods of class and box locations at once in one go which noticeably \ndecreases the time, for inference [14]. \no Applications of SSD: \n• SSDs are especially useful for tasks that involve detecting to medium sized \nobjects efficiently like aerial surveillance systems that operate on the move \nor, in industrial automation settings [14]. \n \n23 \n \nFigure 2.8. Architecture of SSD [14] \nComparison of YOLO and SSD \nBoth YOLO and SSD are leaders in real -time object detection, but they excel in different \nscenarios: \nTable 2.1. Comparison of YOLO and SSD \nFeature YOLO SSD \nSpeed Fater due to single prediction \nlayer \nSlightly slower but still \nefficient \nAccuracy High for large, distinct objects Better for small and medium \nobjects \nArchitecture Single prediction grid Multi-scale feature maps \nEase of Training Requires fewer \nhyperparameters \nSlightly more complex \nApplication Autonomous driving, robots Mobile systems, industrial \nvision \n \n  \n\n \n24 \nAdvances in Real-Time Object Detection \nThe progress of real time object detection hasn't halted at Yolo and SSD; there have been \nenhancements that combine elements from these frameworks with new developments, in deep \nlearning technologies: \no EfficientDet (2020): \n▪ Combines EfficientNet as the backbone with a BiFPN (Bi -directional \nFeature Pyramid Network) for better multi-scale feature fusion. \n▪ Achieves state -of-the-art performance with a focus on computational \nefficiency, making it ideal for edge devices [14]. \no YOLOv5 and Beyond: \n• YOLOv5 integrates enhancements such as auto-learning anchor boxes and \nhyperparameter tuning. \n• It also emphasizes lightweight deployment, making it highly adaptable for \nmobile and embedded systems [13, 14]. \no Transformer-Based Real-Time Detectors: \n▪ Emerging approaches like Detection Transformers (DETR) leverage \nTransformers for global context modeling while maintaining real -time \nperformance. These models aim to overcome the limitations of \nconvolutions-based frameworks in complex scenarios [6, 7, 8]. \nChallenges in Real-Time Object Detection \n• Balancing Speed and Accuracy in Detection Tasks; When trying to speed up the process \nof detection tasks; there is usually a tradeoff, with the accuracy of detecting overlapping \nobjects [13].  \n• Optimizing models is crucial when installing real time detectors on edge devices with \nrestricted capacity due to resource limitations [14].  \n \n25 \n• Generalizing performance, across scenarios and datasets continues to present a challeng e \n[14]. \nConclusion \nReal time object detection systems such as YOLO and SSD have transformed the field by \nstriking a balance between speed and accuracy for use in real world settings with practical \napplications. Their designs have become benchmarks for effectiveness and scalability in inspiring \ndevelopments in the field. Through improvements in combined models and focus mechanisms, the \nfuture of real time object detection offers increased accuracy, flexibility and computational \nefficiency which will lead to its incorporation, into computer vision systems of the future. \n  \n \n26 \n2.2 CNN’s and Its Role in Object Detection \nConvolutional Neural Networks (CNNs) are a class of deep learning models designed for \nprocessing structured grid data, such as images. They are really effective in object detection due \nto their ability to automatically learn spatial features through convolutional layers. These layers \nextract low-level features (e.g., edges, corners) and build high-level representations (e.g., shapes, \nobjects). In object detection, CNNs play a crucial role in identifying and localizing objects within \nan image. Architectures like YOLO, Faster R -CNN, and SSD use CNNs as backbones to extract \nmeaningful features, which are then passed to classification and regression  layers to predict  \ncategories and bounding box coordinates [9, 14]. \n2.2.1 Two-Stream Convolutional Neural Networks (CNNs) \nSimonyan and Zisserman introduced the Two Stream CNN design in 2014 as a \ngroundbreaking approach to address both spatial and temporal analysis, in video data effectively. \nThe design comprises two parallel networks: \n• Spatial Stream:  Processes individual RGB frames to extract spatial features, \ncapturing static object appearance and context. \n• Temporal Stream: Processes stacked optical flow fields, which represent the motion \nbetween consecutive frames, to model temporal dynamics. \nThe results from both streams are combined either by adding them or by merging them to \ncreate forecasts, for action recognition assignments successfully separating spatial and temporal \naspects to extract features independently in each area [1]. \n \nFigure 2.9. Architecture of Two-Stream CNNs [1] \n \n\n \n27 \n \n2.2.2 3D Convolutional Neural Networks (3D CNNs) \nTraditional 2-dimensional convolutions are extended by 3D CNNs to include the aspect of \ntime as well as space to extract both spatial and temporal features simultaneously This method is \nespecially useful for representing intricate motion sequences and understanding the relationships, \nbetween space and time [3]. \nC3D (Convolutional 3D Networks) \nTran et al suggested the concept of  D3D which employs three convolutions across a series \nof frames to extract spatial and temporal characteristics straight from unprocessed video content. \nThe design includes layers of three-dimensional convolution and pooling operations along with \nfully connected layers, for categorization purposes. The success of D3D showcased the ability of \nthree convolutions to accurately grasp motion details and surpass the performance of conventional \ntwo-dimensional Convolution Neural Networks (CNN) in multiple object detection evaluations \n[3]. \nI3D (Inflated 3D Networks) \nCarreira and Zisserman introduced I3D in 2017 as an extension of the 2-dimensional CNN \nmodels by transforming all the 2-dimensional filters and pooling kernels into a 3-dimensional \nformat to enable the network to grasp spatiotemporal characteristics while making use of pre \ntrained 2 dimensional models for better performance in action recognition tasks using two streams. \nOne for RGB and another, for optical flow data [4]. \n \n28 \n \nFigure 2.10. Architecture of I3D Network [4] \n2.2.3 SlowFast Networks \nDeveloped by Feichtenhofer et al. in 2019, SlowFast networks address the need to capture \nobjects occurring at different temporal frequencies. The architecture comprises two pathways: \n• Spatial Stream:  Operates at a low frame rate, focusing on capturing spatial \nsemantics and slow-evolving features. \n• Temporal Stream: Operates at a high frame rate, capturing rapidly changing motion \ninformation. \nThe merging of the two pathways happens at stages to enable the system to grasp both \ngradual and rapid temporal aspects efficiently. This approach results in outcomes in tasks related \nto recognizing object categories by capturing diverse motion patterns effectively [5]. \n \n\n \n29 \n \nFigure 2.11. Architecture of SlowFast Network [5] \n \nIncorporating SlowFast networks into object detection systems enhances the ability to detect \nobjects with varying motion patterns, improving robustness in dynamic environments. \n2.2.4 Integration of CNN in Object Detection \nThe techniques created for detecting objects in images have had an impact on spotting \nobjects in videos as well. Using structures that can understand how things change over time helps \nobject detection systems deal with problems, like things blocking the view blurry movements and \nobjects looking different at times. Combining features related to space and time helps identify and \nplace objects accurately in complicated scenes [1, 3, 5]. \n \nTable 2.2. Comparison of Object Detection Models [1, 3, 5] \nModel Key Features Advantages Limitations \nTwo-Stream CNNs Separate spatial and \ntemporal streams \nEffective motion \nmodeling \nComputationally \nintensive due to \noptical flow \nC3D 3D convolutions over \nspatiotemporal data \nCaptures motion and \nappearance jointly \nHigh computational \ncost \nI3D Inflated 3D \nconvolutions with \npre-trained weights \nLeverages pre-trained \nmodels for improved \naccuracy \nRequires large \ndatasets for optimal \nperformance \nSlowFast Networks Dual pathways for \nslow and fast features \nModels multi -scale \ntemporal dynamics \nIncreased complexity \nand resource \nrequirements \n \n\n \n30 \n \nThe advancement of object detection models has significantly influenced the progress of \nobject detection systems in settings. Through structures such as Two Stream CNNs and SlowFast \nnetworks that cater to temporal features efficiently integrating these models into object detection \npipelines will improve the precision in identifying and categorizing objects, amidst movement and \ninteractions promoting the growth of smarter and more responsive computer vision applications.  \n  \n \n31 \n2.3 Advancements in Transformers for Video Analysis \nTransformers have been seen as a method in analyzing videos by tackling the complexities \nof spatial and temporal connections effectively. They were initially created for processing natural \nlanguage but are now utilized for video analysis thanks to their ability to handle long range \ndependencies using self-focus mechanisms. This section explores the development of Transformer \nmodels in video analysis with an emphasis on Vision Transformers (ViTs) , Video Swin \nTransformers well as VidTr and various cutting-edge Transformer based designs [7, 8]. \n2.3.1 Video Transformers (ViTs) \nDosovitskiy introduced the Vision Transformer (ViTs) in 2020 as a shift away from \ntraditional convolutional designs like CNNs that focus on local operations within images. Unlike \nCNNs that work with patches overlapped in images for processing the ViTs trea t image inputs as \nsequences of distinct and non-overlapping patches. Each patch is considered as a token  \ntransformed into a feature space before being passed through multiple Transformer layers allowing \nthe model to capture global connections across the entirety of the image using multi head self-\nattention [6]. \nViTs Role in Video Analysis \nViTs were first created for images but have since been adapted for analyzing videos by \nconsidering video frames as sequences of spatiotemporal patches that capture specific \nspatiotemporal features in a localized manner; furthermore the self -awareness mechanism takes \ninto account both within frame and, between frame relationships. \n• Advantages: Superior capability to model long -range dependencies compared to \nCNNs. Effective handling of global context across frames, essential for capturing \nmotion patterns and object interactions [6, 7]. \n• Limitations: Requires large -scale datasets for pretraining, such as ImageNet or \nKinetics. High computational cost due to the quadratic complexity of self-attention \n[6, 7]. \n \n32 \nApplications in Video Tasks \nViTs have proven effective in applications like identifying actions, in videos and \nclassifying them based on content over time by incorporating temporal dimensions into the patch-\nbased method for a stronger grasp of spatial and temporal connections [7].  \n \nFigure 2.12. Architecture of ViT [7] \n \nTable 2.3. Comparison of ViT and CNN Architectures in Video Analysis \nAspect Vision Transformer CNNs \nFeature Extraction Global context through self -\nattention \nLocal context through \nconvolutional filters \nTemporal Modeling Captures long -range \ndependencies via attention \nLimited to local temporal \ncontext \nScalability Requires large datasets and \ncomputational resources \n \nMore efficient with smaller \ndatasets \n\n \n33 \n2.3.2 Video Swin Transformers \nBuilding on the success of ViT, Swin Transformers introduced by Liu et al. in 2021 \nintroduced a hierarchical architecture and shifted windows to improve efficiency. Video Swin \nTransformers extend this architecture to spatiotemporal data, allowing localized attention through \n3D shifted windows. Some of the key features include: \n• Hierarchical Structure: Processes features at multiple scales, improving the \ndetection of objects and actions at varying resolutions. \n• Shifted Windows: Instead of global attention, it computes attention within localized \nwindows, reducing computational overhead while retaining critical spatial and \ntemporal relationships. \n• 3D Windows: The extension to 3D allows seamless integration of temporal \ninformation, essential for video tasks [7]. \nApplications \nThe Video Swin Transformer has shown state -of-the-art performance on datasets like \nKinetics and Something -Something V2, excelling in tasks such as action recognition, video \nsegmentation, and motion analysis. \n \nFigure 2.13. Architecture of Video Swin Transformer [7] \nTable 2.4. Performance of Video Swin Transformer vs Traditional CNNs [7, 4, 5] \nModel Top 1 Accuracy (%) Computational Cost (GFLOP) \nVideo Swin Transformer 82.7 282 \n3D ResNet-50 76.4 304 \nI3D 78.7 306 \n\n \n34 \n2.3.3 Video Transformers Without Convolutions \nVidTr represents a significant advancement in video analysis by eliminating convolutional \noperations and relying solely on Transformer architectures to model spatiotemporal data. Proposed \nby Li et al. in 2021, VidTr introduces a pure Transformer-based model for video classification. It \nprocesses video frames as sequences of patches and applies self -attention mechanisms to capture \nboth spatial and temporal dependencies. By removing convolutions, VidTr aims to leverage the \nfull potential of Transformers in modeling complex video data [8]. \n \nFigure 2.14. Architecture of VidTr [8] \nSome of the key features include: \n• Separable Attention : VidTr employs separable attention mechanisms to handle \nspatial and temporal dimensions independently, reducing computational \ncomplexity. \n• Scalability: The model can be scaled to handle longer video sequences by adjusting \nthe number of attention layers and patch sizes. \n• Performance: VidTr demonstrates competitive performance on video classification \nbenchmarks, highlighting the efficacy of Transformer-based architectures in video \nanalysis [8]. \n  \n\n \n35 \nTable 2.5. Comparison of VidTr and CNN-Based Models on Video Classification [8] \nModel Top 1 Accuracy (%) Parameters (Millions) \nVidTr 79.5 45 \nC3D 74.5 78 \nIED 77.9 50 \n2.3.4 Other Transformer-Based Models \nSeveral other Transformer-based architectures have contributed to video analysis: \n• TimeSformer: Models temporal relationships independently across frames, achieving \nhigh accuracy on action recognition tasks. \n• MViT (Multiscale Vision Transformers): Introduces a multiscale approach for both \nspatial and temporal modeling. \n• DETR (Detection Transformer): Adapts Transformer -based detection for video data, \nexcelling in temporal object tracking. \n2.3.5 Conclusion \nTransformer based  models have greatly improved video analysis by overcoming the \nconstraints of techniques in representing long range connections and time related behaviors \neffectively. Through examples, like ViTs and Video Swin Transformers , showcases how \nTransformers excel in grasping temporal associations enabling the development of more \nproductive video analysis systems. These advancements have laid the groundwork for studies \nfocused on refining Transformers for real-world video applications. \n  \n \n36 \n2.4 Applications and Challenges in Video Based Object Detection \nDetecting the objects in videos plays a role in contemporary computer vision technology \nby helping machines comprehend moving scenarios swiftly or for extended durations of time \nacross different environments. Its uses are diverse. From bolstering safety measures in self-driving \ncars to enhancing efficacy in monitoring and robotic tasks. Nevertheless, the changing nature of \nvideo information poses distinctive hurdles such as obstructions blurring caused by motion, \nlimitations, in computing capabilities and the shortage of meticulously annotated datasets that are \nof high quality. This part covers an examination of the uses and obstacles in identifying objects in \nvideos through visual analysis methods [3, 5, 7]. \n2.4.1 Applications \nAutonomous Driving \nObject detection plays a pivotal role in autonomous vehicles, enabling systems to perceive \nand navigate complex environments. Vehicles rely on real -time detection of objects such as \npedestrians, cyclists, vehicles, and road signs to make split-second decisions. \n• Key Functions: \no Detecting and tracking obstacles in real-time. \no Identifying traffic signs, signals, and lane markings for navigation. \no Monitoring surrounding vehicles to predict their trajectories and avoid collisions. \n• Challenges in Autonomous Driving: \no High-speed scenarios exacerbate motion blur, making it harder to detect objects \naccurately. \no Occlusions caused by large vehicles or environmental factors like fog and rain can \nobscure critical visual information. \n• Advancements: \no Leveraging hybrid architectures such as 3D CNNs and Transformers has improved \nspatiotemporal modeling, enabling better detection in dynamic environments [7]. \n \n \n37 \n \n \nFigure 2.15. Object Detection in a Tesla Car [7] \nSurveillance Systems \nVideo-based object detection is a cornerstone of modern surveillance systems, ensuring \nsafety in public spaces, workplaces, and private properties. \n• Applications in Surveillance: \no Identifying and tracking suspicious activities or individuals. \no Monitoring crowd behavior to detect anomalies like fights or stampedes. \no Ensuring compliance with security protocols, such as face mask detection during \npandemics. \n• Challenges in Surveillance: \no Real-time detection over prolonged periods requires high computational resources. \no Privacy concerns necessitate anonymization techniques, adding complexity to the \nsystem. \n• Advancements: \no Using multi -stream architectures like SlowFast networks has enhanced the \ndetection of fast and slow-moving objects in crowded environments [3, 5, 7]. \n \n \n \n\n \n38 \n \nFigure 2.16. Real-Time Object Detection in Surveillance Systems [7] \nRobotics \nIn robotics, object detection is essential for enabling robots to interact effectively with their \nenvironment. Applications include industrial automation, healthcare assistance, and autonomous \ndrones. \n• Key Functions: \no Detecting and manipulating objects in industrial settings. \no Navigating through complex terrains by recognizing obstacles. \no Assisting healthcare workers by detecting and delivering medical supplies. \n• Challenges in Robotics: \no Varying lighting conditions and object appearances can hinder detection accuracy. \no Real-time constraints require low -latency detection algorithms for effective \noperation. \n• Advancements: \no Lightweight detection models optimized for edge devices have enabled deployment \non resource-constrained robotic platforms [14]. \n \n \n\n \n39 \n \nFigure 2.17. Real-Time Object Detection in Robot [14] \n2.4.2 Challenges in Video-Based Object Detection \nOcclusions and Motion Blur \nOcclusions occur when objects are partially or fully blocked by other objects in the scene, \nmaking it difficult to detect or track them accurately. Motion blur, caused by rapid object \nmovement or camera motion, further complicates detection. \n• Impact on Detection: \no Occlusions can result in fragmented tracking or missed detections. \no Motion blur distorts object appearance, leading to classification errors. \n• Solutions: \no Utilizing temporal information across multiple frames helps infer occluded \nobjects' positions. \no Advanced architectures like VidTr leverage attention mechanisms to model \nobject trajectories effectively [15]. \n \n \n \n\n \n40 \n \nFigure 2.18. Illustration showing blurred and occluded objects in a dynamic scene [15] \nReal-Time Constraints and Computational Costs \nReal-time applications, such as autonomous driving and surveillance, demand high -speed \nprocessing to ensure timely decision -making. Achieving this requires significant computational \nresources, particularly for high-resolution video data. \n• Challenges: \no Balancing accuracy with inference speed. \no Handling large volumes of data in real -time without compromising \nperformance. \n• Solutions: \no Real-time detectors like YOLO and SSD prioritize speed through single -shot \narchitectures. \no Efficient Transformers, such as Video Swin Transformers, incorporate \nlocalized attention mechanisms to reduce computational overhead [15]. \n \n\n \n41 \nTable 2.6. Comparison of Real-Time Object Detectors [13, 14, 17] \nModel Speed (FPS) Accuracy (mAP) Computational Cost \n(GFLOPs) \nYOLOv5 45 50.7 28 \nSSD 22 48.5 50 \nVideo Swin 30 55.2 282 \n \nDataset Limitations and Annotation Challenges \nHigh-quality datasets are essential for training robust video-based object detection models. \nHowever, creating such datasets involves significant challenges, including: \n• Annotation Complexity: \no Annotating video frames is labor -intensive and time -consuming, especially for \nlarge datasets. \no Temporal annotations require precise labeling of object interactions and trajectories \nacross frames. \n• Dataset Bias: \no Existing datasets may lack diversity in object types, environments, or lighting \nconditions, limiting model generalization. \n• Solutions: \no Leveraging synthetic datasets generated by GANs or simulation environments can \nalleviate annotation bottlenecks. \no Semi-supervised learning methods can make better use of unannotated or partially \nannotated video data [3, 8]. \nTable 2.7. Popular Video Datasets for Object Detection \nDataset Number of Videos Classes Challenges \nKinetics 650,000 400 Diverse actions, high \ncomplexity \nActivityNet 20,000 200 Limited temporal \nannotations \nUCF101 13,320 101 Limited diversity, \nsmall scale \n \n42 \nConclusion \nVideo based object detection is used in driving, surveillance systems and robotics to \nenhance safety, efficiency and decision-making processes. However, there are obstacles like \nobstructions blurred movements, real time demands and limited data sets. To tackle these hurdles, \nwe need to improve model structures, optimize methods and expand data sets. By conquering these \nchallenges video-based object detection will progress further leading to smarter and more reliable \nsystems, across various fields. \n2.5 Gaps in Existing Research \nWhile there have been strides in video-based object detection technology advancements \nthere are still key challenges that prevent its seamless integration into real life situations leaving \nplenty of room for enhancement. One major obstacle is the inefficiencies tied to combining \ntemporal features. T he top tier techniques today heavily depend on Convolutional Neural \nNetworks (CNNs) to extract features effectively and excel at capturing detailed information, within \neach frame of a video segment. CNNs have a challenge when it comes to capturing how thin gs \nchange over time in videos due to their design limitations in modeling relationships between \nframes accurately for recognizing movements and trajectories of objects in dynamic video clips. \nAlthough methods like RNN and temporal convolutions have tried to improve modeling in videos \nthey often end up sacrificing the quality of spatial details causing a less comprehensive grasp of \nthe video context. For instance while RNN performs well with data it struggles with managing \nlarge amounts of data and maintaini ng detailed spatial information, over extended periods. \nExtending operations into the temporal domain with 3 dimensional CNN models has its limitations \nas it mainly focuses only short-term dependencies and may fall short when detecting objects, in \nintricate and lengthy activities. \n \nA major issue is that current video object detection models lack scalability due to their \ncomputational requirements and memory usage limitations which hinder their usability in \nenvironments with limited resources, like surveillance systems or autonomous vehicles that often \ndeal with high resolution video feeds. For example in self -driving cars real time identification of \nobjects necessitates models to handle video feeds concurrently and respond swiftly to make rapid \n \n43 \njudgements. However current designs are not tailored for these paced situations causing potential \nsafety risks. Moreover devices at the edge and portable gadgets commonly employed in the \nInternet of Things and embedded setups lack the computing power to accommodate these complex \nmodels which hinders their integration, into decentralized systems. The problem of scalability is \nworsened by the energy usage linked to cutting edge models that restricts their use in eco-friendly \nand energy efficient setups. \n \nVideo object detection models struggle to adapt to real world situations due to their limited \ntraining data that often lacks diversity in conditions, like lighting and camera angles as well as the \ntypes of objects included for identification purposes. Detecting objects in a surveillance video or \nin challenging weather conditions like fog or rain can be quite tough for models trained on daylight \ndata sets. Moreover, differences in how objects look. Such as size, position and partial blockages. \nMake it even harder often resulting in missed detections or incorrectly identifying objects. This \nchallenge is especially noticeable in city settings where objects often overlap, move erratically and \nhave different physical characteristics. \n \nRelying on annotated datasets can slow down progress in video object detection research \nas it takes a lot of effort and time to create high quality labels for video data compared to static \nimages labeling process. Temporal annotations do not need  object labels but also detailed \ntrajectory information across frames which can be quite complex, in dynamic settings. \nFurthermore, current datasets do not have a wide variety of object categories settings and \ncircumstances which also restricts the ability of models trained on them to apply broadly semi \nsupervised and unsupervised learning techniques have demonstrated potential in addressing these \nchallenges but their use, in video object detection is still relatively restricted. \n \nProcessing data efficiently remains a major hurdle to overcome in the field of computing. \nVision Transformers (ViTs) and Video Swin Transformers have shown progress in capturing \nrelationships and dependencies over long distances. However, their computation complexity \nincreases quadratically as the input size grows leading to costs especially for lengthy video \nsequences or high resolution frames. This drawback hinders their use, in real time processing or \nedge computing scenarios. Ensuring video object detection models can keep up with the demands \n \n44 \nof applications without compromising performance hinges, on developing more effective attention \nmechanisms and streamlined architectures. \nTable 2.8. Comparison of Current Approaches to Video-Based Object Detection \nApproach Integration of \nSpatial and \nTemporal \nFeatures \nScalability Generalization \nto Real-World \nScenarios \nGeneralization \nto Real-World \nScenarios \n2D CNN + RNN Moderate Low Low High \n3D CNN Limited \nTemporal \nIntegration \nLow Moderate Very High \nTransformer-\nBased Models \nStrong Moderate High Moderate \nHybrid \nArchitectures \nHigh Low Moderate High \n \nTable 2.9. Performance Challenges in Real-World Applications \nChallenge Impact Example Scenario \nInefficient Temporal \nModeling \nMissed detections in complex \nmotion \nFast-moving vehicles in \nautonomous driving \nHigh Computational Demand Limited deployment on edge \ndevices \nReal-time surveillance in \nremote areas \nDataset Bias Poor generalization to diverse \nenvironments \nObject tracking in varied \nlighting conditions \n \nThe gaps highlight the importance of advancements in merging space time characteristics \nand enhancing computational performance while creating tailored models that merge the benefits \nof recognizing actions and detecting objects together is essential for pushing forward video-based \nobject detection to cater to the requirements of practical applications, in various areas. \n2.6 Conclusion \nIn looking at studies on video-based object detection methods have shown progress but also \nareas that still need improvement. Convolutional and recurrent neural networks have helped with \ntemporal modeling but their structures are not seamless and struggle with scalability. Transformer \nbased models offer an option by effectively capturing distant connections and context links; \nhowever they come with high computational costs and struggles in adapting to specific object \n \n45 \ndetection needs. In this field there is a need for innovative models that can perform object detection \neffieiently.  \n \nThe identified shortcomings point to the need for a framework that can tackle issues such as \ninefficient spatial temporal integration and scalability limitations while also incorporating hybrid \narchitectures they lack currently. This study  uses a 3D ResNet-Transformer framework intended \nto unify spatial and temporal feature modeling challenges efficiently by harness the strong spatial \nfeature extraction abilities of 3D ResNet along, with the extensive temporal modeling capabilities \nof Transformer self-attention mechanisms the method seeks to address the drawbacks of current \nmodels effectively.  \n \nIncludes various temporal scales to enhance the precision of object detection, in different \nand changing settings. This mixed approach is created to expand efficiently for real life uses like \nself-driving cars, surveillance and robotics tackling both the i ssues related to processing power \nand practical deployment. Drawing from research findings and addressing the existing limitations \nidentified in the literature this study aims to progress the current standards of video-based object \ndetection by providing a reliable and adaptable solution, for challenging real life situations.   \n \n46 \n3. METHODOLOGY \nIn my research project I set out to tackle the difficulties associated with identifying objects \nin videos by fine-tuning a model that combines 3 dimensional convolutional neural networks \n(CNNs) and Transformers. This unique blend of technologies known as the 3 D ResNet \nTransformer aims to effectively capture both spatial and temporal characteristics, within video \nsequences without sacrificing computational speed. Through this process I outline the method I \nused to develop, execute and assess the effectiveness of this model. This model used the base \narchitecture of 3D ResNet – Transformer, effective adjustments are made in terms of various \nparameters to fine-tune the model for autonomous driving.  The base model architecture is taken \nform the paper: Integrating 3D convolutional neural networks and transformer for  video action \nrecognition by Yan Cheng, Lingfeng Wan [2]. \n \nThe main aim was to utilize the spatial and temporal feature extraction abilities of 3D ResNet \nand improve the overall temporal relationships with the self-attention mechanism of Transformers. \nIn order to validate the model’s performance a variety of experiments were carried out using  the \ndatasets, UFC101, COCO, Waymo Open. The subsequent sections provide in-depth information \non the datasets, model structure, application details and assessment methodology. \n3.1 Dataset Description \nTo train and evaluate th e 3D ResNet-Transformer, I used UCF101, COCO and Waymo \nOpen. These datasets are benchmarks in the  object recognition for autonomous driving domain \ndue to their diverse range of categories and challenging scenarios. \n3.1.1 UCF101 Dataset \nThis The UCF101 dataset contains 13, 320 video clips grouped into 10 1 categories \nincluding sports activities and interactions involving humans and objects or human to human \ninteractions. Divided into 9,537 training videos and 3, 783 test videos . Dealing with this dataset \npresented challenges like differences between classes, changing backgrounds and varying video \nquality levels. \n \n47 \n3.1.2 COCO-VID Dataset \nThe COCO-VID dataset is based on the well -known COCO dataset which was initially \nused for image based object detection and segmentation, to meet the need for object detection and \ntracking in video sequences . It includes temporal annotations which allow to analyze the \nmovement of objects and their interactions in adjacent frames.  COCO-VID has the same \ncharacteristics as COCO in terms of diversity since it includes various objects and complex scenes, \nbut it has been extended to include temporal information to support video object detection. This \ndataset is very relevant for the development and testing of algorithms that are intended to identify \nand follow objects in three dimensional spaces with much background. \n3.1.3 Waymo Open Dataset \nWaymo Open Dataset is a comprehensive dataset which aims at supporting autonomous \ndriving research with a large amount of high quality sensor data, such as LiDAR, camera and 3D \nannotations.  It includes a number of training categories which are important for the autonomous \nsystems to function in the real world such as vehicles, pedestrians, cyclists and signs. The dataset \ncontains both 2D and 3D bounding box labels and per frame object tracks for scene understanding. \nIt covers a number of different environments including urban, suburban and highway which makes \nit ideal for model training for various tasks like object detection, semantic  segmentation and \nbehavior prediction. Thus, the Waymo Open Dataset with its precise annotations and multimodal \ndata is essential for the development of the state -of-the-art perception algorithms in self-driving \nsystems. \n \n \nTable 3.1. Comparative summary of the datasets \nDataset Categories Training Set Test Set \nUCF101 101 9,537 3,783 \nCOCO 80 118K 41K \nWaymo Open 4 1000+ 200+ \n \n \n48 \n3.2 Model Framework \nTo address the challenges of spatiotemporal feature extraction and long-range dependency \nmodeling, I fine-tuned the 3D ResNet -Transformer model, which consists of two main \ncomponents: a 3D ResNet backbone and a Transformer encoding layer. Below, I describe the task \nand architecture in detail. \n3.2.1 Task Description \nThe object detection task involves predicting the class of a given video segment. Formally, \nlet D =  {(xI, yi)}{i=1}\nn  represent the dataset, where 𝑥𝑖  is a video segment and 𝑦𝑖 is the \ncorresponding action label [2]. My goal was to design a model g such that g(𝑥𝑖)= 𝑦𝑖 for any unseen \n𝑥𝑖  . The model outputs a probability distribution over all possible classes using a softmax classifier. \n3.2.2 3D ResNet Backbone \nThe 3D ResNet component of my model was responsible for extracting local \nspatiotemporal features from video data. I used a 3D ResNet -34 architecture, which applies 3D \nconvolutions to process video segments. The input to the 3D ResNet was a tensor of shape \n(N,C,T,H,W), where N is the batch size,  C is the number of channels (RGB), T is the temporal \ndimension, and H, W are the spatial dimensions. \n \nEach 3D convolutional layer applied a kernel K of size (kt, kh, kw) to capture both spatial \nand temporal features. The operation is defined as: \n𝑌(𝑖, 𝑗, 𝑘) =  ∑ ∑ ∑ ∑ 𝑊(𝑝, 𝑞, 𝑟)\n𝑅\n{𝑟=1}\n𝑄\n{𝑞=1}\n𝑃\n{𝑝=1}\n𝐶\n{𝑐=1}\n⋅ 𝑋(𝑖 + 𝑝, 𝑗 + 𝑞, 𝑘 + 𝑟) +  𝑏 \n \nHere, W represents the kernel weights, X is the input tensor, and b is the bias term. \nTo address vanishing gradients in deep networks, I incorporated residual connections, \nallowing gradients to flow more effectively during backpropagation. This design ensured that the \nnetwork could extract rich spatiotemporal features across layers [2]. \n \n49 \n3.2.3 Transformer Encoding Layers \nAfter processing the video through the 3D ResNet, I passed the extracted features to a \nTransformer encoding layer. This layer utilized a multi -head self-attention mechanism to model \nglobal temporal relationships. The self -attention mechanism calculates the  interaction between \ninput features as: \n                    Attention(𝑄, 𝐾, 𝑉) = softmax (\n𝑄𝐾𝑇\n√𝑑𝑘\n) 𝑉                                          3.2 \n \nwhere Q,  K, V are the query, key, and value matrices derived from the input, and 𝑑𝑘  is the \ndimensionality of K. To preserve temporal ordering, I added positional encodings to the input \nfeatures: \n                                           𝑃𝐸(pos, 2𝑖 + 1) = cos (pos ⋅\n10000\n2𝑖\n𝑑\n 1 )                                             3.3 \n                                           𝑃𝐸(pos, 2𝑖 + 1) = cos (pos ⋅\n10000\n2𝑖\n𝑑\n1 )                                             3.4 \nThe final output is a tensor of shape (N,  Classification_Num) where  Classification_Num \ncorresponds to the number of action classes [2]. \n \nFig 3.1. Methodology Overview \n\n \n50 \n \nFigure 3.2. Base Architecture of 3D ResNet-Transformer Model [2] \n3.2.4 DeepStream SDK for Object Detection \nWhen aiming to improve the object detection abilities of the suggested 3D ResNet -\nTransformer model I utilized NVIDIAs DeepStream SDK as a component of the deployment \nprocess.DeepStream SDK stands out as a robust, finely tuned platform designed for creating video \nanalytics applications powered by AI making it a great fit for carrying out real time object detection \ntasks.This segment goes over how DeepStream SDK was incorporated into the methodology \ndiscusses its advantages and highlights how it plays a role,  in enhancing the performance of the \nobject detection system. \n \nNVIDIA has created the DeepStream SDK to help with streaming analytics for AI \napplications such as video analysis in areas like retail and surveillance, in smart cities and \nautonomous machines. DeepStream makes use of NVIDIAs TensorRT to speed up learning \nmodels and utilizes GStreamer to handle video streams efficiently on NVIDIA GPUs for real time \n\n \n51 \ninference and data processing. For this purpose, all the data training and model building is done in \na GPU powered by NVIDIA named Jetson AGX Orin. \n• Key Features of DeepStream SDK: \no Scalable Video Analytics: Handles multiple video streams simultaneously with \nGPU acceleration. \no Hardware Efficiency: Provides support for Jetson devices and NVIDIA GPUs, \nenabling object detection applications to run on embedded systems. \no GStreamer Integration: Uses GStreamer plugins to manage streaming data, \nincluding input preprocessing, model inference, and post-processing stages. \nModel Preparation for DeepStream \nAfter completing the training of the 3 D ResNet-transformer model for recognizing video \nactions and detecting objects in videos successfully I proceeded to utilize TensorRT for converting \nthe trained model into a more efficient format suitable for seamless integration, with DeepStreams \noperations. \n• The conversion includes: \no Model Quantization: Model was quantized to use FP16 precision, which reduces \ncomputation while maintaining high accuracy. \no TensorRT Conversion: Model was converted into a TensorRT -optimized engine, \nenhancing inference speed through reduced precision operations and model \noptimization. \nPipeline Development using DeepStream \nI then developed a GStreamer -based DeepStream Pipeline to implement real -time object \ndetection. The pipeline consists of several components that process video frames from the input to \ngenerate detection outputs. Here are the stages of the pipeline: \n• Video Stream Input: \no The video data was sourced from a live feed and pre-recorded files. \n \n52 \no DeepStream’s nvstreammux plugin was used to merge multiple video feeds into a \nsingle stream, hence proceeds with efficient resource usage during processing. \n• Preprocessing: \no It involved resizing, normalization, and frame batching using the nvvideoconvert \nand nvdspreprocess plugins. This made the video data suitable for input to the 3D \nResNet-Transformer model. \n• Inference with Optimized Model: \no The optimized 3D ResNet -Transformer model was integrated using the nvinfer \nplugin. This plugin used the TensorRT engine for efficient real -time inference on \nincoming video frames. \no The self-attention mechanism of the Transformer layer was useful for enhancing \nfeature relationships in scenes involving overlapping or occluded objects. \n• Post-Processing and Object Tracking: \no The results were processed using DeepStream’s nvtracker plugin to maintain \nunique IDs for detected objects across frames. \no Additional custom plugins were used to implement post -processing logic, such as \nnon-maximum suppression (NMS), to refine bounding box predictions. \n• Output Display and Analytics: \no The processed video was displayed using the nveglglessink plugin, while analytics \ndata was logged for further evaluation. \n \n53 \n \nFigure 3.3. DeepStream Pipeline for the 3D ResNet-Transformer Model \nAdvantages of Using DeepStream SDK \nThe incorporation of DeepStream SDK into the object detection workflow offered many \nadvantages: \n• Real-Time Inference: \no DeepStream SDK, in combination with TensorRT optimization, enabled real -time \ninference for high-resolution video streams. \n• High Throughput and Scalability: \no DeepStream’s ability to process multiple streams simultaneously allowed the \nmodel to be deployed on multiple input sources concurrently, leading to high \nthroughput in video analytics. \n\n \n54 \no By running the system on NVIDIA Jetson AGX Orin, latency is greatly reduced \ncompared to running on traditional GPUs. \n• Efficient Resource Utilization: \no Using FP16 precision with TensorRT helped balance computational load while \nmaintaining accuracy. This made it easy to run the model on GPU -accelerated \ndevices with less computational resources. \no DeepStream's efficient memory management and GStreamer integration helped \nreduce the overhead involved in frame handling and batch processing. \n• Integrated Object Tracking: \no Using the nvtracker plugin for object tracking allowed the model to maintain \nconsistency in object IDs across frames. \n3.3 Autonomous Drone Setup \nThis section provides the hardware and software configuration for the autonomous drone \nsystem. The setup integrates multiple  components to achieve autonomous flight, navigation, and \nobstacle detection. Each system was carefully selected to meet the project's requirements for \nefficiency, reliability, and scalability. \n3.3.1 Drone Frame and Propulsion System \n  The drone is built on a robust quad -propeller chassis designed to accommodate all  \ncomponents securely while ensuring optimal aerodynamics and stability. This drone’s parts are  \nordered from an external source “Holybro”. The propulsion system includes four brushless motors \nand electronic speed controllers (ESCs), providing efficient thrust and maneuverability. Power is \nsupplied by a 5A LiPo battery pack. Drone's navigation and control is done by Pixhawk 6X flight \ncontroller, a powerful and versatile module for autonomous flight. The Pixhawk 6X is responsible \nfor stabilizing the drone, executing flight plans, and integrating data from sensors. It provides real-\ntime feedback to maintain stability during flight and interfaces seamlessly with other components. \nThis is connected to the Jetson Nano via ethernet port to communicate. \n \n \n55 \n3.3.2 Computing Unit \n  The drone uses a Jetson Nano as its onboard computing unit. This module is specifically \nchosen for its GPU-accelerated processing capabilities and lightweight, enabling the execution of \ndeep learning algorithms for real -time object detection and path planning. The Jetson Nano \nprocesses data and communicates with the Pixhawk 6X to adjust flight parameters dynamically.  \nFlight parameters are passed by generating a p roto.txt file and passing it to the flight controller. \nJetson Nano runs a companion computer software stack  called MAVROS. This is a ROS (Robot \nOperating System) node that acts as a bridge between the Jetson Nano and the Pixhawk. \n3.3.3 Obstacle Detection and Depth Sensing \n  To achieve precise obstacle avoidance, the drone is equipped with an Intel RealSense D435 \ndepth camera. The camera captures high -resolution depth maps and RGB images, allowing the \nsystem to detect obstacles in the drone's path and calculate alternative paths. Its lightweight design \nand compatibility with the Jetson Nano make it an ideal choice for this application. \n3.3.4 GPS Navigation \n  The drone is equipped with a GPS module to support autonomous waypoint navigation and \ngeofencing. The GPS module provides accurate location data, enabling the Pixhawk 6X to execute \npre-programmed flight missions with precision. The GPS data is also utilized to maintain the \ndrone's position during hover and adjust its route dynamically in response to obstacles. \n3.3.5 Manual Control \n  Manual control is  done by a FlySky transmitter and receiver , providing  direct control \nduring testing phases or emergency operations. Th is step is crucial as it enables reliable \ncommunication between the operator and the drone for takeoff, landing, and overrides. The \ntransmitter operates on a stable frequency range to minimize interference and maintain consistent \nconnectivity. \n \n56 \n \nFig 3.4. Hardware Setup for Autonomous Flight \n \n  \n \nFig 3.5. Pixhawk 6X Flight Controller (left), Jetson Nano (top middle), Flysky Transmitter \n(right), Intel RealSense D435 Depth Camera (bottom) \n\n \n57 \n3.4 Implementation Details \n3.4.1 Preprocessing \nI preprocessed the video data by sampling frames at 16 frames per second (fps) and \nresizing them to 112×112  pixels. I normalized the pixel values to zero mean and unit \nvariance for consistent input representation. \n3.4.2 Training \nThe 3D ResNet component was initialized using weights pre -trained on the Kinetics-400 \ndataset. Categorical cross-entropy loss has been used to train the model [2]: \n                      𝑳 = −\n𝟏\n𝑵 ∑ ∑ 𝒚𝒊,𝒌\n𝑲\n𝒌=𝟏 𝐥𝐨𝐠(𝒚𝒊,𝒌̂ )𝑵\n𝒊=𝟏                                                       3.5 \nHere, 𝑦𝑖,𝑘 is the true label, and (𝑦𝑖,𝑘̂ ) is the predicted probability for class k. \nI optimized the model using stochastic gradient descent (SGD) with momentum  μ=0.93 and a \nlearning rate schedule of 10(−2). \n3.4.3 Hyperparameter Optimization & Performance \nHyperparameter tuning is crucial for enhancing the efficiency of the 3D ResNet -\nTransformer model. There were several hyperparameters considered during training sessions and \neach affected them differently.  \nLearning Rate: \nThe learning rate controls the step size during gradient descent. I experimented with a range \nof initial learning rates, including 10(−1), 10(−2), 10(−3).  \no Impact: \n▪ A high initial learning rate 10(−1) caused instability, leading to fluctuating \nloss values during training. \n \n58 \n▪ A moderate rate 10(−2) provided smooth convergence and yielded the best \nresults. \n▪ A low rate 10(−3) resulted in slower convergence and suboptimal \nperformance due to insufficient exploration of the parameter space. \nBatch Size: \nThe batch sizes of 16, 32 and 64 were tested to balance computational efficiency and \ngradient stability. Larger batch sizes improved gradient estimation accuracy but required more \nmemory, which limited their practicality. \no Impact: \n▪ A batch size of 32 provided the best trade -off between computational \nefficiency and performance. Larger batches led to marginally better \naccuracy but increased memory usage significantly, while smaller batches \ncaused noisier gradient updates. \nDropout Rate: \nThe Dropout was used to regularize the transformer encoding layer, with rates of 0.1 , 0.2 \nand 0.5 being tested. \no Impact: \n▪ A dropout rate of 0.1 balanced regularization and information retention, \nachieving the best results.  \n▪ Higher dropout rates caused underfitting, as too many features were \nrandomly zeroed during training, reducing the model’s capacity. \nWeight Initialization: \nThe 3D ResNet backbone was initiated with weights pre -trained on the Kinerics-400 \ndataset, while the transformer layers were initialized using pretrained Vision Transformer model \non ImageNet dataset. \n \n59 \no Impact: \n▪ Using pre -trained weights significantly accelerated convergence and \nimproved accuracy on datasets compared to training from scratch. \nOptimizer: \nThe optimizer used was Stochastic Gradient Descent (SGD) with momentum  0.93. \nAlternative optimizers such as Adam and RMSprop were also tested but performed slightly worse.  \no Impact: \n▪ SGD with momentum provided smoother convergence and better \ngeneralization, as it efficiently traversed the loss landscape while avoiding \nlocal minima. \nNumber of Transformer Attention Heads: \nThe number of heads in the transformer’s multi -headed attention mechanism determines \nhow many parallel attention distributions are computed. The graph shows the diminishing results \nof increasing attention heads. \n \nFig 3.6. Attention heads Performance on Each Dataset \n \n\n \n60 \nSummary of Optimized Hyperparameters: \nThe following configuration provided the best performance: \n• Learning Rate: 10(−2) \n• Batch Size: 32 \n• Attention Heads: 8 \n• Dropout Rate: 0.1 \n• Weight Initialization: Pre -trained Kinetics-400; Vision Transformer on ImageNet for \nTransformer \n• Optimizer: SGD with momentum. \n3.5 Evaluation \nI evaluated the model  using the datasets mentioned above using accuracy as the primary \nmetric: Accuracy =\n𝑻𝒄\n𝑻 × 𝟏𝟎𝟎%. Where 𝑇𝑐 is the number of correctly classified samples and T is \nthe total number of test samples. The model achieved  mAP of 47.1 on COCO dataset, 67.2 on \nWaymo Open and 61.5 on UCF101 dataset. \n  \n \n61 \n4. RESULTS & DISCUSSION \nIn this section of the study a detailed assessment is given of the 3D ResNet-transformer \nmodel, for detecting objects effectively. The model’s performance is examined using datasets and \nits outcomes are contrasted with current techniques. Significant discoveries emphasize the model’s \ncapacity to manage spatiotemporal connections optimize computational effectiveness and adapt \nacross various tasks.  \n \nIn the domain of  object detection, the model displayed accuracy levels when tested on the  \ndatasets COCO, Waymo Open and UCF101. Further insights into these outcomes are elaborated \nin the sections with thorough analysis complemented by tables, graphs and detailed conversations. \n4.1.1 Dataset Performance \nThe 3D ResNet-Transformer underwent Testing using the same datasets known as common \nbenchmarks, in  object detection . The assessment criteria applied were the accuracy metric \ntypically used for evaluating classification models. The outcomes of the suggested model were \ncontrasted with baseline models. \nTable 4.1. Accuracy Comparison on UCF101 \nModel UCF101 Accuracy \n(mAP) \nWaymo Open \n(mAP) \nCOCO (mAP) \nTwo-Stream CNN 44.0  [2] 32.4 23.5 \nRNN-FV+iDT 56.5  [2] 42.7 30.8 \nSTC-ResNext101 67.6  [2] 58.5 39.7 \nD3D 70.1  [2] 64.9 46.8 \nVidTr 69.2  [2] 66.0 47.5 \n3D ResNet-\nTransformer (with \nfine-tuning) \n61.5 67.2 47.1 \n \nThe findings show that fine-tuned model performs similar to the best model D3D. This fine-tuned \nmodel is trained using 3 datasets. D3D integrates dynamic convolutions to optimize both spatial \nand temporal feature representations efficiently.  This maybe the reason for better ac curacy, 3D \nResNet-Transformer model benefits from training on three datasets, which enhances \n \n62 \ngeneralization but may introduce some domain mismatch if the categories differ significantly from \nUCF101. \n4.1.2 Ablation Study \nTo quantify the contributions of the 3D ResNet and Transformer components, I conducted \nan ablation study. The results, presented in Table 4.2, highlight the necessity of integrating  the \nmodules. \nTable 4.2. Impact of 3D ResNet and Transformer Components \nComponent COCO (mAP) UCF101 (%) Waymo Open (mAP) \nTransformer Only 43.5 95.0 64.2 \n3D ResNet -\nTransformer \n47.1 96.3 67.2 \n \nThe research findings indicate that although the Transformer encoding layer performs well \non its own, incorporating the 3D ResNet substantially improves accuracy when analyzing several \ndatasets. This confirms the impact of blending spatiotemporal feature extraction, with long range \ndependency modeling. \n4.1.3 Object Detection Results \nTo test the model’s capability in object detection, I fine -tuned it on the COCO dataset, \nwhich is a benchmark for evaluating object detection models. The evaluation metric used was \nMean Average Precision (mAP), calculated over 80 object classes.  Some example figures are \nshown below. All these were captured using the proposed model. \n \n63 \n \nFig 4.1. mAP (mean Average Precision) Metrics for Different Models across the COCO, \nWaymo Open, and UCF101 datasets \n \n \nFigure 4.2. Trail Detection and Follow Captured form the Camera on a Drone \n \nTable 4.2. mAP Metrics of Some Datasets Over 3D ResNet-Transformer Model and Others \nModel mAP (%) \nFaster R-CNN 42.3 \nYOLOv7 45.1 \n3D ResNet-Transformer 47.1 \n\n \n64 \n \nThe results indicate that the proposed model outperforms popular object detection architectures \nlike Faster R -CNN and YOLOv 7. The improvement in mAP is attributed to the ability of the \nTransformer encoding layer to capture contextual relationships between objects. \n \n \nFigure 4.3. Image of the Drone with Intel RealSense Camera mounted \n \nFigure 4.4. Semi-dense SLAM Maps for Obstacle Avoidance \n \n\n \n65 \n \nFigure 4.5. Information Monitoring from Jetson Device \n4.1.4 Performance Summary \nThe results confirm the effectiveness of the 3D ResNet -Transformer in handling object \ndetection tasks as well as the ability of autonomous flying of drone: \n• State-of-the-Art Results: The model achieves the better consistent accuracy on various \ndatasets and competitive mAP on object detection benchmarks. \n• Versatility: The hybrid architecture adapts seamlessly to diverse tasks, highlighting its \nflexibility. \n4.1.5 Challenges and Limitations \nDespite its strengths, the model has certain limitations: \n• Computational Complexity: The hybrid design increases the computational overhead, \nwhich may hinder real-time applications. \n• Dataset Dependency: Performance depends heavily on the quality and diversity of \ntraining datasets. \n\n \n66 \nThe outcomes underscore the performance of the fine-tuned 3D ResNet-Transformer Model and \nits room for improvement in the future showcasing that the integration of 3 D ResNet and \nTransformer creates a sturdy structure suitable, for tasks dealing with intricate visual information. \n  \n \n67 \n5. CONCLUSION & FUTURE WORK \nThe main goal of this research was to create a hybrid model called 3D ResNet-Transformer \nfor detecting objects in videos more efficiently and use the same model for autonomous systems. \nBy merging the ability of 3D ResNet to extract features with the self -focus mechanism of \nTransformers this model effectively tackled the issues related to intricate temporal connections \nand spatial feature associations, in video datasets. This project al so integrated the NVIDIA \nDeepStream SDK for implementation in real world situations like advanced video analysis and \nedge computing applications. This chapter outlines the discoveries and impacts of this study. \n5.1 Achievements in Object Detection \nThe 3 D ResNet-Transformer model underwent  testing on UCF101, COCO and Waymo \nOpen and demonstrated leading edge performance levels by attaining Map scores of 61.5, 47.1 and \n67.2 respectively. This model proved it’s consistency over other established methods such, as the \nTwo Stream CNNs, RNN FVs+iDTs and STCs ResNext101 versions showcasing notable \nadvancements. Results indicate the following achievements: \n• The combination of 3D convolutional layers and Transformer encoding allowed the system \nto effectively grasp both nearby spatial and temporal patterns as well, as distant \nconnections demonstrating higher levels of precision. \n• The collaborative impact of merging two frameworks has contributed to addressing the \nissues posed by occlusions and simultaneous actions often encountered in tasks related to \nrecognizing actions. \n5.2 Object Detection Capabilities and Deployment \nThe suggested framework was customized for object detection assignments utilizing the \nCOCO dataset. Yielded a Mean Average Precision (mAP) of 47.1 over several established models \nsuch as Faster R-CNN and YOLO v 7. This model could be deployed in real-world scenarios with \nthe following outcomes:  \n \n68 \n• The integration of NVIDIA TensorRT to optimize the model for inference led to substantial \nimprovements in speed, enabling real-time detection. \n• Applying the DeepStream SDK to implement the model ’s deployment offered benefits \nsuch as boosting the frame rate to 30 FPS and cutting inference latency. These \nenhancements make the model well suited for performing real time analysis in fields such \nas video monitoring and self-driving cars and autonomous flying drones. \n• The model was used on Jetson edge devices, it showed scalability in its design that allowed \nit to be versatile, in low power settings while still maintaining detection capabilities. \n• By employing DeepStream SDK and TensorRT, the model was able to meet real -time \nrequirements, thereby making it suitable for practical deployment in edge devices and \nlarge-scale systems. \n5.3 Future Work \nThe model showed promising performance however there are multiple ways for additional \ninvestigation and improvement to be explored further in depth below are some exciting paths \nfor future research that could greatly enhance the potential of this model and overcome its \nexisting constraints. \nEnhancing Transformer Architectures: \nThe self-focus mechanism in the Transformer is computationally intensive when dealing \nwith lengthy video sequences; henceforth there could be potential, for investigating alternative \nTransformer structures that offer greater efficiency in such scenarios. \nReal-Time and Low Latency Optimizations: \nAlthough the current model has achieved frame rates there is room for additional \nimprovements to ensure that the model is optimized for quick and real-time deployment. \n• Implementing pruning techniques to remove redundant weights and quantization to INT8 \nprecision can lead to significant speed improvements without major losses in accuracy. \n \n69 \n• Adaptive inference mechanisms, where not all frames are fully processed, could reduce the \ncomputational load further while retaining detection accuracy. Dynamic computation \ngraphs and adaptive frame skipping could be employed to achieve this. \nApplication to Multi-Modal Learning: \nThe current model is focused only on visual data , future extensions could involve the \naddition of multimodal inputs. \n• Including audio signals along with videos can enhance detection. \n• Adding text inputs, such as captions or context descriptions, to enhance scene \nunderstanding. \nImproving Spatiotemporal Consistency in Object Detection: \nTo further improve the spatiotemporal consistency of the detected actions: \n• Adding memory modules could enable the model to remember crucial previous features. \n• Adding Long Short-Term Memory (LSTM) layers with the Transformer encoder may \noffer enhanced temporal modeling by explicitly modeling time-based dependencies. \nBroader Applications in Real-World Scenarios: \nAdapting the model for pedestrian detection  in autonomous vehicles may improve safety \nby accurately predicting actions and movement. U sing the model for monitoring patients and \nrecognizing activities, such as falls or abnormal behaviors, would help healthcare professionals \nin providing timely interventions. Applying the model to sports action recognition and tracking \ncould provide insights into player movements,  strategies, and statistics  that can be used for \nfurther modifications and help in better gameplay. \n  \n \n70 \n5.4 Concluding Reflections \nDeveloping and perfecting the proposed 3D ResNet model combined with Transformer has \nbeen a demanding yet enlightening journey that provided valuable insights into video analysis \nand object detection fields. The unique fusion of convolutional neural networks and \nTransformers attention mechanisms in this proposed model has led to an effective solution for \nunderstanding intricate relationships within video data. \n \nOne important achievement of this study was addressing the difficulties  in videos – like \nocclusions and different camera perspectives as well as changes in the background – by using \na mixed method approach. The integration of 3D ResNet and Transformer layers enabled the \nextraction of spatiotemporal features and modeling long range temporal patterns to create a \nstrong framework that performed exceptionally well considering the consistent accuracy over \n3 datasets. \n \nThe DeepStream SDK was crucial in getting the system ready for use and improving its \nscalability and hardware efficiency. It demonstrates how sophisticated AI models can be \nseamlessly incorporated into applications in the real world to show the blend of innovative \nresearch and real world implementation.  \n \nThe study also pointed out how flexible the 3D ResNet. Transformer model is in adjusting \nto types of data and real-world situations it faces. When used on Jetson edge devices it showed \nits ability to work well in environments with limited resources which means it can be used for \ndistributing complex video analysis tasks to the edges. Its ability to handle challenges and \nadapt well makes it a great asset for fields like healthcare monitoring and analyzing sports and \nretail behavior.  \n \nThe future presents chances to improve the existing solution further by exploring \nalternative Transformer architectures that could enhance efficiency and reduce resource \nrequirements in edge computing environments. Ensuring real time performance and reduci ng \nlatency through techniques like pruning and quantization are measures to enhance the model’s \npreparedness, for widespread implementation. Overall, the 3D ResNet-Transformer model is \n \n71 \na step forward in object detection. By integrating spatial and temporal feature extraction with \nlong range attention mechanisms fine tuning it with TensorRT and implementing it using \nDeepStream SDK the model successfully meets the demands for both performance and real \ntime processing. \n  \n \n72 \nREFERENCES \n1. K. Simonyan and A. Zisserman, \"Two -Stream Convolutional Networks for Action \nRecognition in Videos,\" in Advances in Neural Information Processing Systems \n(NeurIPS), Montreal, QC, Canada, Dec. 2014. \n2. Y. Cheng and L. Wan, \"Integrating 3D Convolutional Neural Networks and \nTransformer for Video Action Recognition,\" College of Computer Information \nEngineering, Jiangxi Normal University, Nanchang, China, 2023. \n3. D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, \"Learning Spatiotemporal \nFeatures with 3D Convolutional Networks,\" in Proceedings of the IEEE International \nConference on Computer Vision (ICCV), Santiago, Chile, Dec. 2015. \n4. J. Carreira and A. Zisserman, \"Quo Vadis, Action Recognition? A New Model and the \nKinetics Dataset,\" in Proceedings of the IEEE Conference on Computer Vision and \nPattern Recognition (CVPR), Honolulu, HI, USA, Jul. 2017. \n5. C. Feichtenhofer, H. Fan, J. Malik, and K. He, \"SlowFast Networks for Video \nRecognition,\" in Proceedings of the IEEE International Conference on Computer \nVision (ICCV), Seoul, South Korea, Oct. 2019. \n6. A. Dosovitskiy et al., \"An Image is Worth 16x16 Words: Transformers for Image \nRecognition at Scale,\" in International Conference on Learning Representations \n(ICLR), Virtual Event, May 2021. \n7. Z. Liu et al., \"Swin Transformer: Hierarchical Vision Transformer Using Shifted \nWindows,\" in Proceedings of the IEEE International Conference on Computer Vision \n(ICCV), Montreal, QC, Canada, Oct. 2021. \n8. Y. Li et al., \"VidTr: Video Transformer Without Convolutions,\" in Proceedings of the \nIEEE International Conference on Computer Vision Workshops (ICCVW), Montreal, \nQC, Canada, Oct. 2021. \n \n73 \n9. K. He, X. Zhang, S. Ren, and J. Sun, \"Deep Residual Learning for Image Recognition,\" \nin Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition \n(CVPR), Las Vegas, NV, USA, Jun. 2016. \n10. H. Wang et al., \"Non-Local Neural Networks,\" in Proceedings of the IEEE Conference \non Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, Jun. \n2018. \n11. A. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, \"HMDB: A Large Video \nDatabase for Human Motion Recognition,\" in Proceedings of the IEEE International \nConference on Computer Vision (ICCV), Barcelona, Spain, Nov. 2011. \n12. K. Soomro, A. R. Zamir, and M. Shah, \"UCF101: A Dataset of 101 Human Actions \nClasses from Videos in the Wild,\" arXiv preprint arXiv:1212.0402, Dec. 2012. \n13. J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \"You Only Look Once: Unified, \nReal-Time Object Detection,\" in Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition (CVPR), Las Vegas, NV, USA, Jun. 2016. \n14. W. Liu et al., \"SSD: Single Shot MultiBox Detector,\" in European Conference on \nComputer Vision (ECCV), Amsterdam, Netherlands, Oct. 2016. \n15. Y. Zhu et al., \"Hidden Two-Stream Convolutional Networks for Action Recognition,\" \nin Proceedings of the British Machine Vision Conference (BMVC), Swansea, UK, Sep. \n2016. \n16. P. Zhou et al., \"Temporal Relational Reasoning in Videos,\" in Proceedings of the \nEuropean Conference on Computer Vision (ECCV), Munich, Germany, Sep. 2018. \n17. R. Goyal et al., \"The Kinetics Human Action Video Dataset,\" arXiv preprint \narXiv:1705.06950, May 2017. \n18. X. Wang et al., \"Temporal Context Networks for Action Recognition in Videos,\" in \nProceedings of the IEEE International Conference on Computer Vision (ICCV), \nVenice, Italy, Oct. 2017. \n \n74 \n19. F. Caron et al., \"Emerging Properties in Self -Supervised Vision Transformers,\" in \nProceedings of the IEEE International Conference on Computer Vision (ICCV), \nMontreal, QC, Canada, Oct. 2021. \n20. K. Simonyan and A. Zisserman, \"Two -Stream Convolutional Networks for Action \nRecognition in Videos,\" in Advances in Neural Information Processing Systems \n(NeurIPS), Montreal, QC, Canada, Dec. 2014. \n21. D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, \"Learning Spatiotemporal \nFeatures with 3D Convolutional Networks,\" in Proceedings of the IEEE International \nConference on Computer Vision (ICCV), Santiago, Chile, Dec. 2015. \n22. J. Carreira and A. Zisserman, \"Quo Vadis, Action Recognition? A New Model and the \nKinetics Dataset,\" in Proceedings of the IEEE Conference on Computer Vision and \nPattern Recognition (CVPR), Honolulu, HI, USA, Jul. 2017. \n23. C. Feichtenhofer, H. Fan, J. Malik, and K. He, \"SlowFast Networks for Video \nRecognition,\" in Proceedings of the IEEE International Conference on Computer \nVision (ICCV), Seoul, South Korea, Oct. 2019. \n24. A. Dosovitskiy et al., \"An Image is Worth 16x16 Words: Transformers for Image \nRecognition at Scale,\" in International Conference on Learning Representations \n(ICLR), Virtual Event, May 2021. \n25. Z. Liu et al., \"Swin Transformer: Hierarchical Vision Transformer Using Shifted \nWindows,\" in Proceedings of the IEEE International Conference on Computer Vision \n(ICCV), Montreal, QC, Canada, Oct. 2021. \n26. Y. Li et al., \"VidTr: Video Transformer Without Convolutions,\" in Proceedings of the \nIEEE International Conference on Computer Vision Workshops (ICCVW), Montreal, \nQC, Canada, Oct. 2021. \n27. K. He, X. Zhang, S. Ren, and J. Sun, \"Deep Residual Learning for Image Recognition,\" \nin Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition \n(CVPR), Las Vegas, NV, USA, Jun. 2016. \n \n75 \n28. H. Wang et al., \"Non-Local Neural Networks,\" in Proceedings of the IEEE Conference \non Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, Jun. \n2018. \n29. A. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, \"HMDB: A Large Video \nDatabase for Human Motion Recognition,\" in Proceedings of the IEEE International \nConference on Computer Vision (ICCV), Barcelona, Spain, Nov. 2011. \n30. K. Soomro, A. R. Zamir, and M. Shah, \"UCF101: A Dataset of 101 Human Actions \nClasses from Videos in the Wild,\" arXiv preprint arXiv:1212.0402, Dec. 2012. \n31. J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \"You Only Look Once: Unified, \nReal-Time Object Detection,\" in Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition (CVPR), Las Vegas, NV, USA, Jun. 2016. \n32. W. Liu et al., \"SSD: Single Shot MultiBox Detector,\" in European Conference on \nComputer Vision (ECCV), Amsterdam, Netherlands, Oct. 2016. \n33. Y. Zhu et al., \"Hidden Two-Stream Convolutional Networks for Action Recognition,\" \nin Proceedings of the British Machine Vision Conference (BMVC), Swansea, UK, Sep. \n2016. \n34. P. Zhou et al., \"Temporal Relational Reasoning in Videos,\" in Proceedings of the \nEuropean Conference on Computer Vision (ECCV), Munich, Germany, Sep. 2018. \n35. R. Goyal et al., \"The Kinetics Human Action Video Dataset,\" arXiv preprint \narXiv:1705.06950, May 2017. \n36. Y. Wang et al., \"TACT: Temporal Attention and Contextual Transformer for Video \nObject Detection,\" in Proceedings of the IEEE/CVF Winter Conference on \nApplications of Computer Vision (WACV), Waikoloa, HI, USA, Jan. 2021. \n37. D. Bertasius, H. Wang, and L. Torresani, \"Is Space -Time Attention All You Need for \nVideo Understanding?\" in Proceedings of the International Conference on Machine \nLearning (ICML), Virtual Event, Jul. 2021. \n \n76 \n38. X. Wang et al., \"Temporal Context Networks for Action Recognition in Videos,\" in \nProceedings of the IEEE International Conference on Computer Vision (ICCV), \nVenice, Italy, Oct. 2017. "
  },
  {
    "source": "PEEB_Poster_Ryiaz_2023.pdf",
    "content": "Acknowledgment\nResearch Goal\nEnhancing First-Year Engineering Student Engagement through the Development of an Open-Hardware Teaching Platform: A Pilot Study\n26thAnnual Student Research and Creative Endeavors SymposiumIRB Approval -Purdue University: 2022-1280\nPresenter: Riyaz ShaikAdvisor: Claudio Freitas, PhD. Electrical and Computer Engineering Department\nConclusion and Future Work\nReference\nBackground Preliminary ResultsDiscussion\n•The success of undergraduate engineering students depends on high levels of engagement and motivation [1][2]•One of the most effective pedagogical approaches to teaching engineering and increasing self-concept and self-efficacy is active learning [3][4]•The use of technologies that promote active learning through hands-on activities in classroom is now an integral aspect of active learning in Science, Technology, Engineering, and Math (STEM) learning.•Making sense of the use of technology in STEM is not straightforward. Integrating technology into engineering courses without reassessing and reimagining core educational features is challenging.\n[1] B. F. French, J. C. Immekus, and W. C. Oakes, “An Examination of Indicators of Engineering Students’ Success and Persistence,” Journal of Engineering Education, vol. 94, no. 4. Wiley, pp. 419–425, Oct. 2005.[2] D. Wilson et al., “The Link between Cocurricular Activities and Academic Engagement in Engineering Education,” Journal of Engineering Education, vol. 103, no. 4. Wiley, pp. 625–651, Oct. 2014.[3]J. M. Braxton, W. A. Jones, A. S. Hirschy, and H. V. Hartley III, “The role of active learning in college student persistence,” New Directions for Teaching and Learning, vol. 2008, no. 115. Wiley, pp. 71–83, Jun. 2008. doi: 10.1002/tl.326.[4] Hyun, J., Ediger, R., & Lee, D. Students’ Satisfaction on Their Learning Process in Active Learning and Traditional Classrooms. International Journal of Teaching and Learning in Higher Education, 29(1), 108–118. 2017.\n•We propose an educational framework to foster meaningful learning experiences and hands-on activities in a first-year engineering course at Purdue Fort Wayne (PFW) called Engineering Fundamentals II (ENGR128).•ENGR128is a foundational 4-credit course required for all undergraduate engineering students, aimed to develop problem-solving skills through lectures, studios, and lab.•This educational framework combines open-hardware technology and innovative story-based pedagogy to enhance learning experiences.•The research scope in this poster is centered on the development of an educational kit that combines prototype and existing work by students during Spring 2023.\n•Initial concept: Students demonstrated a significant level of engagement and excitement while using the kit. As one student said: “This kit makes me feel more intelligent.” •Challengesencountered: Components exhibiting functional difficulties, power-related challenges, inefficient layout, and limited size.\n•Enhanced learning experience: Students using the kit demonstrated a progressive improvement in coding comprehension and its application across various applications in engineering.•Continuous feedback: Student input will be collected with consent to refine and improve kit's effectiveness and usability.•Open resource goal: Plans to publish this tool as an accessible, open resource tool for global access, and simplified learning.•Curriculum integration: The course will be adjusted to incorporate additional applications to cover additional theories covered during ENGR128 classes.\nKit prototype\nExamples of students’ projects using the kit\n•The educational kit (see Figure 1) comprises a versatile, modular tool included in a compact box. This adaptable box can be expanded for diverse applications, including water systems (water shield), food applications (food shield), safety systems (safety shield), and fundamental electronics (SOS shield). The kit features sensors, power systems, electronic components, and easily integrable modules.\nSOS shield\nSafety shield\nWater shield\nSafety shield\nFigure 1. Educational Kit and different shields that can be incorporated into the kit•Figures 2 and 3 showcase examples of projects created by ENGR128 students during the Spring 2023 semester.\nFigure 2. Water control systemFigure 3. Morse code system\nWe would like to thank the Department of Electrical and Computer Engineering at PFW for their funding which has enabled the successful development of the kit."
  },
  {
    "source": "2003.10432v3.pdf",
    "content": "Atlas: End-to-End 3D Scene Reconstruction\nfrom Posed Images\nZak Murez1, Tarrence van As2∗, James Bartolozzi∗, Ayan Sinha1, Vijay\nBadrinarayanan3∗, and Andrew Rabinovich2∗\n1 Magic Leap Inc., CA, USA zak@murez.com,asinha@magicleap.com\n*Work done at Magic Leap bartolozzij@gmail.com\n2 InsideIQ Inc., CA, USA {tarrence,andrew}@insideiq.team\n3 Wayve.ai, London, UK vijay@wayve.ai\nAbstract. We present an end-to-end 3D reconstruction method for a\nscene by directly regressing a truncated signed distance function (TSDF)\nfrom a set of posed RGB images. Traditional approaches to 3D recon-\nstruction rely on an intermediate representation of depth maps prior to\nestimating a full 3D model of a scene. We hypothesize that a direct re-\ngression to 3D is more eﬀective. A 2D CNN extracts features from each\nimage independently which are then back-projected and accumulated\ninto a voxel volume using the camera intrinsics and extrinsics. After ac-\ncumulation, a 3D CNN reﬁnes the accumulated features and predicts the\nTSDF values. Additionally, semantic segmentation of the 3D model is ob-\ntained without signiﬁcant computation. This approach is evaluated on\nthe Scannet dataset where we signiﬁcantly outperform state-of-the-art\nbaselines (deep multiview stereo followed by traditional TSDF fusion)\nboth quantitatively and qualitatively. We compare our 3D semantic seg-\nmentation to prior methods that use a depth sensor since no previous\nwork attempts the problem with only RGB input.\nKeywords: Multiview Stereo; TSDF; 3D Reconstruction\n1 Introduction\nReconstructing the world around us is a long standing goal of computer vi-\nsion. Recently many applications have emerged, such as autonomous driving and\naugmented reality, which rely heavily upon accurate 3D reconstructions of the\nsurrounding environment. These reconstructions are often estimated by fusing\ndepth measurements from special sensors, such as structured light, time of ﬂight,\nor LIDAR, into 3D models. While these sensors can be extremely eﬀective, they\nrequire special hardware making them more cumbersome and expensive than\nsystems that rely solely on RGB cameras. Furthermore, they often suﬀer from\nnoise and missing measurements due to low albedo and glossy surfaces as well\nas occlusion.\nAnother approach to 3D reconstruction is to use monocular [18,31,32], binoc-\nular [3,5] or multivew [23,27,28,51] stereo methods which take RGB images (one,\narXiv:2003.10432v3  [cs.CV]  14 Oct 2020\n2 Z. Murez et al.\ntwo, or multiple respectively) and predict depth maps for the images. Despite\nthe plethora of recent research, these methods are still much less accurate than\ndepth sensors, and do not produce satisfactory results when fused into a 3D\nmodel.\nFig. 1: Overview of our method. Features from each image are backprojected\nalong rays and accumulated into a feature volume. Then a 3D CNN reﬁnes the\nfeatures and regresses a TSDF volume. Finally a mesh is extracted from the\nTSDF. Semantic Labels can also be output.\nIn this work, we observe that depth maps are often just intermediate rep-\nresentations that are then fused with other depth maps into a full 3D model.\nAs such, we propose a method that takes a sequence of RGB images and di-\nrectly predicts a full 3D model in an end-to-end trainable manner. This allows\nthe network to fuse more information and learn better geometric priors about\nthe world, producing much better reconstructions. Furthermore, it reduces the\ncomplexity of the system by eliminating steps like frame selection, as well as\nreducing the required compute by amortizing the cost over the entire sequence.\nOur method is inspired by two main lines of work: cost volume based multi\nview stereo [28,57] and Truncated Signed Distance Function (TSDF) reﬁnement\n[12, 15]. Cost volume based multi view stereo methods construct a cost volume\nusing a plane sweep. Here, a reference image is warped onto the target image\nfor each of a ﬁxed set of depth planes and stacked into a 3D cost volume. For\nthe correct depth plane, the reference and target images will match while for\nother depth planes they will not. As such, the depth is computed by taking the\nargmin over the planes. This is made more robust by warping image features\nextracted by a CNN instead of the raw pixel measurements, and by ﬁltering the\ncost volume with another CNN prior to taking the argmin.\nTSDF reﬁnement starts by fusing depth maps from a depth sensor into an\ninitial voxel volume using TSDF fusion [10], in which each voxel stores the trun-\ncated signed distance to the nearest surface. Note that a triangulated mesh can\nthen be extracted from this implicit representation by ﬁnding the zero cross-\ning surface using marching cubes [34]. TSDF reﬁnement methods [12, 15] take\nthis noisy, incomplete TSDF as input and reﬁne it by passing it through a 3D\nconvolutional encoder-decoder network.\nSimilar to cost volume multi view stereo approaches, we start by using a\n2D CNN to extract features from a sequence of RGB images. These features\nAtlas 3\nare then back projected into a 3D volume using the known camera intrinsics\nand extrinsics. However, unlike cost volume approaches which back project the\nfeatures into a target view frustum using image warping, we back project into\na canonical voxel volume, where each pixel gets mapped to a ray in the volume\n(similar to [46]). This avoids the need to choose a target image and allows us to\nfuse an entire sequence of frames into a single volume. We fuse all the frames into\nthe volume using a simple running average. Next, as in both cost volume and\nTSDF reﬁnement, we pass our voxel volume through a 3D convolutional encoder-\ndecoder to reﬁne the features. Finally, as in TSDF reﬁnement, our feature volume\nis used to regress the TSDF values at each voxel (see Figure 1).\nWe train and evaluate our network on real scans of indoor rooms from the\nScannet [11] dataset. Our method signiﬁcantly outperforms state-of-the-art multi\nview stereo baselines [28,51] producing accurate and complete meshes.\nAs an additional bonus, for minimal extra compute, we can add an addi-\ntional head to our 3D CNN and perform 3D semantic segmentation. While the\nproblems of 3D semantic and instance segmentation have received a lot of atten-\ntion recently [21,25], all previous methods assume the depth was acquired using\na depth sensor. Although our 3D segmentations are not competitive with the\ntop performers on the Scannet benchmark leader board, we establish a strong\nbaseline for the new task of 3D semantic segmentation from multi view RGB.\n2 Related Work\n2.1 3D reconstruction\nReconstructing a 3D model of a scene usually involves acquiring depth for a\nsequence of images and fusing the depth maps using a 3D data structure. The\nmost common 3D structure for depth accumulation is the voxel volume used by\nTSDF fusion [10]. However, surfels (oriented point clouds) are starting to gain\npopularity [44,55]. These methods are usually used with a depth sensor, but can\nalso be applied to depth maps predicted from monocular or stereo images.\nWith the rise of deep learning, monocular depth estimation has seen huge\nimprovements [18, 31, 32], however their accuracy is still far below state-of-the-\nart stereo methods. A popular classical approach to stereo [23] uses mutual\ninformation and semi global matching to compute the disparity between two\nimages. Similar approaches have been incorporated into SLAM systems such as\nCOLMAP [42,43] and CNN-SLAM [50]. More recently, several end-to-end plane\nsweep algorithms have been proposed. DeepMVS [27] uses a patch matching\nnetwork. MVDepthNet [51] constructs the cost volume from raw pixel measure-\nments and performs 2D convolutions, treating the planes as feature channels.\nGPMVS [26] builds upon this and aggregates information into the cost volume\nover long sequences using a Gaussian process. MVSNet [57] and DPSNet [28]\nconstruct the cost volume from features extracted from the images using a 2D\nCNN. They then ﬁlter the cost volume using 3D convolutions on the 4D tensor.\nR-MVSNet [58] reduces the memory requirements of MVSNet by replacing the\n4 Z. Murez et al.\n3D CNN with a recurrent CNN, while P-MVSNet [6] starts with a low resolution\nMVSNet and then iteratively reﬁnes the estimate using their point ﬂow mod-\nule. All of these methods require choosing a target image to predict depth for\nand then ﬁnding suitable neighboring reference images. Recent binocular stereo\nmethods [3, 5] use a similar cost volume approach, but avoid frame selection by\nusing a ﬁxed baseline stereo pair. Depth maps over a sequence are computed\nindependently (or weakly coupled in the case of [26]). In contrast to these ap-\nproaches, our method constructs a single coherent 3D model from a sequence of\ninput images directly.\nWhile TSDF fusion is simple and eﬀective, it cannot reconstruct partially\noccluded geometry and requires averaging many measurements to reduce noise.\nAs such, learned methods have been proposed to improve the fusion. OctNet-\nFusion [40] uses a 3D encoder-decoder to aggregate multiple depth maps into a\nTSDF and shows results on single objects and portions of scans. ScanComplete\n[15] builds upon this and shows results for entire rooms. SG-NN [12] improves\nupon ScanComplete by increasing the resolution using sparse convolutions [21]\nand training using a novel self-supervised training scheme. 3D-SIC [24] focuses\non 3D instance segmentation using region proposals and adds a per instance\ncompletion head. Routed fusion [54] uses 2D ﬁltering and 3D convolutions in\nview frustums to improve aggregation of depth maps.\nMore similar in spirit to ours are networks that take one or more images and\ndirectly predict a 3D representation. 3D-R2N2 [9] encodes images to a latent\nspace and then decodes a voxel occupancy volume. Octtree-Gen [49] increases\nthe resolution by using an octtree data structure to improve the eﬃciency of\n3D voxel volumes. Deep SDF [38] chooses to learn a generative model that can\noutput an SDF value for any input position instead of discretizing the volume.\nThese methods encode the input to a small latent code and report results on\nsingle objects, mostly from shapenet [4]. This small latent code is unlikely to\ncontain enough information to be able to reconstruct an entire scene (follow up\nwork [2], concurrent with ours, addresses this problem, but they do not apply\nit to RGB only reconstruction). Pix2Vox [56] encodes each image to a latent\ncode and then decodes a voxel representation for each and then fuses them.\nThis is similar to ours, but we explicitly model the 3D geometry of camera rays\nallowing us to learn better representations and scale to full scenes. SurfNet [45]\nlearns a 3D oﬀset from a template UV map of a surface. Point set generating\nnetworks [17] learns to generate point clouds with a ﬁxed number of points.\nPixel2Mesh++ [52] uses a graph convolutional network to directly predict a\ntriangulated mesh. Mesh-RCNN [20] builds upon 2D object detection [22] and\nadds an additional head to predict a voxel occupancy grid for each instance and\nthen reﬁnes them using a graph convolutional network on a mesh.\nBack projecting image features into a voxel volume and then reﬁning them\nusing a 3D CNN has also been used for human pose estimation [29, 59]. These\nworks regress 3D heat maps that are used to localize joint locations.\nDeep Voxels [46] and the follow up work of scene representation networks [47]\naccumulate features into a 3D volume forming an unsupervised representation\nAtlas 5\nof the world which can then be used to render novel views without the need to\nform explicit geometric intermediate representations.\n2.2 3D Semantic Segmentation\nIn addition to reconstructing geometry, many applications require semantic la-\nbeling of the reconstruction to provide a richer representation. Broadly speaking,\nthere are two approaches to solving this problem: 1) Predict semantics on 2D\ninput images using a 2D segmentation network [1,7,22] and back project the la-\nbels to 3D [35–37] 2) Directly predict the semantic labels in the 3D space. All of\nthese methods assume depth is provided by a depth sensor. A notable exception\nis Kimera [41], which uses multiview stereo [23] to predict depth, however, they\nonly show results on synthetic data and ground truth 2D segmentations.\nSGPN [53] formulates instance segmentation as a 3D point cloud clustering\nproblem. Predicting a similarity matrix and clustering the 3D point cloud to\nderive semantic and instance labels. 3D-SIS [25] improves upon these approaches\nby fusing 2D features in a 3D representation. RGB images are encoded using a\n2D CNN and back projected onto the 3D geometry reconstructed from depth\nmaps. A 3D CNN is then used to predict 3D object bounding boxes and semantic\nlabels. SSCN [21] predicts semantics on a high resolution voxel volume enabled\nby sparse convolutions.\nIn contrast to these approaches, we propose a strong baseline to the relatively\nuntouched problem of 3D semantic segmentation without a depth sensor.\n3 Method\nOur method takes as input an arbitrary length sequence of RGB images, each\nwith known intrinsics and pose. These images are passed through a 2D CNN\nbackbone to extract features. The features are then back projected into a 3D\nvoxel volume and accumulated using a running average. Once the image features\nhave been fused into 3D, we regress a TSDF directly using a 3D CNN (See\nFig. 2). We also experiment with adding an additional head to predict semantic\nsegmentation.\n3.1 Feature Volume Construction\nLet It ∈ R3×h×w be an image in a sequence of T RGB images. We extract\nfeatures Ft = F(It) ∈Rc×h×w using a standard 2D CNN where c is the feature\ndimension. These 2D features are then back projected into a 3D voxel volume\nusing the known camera intrinsics and extrinsics, assuming a pinhole camera\nmodel. Consider a voxel volume V ∈Rc×H×W×D\nVt(:,i,j,k ) = Ft(:,ˆi,ˆj), with (1)\n6 Z. Murez et al.\nSHARED WEIGHTS\n[3,h,w]\nImage n\n[3,h,w]\nImage 2\n[3,h,w]\nImage 1\n2D\nCNN\n2D\nCNN\n2D\nCNN\n2D\nCNN\n[c,h,w]\nFeatures 1\nBACKPROJECTION\n[c,h,w]\nFeatures 2\nBACKPROJECTION\n[c,h,w]\nFeatures n\nBACKPROJECTION\n[c,H,W,D]\nFeatures 1\n[c,H,W,D]\nFeatures 2\n[c,H,W,D]\nFeatures n\n[c,H,W,D]\nAccumulated\nFeatures\n3D\nCNN\n[1,H,W,D]\nTSDF\n[q,H,W,D]\nLabels\nFig. 2: Schematic of our method. Features are extracted from a sequence of im-\nages using a 2D CNN and then back projected into a 3D volume. These volumes\nare accumulated and then passed through a 3D CNN to directly regress a TSDF\nreconstruction of the scene. We can also jointly predict the 3D semantic segmen-\ntation of the scene.\n[ˆi\nˆj\n]\n= ΠKtPt\n\n\ni\nj\nk\n1\n\n, (2)\nwhere Pt and Kt are the extrinsics and intrinsics matrices for image t respec-\ntively, Π is the perspective mapping and : is the slice operator. Here ( i,j,k ) are\nthe voxel coordinates in world space and (ˆi,ˆj) are the pixel coordinates in image\nspace. Note that this means that all voxels along a camera ray are ﬁlled with\nthe same features corresponding to that pixel.\nThese feature volumes are accumulated over the entire sequence using a\nweighted running average similar to TSDF fusion as follows:\n¯Vt =\n¯Vt−1 ¯Wt−1 + Vt\n¯Wt−1 + Wt\n, (3)\n¯Wt = ¯Wt−1 + Wt. (4)\nFor the weights we use a binary mask Wt(i,j,k ) ∈{0,1}which stores if voxel\n(i,j,k ) is inside or outside the view frustum of the camera.\n3.2 3D Encoder-Decoder\nOnce the features are accumulated into the voxel volume, we use a 3D convo-\nlutional encoder-decoder network to reﬁne the features and regress the output\nTSDF (Fig. 3). Each layer of the encoder and decoder uses a set of 3x3x3 residual\nblocks. Downsampling is implemented with 3x3x3 stride 2 convolution, while up-\nsampling uses trilinear interpolation followed by a 1x1x1 convolution to change\nthe feature dimension. The feature dimension is doubled with each downsam-\npling and halved with each upsampling. All convolution layers are followed by\nAtlas 7\nFig. 3: Our 3D encoder-decoder architecture. Blue boxes denote residual blocks,\ngreen boxes are stride 2 convolutions and red boxes are trilinear upsampling. The\narrows from the encoder to the decoder indicate skip connections. Our network\npredicts TSDFs in a coarse to ﬁne manner with the previous resolution being\nused to sparsify the next resolution (shown as small arrows in the decoder).\nbatchnorm and relu. We also include additive skip connections from the encoder\nto the decoder.\nAt the topmost layer of the encoder-decoder, we use a 1x1x1 convolution\nfollowed by a tanh activation to regress the ﬁnal TSDF values. For our semantic\nsegmentation models we also include an additional 1x1x1 convolution to predict\nthe segmentation logits.\nWe also include intermediate output heads at each decoded resolution prior\nto upsampling. These additional predictions are used both for intermediate su-\npervision to help the network train faster, as well as to guide the later resolutions\nto focus on reﬁning predictions near surfaces. At each resolution, any voxel that\nis predicted beyond a fraction (.99) of the truncation distance is clamped to one\nat the following resolutions. Furthermore, loss is only backpropageted for non-\nclamped voxels. Without this, the loss at the higher resolutions is dominated\nby the large number of empty space voxels and the network has a harder time\nlearning ﬁne details.\nNote that since our features are back projected along entire rays, the voxel\nvolume is ﬁlled densely and thus we cannot take advantage of sparse convolutions\n[21] in the encoder. However, the multiscale outputs can be used to sparsify the\nfeature volumes in the decoder allowing for the use of sparse convolutions similar\nto [12]. In practice, we found that we were able to train our models at 4cm3 voxel\nresolution without the need for sparse convolutions.\n8 Z. Murez et al.\n4 Implementation Details\nWe use a Resnet50-FPN [33] followed by the merging method of [30] with 32\noutput feature channels as our 2D backbone. Our 3D CNN consists of a four\nscale resolution pyramid where we double the number of channels each time we\nhalf the resolution. The encoder consists of (1,2,3,4) residual blocks at each scale\nrespectively, and the decoder consists of (3,2,1) residual blocks.\nWe supervise the multiscale TSDF reconstructions usingℓ1 loss to the ground\ntruth TSDF values. Following [14], we log-transform the predicted and target\nvalues before applying the ℓ1 loss, and only backpropagate loss for voxels that\nwere observed in the ground truth (i.e. have TSDF values strictly less than\n1.) However, to prevent the network from hallucinating artifacts behind walls,\noutside the room, we also mark all the voxels where their entire vertical column\nis equal to 1 and penalize in these areas too. The intuition for this is that if the\nentire vertical column was not observed it was probably not within the room.\nTo construct the ground truth TSDFs we run TSDF fusion at each resolution\non the full sequences, prior to training.\nWe train the network end-to-end using 50 images selected randomly through-\nout the full sequence. We use a voxel size of 4cm3 with a grid of (160 ×160 ×64)\nvoxels, corresponding to a volume of (6 .4 ×6.4 ×2.56) meters. At test time,\nwe accumulate the feature volumes in place (since we do not need to store the\nintermediate activations for backpropagation), allowing us to operate on arbi-\ntrary length sequences (often thousands of frames for ScanNet) and we use a\n400x400x104 sized voxel grid corresponding to a volume of (16 ×16 ×4.16) me-\nters. We use the ADAM optimizer with a learning rate of 5e−4 and 16bit mixed\nprecision operations. Training the network takes around 24 hours on 8 Titan RTX\nGPUs with a batch size of 8 (1 sequence per GPU) and synchronized batchnorm.\nOur model is implemented with PyTorch and PyTorch Lightning [16].\n5 Results\nWe evaluate our method on ScanNet [11], which consists of 2.5M images across\n707 distinct spaces. Standard train/validation/test splits are adopted. The 3D\nreconstructions are benchmarked using standard 2D depth metrics (Table 2) and\n3D metrics (Table 3), which are deﬁned in Table 1. We also show qualitative\ncomparisons in Figure 6 where our method really stands out.\nWe compare our method to 4 state-of-the-art baselines: COLMAP [42, 43],\nMVDepthNet [51], GPMVS [26], and DPSNet [28]. For COLMAP we use the\ndefault dense reconstruction parameters but use the ground truth poses provided\nby Scannet. For each of the learned methods we ﬁne tuned the models provided\nby the authors on Scannet. At inference time, 6 reference frames were selected\ntemporally with stride 10 centered around the target view. We also mask the\nboundary pixels since the networks have visible edge eﬀects that cause poor\ndepth predictions here (leading to 92.8% completeness).\nTo evaluate these in 3D we fuse the predicted depth maps using two tech-\nniques: TSDF Fusion [10] and point cloud fusion. For COLMAP we use their\nAtlas 9\ndefault point cloud fusion, while for the other methods we use the implementa-\ntion of [19]. We found point cloud fusion was more robust to the outliers present\nin the depth predictions than our implementation of TSDF Fusion. As such, we\nonly report the point cloud fusion results in Table 3 which are strictly better\nthan the TSDF Fusion results (Note that the L1 metric is computed using the\nTSDF Fusion approach as it is not computed in the point cloud fusion approach).\nA\nB\nGround Truth Ours\nFig. 4: Our method learns to ﬁll holes that are missing from the ground truth.\nThese holes arise from two causes: A) limitations of depth sensors on low albedo\nand specular surfaces, and B) unobserved regions caused by occlusion and incom-\nplete scans. While other multiview stereo method often learn to predict depth for\nthese troublesome surfaces, they are not able to complete unobserved geometry.\nAs seen in Figure 4 our method is able to ﬁll holes that are missing from\nthe ground truth. These holes arise from two causes: A) limitations of depth\nsensors on low albedo and specular surfaces, and B) unobserved regions caused by\nocclusion and incomplete scans. While other multiview stereo method often learn\nto predict depth for these troublesome surfaces, they are not able to complete\nunobserved geometry. On the other hand, since our method directly regresses\nthe full TSDF for a scene, it is able to reason about and complete unobserved\nregions. However, this means that we must take extra care when evaluating\nthe point cloud metrics, otherwise we will be falsely penalized in these regions.\nWe remove geometry that was not observed in the ground truth by taking the\nrendered depth maps from our predicted mesh and re-fuse them using TSDF\n10 Z. Murez et al.\nFusion into a trimmed mesh. This guarantees that there is no mesh in areas that\nwere not observed in the ground truth.\nOur method achieves state-of-the-art on about half of the metrics and is\ncompetitive on all metrics. However, as seen in Figure 6, qualitatively our results\nour signiﬁcantly better than previous methods. While theL1 metric on the TSDF\nseems to reﬂect this performance gap better, the inability of the other metrics\nto capture this indicates a need for additional more perceptual metrics.\nAs mentioned previously, we augment the existing 3D-CNN with a semantic\nsegmentation head, requiring only a single 1×1×1 convolution, to be able to not\nonly reconstruct the 3D structure of the scene but also provide semantic labels\nto the surfaces. Since no prior work attempts to do 3D semantic segmentation\nfrom only RGB images, and there are no established benchmarks, we propose\na new evaluation procedure. The semantic labels from the predicted mesh are\ntransferred onto the ground truth mesh using nearest neighbor lookup on the\nvertices, and then the standard IOU metric can be used. The results are reported\nin Table 4 and Fig. 7 (note that this is an unfair comparison since all prior\nmethods include depth as input).\nTable 1: Deﬁnitions of metrics: nis the number of pixels with both valid ground\ntruth and predictions, d and d∗ are the predicted and ground truth depths (the\npredicted depth from our method is computed by rendering the predicted mesh).\nt and t∗ are the predicted and ground truth TSDFs while p and p∗ are the\npredicted and ground truth point clouds.\n2D 3D\nAbs Rel 1\nn\n∑|d−d∗|/d∗ L1 mean t∗<1|t−t∗|\nAbs Diﬀ 1\nn\n∑|d−d∗| Acc mean p∈P (minp∗∈P∗ ||p−p∗||)\nSq Rel 1\nn\n∑|d−d∗|2/d∗ Comp mean p∗∈P∗ (minp∈P ||p−p∗||)\nRMSE\n√\n1\nn\n∑|d−d∗|2 Prec mean p∈P (minp∗∈P∗ ||p−p∗||<.05)\nδ <1.25i 1\nn\n∑(max ( d\nd∗ ,d∗\nd ) <1.25i) Recal mean p∗∈P∗ (minp∈P ||p−p∗||<.05)\nComp % valid predictions F-score 2×Perc×Recal\nPerc+Recal\nTable 2: 2D Depth Metrics\nMethod AbsRel AbsDiﬀ SqRel RMSE δ <1.25 δ <1.252 δ <1.253 Comp\nCOLMAP [43] .137 .264 .138 .502 .834 .908 .938 .871\nMVDepthNet [51] .098 .191 .061 .293 .896 .977 .994 .928\nGPMVS [26] .130 .239 .339 .472 .906 .967 .980 .928\nDPSNet [28] .087 .158 .035 .232 .925 .984 .995 .928\nOurs (plain) .061 .120 .042 .248 .940 .972 .985 .999\nOurs (semseg) .065 .124 .043 .251 .936 .971 .986 .999\nAtlas 11\nTable 3: 3D Geometry Metrics\nMethod L1 Acc Comp Prec Recal F-score\nCOLMAP [43] .599 .135 .069 .505 .634 .558\nMVDepthNet [51] .518 .240 .040 .208 .831 .329\nGPMVS [26] .475 .879 .031 .188 .871 .304\nDPSNet [28] .421 .284 .045 .223 .793 .344\nOurs (plain) .162 .130 .065 .383 .725 .499\nOurs (semseg) .172 .124 .074 .413 .711 .520\nTable 4: 3D Semantic Label Benchmark\nMethod mIOU\nScanNet [11] 30.6\nPointNet++ [39] 33.9\nSPLATNet [48] 39.3\n3DMV [13] 48.4\n3DMV-FTSDF 50.1\nPointNet++SW 52.3\nSparseConvNet [21] 72.5\nMinkowskiNet [8] 73.4\nOurs 34.0\nScanNet 3D Semantic Segmentation metrics. We transfer our labels from the predicted\nmesh to the ground truth mesh using nearest neighbors.\nFrom the results in Table 4 we see that our approach is surprisingly com-\npetitive with (and even beats some) prior methods that include depth as input.\nHaving depth as an input makes the problem signiﬁcantly easier because the\nonly source of error is from the semantic predictions. In our case, in order to\ncorrectly label a vertex we must both predict the geometry correct as well as the\nsemantic label. From Fig. 7 we can see that mistakes in geometry compounds\nwith mistakes in semantics which leads to lower IOUs.\nIn Figure 5 we show an example of how our method degrades as the number\nof frames is reduced at inference time. We see that there is almost no degradation\nwith as few as 25 frames. See accompanying video for more examples.\n5.1 Inference Time\nSince our method only requires running a small 2D CNN on each frame, the\ncost of running the large 3D CNN is amortized over a sequence of images. On\nthe other hand, MVS methods must run all their compute on every frame. Note\nthat they must also run depth map fusion to accumulate the depth maps into a\nmesh, but we do not include this additional time here. We report inference times\nusing 2 neighbors. All models are run on a single NVidia TiTan RTX GPU. From\n12 Z. Murez et al.\n25\tFrames 50\tFrames\nAll\tFrames Ground\tTruth\nFig. 5: Quality as a function of number of input frames at inference time. There\nis almost no degradation with as few as 25 frames (out of 784 total).\nTable 5 we can see that after approximately 4 frames, ours becomes faster than\nDPSNet (note that most Scannet scenes are a few thousands of frames).\nTable 5: Inference Time\nMethod Per Frame Time (sec) Per Sequence Time (sec)\nCOLMAP [43] 2.076 0\nMVDepthNet [51] 0.048 0\nGPMVS [26] 0.051 0\nDPSNet [28] 0.322 0\nOurs .071 .840\n6 Conclusions\nIn this work, we present a novel approach to 3D scene reconstruction. Notably,\nour approach does not require depth inputs; is unbounded temporally, allowing\nthe integration of long frame sequences; completes unobserved geometry; and\nsupports the eﬃcient prediction of other quantities such as semantics. We have\nexperimentally veriﬁed that the classical approach to 3D reconstruction via per\nview depth estimation is inferior to direct regression to a 3D model from an input\nRGB sequence. We have also demonstrated that without signiﬁcant additional\ncompute, a semantic segmentation objective can be added to the model to ac-\ncurately label the resultant surfaces. In our future work, we aim to improve the\nback projection and accumulation process. One approach is to allow the network\nto learn where along a ray to place the features (instead of uniformly). This will\nimprove the models ability to handle occlusions and large multi room scenes.\nWe also plan to add additional tasks such as instance segmentation and intrinsic\nimage decomposition. Our method is particularly well suited for intrinsic image\ndecomposition because the network has the ability to reason with information\nfrom multiple views in 3D.\nAtlas 13\nCOLMAP DPSNet Ours Ground Truth\nFig. 6: Qualitative 3D reconstruction results.\n14 Z. Murez et al.\nOurs Transferred Ground Truth\nFig. 7: Qualitative 3D semantic segmentations. Left to right: Ours, our labels\ntransferred to the ground truth mesh, ground truth labels. We are able to accu-\nrately segment the 3D scene despite not using a depth sensor.\nAtlas 15\nReferences\n1. Badrinarayanan, V., Kendall, A., Cipolla, R.: Segnet: A deep convolutional\nencoder-decoder architecture for image segmentation (2015)\n2. Chabra, R., Lenssen, J.E., Ilg, E., Schmidt, T., Straub, J., Lovegrove, S., New-\ncombe, R.: Deep local shapes: Learning local sdf priors for detailed 3d reconstruc-\ntion. arXiv preprint arXiv:2003.10983 (2020)\n3. Chabra, R., Straub, J., Sweeney, C., Newcombe, R., Fuchs, H.: Stereodrnet: Dilated\nresidual stereonet. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition. pp. 11786–11795 (2019)\n4. Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z.,\nSavarese, S., Savva, M., Song, S., Su, H., et al.: Shapenet: An information-rich\n3d model repository. arXiv preprint arXiv:1512.03012 (2015)\n5. Chang, J.R., Chen, Y.S.: Pyramid stereo matching network. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition. pp. 5410–5418\n(2018)\n6. Chen, R., Han, S., Xu, J., Su, H.: Point-based multi-view stereo network. In: Pro-\nceedings of the IEEE International Conference on Computer Vision. pp. 1538–1547\n(2019)\n7. Cheng, B., Collins, M.D., Zhu, Y., Liu, T., Huang, T.S., Adam, H., Chen, L.C.:\nPanoptic-deeplab. arXiv preprint arXiv:1910.04751 (2019)\n8. Choy, C., Gwak, J., Savarese, S.: 4d spatio-temporal convnets: Minkowski convo-\nlutional neural networks (2019)\n9. Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S.: 3d-r2n2: A uniﬁed approach\nfor single and multi-view 3d object reconstruction. In: European conference on\ncomputer vision. pp. 628–644. Springer (2016)\n10. Curless, B., Levoy, M.: A volumetric method for building complex models from\nrange images. In: Proceedings of the 23rd annual conference on Computer graphics\nand interactive techniques. pp. 303–312 (1996)\n11. Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nießner, M.: Scannet:\nRichly-annotated 3d reconstructions of indoor scenes. In: Proc. Computer Vision\nand Pattern Recognition (CVPR), IEEE (2017)\n12. Dai, A., Diller, C., Nießner, M.: Sg-nn: Sparse generative neural networks for self-\nsupervised scene completion of rgb-d scans. arXiv preprint arXiv:1912.00036 (2019)\n13. Dai, A., Nießner, M.: 3dmv: Joint 3d-multi-view prediction for 3d semantic scene\nsegmentation (2018)\n14. Dai, A., Qi, C.R., Nießner, M.: Shape completion using 3d-encoder-predictor cnns\nand shape synthesis (2016)\n15. Dai, A., Ritchie, D., Bokeloh, M., Reed, S., Sturm, J., Nießner, M.: Scancomplete:\nLarge-scale scene completion and semantic segmentation for 3d scans. In: Proceed-\nings of the IEEE Conference on Computer Vision and Pattern Recognition. pp.\n4578–4587 (2018)\n16. Falcon, W.: Pytorch lightning. GitHub. Note:\nhttps://github.com/PyTorchLightning/pytorch-lightning Cited by 3 (2019)\n17. Fan, H., Su, H., Guibas, L.J.: A point set generation network for 3d object recon-\nstruction from a single image. In: Proceedings of the IEEE conference on computer\nvision and pattern recognition. pp. 605–613 (2017)\n18. Fu, H., Gong, M., Wang, C., Batmanghelich, K., Tao, D.: Deep ordinal regression\nnetwork for monocular depth estimation. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition. pp. 2002–2011 (2018)\n16 Z. Murez et al.\n19. Galliani, S., Lasinger, K., Schindler, K.: Massively parallel multiview stereopsis by\nsurface normal diﬀusion. In: Proceedings of the IEEE International Conference on\nComputer Vision. pp. 873–881 (2015)\n20. Gkioxari, G., Malik, J., Johnson, J.: Mesh r-cnn. In: Proceedings of the IEEE\nInternational Conference on Computer Vision. pp. 9785–9795 (2019)\n21. Graham, B., Engelcke, M., van der Maaten, L.: 3d semantic segmentation with\nsubmanifold sparse convolutional networks. In: Proceedings of the IEEE conference\non computer vision and pattern recognition. pp. 9224–9232 (2018)\n22. He, K., Gkioxari, G., Doll´ ar, P., Girshick, R.: Mask r-cnn. In: Proceedings of the\nIEEE international conference on computer vision. pp. 2961–2969 (2017)\n23. Hirschmuller, H.: Stereo processing by semiglobal matching and mutual informa-\ntion. IEEE Transactions on pattern analysis and machine intelligence 30(2), 328–\n341 (2007)\n24. Hou, J., Dai, A., Nießner, M.: 3d-sic: 3d semantic instance completion for rgb-d\nscans. arXiv preprint arXiv:1904.12012 (2019)\n25. Hou, J., Dai, A., Nießner, M.: 3d-sis: 3d semantic instance segmentation of rgb-d\nscans (2018)\n26. Hou, Y., Kannala, J., Solin, A.: Multi-view stereo by temporal nonparametric\nfusion. In: Proceedings of the IEEE International Conference on Computer Vision.\npp. 2651–2660 (2019)\n27. Huang, P.H., Matzen, K., Kopf, J., Ahuja, N., Huang, J.B.: Deepmvs: Learning\nmulti-view stereopsis. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition. pp. 2821–2830 (2018)\n28. Im, S., Jeon, H.G., Lin, S., Kweon, I.S.: Dpsnet: End-to-end deep plane sweep\nstereo. In: 7th International Conference on Learning Representations, ICLR 2019.\nInternational Conference on Learning Representations, ICLR (2019)\n29. Iskakov, K., Burkov, E., Lempitsky, V., Malkov, Y.: Learnable triangulation of\nhuman pose. In: Proceedings of the IEEE International Conference on Computer\nVision. pp. 7718–7727 (2019)\n30. Kirillov, A., Girshick, R., He, K., Doll´ ar, P.: Panoptic feature pyramid networks. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition.\npp. 6399–6408 (2019)\n31. Lasinger, K., Ranftl, R., Schindler, K., Koltun, V.: Towards robust monocu-\nlar depth estimation: Mixing datasets for zero-shot cross-dataset transfer. arXiv\npreprint arXiv:1907.01341 (2019)\n32. Lee, J.H., Han, M.K., Ko, D.W., Suh, I.H.: From big to small: Multi-scale local\nplanar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326\n(2019)\n33. Lin, T.Y., Doll´ ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature\npyramid networks for object detection. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. pp. 2117–2125 (2017)\n34. Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3d surface con-\nstruction algorithm. ACM siggraph computer graphics 21(4), 163–169 (1987)\n35. McCormac, J., Clark, R., Bloesch, M., Davison, A., Leutenegger, S.: Fusion++:\nVolumetric object-level slam. In: 2018 international conference on 3D vision (3DV).\npp. 32–41. IEEE (2018)\n36. McCormac, J., Handa, A., Davison, A., Leutenegger, S.: Semanticfusion: Dense 3d\nsemantic mapping with convolutional neural networks. In: 2017 IEEE International\nConference on Robotics and automation (ICRA). pp. 4628–4635. IEEE (2017)\nAtlas 17\n37. Narita, G., Seno, T., Ishikawa, T., Kaji, Y.: Panopticfusion: Online volumetric\nsemantic mapping at the level of stuﬀ and things. arXiv preprint arXiv:1903.01177\n(2019)\n38. Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: Deepsdf: Learning\ncontinuous signed distance functions for shape representation. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition. pp. 165–174\n(2019)\n39. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learn-\ning on point sets in a metric space (2017)\n40. Riegler, G., Osman Ulusoy, A., Geiger, A.: Octnet: Learning deep 3d representa-\ntions at high resolutions. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition. pp. 3577–3586 (2017)\n41. Rosinol, A., Abate, M., Chang, Y., Carlone, L.: Kimera: an open-source library\nfor real-time metric-semantic localization and mapping. In: IEEE Intl. Conf. on\nRobotics and Automation (ICRA) (2020)\n42. Sch¨ onberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Conference\non Computer Vision and Pattern Recognition (CVPR) (2016)\n43. Sch¨ onberger, J.L., Zheng, E., Pollefeys, M., Frahm, J.M.: Pixelwise view selection\nfor unstructured multi-view stereo. In: European Conference on Computer Vision\n(ECCV) (2016)\n44. Sch¨ ops, T., Sattler, T., Pollefeys, M.: Surfelmeshing: Online surfel-based mesh\nreconstruction. IEEE Transactions on Pattern Analysis and Machine Intelligence\n(2019)\n45. Sinha, A., Unmesh, A., Huang, Q., Ramani, K.: Surfnet: Generating 3d shape\nsurfaces using deep residual networks. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. pp. 6040–6049 (2017)\n46. Sitzmann, V., Thies, J., Heide, F., Nießner, M., Wetzstein, G., Zollhofer, M.: Deep-\nvoxels: Learning persistent 3d feature embeddings. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. pp. 2437–2446 (2019)\n47. Sitzmann, V., Zollh¨ ofer, M., Wetzstein, G.: Scene representation networks: Con-\ntinuous 3d-structure-aware neural scene representations. In: Advances in Neural\nInformation Processing Systems (2019)\n48. Su, H., Jampani, V., Sun, D., Maji, S., Kalogerakis, E., Yang, M.H., Kautz, J.:\nSplatnet: Sparse lattice networks for point cloud processing (2018)\n49. Tatarchenko, M., Dosovitskiy, A., Brox, T.: Octree generating networks: Eﬃcient\nconvolutional architectures for high-resolution 3d outputs. In: Proceedings of the\nIEEE International Conference on Computer Vision. pp. 2088–2096 (2017)\n50. Tateno, K., Tombari, F., Laina, I., Navab, N.: Cnn-slam: Real-time dense monoc-\nular slam with learned depth prediction. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition. pp. 6243–6252 (2017)\n51. Wang, K., Shen, S.: Mvdepthnet: real-time multiview depth estimation neural net-\nwork. In: 2018 International Conference on 3D Vision (3DV). pp. 248–257. IEEE\n(2018)\n52. Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., Jiang, Y.G.: Pixel2mesh: Gener-\nating 3d mesh models from single rgb images. In: Proceedings of the European\nConference on Computer Vision (ECCV). pp. 52–67 (2018)\n53. Wang, W., Yu, R., Huang, Q., Neumann, U.: Sgpn: Similarity group proposal\nnetwork for 3d point cloud instance segmentation. In: In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition. p. 2569–2578 (2018)\n54. Weder, S., Sch¨ onberger, J., Pollefeys, M., Oswald, M.R.: Routedfusion: Learning\nreal-time depth map fusion. arXiv preprint arXiv:2001.04388 (2020)\n18 Z. Murez et al.\n55. Whelan, T., Leutenegger, S., Salas-Moreno, R., Glocker, B., Davison, A.: Elastic-\nfusion: Dense slam without a pose graph. Robotics: Science and Systems\n56. Xie, H., Yao, H., Sun, X., Zhou, S., Zhang, S.: Pix2vox: Context-aware 3d recon-\nstruction from single and multi-view images. In: Proceedings of the IEEE Interna-\ntional Conference on Computer Vision. pp. 2690–2698 (2019)\n57. Yao, Y., Luo, Z., Li, S., Fang, T., Quan, L.: Mvsnet: Depth inference for unstruc-\ntured multi-view stereo. In: Proceedings of the European Conference on Computer\nVision (ECCV). pp. 767–783 (2018)\n58. Yao, Y., Luo, Z., Li, S., Shen, T., Fang, T., Quan, L.: Recurrent mvsnet for high-\nresolution multi-view stereo depth inference. In: Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition. pp. 5525–5534 (2019)\n59. Zimmermann, C., Ceylan, D., Yang, J., Russell, B., Argus, M., Brox, T.: Freihand:\nA dataset for markerless capture of hand pose and shape from single rgb images.\nIn: Proceedings of the IEEE International Conference on Computer Vision. pp.\n813–822 (2019)"
  },
  {
    "source": "doc_1.pdf",
    "content": "Slide 1: \nObject detection involves identifying and locating objects within video or image frames, playing a \ncrucial role in interpreting visual data.  \nDetecting and analyzing objects in videos is a challenging task in computer vision due to the dynamic \nand constantly changing nature of video data. Current object detection techniques, primarily designed \nfor static images, struggle to effectively capture motion patterns and long-term relationships between \nframes, which leads to suboptimal performance in dynamic settings. The challenge lies in developing \na framework that integrates spatial and temporal features seamlessly, while effectively addressing \nocclusions, motion blur, and real-time processing constraints. \nSlide 8: \nHOG is a method that involves breaking down the image into smaller parts, calculating gradients, and \norganizing the information into a histogram. The process involves the following steps:  \n1. Image Preprocessing: \no Grayscale Conversion: The input image is first converted to grayscale to simplify the \ncalculation of gradients, reducing computational complexity while retaining edge \ninformation. \n2. Gradient Calculation: \no Compute the gradient of each pixel in the image. This is typically done using Sobel filters \nto calculate changes in intensity in both x (horizontal) and y (vertical) directions. \no The gradients are computed for each pixel, resulting in two components: the magnitude \nand the direction (or orientation). \n3. Dividing the Image into Cells: \no The image is divided into smaller, non-overlapping cells (e.g., 8x8 pixels). \no For each cell, a histogram of gradient directions is calculated. \n4. Histogram Creation: \no Within each cell, orientation bins are defined (e.g., 0° to 180° or 0° to 360°, divided into \nbins of 20° each). \no Each pixel's gradient contributes to the histogram based on its magnitude and direction. \nThe pixel's magnitude is used as the weight for the corresponding orientation bin.  \n5. Normalization of Blocks: \no To make the descriptor robust to changes in illumination or contrast, neighboring cells are \ngrouped into blocks (e.g., 2x2 cells). Each block's histogram is then normalized by \ncalculating the L2 norm. \no This normalization ensures that the feature descriptor is invariant to local brightness and \ncontrast changes. \n6. Feature Vector Formation: \no After the entire image is processed, the histograms from all the cells are concatenated to \nform a feature vector that represents the HOG descriptor for the image. \n2. Using SVM as Classifier with HOG Features \nAfter extracting HOG features, they are used as input to a classifier for object detection or \nrecognition. The Support Vector Machine (SVM) is a popular choice because of its effectiveness in \nbinary classification tasks. \n1. Training SVM: \no HOG Features Extraction: Extract HOG features from a labeled dataset, consisting of \npositive (e.g., images with people) and negative (e.g., images without people) samples.  \no SVM Training: Train an SVM using these HOG feature vectors. SVM is a linear classifier \nthat finds the hyperplane that best separates the positive and negative samples in the \nfeature space. \n2. Objective of SVM: \no The goal of SVM is to find the optimal hyperplane that maximizes the margin between the \npositive and negative classes. The wider the margin, the better the generalization. \no Kernel Trick: If the data is not linearly separable, an SVM with a non-linear kernel (e.g., \nRBF kernel) can be used to map the data into a higher-dimensional space where it \nbecomes linearly separable. \n3. Prediction Phase: \no HOG Features Extraction: During prediction, the test image is processed to extract HOG \nfeatures. \no SVM Classification: The SVM classifier then takes these HOG features and predicts the \nclass label (e.g., whether a person is present in the image or not). \nSlide 9: \nare a specialized type of deep neural network designed to work with grid-like data, such as images. CNNs \nare particularly effective for computer vision tasks, including image classification, object detection, and \nsegmentation. Here is an overview of the architecture of CNNs: \nKey Components of CNN Architecture \n1. Input Layer: \no The input to a CNN is usually an image represented as a tensor of dimensions (height, \nwidth, depth). \n▪ For example, a RGB image of size 32x32 has a tensor shape of (32, 32, 3). \no The depth of the image refers to the number of channels (e.g., 3 for RGB). \n2. Convolutional Layer: \no The Convolutional Layer is the core building block of a CNN. It uses a set of learnable \nfilters (kernels) to convolve over the input image. \no Filters/Kernels: \n▪ Typically of size 3x3 or 5x5. \n▪ Each filter is applied across the height and width of the image, generating a \nfeature map. \no Convolution Operation: \n▪ The kernel slides over the input, and the dot product between the kernel weights \nand the input values is computed. \n▪ This results in a feature map that detects specific patterns, such as edges or \ntextures. \no Stride and Padding: \n▪ Stride: Determines how much the filter moves over the input image. A stride of 1 \nmeans the filter moves by one pixel at a time. \n▪ Padding: Adds zero-padding to the edges of the input image to maintain spatial \ndimensions. \n3. Activation Function (ReLU): \no After convolution, an activation function is applied to introduce non-linearity. \no ReLU (Rectified Linear Unit): \n▪ The most common activation used in CNNs. \n▪ It replaces all negative values in the feature map with zero, which helps prevent \nthe model from being linear and makes learning faster. \n▪ Formula: ReLU(x) = max(0, x). \n4. Pooling Layer (Downsampling): \no Pooling layers are used to reduce the spatial dimensions (height and width) of the feature \nmaps, which helps in reducing the number of parameters and computational cost.  \no Max Pooling: \n▪ The most common pooling operation. \n▪ Divides the input into rectangular regions (e.g., 2x2) and selects the maximum \nvalue from each region. \n▪ It helps preserve the dominant features while reducing the size of the feature \nmaps. \no Average Pooling: \n▪ Computes the average value of each region. \no Pooling reduces dimensionality, which makes the network less sensitive to small changes \nor translations in the input image. \n5. Fully Connected (FC) Layer: \no After several convolutional and pooling layers, the output feature maps are flattened into \na 1D vector. \no The fully connected layer takes this flattened vector and uses it to classify the input \nimage. \no These layers are similar to traditional neural networks, where all the neurons are \nconnected to each other. \no Softmax Activation: \n▪ In classification tasks, the final FC layer usually has a Softmax activation function \nthat outputs probabilities for each class. \nSlide 10:  \nInput Image and Region Proposals: \n• The process begins with an input image from which possible regions containing objects need to \nbe identified. \n• Selective Search Algorithm: \no R-CNN uses Selective Search to generate approximately 2000 region proposals. \no Selective Search is a method that combines hierarchical grouping of superpixels to \nidentify potential regions of interest. This step aims to find the parts of the image that are \nlikely to contain objects, known as Region Proposals. \n• The result is a set of bounding boxes representing potential objects in the image. \n  Feature Extraction with CNN: \n• Each region proposal is then resized to a fixed size (e.g., 224x224 pixels) to ensure compatibility \nwith the CNN. \n• A pre-trained CNN (e.g., AlexNet or VGG16) is used to extract features from each of these region \nproposals. \no The CNN takes each resized proposal and passes it through multiple convolutional \nlayers, pooling layers, and fully connected layers to generate a feature vector. \n• This feature extraction process is repeated for each region proposal, which makes R-CNN \ncomputationally expensive and slow. \n  Classification with SVM: \n• Once the feature vectors are extracted from the region proposals, they are used as input to \nSupport Vector Machines (SVMs) for classification. \n• An SVM is trained for each class of objects to determine whether a region contains a particular \nclass or is background. \no For example, separate SVMs would be trained to classify regions as dog, car, or \nbackground. \n• This classification step is responsible for determining which regions contain objects and which do \nnot. \n  Bounding Box Regression: \n• The bounding boxes from the Selective Search process may not be perfectly aligned with the \nactual objects. \n• To improve accuracy, Bounding Box Regression is applied. \no A separate regression model is used to refine the coordinates of each bounding box to \nbetter fit the object detected. \n• This step helps to adjust the bounding box coordinates to make the predicted box more accurate \nwith respect to the ground truth. \nSlide 11: \nInput Image and Region Proposals: \n• The process begins with an input image from which possible regions containing objects need to \nbe identified. \n• Selective Search Algorithm: \no R-CNN uses Selective Search to generate approximately 2000 region proposals. \no Selective Search is a method that combines hierarchical grouping of superpixels to \nidentify potential regions of interest. This step aims to find the parts of the image that are \nlikely to contain objects, known as Region Proposals. \n• The result is a set of bounding boxes representing potential objects in the image. \n  Feature Extraction with CNN: \n• Each region proposal is then resized to a fixed size (e.g., 224x224 pixels) to ensure compatibility \nwith the CNN. \n• A pre-trained CNN (e.g., AlexNet or VGG16) is used to extract features from each of these region \nproposals. \no The CNN takes each resized proposal and passes it through multiple convolutional \nlayers, pooling layers, and fully connected layers to generate a feature vector. \n• This feature extraction process is repeated for each region proposal, which makes R-CNN \ncomputationally expensive and slow. \n  Classification with SVM: \n• Once the feature vectors are extracted from the region proposals, they are used as input to \nSupport Vector Machines (SVMs) for classification. \n• An SVM is trained for each class of objects to determine whether a region contains a particular \nclass or is background. \no For example, separate SVMs would be trained to classify regions as dog, car, or \nbackground. \n• This classification step is responsible for determining which regions contain objects and which do \nnot. \n  Bounding Box Regression: \n• The bounding boxes from the Selective Search process may not be perfectly aligned with the \nactual objects. \n• To improve accuracy, Bounding Box Regression is applied. \no A separate regression model is used to refine the coordinates of each bounding box to \nbetter fit the object detected. \n• This step helps to adjust the bounding box coordinates to make the predicted box more accurate \nwith respect to the ground truth. \nSlide 12: \nFaster R-CNN is composed of two main modules: a Region Proposal Network (RPN) and a Fast R-CNN \ndetector. Let’s walk through its architecture in detail: \nArchitecture Overview \nFaster R-CNN can be broken down into the following key components: \n1. Base Convolutional Network: \no The input image is passed through a shared convolutional backbone network (e.g., \nVGG16 or ResNet). \no This convolutional feature map is generated for the entire image, which serves as the \nbasis for both generating region proposals and for further object detection.  \n2. Region Proposal Network (RPN): \no The Region Proposal Network (RPN) is a fully convolutional network that generates \nregion proposals directly from the shared feature map. The RPN allows Faster R-CNN to \nbe fully integrated without relying on external methods (like Selective Search) to generate \nproposals. \no The RPN operates as follows: \n▪ Sliding Window Approach: \n▪ A sliding window is applied over the shared feature map. At each window \nlocation, a set of anchors (bounding boxes) of different sizes and aspect \nratios is used to predict objectness (whether an object is present or not) \nand bounding box adjustments. \n▪ Anchors: \n▪ For each sliding window location, a predefined set of anchor boxes (e.g., \n9 anchors with different scales and aspect ratios) is used. \n▪ Anchors help generate multiple region proposals at each location, which \nis useful for capturing objects of different sizes and shapes. \n▪ Two Outputs from RPN: \n1. Objectness Score: For each anchor, the RPN predicts whether the anchor contains an object or is \njust background. \n2. Bounding Box Regression: The RPN also predicts the bounding box offsets to adjust each anchor \nto better fit the detected object. \n3. RoI Pooling Layer: \no The Region of Interest (RoI) Pooling Layer is used to extract fixed-size feature maps from \nthe shared convolutional feature map for each region proposal generated by the RPN.  \no Each region proposal is projected onto the feature map, and the RoI pooling converts \neach of these regions into a fixed size (e.g., 7x7). This ensures that the subsequent \nlayers can process each proposal regardless of its original size. \n4. Classification and Bounding Box Regression (Fast R-CNN Detector): \no After RoI pooling, the pooled features are passed through a set of fully connected layers \nto produce two outputs for each region proposal: \n▪ Classification Scores: Predicts the class label for each region (e.g., car, person, \nbackground). \n▪ Bounding Box Regressor: Adjusts the bounding box coordinates to more \naccurately fit the detected object. \no This is the final step, where each region proposal is either classified as belonging to a \nparticular object class or as background. \nSlide 14 \nYOLO treats object detection as a single regression problem. This results in significantly faster detection \nspeeds, making YOLO ideal for applications requiring real-time processing. Here is a detailed explanation \nof the YOLO architecture and its components: \nKey Components and Architecture of YOLO \n1. Single Convolutional Neural Network Architecture: \no YOLO uses a single Convolutional Neural Network (CNN) that processes the entire \nimage at once. \no It divides the input image into a grid and, for each grid cell, predicts bounding boxes, \nconfidence scores, and class probabilities. \n2. Image Division into Grid Cells: \no The input image is divided into an S x S grid (e.g., 7x7 grid). \no Each grid cell is responsible for detecting objects whose center lies within that cell. \no This division allows YOLO to predict multiple objects in a single forward pass. \n3. Bounding Box Prediction: \no Each grid cell predicts B bounding boxes, along with a confidence score for each box. \no The confidence score represents the probability that the bounding box contains an object \nand how accurate the bounding box is. \no Each bounding box is described using: \n▪ x, y: Coordinates of the center of the bounding box, relative to the grid cell. \n▪ w, h: Width and height of the bounding box, relative to the entire image.  \n▪ Confidence: Indicates how likely it is that the bounding box contains an object \nand how precise the box is. \n4. Class Prediction: \no Each grid cell also predicts class probabilities for the objects within it. \no These class probabilities represent the likelihood of the object being of a particular class \n(e.g., car, person, dog). \no If there are C classes to detect, each grid cell predicts C class probabilities. \no The final score for each predicted bounding box is the product of the confidence score \nand the class probability. \nSlide 15 \nThe primary advantage of SSD is its ability to predict object classes and locations in a single forward \npass, eliminating the need for region proposal steps. Here's a detailed explanation of the SSD \narchitecture and how it works: \nKey Components and Architecture of SSD \nThe SSD architecture builds upon a base network (like VGG16) to extract features and uses additional \nconvolutional layers to predict both object classes and bounding boxes directly at different scales. Here \nare the key components and how the SSD network functions: \n1. Base Network (Feature Extractor): \no SSD starts with a base network to extract features from the input image. Typically, \nVGG16 is used, but with modifications. \no The initial layers of VGG16 are used for feature extraction to learn the spatial features, \nbut the fully connected layers are removed, making it a fully convolutional network. \no The output from the base network serves as feature maps for detecting objects.  \n2. Extra Convolutional Layers: \no After the base network, additional convolutional layers are added to create feature maps \nat multiple scales. \no These additional layers allow SSD to detect objects at different resolutions, making it \ncapable of detecting both small and large objects. \no Each of these convolutional layers has progressively smaller feature maps, which capture \ninformation at different scales. \n3. Multiscale Feature Maps for Detection: \no SSD leverages multiple feature maps of different sizes to detect objects at different \nscales. \no For example, SSD might use the feature maps from layers with dimensions such as \n38x38, 19x19, 10x10, etc. \no Higher resolution feature maps are used for detecting small objects, whereas lower \nresolution maps are used for detecting large objects. \n4. Convolutional Predictors for Bounding Boxes and Class Scores: \no For each feature map (from both the base network and extra layers), convolutional \npredictors are applied to generate bounding box offsets and class scores. \no At each location in the feature map, SSD predicts a set of bounding boxes (also known \nas default boxes or anchor boxes) of different aspect ratios. \no For each bounding box, SSD predicts: \n▪ Coordinates (x, y, w, h): The location of the bounding box. \n▪ Confidence Scores: Probability of an object being present in the bounding box. \n▪ Class Labels: Scores for all object classes, from which the highest is selected.  \n5. Default Boxes (Anchor Boxes): \no Each feature map cell predicts a set of default boxes at different aspect ratios (e.g., 1:1, \n2:1, 1:2). \no The network predicts offsets for these default boxes to match them with the actual ground \ntruth boxes, allowing for flexibility in detecting objects of various shapes and sizes.  \no For example, a 19x19 feature map may have several default boxes per cell, resulting in \nmany bounding boxes for different scales and aspect ratios. \n6. Non-Maximum Suppression (NMS): \no SSD generates a large number of bounding box predictions, which often results in \noverlapping detections. \no To select the best bounding boxes, Non-Maximum Suppression (NMS) is used. NMS \neliminates redundant boxes and retains only those with the highest confidence scores \nand minimal overlap, which helps to reduce false positives. \nSlide 16 \nInput Representation \n• 3D Volume Input: \no The input to a 3D CNN is a 3D volume or sequence of images (e.g., video frames). \no For videos, the input can be represented as a 4D tensor with dimensions (T, H, W, C): \n▪ T: Temporal dimension (number of frames). \n▪ H: Height of each frame. \n▪ W: Width of each frame. \n▪ C: Number of channels (e.g., 3 for RGB). \n2. 3D Convolutional Layers \n• 3D Convolutions: \no In 2D CNNs, convolution is performed over the height and width of the image, whereas in \n3D CNNs, the convolution operation is performed over the height, width, and depth \n(temporal axis). \no A 3D kernel (filter) is used, which has dimensions (Kt, Kh, Kw), where: \n▪ Kt: Temporal extent of the filter. \n▪ Kh and Kw: Spatial dimensions of the filter. \no The filter slides across the 3D input, moving in both spatial directions (height and width) \nand along the temporal dimension, thereby capturing spatio-temporal features. \no Output Feature Maps: \n▪ The output from a 3D convolution is a set of 3D feature maps that preserve \nspatial and temporal relationships in the input data. \n3. 3D Pooling Layers \n• Pooling in 3D: \no Similar to 2D pooling in traditional CNNs, 3D pooling layers are used to downsample the \nfeature maps, reducing their spatial and temporal dimensions. \no 3D Max Pooling or 3D Average Pooling operations are used, where the pooling operation \nis applied across the height, width, and depth. \no This helps in reducing computational cost and retaining the most important features while \nignoring minor variations in the input. \n4. Fully Connected Layers \n• After several 3D convolutional and pooling layers, the feature maps are flattened into a 1D vector. \n• These vectors are then passed through fully connected layers for classification or regression \ntasks. \n• For example, in action recognition, the fully connected layers output a probability distribution over \nthe possible actions. \n5. Activation Functions \n• ReLU (Rectified Linear Unit) is commonly used as an activation function in 3D CNNs, introducing \nnon-linearity to the model. \n• The ReLU activation helps in speeding up training and avoids the vanishing gradient problem by \nsetting negative values to zero. \nSlide 17 \nVideo Transformers rely on attention mechanisms to focus on different parts of video frames, allowing \nthem to capture long-range dependencies and relationships over time. Here’s an in-depth explanation of \nthe Video Transformer architecture: \nKey Concepts of Video Transformer Architecture \n1. Self-Attention Mechanism: \no Self-Attention is the core component of Transformer models. It allows each token (e.g., a \nvideo patch or frame) to attend to every other token, capturing global context. \no In video analysis, self-attention enables the model to understand spatial relationships \nwithin frames and temporal dependencies across frames. \n2. Input Representation: \no A video consists of a sequence of frames that can be represented in a 4D tensor format \n(T, H, W, C): \n▪ T: Number of frames (temporal dimension). \n▪ H and W: Height and width of each frame. \n▪ C: Number of channels (e.g., 3 for RGB). \no The input video is split into spatio-temporal patches to feed into the Transformer. \n3. Embedding the Patches: \no Patch Embeddings: \n▪ The video frames are split into smaller patches (e.g., 16x16 pixels). Each patch is \nthen flattened into a 1D vector. \n▪ These vectors are then passed through a linear projection layer to create patch \nembeddings. \no Positional Encoding: \n▪ To provide the model with spatial and temporal position information, positional \nencodings are added to the patch embeddings. \n▪ This helps the model understand the order of the frames (temporal) and the \nposition of the patches within each frame (spatial). \n4. Transformer Encoder: \no The Transformer Encoder is the core of the Video Transformer model. It consists of \nmultiple layers, each with: \n▪ Multi-Head Self-Attention: \n▪ The self-attention mechanism is applied to the patch embeddings across \nboth spatial and temporal dimensions. \n▪ Multi-head attention enables the model to capture different types of \nrelationships in the video, such as interactions between objects and \nmotion. \n▪ Feed-Forward Neural Network: \n▪ A feed-forward network (FFN), consisting of fully connected layers, \nprocesses each embedding independently after the attention layer. \n▪ This adds more non-linearity and helps learn complex representations. \n▪ Layer Normalization and Residual Connections: \n▪ Each layer is followed by Layer Normalization to stabilize training. \n▪ Residual Connections are used to ease the flow of gradients and avoid \nvanishing or exploding gradient issues, making training deeper networks \nfeasible. \n5. Temporal and Spatial Attention: \no The Video Transformer needs to focus on both spatial relationships (within frames) and \ntemporal relationships (across frames). \no Factorized Attention: \n▪ Some Video Transformer architectures implement factorized attention, where \nspatial attention is applied to each frame separately, followed by temporal \nattention across frames. \no Joint Spatio-Temporal Attention: \n▪ Another approach is to use joint spatio-temporal attention, where attention is \ncalculated across both spatial and temporal dimensions simultaneously. \n▪ This enables the model to learn both spatial (object-level) and temporal (action-\nlevel) relationships simultaneously. \n6. Classification Head: \no After processing all the patches through multiple Transformer layers, the output \nembeddings are passed to a classification head. \no Typically, a special classification token (CLS token) is used to represent the aggregated \ninformation of the entire video. \no The final classification layer outputs probability scores for each class (e.g., action \ncategories like walking, running, jumping). \nSlide 22 \nProposed model: \nVideo Input: \n• Input Data: The input is a sequence of video frames represented by a tensor with dimensions (N, \n3, 64, 112, 112): \no N: Batch size. \no 3: Number of channels (e.g., RGB). \no 64: Number of frames in the input sequence (temporal dimension). \no 112, 112: Height and width of each frame. \n2. 3D ResNet Block: \nThe 3D ResNet serves as the backbone to extract spatial and short-term temporal features. It consists of \nseveral convolutional blocks, each with its specific purpose: \n• Conv Block 1 (N, 64, 56, 56): \no The first convolutional block extracts basic features from the input. The dimensions are \nreduced to (N, 64, 56, 56), meaning that the frames have been downsampled spatially. \no This block is repeated 3 times to capture more detailed low-level features. \n• Conv Block 2 (N, 128, 28, 28): \no The second convolutional block reduces the dimensions further to (N, 128, 28, 28). \no This layer extracts deeper spatial and temporal features from the video, and it is repeated \n4 times. \n• Conv Block 3 (N, 256, 14, 14): \no The third block continues to downsample and extract more abstract features. The output \ndimensions become (N, 256, 14, 14). \no This block is repeated 6 times to extract more refined features, enhancing the model’s \nunderstanding of the spatial and temporal relationships. \n• Conv Block 4 (N, 512, 8, 8): \no The final convolutional block reduces the spatial dimensions to (N, 512, 8, 8). \no This block is repeated 3 times and provides a high-level representation of the input video \nthat is rich in both spatial and short-term temporal information. \n• Average Pooling (AvgPool3d): \no After the convolutional blocks, an average pooling layer is applied to reduce the output \ndimensions to (N, 512, 8, 1, 1). \no This reduces the spatial and temporal dimensions to a manageable size, summarizing \nthe extracted features from the previous layers. \n3. Transformer Encoding: \nThe Transformer Encoder is used to capture long-term dependencies and relationships between frames, \nwhich is crucial for understanding temporal patterns in video sequences.  \n• Input to Transformer: \no The output from the 3D ResNet is flattened and fed into the Transformer. Each frame is \nrepresented as a feature vector, and positional encodings are added to maintain temporal \ninformation. \n• Multi-Head Attention: \no The multi-head attention mechanism allows the Transformer to focus on different parts of \nthe input sequence, learning complex temporal relationships across frames.  \no Each head learns a different representation, enabling the model to understand different \ntemporal patterns. \n• Add & Norm, Feed Forward, and Layer Normalization: \no Add & Norm layers are used to implement residual connections, which help stabilize \ntraining by preventing vanishing gradients. \no Feed Forward layers are applied to add non-linearity, helping the model learn complex \nfeatures. \no Layer Normalization is applied to normalize the outputs, further stabilizing the learning \nprocess. \n• Stacking Transformer Encoders: \no Multiple Transformer Encoder layers (N times) are stacked to enhance the model’s ability \nto learn long-range temporal dependencies. Each encoder layer has multi-head attention, \nfeed-forward networks, and normalization components. \n4. Classification Head: \n• Softmax Layer: \no The final output from the Transformer Encoder is passed through a fully connected layer \nwith a softmax activation function. \no This layer produces probability scores for different classes, allowing the model to predict \nthe action or event occurring in the video. \n \n \n "
  },
  {
    "source": "2112.08274v3.pdf",
    "content": "Putting People in their Place: Monocular Regression of 3D People in Depth\nYu Sun1* Wu Liu2† Qian Bao2 Yili Fu1† Tao Mei2 Michael J. Black3\n1Harbin Institute of Technology, Harbin, China 2 Explore Academy of JD.com, Beijing, China\n3Max Planck Institute for Intelligent Systems, T¨ubingen, Germany\nyusun@stu.hit.edu.cn, liuwu1@jd.com, baoqian@jd.com, meylfu@hit.edu.cn\ntmei@jd.com, black@tuebingen.mpg.de\nFigure 1. Monocular reconstruction of multiple 3D people with coherent depth reasoning.We introduce BEV , a monocular one-stage\nmethod with an efﬁcient new “bird’s-eye-view” representation that enables the network to explicitly reason about people in 3D.\nAbstract\nGiven an image with multiple people, our goal is to di-\nrectly regress the pose and shape of all the people as well\nas their relative depth. Inferring the depth of a person in an\nimage, however, is fundamentally ambiguous without know-\ning their height. This is particularly problematic when the\nscene contains people of very different sizes, e.g. from in-\nfants to adults. To solve this, we need several things. First,\nwe develop a novel method to infer the poses and depth\nof multiple people in a single image. While previous work\nthat estimates multiple people does so by reasoning in the\nimage plane, our method, called BEV , adds an additional\nimaginary Bird’s-Eye-View representation to explicitly rea-\nson about depth. BEV reasons simultaneously about body\n*This work was done when Yu Sun was an intern at Explore Academy\nof JD.com.\n†Corresponding author.\ncenters in the image and in depth and, by combing these,\nestimates 3D body position. Unlike prior work, BEV is a\nsingle-shot method that is end-to-end differentiable. Sec-\nond, height varies with age, making it impossible to resolve\ndepth without also estimating the age of people in the im-\nage. To do so, we exploit a 3D body model space that lets\nBEV infer shapes from infants to adults. Third, to train BEV ,\nwe need a new dataset. Speciﬁcally, we create a “Relative\nHuman” (RH) dataset that includes age labels and rela-\ntive depth relationships between the people in the images.\nExtensive experiments on RH and AGORA demonstrate the\neffectiveness of the model and training scheme. BEV out-\nperforms existing methods on depth reasoning, child shape\nestimation, and robustness to occlusion. The code 1 and\ndataset2 are released for research purposes.\n1https://github.com/Arthur151/ROMP\n2https://github.com/Arthur151/Relative_Human\narXiv:2112.08274v3  [cs.CV]  20 Apr 2022\n1. Introduction\nIn this article, we focus on simultaneously estimating the\n3D pose and shape of all people in an RGB image along\nwith their relative depth. There has been rapid progress [22]\non regressing the 3D pose and shape of individual (cropped)\npeople [4, 15, 16, 18, 19, 26, 29, 35, 44, 45, 47, 49] as well\nas the direct regression of groups [11, 34]. Neither class\nof methods explicitly reasons about the depth of people\nin the scene. Such depth reasoning is critical to enable a\ndeeper understanding of the scene and the multi-person in-\nteractions within it. To address this, we propose a uniﬁed\nmethod that jointly regresses multiple people and their rel-\native depth relations in one shot from an RGB image.\nWhile previous multi-person methods perform well in\nconstrained experimental settings, they struggle with severe\nocclusion, diverse body size and appearance, the ambiguity\nof monocular depth, and in-the-wild cases [11, 25, 38, 48].\nThese challenges lead to unsatisfactory performance in\ncrowded scenes, including detection misses, similar predic-\ntions for overlapping people, and all predictions having a\nsimilar height. We observe two inter-related limitations that\nresult in these failures. First, the architecture of the regres-\nsion networks is closely tied to the 2D image, while the peo-\nple actually inhabit 3D space. We address this with a new\narchitecture that reasons in 3D. Second, depth estimation is\nfundamentally ambiguous due to the unknown height of the\npeople in the image and it is difﬁcult to obtain training data\nof images with ground-truth height and depth. To address\nthis, we present a new dataset and novel losses that allow\ntraining without having metric depth.\nWe observe that crowded scenes contain rich information\nabout the relative relationships between people, which can\nbe exploited for both training and validation of depth rea-\nsoning. However, we still lack a powerful representations to\nlearn from these cases. A few learning-based methods have\nbeen proposed for reasoning about the depth of predicted\nbody meshes [11] or 3D poses [25, 38, 48]. Unfortunately,\nthey all reason about depth via 2D representations, such as\nRoI-aligned features [11, 25] or a 2D depth map [38, 48].\nThese regression-based 2D representations have inherent\ndrawbacks for representing the 3D world. The lack of an\nexplicit 3D representation in the networks makes it chal-\nlenging for these methods to deal with crowded scenes in\nwhich people overlap at different depths. Therefore, we ar-\ngue that an explicit 3D representation is needed.\nTo achieve this, we develop BEV (for Bird’s Eye View),\na uniﬁed one-stage method for monocular reconstruction\nand depth reasoning of multiple 3D people. We take inspira-\ntion from ROMP [34], a one-stage, multi-person, regression\nmethod that directly estimates multiple 2D front-view maps\nfor 2D human detection, positioning, and mesh parameter\nregression without depth reasoning. With ROMP, the net-\nwork can only reason about the 2D location of people in\nthe image plane. To go beyond this, we need to enable the\nnetwork to efﬁciently reason about depth as well. To that\nend, we introduce a new imaginary 2D “bird’s-eye-view”\nmap that represents the likely centers of bodies in depth. To\nbe clear, BEV takes only a single 2D image; the overhead\nview is inferred, not observed. BEV uses a powerful and\nefﬁcient localization pipeline, performing bird’s-eye-view-\nbased coarse detection and ﬁne localization in parallel. We\nemploy the 2D heatmaps for coarse detection from both the\nfront (image) and bird’s eye views. BEV combines these\nheatmaps to obtain a 3D heatmap, as illustrated in Fig. 2.\nBy learning the front and the bird’s-eye view together, BEV\nexplicitly models how people appear in images and in depth.\nThis enables BEV to learn from available 2D and 3D anno-\ntations. BEV also uses a novel 3D Offset map to reﬁne the\ninitial coarse detections. From these coarse and ﬁne maps,\nwe obtain the 3D translation of all people in the scene. BEV\ntransforms these predictions from the latent 3D Center-map\nspace to an explicit camera-centric 3D space. Given these\n3D translation predictions, BEV samples the features of all\nthe people from a predicted mesh feature map and regresses\nthe ﬁnal SMPL [23] parameters. Distinguishing people at\ndifferent depths enables BEV to estimate multiple people\neven with severe occlusion as illustrated in Fig. 1.\nEven with a powerful 3D representation, we need an\nappropriate training scheme to ensure generalization. The\nmain reason is that without knowing subject height, we lack\neffective constraints to alleviate the depth/height ambigu-\nity under perspective projection. In particular, height varies\nwith age, making it impossible to resolve depth without also\nestimating the age of people in the image. The ambiguity\ncauses incorrect depth estimates for children and infants,\nlimiting the generalization of existing methods. Unfortu-\nnately, existing 3D datasets with multiple people have lim-\nited diversity in height and age, so they cannot be used to\nimprove or evaluate generalization.\nSince collecting ground-truth 3D data in the wild is difﬁ-\ncult, we instead train BEV using cost-effective weak labels\nof in-the-wild images. Speciﬁcally, we collect a dataset,\nnamed “Relative Human” (RH), that contains weak anno-\ntations of depth layers and human ages categorized into\nthe groups adult, teenager, child, and infant. Moreover,\nwe propose a weakly supervised training scheme (WST)\nto effectively learn from these weak supervision signals.\nFor instance, we use a piece-wise loss function that ex-\nploits the depth layers to penalize incorrect relative depth\norders. Exploiting age information to constrain height is\ntricky. While age and height are correlated, heights can vary\nsigniﬁcantly within the same age group. Consequently, we\ndevelop an ambiguity-compatible mixed loss function that\nencourages body shapes with heights that lie within an ap-\npropriate range for each age group.\nWe evaluate BEV on three multi-person datasets: in-the-\nwild using the 2D RH dataset and in 3D using the real CMU\nPanoptic [13] and the synthetic AGORA [28] datasets.\nOn RH, compared with previous methods [11, 25, 38, 48],\nBEV is more accurate in relative depth reasoning and pose\nestimation. On CMU Panoptic, BEV outperforms pre-\nvious methods [6, 11, 34, 42, 43] in 3D pose estimation.\nOn AGORA, BEV signiﬁcantly improves detection and\nachieves state-of-the-art results on “AGORA kids” in terms\nof the mesh reconstruction error. Also, ﬁne-tunning on RH\nin a weakly supervised manner signiﬁcantly improves the\nresults for all age groups, especially for young people.\nIn summary, the main contributions are: (1) We construct\na 3D representation to alleviate the monocular depth am-\nbiguity via combining a front-view representation with an\nimaginary bird’s eye view. (2) We collect the Relative Hu-\nman dataset with weak annotations of in-the-wild images,\nwhich facilitates the training and evaluation on monocular\ndepth reasoning in multi-person scenes. (3) We develop\na weakly supervised training scheme to learn from weak\ndepth annotations and to exploit age information.\n2. Related Work\nMonocular 3D mesh regression from natural scenes.\nHere, we focus on regressing a 3D body mesh using a para-\nmetric model like SMPL from a single RGB image. Most\nmethods can be divided into multi-stage or single-stage ap-\nproaches. For general multi-person cases, most existing\nmethods [4, 15, 19, 26, 29] are based on a typical two-stage\nframework, which ﬁrst detects people and then estimates\nthe parameters of each person separately. Recent methods\nfocus on exploring various supervision [33] signals, such as\ntemporal coherence [16], contour alignment [7,31,39], self-\ncontact [27], ground constraints [32, 40], or global human\ntrajectory [41] to enhance the geometric/dynamic consis-\ntency. However, for depth reasoning about all people in the\nscene, these multi-stage methods are not ideal. The process-\ning of individual cropped people cannot exploit the scene\ncontext or reason about depth ordering.\nA few one-stage methods [24, 34] estimate multiple 3D\npeople simultaneously. Given a single image, ROMP [34]\noutputs a 2D Body Center Heatmap, Camera Map, and\nParameter Map for 2D human detection, positioning, and\nmesh parameter regression, respectively. At the position\nparsed from the 2D Body Center heatmap, ROMP samples\nthe ﬁnal mesh parameters from the Camera and Parameter\nmaps. These one-stage methods enjoy a holistic view of the\nimage, which is more suitable for depth reasoning. How-\never, they are based on 2D representations that do not rep-\nresent depth. Like most methods, they model adults (with\nSMPL), train on images of adults, and therefore only predict\nadults. To tackle the limitations of their 2D representation\nand age bias, we propose BEV and its training scheme of\nlearning age priors that constrain body height.\nMonocular depth reasoning. Most previous meth-\nods place bodies in depth via post-processing. Due to their\n2D-based pipeline and lack of height prior for different age\ngroups, their results are unsatisfying. A few learning-based\nmethods, like 3DMPPE [25] and CRMH [11], address\nmulti-stage depth reasoning. 3DMPPE uses image features\nto reﬁne the bounding-box-based depth predictions. CRMH\nlearns from instance segmentation to distinguish the relative\ndepth between overlapping people. However, instance seg-\nmentation is expensive and unable to promote the learning\nof depth relations in cases without overlapping. SMAP [48]\nand HMOR [38] employ a 2D depth map to represent the\nroot depth of 3D pose at each pixel. However, in crowded\nscenes, these 2D representations are ambiguous. In con-\ntrast, BEV adopts a novel bird’s-eye-view-based 3D repre-\nsentation to distinguish people at different depths, therefore,\nit is more robust to the overlapping cases. Most recently,\nUgrinovic et al. [36] propose an optimization-based method\nto reﬁne the 3D translation of estimated body meshes. They\nﬁt the 3D body mesh to the detected 2D poses and force the\nfeet to touch the ground. In contrast, our learning-based,\none-stage, framework is more efﬁcient and ﬂexible, and can\nadapt to more scenarios, such as jumping. Albiero et al. [2]\nestimate the depth of all faces in a crowd in one shot by re-\ngressing their 6DoF pose; they do not deal with shape vari-\nation or articulation.\n3. Method\n3.1. Overview\nThe overall framework is illustrated in Fig. 2. BEV\nadopts a multi-head architecture. Given a single RGB im-\nage as input, BEV outputs 5 maps. For coarse-to-ﬁne local-\nization, we use the ﬁrst 4 maps, which are the Body Cen-\nter heatmaps and the Localization Offset maps in the front\nview and bird’s-eye view. We ﬁrst expand the front-/bird’s-\neye-view maps in depth/height and then combine them to\ngenerate the 3D Center/Offset maps. For coarse detection,\nwe extract the rough 3D position of people from the 3D\nCenter map. For ﬁne localization, we sample the offset vec-\ntors from the 3D Offset map at the corresponding 3D center\nposition. Adding these gives the 3D translation prediction.\nFor 3D mesh parameter regression, we use the estimated\n3D translation (xi,yi,di) and the Mesh Feature map. The\ndepth value di of 3D translation is mapped to a depth en-\ncoding. At (xi,yi), we sample a feature vector from the\nMesh Feature map and add it to the depth encoding for ﬁ-\nnal parameter regression. Finally, we convert the estimated\nparameters to body meshes using the SMPL+A model.\n3.2. SMPL+A: Mesh Representation for All Ages\nThe SMPL [23] and SMIL [9] models are developed to\nparameterize 3D body meshes of adults and infants into\nFigure 2. Overview. Given an RGB image, BEV ﬁrst estimates the 3D translation of all people in the scene via compositing the front-\nview and the bird’s-eye-view predictions. Then guided by the 3D translation, we sample the mesh feature of each person to regress their\nage-aware SMPL+A parameters. See Sec. 3.1 for details.\nlow-dimensional parameters. Recently, AGORA [28] fur-\nther extends SMPL to support children by linearly blend-\ning the SMIL and SMPL template shapes with a weight\nα ∈ [0,1], which we refer to as an “age offset.” While\nblending the templates to address scale and proportion dif-\nferences between adults and children, AGORA uses the\nadult shape space regardless of age. Additionally, AGORA\ndoes not address the representation of infants. We make a\nsmall, but important, change to better support all ages.\nFollowing the notation of SMPL [23], the SMPL+A\nmodel deﬁnes a piece-wise function ⃗B = M(⃗θ,⃗β,α) that\nmaps 3D pose ⃗θ, shape ⃗β, and age offset α to a 3D body\nmesh ⃗B ∈R6890×3. The pose parameters, ⃗θ ∈R6×22, cor-\nrespond to the 6D rotations [50] of the ﬁrst 22 body joints of\nSMPL. The shape parameter ⃗β ∈R10 are the top-10 PCA\ncoefﬁcients of either the SMPL gender-neutral shape space\nor the SMIL shape space.\nThe adult shape space of AGORA produces shape defor-\nmations that are too large for an infant body, resulting in a\ndistorted mesh when posed. Therefore, we use SMIL for\ninfants when the age offsetαis above a threshold tα. When\nα >tα, M(⃗θ,⃗β,α) is the SMIL model MI(⃗θ,⃗β). When\nthe age offset α≤tα, we use the AGORA formulation\nM(⃗θ,⃗β,α) =W(TA(⃗θ,⃗β,α; T,TI),J(⃗β),⃗θ,W),\nTA(·) = (1−α)T + αTI + BS(⃗β) +BP(⃗θ),\n(1)\nwhere W(·) performs linear blend-skinning with weights\nW to convert the T-posed mesh TA(·) to the target pose\n⃗θ based on the skeleton joints J(·). The T-posed mesh\nTA(·) is the weighted sum of the templates (T,TI), shape-\ndependent deformation BS(·), and pose-dependent defor-\nmation BP(·). The age offset α ∈[0,1] is used to inter-\npolate between the adult SMPL template T and the infant\nFigure 3. Example images from the Relative Human (RH) dataset\nwith weak annotations: depth layers (DLs) and age group classiﬁ-\ncation. Examples are a) adults at different DLs, and b) people of\ndifferent age groups at the same DL.\nSMIL template TI. The larger the α, the lower the mesh\ntemplate height.\nThe 3D joints ⃗J of the output mesh are derived via J⃗B,\nwhere J∈ RK×6890 is a sparse weight matrix that linearly\nmaps the vertices ⃗Bto the K body joints. To supervise 3D\njoints ⃗Jwith 2D keypoints, regression methods [15,34] typ-\nically adopt a weak-perspective camera model to project ⃗J\ninto the image plane. For better depth reasoning, we em-\nploy a perspective camera model to perform projection; see\nSup. Mat. for the details of our camera model.\n3.3. Relative Human dataset\nExisting in-the-wild datasets lack groups of overlapping\npeople with annotations. Since acquiring 3D annotations of\nlarge crowds is challenging, we exploit more cost-effective\nweak annotations. We collect a new dataset, named Rela-\ntive Human (RH), to support in-the-wild monocular human\ndepth reasoning.\nThe images are collected from multiple sources to ensure\ndiversity in age, ethnicity, gender, and scene. Most images\nFigure 4. Pre-deﬁned 3D camera anchor maps.\nare collected from the existing 2D pose datasets [20,21,46].\nThey contain few infants so we collect additional open-\nsource family photos from Pexels [1] and then annotate their\n2D poses. As shown in Fig. 3, we annotate the relative depth\nrelationship between all people in the image. We treat sub-\njects whose depth difference is less than one body-width\n(γ = 0.3m) as people in the same layer. We then classify\nall people into different depth layers (DLs). Unlike prior\nwork, which labels the ordinal relationships between pairs\nof joints of individuals [5], DLs capture the depth order of\nmultiple people. Additionally, we label people with four\nage categories: adults, teenagers, children, and babies.\nIn total, we collect about 7.6K images with weak annota-\ntions of over 24.8K people. More than 21% of the subjects\nare young people (5.3K), including teenagers, children, and\nbabies. For more analysis, please refer to Sup. Mat.\n3.4. Representations\nFigure 2 gives an overview of BEV’s representations.\nHeatmaps: We build on the body-center heatmap repre-\nsentation from ROMP [34]. The front-view heatmap of size\nR1×H×W is aligned with the pixel space and represents the\nlikelihood of a body being centered at a 2D location using\nGaussian kernels. We go beyond ROMP to add a second 2D\nheatmap of size R1×D×W that represents an unseen bird’s-\neye-view. This heatmap represents the likelihood of a per-\nson being at some point in depth; this map, however, does\nnot represent metric depth. BEV composes and reﬁnes these\ntwo maps into a 3D heatmap,M3D\nC ∈R1×D×H×W, which\nrepresents the 3D position of the detected human body cen-\nters with 3D Gaussian kernels.\nOffset maps: The discretized Center Heatmaps coarsely\nlocalize the body but we want the network to produce more\nprecise estimates. To improve the granularity of 3D lo-\ncalization, we use additional maps that, at each position,\nadd an estimated offset vector to reﬁne the coarse detec-\ntion. The front-view Offset map of size R3×H×W contains\n3D offset vectors. The bird’s-eye-view Offset map of size\nR1×D×W contains 1D offset vectors for depth correction.\nM3D\nO ∈R3×D×H×W corresponds to the 3D Center map\nand contains a 3D offset vector at each 3D position.\n3D camera anchor maps: Each discretized coordinate\nin the 3D Center map corresponds to a set of camera param-\neters, representing its 3D position in the world. The anchor\nmap serves as a mapping function to transform the coordi-\nnates of the 3D Center map to the 3D position in a prede-\nﬁned perspective camera space. To establish a one-to-one\nmapping from the square Center map to a pyramidal cam-\nera space, as shown in Fig. 4, we voxelize camera space.\nEach voxel center corresponds to a discretized 3D coordi-\nnate in the Center map. The 3D position vector (x,y,d ) of\nvoxel center is the anchor value of 3D camera anchor map.\nV oxels of equal depth form a depth plane, corresponding to\na 2D (x-y) slice of the 3D camera anchor map. During in-\nference, the 3D camera anchor map is sampled at the same\ncoordinate of 3D Center map to obtain the coarse 3D trans-\nlation of the corresponding detection.\nMesh feature map: MF ∈R128×H×W contains a 128-\nD mesh feature vector at each 2D position. These features\nare aligned with the input 2D image at the pixel level. After\na 3D-center-based sampling process, the relevant features\nare used for the regression of SMPL+A parameters.\n3.5. BEV\nTo effectively establish the 3D representation, the front-\nview and the bird’s-eye-view must work together to esti-\nmate the image position and depth of corresponding sub-\njects. Independently estimating the map of two views in\nparallel would inevitably cause misalignment, leading to the\nfailure of 3D heatmap-based detection. To connect the two\nviews, we estimate the bird’s-eye-view maps conditioned on\nthe front-view maps (i.e. Center and Offset maps). Speciﬁ-\ncally, to estimate the bird’s-eye-view maps, we take the con-\ncatenation of the front-view maps and the backbone feature\nmaps as input. The front-view 2D body-centered heatmap\nis used as a form of robust attention to people in the im-\nage, which helps the model focus on exploring depth during\nbird’s-eye view estimation. Then we expand and composite\nthe 2D maps from the front and BEV views to generate the\n3D maps. To integrate 2D features from two views and en-\nhance 3D consistency, we further perform 3D convolution\non the composited 3D maps for reﬁnement.\nNext, we extract the 3D translation from the estimated\n3D maps, M3D\nC ,M3D\nO . High-conﬁdence 3D positions of\nthe 3D Center map are where we sample 3D offset vectors\nfrom the 3D Offset map. From the same 3D position in the\n3D camera anchor maps (Fig. 4), we obtain the 3D anchor\nvalues, which are positions in camera space of the corre-\nsponding 3D center voxel. Adding the 3D offset vectors to\nthe 3D anchor values gives the 3D translation as output.\nFinally, we take the estimated 3D translation (xi,yi,di)\nand Mesh Feature maps MF for parameter regression. We\nsample the pixel-level mesh feature vectors at (xi,yi) of\nMF. Inspired by positional embeddings [37], we learn an\nembedding space to differentiate people at different depths,\nespecially for the overlapping cases. The predicted depth\nvalue di is mapped to a 128-dim encoding vector via an\nembedding layer. We sum up the depth encodings and the\nmesh feature vectors to differentiate the features of people\nat different depths, enabling individual estimates for dif-\nferent subjects. Then we estimate the SMPL+A parame-\nters (⃗θ,⃗β,α) via a fully-connected block. The output body\nmeshes are obtained via M(⃗θ,⃗β,α).\n3.6. Loss Functions\nOur loss functions are divided into two groups illustrated\nin Fig. 2: relative losses (in gold) and the standard mesh\nlosses (in black). BEV is supervised by the weighted sum of\nall loss items. First, we introduce two relative loss functions\nfor weakly supervised training (WST).\nPiece-wise depth layer loss Ldepth. Ldepth is de-\nsigned to supervise the predicted depth di,dj of subject i,j\nby their depth layers ri,rj via\n\n\n\n(di − dj)2, r i = rj\nlog(1 +edi−dj ) ∏((di − dj) − γ(ri − rj)), ri <rj\nlog(1 +edj−di ) ∏(γ(ri − rj) − (di − dj)), ri >rj,\n(2)\nwhere ∏is a binarization function that maps positive values\nto 1 and negative values to 0. ∏ is used to judge whether\nthe BEV prediction is consistent with the depth relationship\nof the ground truth DLs. Ldepthis 0, if the predicted depth\ndifference is within an acceptable range; that is, greater than\nthe product of the DL difference and body-width γ. Other-\nwise, Ldepthwill encourage the model to achieve it.\nPrevious ordinal depth losses [5, 30] encourage the\nmodel to enlarge the depth difference between people at\ndifferent depth layers as much as possible. In contrast, the\npenalty in Ldepth is controlled within a range. This helps\navoid pushing remote subjects too far away.\nAmbiguity-compatible age lossLage. The classiﬁca-\ntion of age categories (infant, child, teenager, adult) is in-\nherently ambiguous, especially for teenagers and children.\nAlso, while height is correlated with age, one can easily ﬁnd\nchildren who are taller than some adults. Consequently, we\nformulate an ambiguity-compatible mixed loss Lage.\nRather than supervise height directly, we supervise the\nα parameter that controls the blending between the SMIL\ninfant body and the SMPL adult body. To do so, we de-\nﬁne ranges of α values for each age group; i.e. (lower-\nbound, middle, upper-bound). We do this using the sta-\ntistical data of heights for each age category that we then\nrelate these to ranges of αvalues. Formally, the ranges are\n(αk\nl,αk\nm,αk\nu),k = 1···4 where kis the annotated age class\nnumber; see Sec. 4 for details.\nBEV is then trained to predict the body shape as well\nas an α value for each person. Given the predicted α and\nground truth age class kg, the loss Lage is deﬁned as\nLage(α) =\n{\n0, α\nkg\nl <α ≤ α\nkg\nu\n(α− α\nkg\nm )2, otherwise. (3)\nOther losses. Following the previous methods [15, 34],\nwe employ the standard mesh losses to supervise the out-\nput maps and regressed SMPL+A parameters. Lcm is the\nfocal loss [34] of the front-view Body Center heatmap. In\nthe same pattern, we further use a 3D focal loss Lcm3D\nto supervise the 3D Center map via converting Lcm’s 2D\noperation to 3D. Lpm consists of three parts, Lθ,Lβ, and\nLprior. Lθ and Lβ are L2 losses of SMPL+A pose ⃗θand\nshape ⃗β parameters respectively. Lprior is the Mixture of\nGaussian pose prior [4, 23] on ⃗θ. To supervise the 3D body\njoints ⃗J, we use Lj3d, which is composed of Lmpj and\nLpmpj. Lmpjis the L2 loss of 3D joints ⃗J. To alleviate the\ndomain gap between training datasets, we follow [34,35] to\ncalculate the L2 loss Lpmpj of the predicted 3D joints af-\nter Procrustes alignment with the ground truth. Lpj2d is\nthe L2 loss of the 2D projection of 3D joints ⃗J. Lastly, w(.)\ndenotes the corresponding weight of these losses.\n4. Experiments\n4.1. Implementation Details\nTraining details.For basic training, we use two 3D pose\ndatasets (Human3.6M [10] and MuCo-3DHP [24]) and four\n2D pose datasets (COCO [21], MPII [3], LSP [12], and\nCrowdPose [20]). We also use the pseudo SMPL anno-\ntations from [14] and WST on RH. Most samples in RH\nare collected from 2D pose datasets [20, 21, 46]. For a fair\ncomparison, we only use the samples that are also used\nfor training in compared methods [11, 18, 19, 25, 34, 48].\nTo compare with [18, 28], we further ﬁne-tune our model\nand ROMP on AGORA. The threshold for the age offset\nis set to tα = 0.8. The age offset ranges (αk\nl,αk\nm,αk\nu)\nare: adults (−0.05,0,0.15), teenagers (0.15,0.3,0.45),\nchildren (0.45,0.6,0.75), and infants (0.75,0.9,1). See\nSup. Mat. for more details.\nEvaluation benchmarks. We evaluate BEV on three\nmulti-person datasets, RH, CMU Panoptic, [13] and\nAGORA [28], containing 257 child scans and signiﬁcant\nperson-person occlusion.\nEvaluation matrix. To evaluate the accuracy of depth\nreasoning, we employ the Percentage of Correct Depth Re-\nlations (PCDR0.2), and set the threshold for equal depth to\n0.2m. To evaluate the accuracy of projected 2D poses on\nRH, we also report the mean Percentage of Correct Key-\npoints ( mPCK0.6\nh ), setting the matching threshold to 0.6\ntimes the head length.\nAlso, following AGORA [28], we evaluate the accu-\nracy of 3D pose/mesh estimation while considering miss-\ning detections. To evaluate the detection accuracy, we re-\nMethod PCDR0.2(%)↑ mPCK0.6\nh ↑Baby Kid Teen Adult All\n3DMPPE†[25] 39.33 51.42 60.91 57.95 57.47 -\nCRMH [11] 34.74 48.37 59.11 55.47 54.83 0.781\nSMAP [48] 31.58 40.29 47.35 41.65 41.55 -\nROMP [34] 30.08 48.41 51.12 55.34 54.81 0.866\nBEV w/o WST 34.27 50.81 54.34 57.43 57.17 0.850\nBEV w/o Ldepth 43.61 51.55 50.88 57.27 55.97 0.794\nBEV w/o Lage 49.09 56.55 60.92 62.47 61.47 0.810\nBEV 60.77 67.09 66.07 69.71 68.27 0.884\nTable 1. Accuracy of relative depth relations (PCDR0.2) and projected\n2D poses (mPCK0.6\nh ) on RH. †uses the ground truth bounding boxes.\nMethod Haggl. Maﬁa Ultim. Pizza Mean\nZanﬁr et. al. [43] 141.4 152.3 145.0 162.5 150.3\nMSC [42] 140.0 165.9 150.7 156.0 153.4\nCRMH [11] 129.6 133.5 153.0 156.7 143.2\nROMP [34] 110.8 122.8 141.6 137.6 128.2\n3DCrowdNet [6] 109.6 135.9 129.8 135.6 127.3\nBEV 90.7 103.7 113.1 125.2 109.5\nTable 2. Comparisons to the state-of-the-art methods\non CMU Panoptic in MPJPE. Results are obtained from\nthe original papers.\nMethod\nKid subset Full set\nDetection↑ Matched↓ All↓ Detection↑ Matched↓ All↓\nF1 score Precision Recall MVE MPJPE NMVE NMJE F1 score Precision Recall MVE MPJPE NMVE NMJE\nPARE [17] 0.55 0.44 0.74 186.4 193.9 338.9 352.5 0.84 0.96 0.75 140.9 146.2 167.7 174.0\nSPIN [28] 0.31 0.21 0.60 186.7 191.7 602.3 618.4 0.77 0.91 0.67 148.9 153.4 193.4 199.2\nSPEC [18] 0.52 0.40 0.73 163.2 171.0 313.8 328.8 0.84 0.96 0.74 106.5 112.3 126.8 133.7\nROMP [34] 0.50 0.37 0.80 156.6 159.8 313.2 319.6 0.91 0.95 0.88 103.4 108.1 113.6 118.8\nBEV w/o WST 0.58 0.44 0.86 146.0 148.3 251.7 255.7 0.93 0.96 0.90 105.6 109.7 113.5 118.0\nBEV 0.55 0.41 0.85 125.9 129.1 228.9 234.7 0.93 0.96 0.90 100.7 105.3 108.3 113.2\nTable 3. Comparison of SOTA methods on AGORA test set. All methods are ﬁne-tuned on the AGORA training set or synthetic data [18]\ngenerated in the same way as AGORA. We ﬁne-tune ROMP [34] using the public implementation; results from the AGORA leaderboard.\nport Precision, Recall, and F1 score. For matched detec-\ntions, we report the Mean Per Joint Position Error (MPJPE)\nand Mean Vertex Error (MVE). To punish misses and false\nalarms in detection, we normalize the MPJPE and MVE by\nF1 score to get Normalized Mean Joint Error ( NMJE) and\nNormalized Mean Vertex Error (NMVE).\n4.2. Comparisons to the state-of-the-art methods\nMonocular depth reasoning.We ﬁrst evaluate BEV on\nmonocular depth reasoning in Tab. 1 using the RH dataset.\nResults in Tab. 1 are obtained using the ofﬁcial implemen-\ntations of compared methods. BEV uses the same train-\ning samples as [34] to perform WST. We ﬁrst compare\nwith the most competitive methods [11,25,48], which solve\ndepth relations in monocular images. We also compare\nwith ROMP [34], for one-stage multi-person mesh recov-\nery. Their 3D translation results are obtained by solving the\nPnP algorithm (RANSAC [8]) between their 3D pose and\nprojected 2D pose predictions. As shown in Tab. 1, BEV\noutperforms all these methods in the accuracy of both depth\nreasoning and projected 2D poses by a large margin.\nMonocular detection and mesh regression.We also\nrun BEV on AGORA and CMU Panpotic to evaluate the de-\ntection and 3D mesh accuracy. We compare with the state-\nof-the-art (SOTA) multi-stage methods [6, 11, 17, 18, 28, 42,\n43] and the one-stage ROMP [34]. Beneﬁting from the su-\nperiority in recall, in Tab. 3, BEV outperforms SOTA meth-\nods on detection by 5.2% and 2.2% in terms of F1 score\non the kid and full subset, respectively. This is evidence\nthat the 3D representation helps alleviate depth ambiguity\nin crowded scenes. On the kid subset, BEV signiﬁcantly\noutperforms previous methods in terms of mesh reconstruc-\ntion. Especially, compared with ROMP [34], BEV reduces\nerrors over 19.6% and 26.9% in terms of matched MVE and\nall NMVE on AGORA kids, indicating that BEV effectively\nreduces the age bias using WST. Also, as shown in Tab. 2,\non CMU Panpotic, BEV signiﬁcantly reduces 3D pose er-\nrors by 13.9% compared to multi-person SOTA methods.\nFor qualitative results, see Fig. 1 and Fig. 5.\n4.3. Ablation Studies\nBird’s-eye-view representation & BEV w/o WST.To\nfurther test the effectiveness of BEV’s 3D representation,\nwe train it without performing WST on RH and compare it\nwith SOTA methods on AGORA and RH. On RH in Tab. 1,\ncompared with CRMH [11], the depth reasoning accuracy\nof BEV w/o WST is 4.1% higher ( PCDR0.2 of all). BEV\nw/o WST outperforms the 2D representation-based network\nROMP [34]. These results point to the effectiveness of our\n3D representation for dealing with monocular depth ambi-\nguity. On AGORA, as shown in Tab. 3, BEV w/o WST sig-\nniﬁcantly outperforms ROMP in all detection metrics. Ad-\nditionally, the strong detection ability of the 3D represen-\ntation makes BEV w/o WST outperform the SOTA meth-\nods [18, 28, 34] in terms of NMVE and NMJE.\nWeakly supervised training (WST) losses, Ldepth\nand Lage. Results in Tab. 1 show that performing WST\nsigniﬁcantly improves the accuracy of depth reasoning, es-\nFigure 5. Qualitative results on AGORA, RH, and Internet images [1]. Note how children and adults are properly placed in depth.\nMethod Relative Human AGORA\nPCDR0.2 mPCK0.6\nh F1 NMVE NMJE\nBEV 68.27 0.884 0.93 108.3 113.2\nw/o FVC 67.99 0.880 0.89 118.9 123.0\nw/o OM 60.76 0.620 0.87 126.6 130.7\nTable 4. Ablation study of front-view condition (FVC) and 3D\nOffset map (OM) on RH and AGORA.\nMethod Dist.↓ X↓ Y↓ Depth↓\nOrdinal loss [38] 0.608 0.153 0.184 0.509\nPiece-wise Ldepth (ours) 0.518 0.128 0.166 0.423\nTable 5. 3D translation error on AGORA validation set with dif-\nferent depth losses.\npecially for the young groups. Also, Tab. 1 shows that sep-\narately using Ldepth or Lage make BEV produce better\ndepth reasoning than BEV w/o WST, and, when using both\nterms, BEV performs best.\n3D Offset map (OM) and Front-view condition (FVC)\nfor 3D localization. FVC is taking the front-view 2D body-\ncentered heatmap as a robust attention signal to explore the\ndepth of detected persons during bird’s-eye view estimation.\nResults in Tab. 4 verify that OM and FVC signiﬁcantly im-\nprove the granularity of 3D localization.\nPiece-wise depth layer lossLdepth v.s. ordinal depth\nloss [38]. Unlike an ordinal depth loss, Ldepth keeps the\npenalty within a reasonable range (see Sec. 3.6). As shown\nin Tab. 5, on AGORA validation set, training with Ldepth\nreduces the 3D translation error, especially in depth.\n5. Conclusion, Limitations, Ethics, Risks\nIn this paper, we introduce BEV , a uniﬁed one-stage\nmethod for monocular regression and depth reasoning of\nmultiple 3D people. By introducing a novel bird’s eye view\nrepresentation, we enable powerful 3D reasoning that re-\nduces the monocular depth ambiguity. Exploiting the cor-\nrelation between body height and depth, BEV learns depth\nreasoning from complex in-the-wild scenes by exploiting\nrelative depth relations and age group classiﬁcation. We\nmake available an in-the-wild dataset to promote the train-\ning and evaluation of monocular depth reasoning in the\nwild. The ablation studies point to the value of the 3D rep-\nresentation and the ﬁne-grained localization in the network,\nthe importance of our training scheme, and the value of the\ncollected dataset. BEV is a preliminary attempt to explore\ncomplex multi-person relationships in the 3D world, and\nwe hope the framework will serve as a simple yet effective\nfoundation for future progress.\nLimitations. While BEV goes beyond current methods\nto cover more diverse ages, it is not trained to capture di-\nverse weights, gender, ethnicity, etc. BEV also assumes a\nconstant focal length. Our labeling approach, however, sug-\ngests that weak labels can produce strong results; i.e. im-\nproved metric accuracy. Note that BEV is not trained or\ndesigned to deal with large “crowds” (e.g. 100’s of people).\nEthics and data.We collected RH images from a free\nphoto website [1] under a Creative Commons license that\nenables sharing. We strove to have a dataset that is diverse\nin age, ethnicity, and gender. Also, our weak annotations\ndo not contain any personal information and the annotators,\nthemselves, are anonymous and were not studied.\nPotential Negative Societal Impacts. Methods for\nmonocular 3D pose and shape estimation might be used\nfor automated surveillance, tracking, and behavior analysis,\nwhich may violate people’s privacy. To help prevent this,\nBEV is released for research only.\nAcknowledgements: This work was supported by the\nNational Key R&D Program of China under Grand No.\n2020AAA0103800.\nDisclosure: MJB has received research funds from Adobe,\nIntel, Nvidia, Facebook, and Amazon and has ﬁnancial\ninterests in Amazon, Datagen Technologies, and Meshca-\npade GmbH. While he was part-time at Amazon during this\nproject, his research was performed solely at Max Planck.\nReferences\n[1] Pexels. https://www.pexels.com. 5, 8\n[2] Vitor Albiero, Xingyu Chen, Xi Yin, Guan Pang, and Tal\nHassner. img2pose: Face alignment and detection via 6dof,\nface pose estimation. In CVPR, pages 7617–7627, 2021. 3\n[3] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\nBernt Schiele. 2D human pose estimation: New benchmark\nand state of the art analysis. In CVPR, pages 3686–3693,\n2014. 6\n[4] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter\nGehler, Javier Romero, and Michael J Black. Keep it SMPL:\nAutomatic estimation of 3D human pose and shape from a\nsingle image. In ECCV, pages 561–578, 2016. 2, 3, 6\n[5] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-\nimage depth perception in the wild. In NeurIPS, pages 730–\n738, 2016. 5, 6\n[6] Hongsuk Choi, Gyeongsik Moon, JoonKyu Park, and Ky-\noung Mu Lee. Learning to estimate robust 3d human mesh\nfrom in-the-wild crowded scenes. In CVPR, 2022. 3, 7\n[7] Sai Kumar Dwivedi, Nikos Athanasiou, Muhammed Ko-\ncabas, and Michael J. Black. Learning to regress bodies from\nimages using differentiable semantic rendering. In ICCV,\npages 11250–11259, 2021. 3\n[8] Martin A Fischler and Robert C Bolles. Random sample\nconsensus: a paradigm for model ﬁtting with applications to\nimage analysis and automated cartography.Communications\nof the ACM, 24(6):381–395, 1981. 7\n[9] Nikolas Hesse, Sergi Pujades, Javier Romero, Michael J\nBlack, Christoph Bodensteiner, Michael Arens, Ulrich G\nHofmann, Uta Tacke, Mijna Hadders-Algra, Raphael Wein-\nberger, et al. Learning an infant body model from rgb-d data\nfor accurate full body motion analysis. In MICCAI, pages\n792–800, 2018. 3\n[10] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6M: Large scale datasets and predic-\ntive methods for 3D human sensing in natural environments.\nTPAMI, 36(7):1325–1339, 2013. 6\n[11] Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, Xiaowei\nZhou, and Kostas Daniilidis. Coherent reconstruction of\nmultiple humans from a single image. InCVPR, pages 5579–\n5588, 2020. 2, 3, 6, 7\n[12] Sam Johnson and Mark Everingham. Learning effective hu-\nman pose estimation from inaccurate annotation. In CVPR,\npages 1465–1472, 2011. 6\n[13] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,\nIain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser\nSheikh. Panoptic Studio: A massively multiview system for\nsocial motion capture. In ICCV, pages 3334–3342, 2015. 3,\n6\n[14] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Ex-\nemplar ﬁne-tuning for 3D human pose ﬁtting towards in-the-\nwild 3D human pose estimation. In ECCV, 2020. 6\n[15] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and\nJitendra Malik. End-to-end recovery of human shape and\npose. In CVPR, pages 7122–7131, 2018. 2, 3, 4, 6\n[16] Muhammed Kocabas, Nikos Athanasiou, and Michael J\nBlack. VIBE: Video inference for human body pose and\nshape estimation. In CVPR, pages 5253–5263, 2020. 2, 3\n[17] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges,\nand Michael J Black. PARE: Part attention regressor for\n3d human body estimation. In ICCV, pages 11127–11137,\n2021. 7\n[18] Muhammed Kocabas, Chun-Hao P. Huang, Joachim Tesch,\nLea M ¨uller, Otmar Hilliges, and Michael J. Black. SPEC:\nSeeing people in the wild with an estimated camera. In\nICCV, pages 11035–11045, 2021. 2, 6, 7\n[19] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and\nKostas Daniilidis. Learning to reconstruct 3D human pose\nand shape via model-ﬁtting in the loop. In ICCV, pages\n2252–2261, 2019. 2, 3, 6\n[20] Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu\nFang, and Cewu Lu. CrowdPose: Efﬁcient crowded scenes\npose estimation and a new benchmark. In CVPR, pages\n10863–10872, 2019. 5, 6\n[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, pages 740–755, 2014. 5, 6\n[22] Wu Liu, Qian Bao, Yu Sun, and Tao Mei. Recent advances in\nmonocular 2d and 3d human pose estimation: A deep learn-\ning perspective. ACM Computing Surveys, 2022. 2\n[23] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J. Black. SMPL: A skinned multi-\nperson linear model. TOG, 34(6):1–16, 2015. 2, 3, 4, 6\n[24] Dushyant Mehta, Oleksandr Sotnychenko, Franziska\nMueller, Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll,\nand Christian Theobalt. Single-shot multi-person 3d pose\nestimation from monocular rgb. In 3DV, pages 120–130,\n2018. 3, 6\n[25] Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee.\nCamera distance-aware top-down approach for 3D multi-\nperson pose estimation from a single RGB image. In CVPR,\npages 10133–10142, 2019. 2, 3, 6, 7\n[26] Gyeongsik Moon and Kyoung Mu Lee. Pose2Pose: 3d posi-\ntional pose-guided 3d rotational pose prediction for expres-\nsive 3d human pose and mesh estimation. arXiv, 2020. 2,\n3\n[27] Lea Muller, Ahmed AA Osman, Siyu Tang, Chun-Hao P\nHuang, and Michael J Black. On self-contact and human\npose. In CVPR, pages 9990–9999, 2021. 3\n[28] Priyanka Patel, Chun-Hao P Huang, Joachim Tesch,\nDavid T Hoffmann, Shashank Tripathi, and Michael J Black.\nAGORA: Avatars in geography optimized for regression\nanalysis. In CVPR, pages 13468–13478, 2021. 3, 4, 6, 7\n[29] Georgios Pavlakos, Nikos Kolotouros, and Kostas Daniilidis.\nTexturePose: Supervising human mesh estimation with tex-\nture consistency. In ICCV, pages 803–812, 2019. 2, 3\n[30] Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis.\nOrdinal depth supervision for 3d human pose estimation. In\nCVPR, pages 7307–7316, 2018. 6\n[31] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas\nDaniilidis. Learning to estimate 3d human pose and shape\nfrom a single color image. In CVPR, pages 459–468, 2018.\n3\n[32] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang,\nSrinath Sridhar, and Leonidas J. Guibas. HuMoR: 3d human\nmotion model for robust pose estimation. In ICCV, pages\n11488–11499, 2021. 3\n[33] Yu Rong, Ziwei Liu, Cheng Li, Kaidi Cao, and Chen Change\nLoy. Delving deep into hybrid annotations for 3d human\nrecovery in the wild. In ICCV, pages 5340–5348, 2019. 3\n[34] Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J Black, and\nTao Mei. Monocular, one-stage, regression of multiple 3d\npeople. In ICCV, pages 11179–11188, 2021. 2, 3, 4, 5, 6, 7\n[35] Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, YiLi Fu, and Tao\nMei. Human mesh recovery from monocular images via a\nskeleton-disentangled representation. In ICCV, pages 5348–\n5357, 2019. 2, 6\n[36] Nicolas Ugrinovic, Adria Ruiz, Antonio Agudo, Alberto\nSanfeliu, and Francesc Moreno-Noguer. Body size and depth\ndisambiguation in multi-person reconstruction from single\nimages. In 3DV, pages 53–63, 2021. 3\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, pages\n5998–6008, 2017. 5\n[38] Can Wang, Jiefeng Li, Wentao Liu, Chen Qian, and Cewu\nLu. HMOR: Hierarchical multi-person ordinal relations for\nmonocular multi-person 3d pose estimation. InECCV, pages\n242–259, 2020. 2, 3, 8\n[39] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J\nBlack. ICON: Implicit Clothed humans Obtained from Nor-\nmals. In CVPR, 2022. 3\n[40] Hongwei Yi, Chun-Hao P. Huang, Dimitrios Tzionas,\nMuhammed Kocabas, Mohamed Hassan, Siyu Tang, Justus\nThies, and Michael J. Black. Human-aware object placement\nfor visual environment reconstruction. In CVPR, 2022. 3\n[41] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and\nJan Kautz. GLAMR: Global occlusion-aware human mesh\nrecovery with dynamic cameras. In CVPR, 2022. 3\n[42] Andrei Zanﬁr, Elisabeta Marinoiu, and Cristian Sminchis-\nescu. Monocular 3D pose and shape estimation of multi-\nple people in natural scenes-the importance of multiple scene\nconstraints. In CVPR, pages 2148–2157, 2018. 3, 7\n[43] Andrei Zanﬁr, Elisabeta Marinoiu, Mihai Zanﬁr, Alin-Ionut\nPopa, and Cristian Sminchisescu. Deep network for the in-\ntegrated 3D sensing of multiple people in natural images. In\nNeurIPS, pages 8410–8419, 2018. 3, 7\n[44] Wang Zeng, Wanli Ouyang, Ping Luo, Wentao Liu, and Xi-\naogang Wang. 3d human mesh regression with dense corre-\nspondence. In CVPR, pages 7054–7063, 2020. 2\n[45] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,\nYebin Liu, Limin Wang, and Zhenan Sun. PyMAF: 3d hu-\nman pose and shape regression with pyramidal mesh align-\nment feedback loop. In ICCV, pages 11446–11456, 2021.\n2\n[46] Song-Hai Zhang, Ruilong Li, Xin Dong, Paul Rosin, Zixi\nCai, Xi Han, Dingcheng Yang, Haozhi Huang, and Shi-Min\nHu. Pose2Seg: Detection free human instance segmentation.\nIn CVPR, pages 889–898, 2019. 5, 6\n[47] Yuxiang Zhang, Zhe Li, Liang An, Mengcheng Li, Tao Yu,\nand Yebin Liu. Lightweight multi-person total motion cap-\nture using sparse multi-view cameras. InCVPR, pages 5560–\n5569, 2021. 2\n[48] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang,\nHujun Bao, and Xiaowei Zhou. SMAP: Single-shot multi-\nperson absolute 3D pose estimation. In ECCV, pages 550–\n566, 2020. 2, 3, 6, 7\n[49] Xingyi Zhou, Arjun Karpur, Chuang Gan, Linjie Luo, and\nQixing Huang. Unsupervised domain adaptation for 3d key-\npoint estimation via view consistency. In ECCV, pages 137–\n153, 2018. 2\n[50] Yi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, and Li\nHao. On the continuity of rotation representations in neural\nnetworks. In CVPR, pages 5745–5753, 2019. 4\nPutting People in their Place: Monocular Regression of 3D People in Depth\n**Supplementary Material**\nFigure 1. More qualitative results on Internet images [1].\n1. Introduction\nIn this material, we provide more implementation de-\ntails, analysis of the “ Relative Human” (RH) dataset, and\nquantitative/qualitative comparisons to the state-of-the-art\nmethods. Additionally, we present more visual results, like\nFig. 1, to show the performance of BEV under different sit-\nuations and to explore its failure modes.\n2. Implementation Details\nIn this section, we introduce the details of our camera\nrepresentation, network architecture, and training details.\n2.1. Normalized Camera Representation\nTo supervise 3D joints ⃗J with 2D poses, existing meth-\nods [13,30] widely adopt a weak-perspective camera model\narXiv:2112.08274v3  [cs.CV]  20 Apr 2022\nFigure 2. Pre-deﬁned 3D camera anchor maps.\nto project ⃗J onto the image plane. For better depth reason-\ning, we employ a perspective camera model to perform this\n2D projection.\nIn most cases, accurate camera parameters for in-the-\nwild images are unavailable. In this situation, to avoid re-\nliance on the camera parameters of 2D projection, we as-\nsume that the input image is captured with a standard cam-\nera without radial distortion. Then we can assign static val-\nues for the ﬁeld of view (FOV) and image size ⃗W of this\nstandard camera. The focal length ⃗f = (fx, fy) can be\ndeﬁned as ⃗W/(2tan(FOV/ 2)). Given the 3D translation\n(xi, yi, di) of i-th subject and the focal length, the 2D pro-\njection ( ⃗ ui, ⃗ vi) of 3D joints ( ⃗Jx\ni , ⃗Jy\ni , ⃗Jd\ni ) is deﬁned as\n⃗ ui = fx( ⃗Jx\ni + xi)\n⃗Jd\ni + di\n, ⃗ vi = fy( ⃗Jy\ni + yi)\n⃗Jd\ni + di\n. (1)\nIn cases where the camera parameters are provided, we\ncan convert the 3D translation estimated in our standard\ncamera space to the given one. With K pairs of estimated\n3D joints ⃗J and their 2D projection (obtained via Eq. 1), we\ncan solve the 3D translation at a speciﬁc camera space via a\nPnP algorithm (e.g. RANSAC [7]).\nHowever, in the image, 3D translation is not as intu-\nitive as the person’s scale used by weak-perspective meth-\nods. For instance, a small 2D scale change in an image\nmay correspond to a large difference in 3D translation in\ncamera space, especially for people who are far away in\ndepth. Therefore, to alleviate this difference, we convert the\n3D translation (xi, yi, di) to a normalized scale-based for-\nmat (si, ty\ni, tx\ni) via a scale factor si = (ditan(FOV/ 2))−1,\nwhere ty\ni = yisi, tx\ni = xisi. The normalized representation\nis proportional to the person’s scale. When FOV=60 ◦, the\nsensitive partsi ∈(0, 2) corresponds to di ∈(0.86, +∞) in\nmeters, which is more suitable for the network to estimate.\nAdditionally, we observe that people in the depth range\n(1m,10m) show more abundant and stable information in\npose, shape, and depth, which deserve more attention. Ad-\nditionally, most of our training samples are within this depth\nrange. As we introduced in the main paper, 3D camera\nanchor maps deﬁne the way we voxelize the 3D camera\nspace. Therefore, we adjust the occupancy ratio of different\ndepths in the channel number of 3D camera anchor maps.\nAs shown in Fig. 2, we ﬁrst split the camera space into 4\nregions in depth and then evenly put the different number\n(shown in the table) of 3D camera anchor maps inside each\nregion. For instance, we put 25/32 3D camera anchor maps\ninside the depth range (1m,10m); this gives more attention\nto this critical depth range. Each anchor map contains the\nnormalized camera values (si, ty\ni, tx\ni) at the corresponding\nposition.\n2.2. Network Architecture\nWe develop a bird’s-eye-view-based coarse-to-ﬁne local-\nization pipeline to estimate the 3D translation of all people\nin the scene in one shot. In Fig. 3, we present the net-\nwork architecture of estimating ﬁve 2D maps and two 3D\nmaps, which are used to generate the ﬁnal results as shown\nin Fig. 2 of the main paper. The input size ⃗W is (512, 512).\nFollowing ROMP [30], we adopt a multi-head architecture\nand use HRNet-32 [3] as backbone. With backbone feature\nmaps of size R32×H×W, we employ three head branches to\nestimate four front-/bird’s-eye-view 2D maps and a Mesh\nFeature map.\nAs illustrated in Fig. 4, our key design is to convert the\nfront-view features to a bird’s eye view via explicit opera-\ntions including height-wise suppression and depth-wise ex-\nploration. As shown in the middle branch of Fig. 3, we\nﬁrst explore the depth information of backbone features via\na Bottleneck block. And then we concatenate the explored\ndepth features and front-view 2D maps as input to the BVH\nbranch. As shown in Fig. 4, we compress the 2D feature\nmaps in height to obtain 1D feature vectors. In the BVH\nbranch (Fig. 3), we employ six 1D convolution blocks to\nexplicitly explore features in depth. Two bird’s-eye-view\nmaps are of size R1×D×W.\nNext, we compose the front-view and bird’s-eye-view\nmaps to generate 3D maps. We extend the front-view maps\nwith an additional depth dimension and repeat D times.\nWe also extend the bird’s-eye-view maps with an additional\nheight dimension and repeat H times. To obtain the 3D\nCenter map, we multiply the bird’s-eye-view Body Center\nheatmap to the front-view one and reﬁne it with a 3D re-\nﬁner (Fig. 3). Then we add the bird’s-eye-view offset map\nto the last channel of the front-view one to reﬁne the depth.\nTo obtain the 3D Offset map, we further use a 3D reﬁner to\nreﬁne the composed 3D maps, which improves the consis-\ntency between features of two views.\n2.3. Datasets\nIn this section, we introduce the datasets we used during\ntraining and evaluation.\nAGORA [27]is a synthetic dataset with accurate annota-\nFigure 3. Network architecture.\nFigure 4. Operations to convert the front-view features to a bird’s\neye view (shown in 3D camera space represented by 3D camera\nanchor maps).\ntions of body meshes and 3D translations, with 4,240 high-\nrealism textured scans in diverse poses and clothes. Impor-\ntantly, it contains 257 child scans. It contains 14K train-\ning and 3K test images. Each image has 5-15 people with\nfrequent occlusions. AGORA-PC [27] is a high occlusion\nsubset of the AGORA validation set. Each image has over\n70% occlusion. We use it to evaluate the performance under\nsevere occlusion. Note that there are no child samples in the\nvalidation set.\nHuman3.6M [8] is a single-person 3D pose dataset. It\ncontains videos of 9 professional actors performing activ-\nities in 17 scenarios. It provides 3D pose annotations for\neach frame. We sample every 5 frames to reduce redun-\ndancy. We use its training set for training.\nMuCo-3DHP [23] is a synthetic multi-person 3D pose\ndataset. It is built on the single-person 3D pose dataset,\nMPI-INF-3DHP [23]. They use segmentation annotations\nto blend multiple single-person images into one. For a fair\ncomparison with 3DMPPE [25], we use the same synthetic\nversion for training.\nOther 2D pose datasets. For better generalization,\nwe also use four 2D pose datasets for training, including\nCOCO [21], MPII [2], LSP [10], and CrowdPose [18]. Be-\nsides, we also use the pseudo-3D annotations [12] for train-\ning.\n2.4. Training Details\nThe size of output maps are H = W = 128 , and\nD = 64. The threshold for the age offset is set to tα = 0.8.\nThe FOV is set to 60◦. The loss weights are wmpj = 200,\nwpmpj = 360 , wpj2d = 400 , wθ = 80 , wβ = 60 ,\nwprior = 1.6, wcm = 100, wcm3d = 1000, wage = 4000,\nand wdepth = 400. We train BEV on a server with four\nTesla V100 GPUs. The batch size is 64. The learning\nrate is 5 e−5. The conﬁdence threshold of the Body Cen-\nter heatmap is 0.12.\nAdditionally, although we strive to alleviate the age bias\nin training samples, the age bias in existing 3D pose datasets\nis severe, and we have to use them to obtain good 3D pose\nestimation. To handle the imbalanced distribution of the\ntraining sample space, we balance the sampling ratio of dif-\nferent datasets and evenly select the training samples from\ndifferent age groups on RH. The sampling ratios of differ-\nent datasets are 16% AGORA, 16% MuCo-3DHP, 16% RH,\n18% Human3.6M, 14% COCO, 8% CrowdPose, 6% MPII,\nand 6% LSP.\nAlso, we adopt a two-step training strategy. We ﬁrst\nlearn monocular 3D pose and shape estimation for 120\nepochs on basic training datasets. Then we add the weak\nannotations of RH to training samples and train for 120\nepochs. If we need to ﬁne-tune on AGORA, we add\nAGORA to the training sequence and train for 80 epochs.\nIn this process, the validation set of the RH is used to select\ncheckpoints with good performance.\n2.5. Processing High-resolution Images\nAs a one-stage method, BEV takes an image of con-\nstant size as input. However, to process the high-resolution\nimages, directly resizing them to a constant size would\nsacriﬁce the performance. Therefore, we develop a slid-\ning window-based pipeline to achieve promising results on\nhigh-resolution images, as shown in Fig. 1 of the main pa-\nper. In detail, we evenly split the image into multiple grids\nand then apply BEV on each grid. This process is similar\nto the sliding window operation of 2D convolution. At each\ngrid, we only take the result whose body center falls in the\ncenter area of the grid. Then we perform non-maximum\nsuppression on the edge between grids to get rid of redun-\ndant predictions. In this process, overlapping predictions\nwith lower center conﬁdence values will be deleted.\n3. Relative Human Dataset\nIn this section, we provide more detailed analyses of our\nRelative Human dataset.\nIn total, we collect about 7,689 images with weak anno-\ntations of 24,814 people. We split them into three groups\n(5218, 635, 1836) for training, validation, and test respec-\ntively. Among these images, about 1,000 images are col-\nlected from a free photo website [1] and we annotate the 2D\nposes deﬁned as Fig. 5. Note that compared with LSP’s 14\nkeypoints, we add keypoints on the face and feet to repre-\nsent their orientations. The remaining images are selected\nfrom existing 2D pose datasets [18,21,33]. We correct some\nerroneous 2D poses from the existing 2D pose dataset and\nadd the missing detections. Note that a large number of im-\nages in CrodPose [18] and OCHuman [33] are selected from\nCOCO [21] and MPII [2], which are also used as training\nsamples by our compared methods [9,15,16,30]. Therefore,\nwe use these common images for training.\nWe classify all people in the image into four age groups,\nbaby, child, teenager, and adult according to the follow-\ning age ranges: baby (0-3), kid (3-8), teenager (8-16), and\nadult (16+). As shown in Tab. 1, we provide the num-\nber of subjects in the four age groups and their propor-\ntions. Compared with the existing multi-person 3D pose\ndatasets [23, 27, 31], RH contains richer subjects and more\nocclusion cases. Therefore, RH is more general and suitable\nfor evaluating depth reasoning in the wild.\nThe consistency of weak annotations. During the col-\nlection of weak annotations, we observe that people’s judg-\nments for such weak labels vary greatly. It is hard to ob-\ntain consistent weak labels through online platforms (e.g.\nAMT). Therefore, ofﬂine, we organized a group of labelers\nand trained them with uniﬁed standards. To test how well\nthey learn the standards, we prepare some pre-labeled data\nas test samples. Ones who pass the test after training were\nemployed for ofﬁcial labeling. In addition, the annotations\nRH splits Babies Children Teenagers Adults\nAll 1534 / 6% 2720 / 10% 1067 / 4% 19493 / 78%\nTrain 942 / 5% 1795 / 10% 690 / 4% 13478 / 79%\nValidation 117 / 5% 209 / 9% 101 / 4% 1680 / 79%\nTest 475 / 8% 716 / 12% 276 / 4% 4335 / 74%\nExisting multi-person 3D pose datasets\nMuPoTS [23] - - - 8 / 100%\n3DPW [31] - - - 18 / 100%\nAGORA [27] - 257 / 6% - 3983 / 94%\nTable 1. Subject number/proportions of four age groups on Rela-\ntive Human (RH) and 3D pose benchmarks.\nFigure 5. The 2D skeleton deﬁnition.\nare double-checked by professional testers and the author.\n4. Discussion\nWhy not estimate the 3D heatmap directly? The main\nchallenge is the lack of sufﬁcient multi-person data with ac-\ncurate 3D translation annotations for supervision, especially\nfor in-the-wild cases. Due to the data lack problem, directly\nlearning 3D heatmap performs poorly. It is hard to effec-\ntively supervise multi-person 3D heatmaps with 2D annota-\ntions. In contrast, our separable representation disentangles\nthe 3D heatmap into the front-view and the bird’s-eye-view\nmaps. In this way, our model can learn robust front-view\nlocalization from abundant 2D in-the-wild datasets. With\nrobust front-view attention, the model can focus on learn-\ning depth reasoning from weak annotations in RH with the\nproposed WST.\nHeatmap reﬁnement and decomposition. Following\nprevious methods [30], we also adopt the powerful heatmap\nrepresentation for detection. While its rough granularity\nlimits its effectiveness in ﬁne localization. Some previous\nmethods explore the reﬁnement and decomposition of the\nheatmap to alleviate this problem. PifPaf [17] estimates off-\nset maps to reﬁne the coarse 2D pose coordinates parsed\nfrom the heatmap. VNect [24] estimates three 2D maps\nInput Method F1 score↑ Precision↑ Recall↑ MVE↓ MPJPE↓ NMVE↓ NMJE↓\nMulti-stage\nPerson crops from 3840x2160\nHMR [13] 0.38 0.27 0.61 209.3 219.4 550.8 577.4\nSMPLify-X‡ [28] 0.57 0.60 0.55 213.3 208.3 374.2 365.4\nEFT [12] 0.43 0.34 0.60 193.5 202.7 450.0 471.4\nSPIN [16] 0.33 0.23 0.61 193.2 203.7 585.5 617.3\nExPose [5] 0.53 0.46 0.61 174.0 176.6 328.3 333.2\nFrankmocap [29] 0.40 0.30 0.62 204.2 203.7 510.5 509.2\nPyMAF [32] 0.27 0.16 0.82 192.0 203.2 711.1 752.6\nPIXIE [6] 0.48 0.39 0.61 174.6 174.7 363.8 364.0\nSPIN⋆ [27] 0.31 0.21 0.60 186.7 191.7 602.3 618.4\nSPEC⋆ [15] 0.52 0.40 0.73 163.2 171.0 313.8 328.8\nPARE [14] 0.55 0.44 0.74 186.4 193.9 338.9 352.5\nPose2Pose⋆† [26] 0.56 0.40 0.91 146.4 153.3 261.4 273.8\nOne-stage\n512x512\nROMP [30] 0.38 0.39 0.37 198.5 207.4 522.4 545.8\nBEV w/o WST 0.41 0.39 0.45 194.4 202.6 474.1 494.1\nROMP⋆ [30] 0.50 0.37 0.80 156.6 159.8 313.2 319.6\nBEV⋆ w/o WST 0.58 0.44 0.86 146.0 148.3 251.7 255.7\nBEV⋆ 0.55 0.41 0.85 125.9 129.1 228.9 234.7\nTable 2. Comparison to existing SOTA methods on the “AGORA kids” test set. Results are obtained from the AGORA leaderboard. ⋆\nis ﬁne-tuning on the AGORA training set or synthetic data [15] generated in the same way as AGORA. ‡ means the optimization-based\nmethod while the rest are learning-based methods. † means the paper is under review.\nInput Method F1 score↑ Precision↑ Recall↑ MVE↓ MPJPE↓ NMVE↓ NMJE↓\nMulti-stage\nPerson crops from 3840x2160\nHMR [13] 0.80 0.93 0.70 173.6 180.5 217.0 226.0\nSMPLify-X ‡ [28] 0.71 0.86 0.60 187.0 182.1 263.3 256.5\nEFT [12] 0.69 0.97 0.54 159.0 165.4 196.3 203.6\nSPIN [16] 0.78 0.91 0.69 168.7 175.1 216.3 223.1\nExPose [5] 0.82 0.96 0.71 151.5 150.4 184.8 183.4\nFrankmocap [29] 0.80 0.93 0.71 204.2 203.7 510.5 509.2\nPyMAF [32] 0.84 0.86 0.82 192.0 203.2 711.1 752.6\nPIXIE [6] 0.82 0.95 0.73 142.2 140.3 173.4 171.1\nSPIN⋆ [27] 0.77 0.91 0.67 168.7 175.1 216.3 223.1\nSPEC⋆ [15] 0.84 0.96 0.74 106.5 112.3 126.8 133.7\nPARE [14] 0.84 0.96 0.75 140.9 146.2 167.7 174.0\nPose2Pose⋆† [26] 0.94 0.94 0.93 84.8 89.8 90.2 95.5\nOne-stage\n512x512\nROMP [30] 0.69 0.97 0.54 161.4 168.1 233.9 242.3\nBEV w/o WST 0.75 0.97 0.61 164.2 169.1 218.9 225.5\nROMP⋆ [30] 0.91 0.95 0.88 103.4 108.1 113.6 118.8\nBEV⋆ w/o WST 0.93 0.96 0.90 105.6 109.7 113.5 118.0\nBEV⋆ 0.93 0.96 0.90 100.7 105.3 108.3 113.2\nTable 3. Comparison to existing SOTA methods on AGORA full test set. Results are obtained from the AGORA leaderboard. ⋆ is ﬁne-\ntuning on the AGORA training set or synthetic data [15] generated in the same way as AGORA. ‡ means the optimization-based method\nwhile the rest are learning-based methods. † means the paper is under review.\ncontaining x/y/z coordinates of the 3D pose at each posi-\ntion. Luvizon et al. [22] employ soft-argmax to decompose\n2D/3D heatmap into 1D for separate supervision, while it\ndoes not deal with multiple overlapping people. Differ-\nent from previous solutions, we propose a novel bird’s-eye-\nview-based representation for multi-person 3D localization.\nAs we introduced above, it disentangles the depth-wise in-\nformation into an individual map for easier learning. We\nalso estimate a 3D offset map to improve the granularity of\n3D localization.\n5. Quantitative and Qualitative Results\nIn this section, we ﬁrst show more comparisons to SOTA\nmethods on AGORA and then provide more qualitative re-\nsults on Internet images, CMU Panpotic [11], AGORA [27],\nand RH.\n5.1. Quantitative Comparisons\nIn Tab. 2 and 3, we show the results of existing SOTA\nmethods on “AGORA kids” and the full test set respec-\ntively. Results in Tab. 2 show that BEV outperforms all\nprevious methods by a large margin in terms of child mesh\nFigure 6. Qualitative results on CMU Panoptic [11] and AGORA [27] datasets.\nFigure 7. Qualitative comparisons to SOTA methods, ROMP [30] and CRMH [9], on RH test set.\nreconstruction. It demonstrates that learning weak annota-\ntions via the proposed weakly-supervised training (WST)\nhelps to alleviate the age bias. Multi-stage methods, like\nPose2Pose [26], beneﬁt from taking high-resolution person\ncrops as input, which helps process the small-scale subjects\nin AGORA. Besides, as a sanity check, we also compare\nwith SOTAs on 3DPW and MuPoTS datasets. While not\ntuned for uncrowded scenes, BEV is on par with the previ-\nous methods on MuPoTs (Tab. 4) and 3DPW (Tab. 5).\nMethod All↑ Matched↑\nCRMH [9] 69.1 72.2\nROMP [30] 69.9 74.6\n3DCrowdNet [4] 72.7 73.3\nBEV 70.2 75.2\nTable 4. Comparisons to the SOTAs on MuPoTS.\nMethod PMPJPE MPJPE MPVE\nHybrIK [19] 48.8 80.0 94.5\nMETRO [20] 47.9 77.1 88.2\nROMP [30] 47.3 76.7 93.4\nBEV 46.9 78.5 92.3\nTable 5. Comparisons to the SOTAs on 3DPW test set.\nMethod F1 score↑ MVE↓ MPJPE↓ NMVE↓ NMJE↓\nROMP [30] 0.695 173.76 170.55 249.96 245.34\nBEV 0.732 169.21 165.27 231.16 225.76\nw/o WST 0.738 171.16 168.12 235.06 230.89\nw/o DC 0.741 170.59 168.12 229.98 225.67\nTable 6. 3D mesh/pose error on AGORA-PC, the high occlusion\n(over 70%) subset of the AGORA validation set (no kids).\n5.2. Ablation Studies\nTo analyse the performance gain of different designs, we\nperform more ablation studies on AGORA−PC, a high oc-\nclusion (over 70%) subset of the AGORA validation set (no\nkids). This has ground truth 3D annotations for detailed\nevaluation while the test set does not. BEV uses the same\ntraining samples as [30]. Comparing BEV and BEV w/o\nWST in Tab. 6 also shows that our gains in high occlusion\nsituations come from the 3D representation.\nBesides, we also evaluate the effectiveness of depth en-\ncoding (DC) for 3D mesh parameter regression. Depth en-\ncoding is developed to transfer people at different depths\nto individual feature spaces. Tab. 6 shows that adding the\ndepth encoding reduces mesh reconstruction error under\nhigh occlusion (over 70%). It demonstrates that achieving\ndepth-aware mesh regression via adding depth encoding is\nbeneﬁcial to alleviating depth ambiguity and improving the\nstability under occlusion.\n5.3. Qualitative Results\nIn Fig. 6, we present more qualitative results on CMU\nPanoptic and AGORA. In Fig. 1, 7, 8, we show the re-\nsults under various crowded scenarios, including queuing,\nstanding side by side, and mixed scenarios. Compared with\nROMP [30] and CRMH [9], BEV performs much better in\ndetection, depth reasoning, and robustness to occlusion, es-\npecially in cases containing children. These results demon-\nstrate the superiority of our 3D representation, WST, and\nperspective camera model. However, we also observe some\nlimitations of BEV from failure cases in Fig. 9. Without\nmodeling the contact between multiple people, BEV may\nmiss obvious contact and cannot avoid mesh intersections.\nBesides, BEV is unable to handle occlusions with few visi-\nble parts and dense small-scale subjects in crowds.\nReferences\n[1] Pexels. https://www.pexels.com.\n[2] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\nBernt Schiele. 2D human pose estimation: New benchmark\nand state of the art analysis. In CVPR, pages 3686–3693,\n2014.\n[3] Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi,\nThomas S Huang, and Lei Zhang. Higherhrnet: Scale-aware\nrepresentation learning for bottom-up human pose estima-\ntion. In CVPR, pages 5386–5395, 2020.\n[4] Hongsuk Choi, Gyeongsik Moon, JoonKyu Park, and Ky-\noung Mu Lee. Learning to estimate robust 3d human mesh\nfrom in-the-wild crowded scenes. In CVPR, 2022.\n[5] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dim-\nitrios Tzionas, and Michael J Black. Monocular expressive\nbody regression through body-driven attention. In ECCV,\npages 20–40, 2020.\n[6] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios\nTzionas, and Michael J Black. Collaborative regression of\nexpressive bodies using moderation. In3DV, pages 792–804,\n2022.\n[7] Martin A Fischler and Robert C Bolles. Random sample\nconsensus: a paradigm for model ﬁtting with applications to\nimage analysis and automated cartography.Communications\nof the ACM, 24(6):381–395, 1981.\n[8] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6M: Large scale datasets and predic-\ntive methods for 3D human sensing in natural environments.\nTPAMI, 36(7):1325–1339, 2013.\n[9] Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, Xiaowei\nZhou, and Kostas Daniilidis. Coherent reconstruction of\nmultiple humans from a single image. InCVPR, pages 5579–\n5588, 2020.\n[10] Sam Johnson and Mark Everingham. Learning effective hu-\nman pose estimation from inaccurate annotation. In CVPR,\npages 1465–1472, 2011.\n[11] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,\nIain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser\nSheikh. Panoptic Studio: A massively multiview system for\nsocial motion capture. In ICCV, pages 3334–3342, 2015.\n[12] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Ex-\nemplar ﬁne-tuning for 3D human pose ﬁtting towards in-the-\nwild 3D human pose estimation. In ECCV, 2020.\n[13] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and\nJitendra Malik. End-to-end recovery of human shape and\npose. In CVPR, pages 7122–7131, 2018.\n[14] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges,\nand Michael J Black. PARE: Part attention regressor for\n3d human body estimation. In ICCV, pages 11127–11137,\n2021.\n[15] Muhammed Kocabas, Chun-Hao P. Huang, Joachim Tesch,\nLea M ¨uller, Otmar Hilliges, and Michael J. Black. SPEC:\nSeeing people in the wild with an estimated camera. In\nICCV, pages 11035–11045, 2021.\n[16] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and\nKostas Daniilidis. Learning to reconstruct 3D human pose\nand shape via model-ﬁtting in the loop. In ICCV, pages\n2252–2261, 2019.\n[17] Sven Kreiss, Lorenzo Bertoni, and Alexandre Alahi. Pifpaf:\nComposite ﬁelds for human pose estimation. InCVPR, pages\n11977–11986, 2019.\n[18] Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu\nFang, and Cewu Lu. CrowdPose: Efﬁcient crowded scenes\npose estimation and a new benchmark. In CVPR, pages\n10863–10872, 2019.\n[19] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang,\nand Cewu Lu. Hybrik: A hybrid analytical-neural inverse\nkinematics solution for 3d human pose and shape estimation.\nIn CVPR, pages 3383–3393, 2021.\n[20] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end hu-\nman pose and mesh reconstruction with transformers. In\nCVPR, pages 1954–1963, 2021.\n[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, pages 740–755, 2014.\n[22] Diogo C Luvizon, David Picard, and Hedi Tabia. 2d/3d pose\nestimation and action recognition using multitask deep learn-\ning. In CVPR, pages 5137–5146, 2018.\n[23] Dushyant Mehta, Oleksandr Sotnychenko, Franziska\nMueller, Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll,\nand Christian Theobalt. Single-shot multi-person 3d pose\nestimation from monocular rgb. In 3DV, pages 120–130,\n2018.\n[24] Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko,\nHelge Rhodin, Mohammad Shaﬁei, Hans-Peter Seidel,\nWeipeng Xu, Dan Casas, and Christian Theobalt. Vnect:\nReal-time 3d human pose estimation with a single rgb cam-\nera. TOG, pages 1–14, 2017.\n[25] Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee.\nCamera distance-aware top-down approach for 3D multi-\nperson pose estimation from a single RGB image. In CVPR,\npages 10133–10142, 2019.\n[26] Gyeongsik Moon and Kyoung Mu Lee. Pose2Pose: 3d posi-\ntional pose-guided 3d rotational pose prediction for expres-\nsive 3d human pose and mesh estimation. arXiv, 2020.\n[27] Priyanka Patel, Chun-Hao P Huang, Joachim Tesch,\nDavid T Hoffmann, Shashank Tripathi, and Michael J Black.\nAGORA: Avatars in geography optimized for regression\nanalysis. In CVPR, pages 13468–13478, 2021.\n[28] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and\nMichael J Black. Expressive body capture: 3d hands, face,\nand body from a single image. In CVPR, pages 10975–\n10985, 2019.\n[29] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmocap:\nA monocular 3d whole-body pose estimation system via re-\ngression and integration. In ICCV, pages 1749–1759, 2021.\n[30] Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J Black, and\nTao Mei. Monocular, one-stage, regression of multiple 3d\npeople. In ICCV, pages 11179–11188, 2021.\n[31] Timo von Marcard, Roberto Henschel, Michael Black, Bodo\nRosenhahn, and Gerard Pons-Moll. Recovering accurate 3D\nhuman pose in the wild using imus and a moving camera. In\nECCV, pages 601–617, 2018.\n[32] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,\nYebin Liu, Limin Wang, and Zhenan Sun. PyMAF: 3d hu-\nman pose and shape regression with pyramidal mesh align-\nment feedback loop. In ICCV, pages 11446–11456, 2021.\n[33] Song-Hai Zhang, Ruilong Li, Xin Dong, Paul Rosin, Zixi\nCai, Xi Han, Dingcheng Yang, Haozhi Huang, and Shi-Min\nHu. Pose2Seg: Detection free human instance segmentation.\nIn CVPR, pages 889–898, 2019.\nFigure 8. Qualitative comparisons to SOTA methods, ROMP [30] and CRMH [9], on Internet images [1].\nFigure 9. Failure cases on Internet images [1]"
  },
  {
    "source": "A Hybrid AES 256 Cryptographic Framework for Securing IoMT Healthcare.pdf",
    "content": "4 \n \nABSTRACT \nThe Internet of Things (IoT) is a network of physical objects that are interconnected through \nsensors and software to exchange data with other devices and systems via the Internet. The Internet \nof Medical Things (IoMT) technologies is one of the IoT branches, where IoMT devices transmit \na vast amount of medical data in real time. Healthcare data is highly susceptible to cyber attackers. \nIn this project, we have designed a hybrid framework combining cryptographic and steganographic \nmethods.  \nCryptography is the technique of transforming plain text or image to ciphertext or cipher image \nusing mathematical techniques. We have implemented Advanced Encryption Standard (AES) 256 \nas cryptographic technique. It  is a secure symmetric encryption algorithm using a sing le 256-bit \nkey to encrypt and decrypt data. Steganography is the process of concealing information in another \nmedium (audio, video, image, etc.). It hides the transmitted data inside a cover object (carrier \nobject). Through the process, a stego-object is created, which carries the hidden information object \nbut appears identical to the cover object. The encoder that we have implemented is a combination \nof AES-256 cryptographic and the Least Significant Bit (LSB) steganographic technique . On the \nencoder side, sensitive medical data is first encrypted with a symmetric key (k) utilizing the AES-\n256 algorithm. This encrypted data is transformed into a bitstream. The bitstream is embedded into \na cover image using the LSB method, resulting in a stego image. The ste go image is then \ntransmitted over the Internet. On the decoder side, the steps are performed in the reverse sequence.  \nThe LSB -embedded data is extracted first. Then the extracted data is decrypted using the \nsymmetric key (k) of AES-256. For performance evaluation of the use case, we have analyzed the \nhistograms of the original image and the stego image. Identical histograms prove that both images \nare alike and undetectable to hackers. \nThe objective of the project is to ensure communication security in IoMT devices through a hybrid \ncryptographic and steganographic technique in medical data transmission. Securing healthcare \ndata will increase the effectiveness of IoMT devices benefiting the patients and developing a smart \nhealthcare system. \n5 \n \n1. INTRODUCTION \nThe Internet of Things (IoT) is a network of physical objects that are embedded with sensors, \nsoftware, and other technologies for the purpose of connecting and exchanging data with other \ndevices and systems over the internet. In, recent years, IoT has increasingly been implemented by \nmany applications affecting our daily life. According to Fabio Duarte, there are over 15 billion \nconnected IoT devices worldwide and the number of IoT devices is expected to double by 2030  \n[1]. These devices are compact, c an communicate in various ways, and have low power \nconsumption which makes them exceptionally efficient in real-life applications.  \nThe Internet of Medical Things (IoMT) is an IoT branch  dedicated to the healthcare industry. \nThe IoMT is the collection of medical devices and applications that connect to healthcare IT \nsystems through online computer networks. Medical devices equipped with Wi -Fi allow the \nmachine-to-machine communication that is the basis of IoMT. These IoMT devices link to clou d \nplatforms, on which captured data can be stored and analyzed. IoMT is also known as healthcare \nIoT. According to market watch, the global IoMT market is poised to value over 187.60 billion by \n2028 end [2]. Beside this, the global I oMT market is projected to grow significantly in terms  of \nrevenue in the near future, as the industry is moving towards the recovery period from the \npandemic. \n1.1 Project Overview \nIoMT is a network that has being used by wireless sensor networks (WSN) and radio \nfrequency identification (RFID) through wireless network and technology to achieve perception \nof information, reliable transmission, and intelligent processing. Hence, ensuring secure data \ntransmission by protecting privacy and safety are essential features of IoMT. This security is \nrelated to tag information (RFID), wireless communication information security, network \ntransmission of information security, privacy, and information processing security. Therefore, it is \nimportant to have a th orough study and research on IoMT device, potential threats, and security \nrequirements. The main purpose of the network security and information protection is to achieve \n6 \n \nconfidentiality and integrity. Security issues are of great importance in enlarging th e scale of \nnetwork and devices. There are some security risks in both consumers and business in IoMT. So, \nsecure data encryption and decryption techniques can be used to reduce security risks. A suitable \nencryption decryption algorithm can play a vital role in reducing the security risks.  \n \nA hybrid encryption and decryption algorithm has been implemented as a use case in this \nproject. Cryptography and steganography are combinedly used as encryption and decryption \ntechnique in this use case . The cryptography converts the plain text into an  incomprehensible \ncipher text whereas steganography conceals the traces of the data. When hackers want to access a \nsystem, they will aim for the weakest point, which is not the  encryption, but the key. Advance \nEncryption Standard (AES)-256 is the most secure encryption technique and till now there is no \nreport of its cracking. But  the key is the weakest point . The key can be secured by using \nsteganography or secure hash algorithm (SHA) which is irreversible.  \n \nThe summary of the contributions that we have achieved are as follows- \n1. A structured systematic review of IoMT device advantages in the healthcare system. \n2. Security requirements that are necessary for IoMT systems as well as different types of \ntechniques to provide secure data collection, transmission, and storage are discussed. \n3. We have designed a hybrid encryption -decryption method combining cryptography and \nsteganography and evaluated the performance of the implemented method. \n \n \n \n \n \n \n \n \n \n \n7 \n \n2. BACKGROUND \nIn the background analysis of this project, we will briefly discuss the context and structure of \nIoMT devices, communication protocols , IoMT related technologies, security requirements and \ntechniques. We will also discuss IoMT device pros cons, and latest IoMT device trends for 2023. \n2.1 Architecture of IoMT \nThe IoMT is an incorporation of medical devices and applications that can connect to \nhealthcare information technology systems using networking technologies. It can reduce \nunnecessary hospital visits and the burden on healthcare systems by connecting patients to their \nphysicians and allowing the transfer of medical data over a secure network.  Each technology has \nits own set of guidelines.  IoMT devices are mainly divided into four layers.  They are the sensor \nlayer, gateway layer, cloud layer, and visualization layer. All layers are illustrated in Figure 1 \nbelow. \n \nFigure 1 IoMT architecture [3] \n \nSensor layer: This layer consists of biometric sensors which are placed in the patient’s body. \nThey transfer data to the gateway layer through wireless protocols like Wi-Fi and Bluetooth. \nGateway layer: Due to the processing and storage limitations of IoMT sensors, the  data is \ntransferred without processing to the second layer, i.e., the gateway layer. In this layer, data can \n\n8 \n \nbe stored for a short time and perform some preprocessing. These devices can be patients’ \nsmartphones, access points, or home monitoring systems. \nCloud layer: The cloud layer is responsible for getting the data from the gateway for storage, \nanalysis, and secure access. The analysis includes data processing and finding any changes in the \npatient’s health. Then it is presented to the physicians or patients for further action.  The access to \nthe sensors can be remotely managed and controlled from this layer. \nVisualization layer: In this layer, the data are presented to the physicians and the patients \nto track their health. This layer also includes the actions recommended by the physician based on \nthe patient’s health conditions. \n2.2 IoMT Communication Protocols \nCommunication protocols play a crucial role on the IoMT architecture as they define how \ndevices and systems communicate with each other. In IoMT, communication protocols enable the \nseamless transfer of medical data between various devices, sensors, gateway s, and cloud -based \nplatforms, ensuring secure and reliable data transmission [13-15]. \n \nOne of the most widely used communication protocols in IoMT is the Bluetooth Low Energy \n(BLE) protocol, which is often used in wearable medical devices such as fitness trackers and blood \nglucose monitors. BLE provides low power consumption, reliable data transmission, and low \nlatency, making it ideal for medical devices that require continuous monitoring. \n \nAnother important communication protocol used in IoMT is the Zigbee  protocol, which is \nused in home healthcare systems and hospital environments. Zigbee is a low-power, wireless mesh \nnetwork protocol that allows devices to communicate with each other through a centralized hub or \ngateway. Zigbee provides reliable and secur e communication between devices, ensuring data \nprivacy and confidentiality. \n \nThe Hypertext Transfer Protocol (HTTP) is another widely used communication protocol in \nIoMT, which is used for web -based communication between medical devices and cloud -based \nservers. HTTP enables secure communication and is used for transferring patient data, images, and \nmedical records securely and efficiently. \n9 \n \nThe Health Level 7 (HL7) protocol is another communication standard that is widely used \nin healthcare settings. HL7 ena bles the exchange of healthcare information between different \nhealthcare systems and devices. It provides a framework for standardizing medical data exchange, \nensuring interoperability between different medical systems. \n \nIn summary, communication protocols are an essential component of the IoMT architecture, \nenabling secure and reliable communication between different devices, systems, and platforms. \nThe choice of protocol depends on the application requirements, device type, and data transfer \nneeds, and un derstanding the various protocols used in IoMT is crucial for developing and \ndeploying effective and efficient IoMT systems. \n2.3 IoMT Related Technologies \nWireless Sensor Networks (WSN) and Radio -Frequency Identification (RFID) systems are \ncrucial technologies on the IoMT. The integration of WSN and RFID with smart objects has \nprovided novel communication capabilities in recent years. Given their advantages and widespread \nuse in IoMT, we provide a brief overview of these technologies in this section. \n \nWireless Sensor Networks (WSN): WSNs in IoMT can be used for various purposes, such \nas monitoring vital signs, tracking patients' movements, detecting falls, and alerting caregivers in \ncase of an emergency. For example, a WSN can be used to monitor a patient's blood pressure, heart \nrate, and body temperature continuously and wirelessly transmit the data to a healthcare provider \nfor analysis. In case of abnormal readings, the provider can be alerted, and necessary action can \nbe taken promptly.  A WSN usually consists of a microprocessor, memory unit, communication \ninterface, and a battery shown in Figure 2. These sensors can be attached to objects/persons \naccording to their specific needs [14,16,17]. WSNs in IoMT can also be used for remote pati ent \nmonitoring, telemedicine, and clinical research. With the help of WSNs, healthcare providers can \nmonitor patients' health status in real -time, regardless of their location. This has significant \nimplications for elderly patients, those with chronic conditions, and those who live in remote areas \nwith limited access to medical facilities.  In conclusion, WSNs in IoMT have the potential to \ntransform healthcare by enabling remote monitoring, early detection of health problems, and \n10 \n \npersonalized care. However, careful consideration must be given to the design and implementation \nof WSNs to address the challenges associated with their use in healthcare. \n \n \nFigure 2 WSN architecture [4] \n \nRFID: RFID technology has found significant applications on the IoMT due to its ability to \ntrack and monitor medical equipment, supplies, and patients' movements. RFID technology uses \nradio waves to communicate between a reader and a tag, which contains information about the \nitem or person being tracked. In IoMT, RFID technology can be used to track medical equipment, \nsuch as surgical tools, to ensure they are properly sterilized and accounted for. It can also be used \nto track medical suppl ies, such as medication, to prevent waste and ensure proper inventory \nmanagement. RFID technology can also be used to track patients' movements within a healthcare \nfacility, enabling healthcare providers to monitor patients' activities, ensure their safety , and \noptimize care delivery. For example, RFID tags can be placed on patients' wristbands, which can \nbe used to monitor their location and activities in real -time [18,19]. However, the deployment of \nRFID in IoMT poses challenges, such as data privacy and security concerns. Healthcare providers \nmust ensure that the data collected by RFID tags are secure and comply with data protection \nregulations. Overall, the use of RFID technology in IoMT has the potential to improve patient \nsafety, optimize care delivery, and streamline medical inventory management. \n2.4 IoMT Security Requirements \nThe patient’s medical data is very sensitive. A set of requirements that can ensure IoMT \nsystems’ security at all layers is needed. The set has been derived from CIANA (Confident iality, \n\n11 \n \nIntegrity, Availability, Non -Repudiation, and Authentication) considerations and consists of the \nfollowing security requirements [20], [21].  \n1. Authentication and authorization: Ensuring that only authorized users and devices can \naccess and modify medical data. \n2. Data privacy and confidentiality : Protecting sensitive medical data from unauthorized \naccess or disclosure. \n3. Integrity and non-repudiation: Ensuring that medical data is not tampered with and that \nthere is a clear audit trail of all data transactions. \n4. Availability and reliability: Ensuring that medical data and devices are always available \nand functional. \n5. Compliance: Ensuring that IoMT systems comply with relevant regulatory requirements, \nsuch as HIPAA and GDPR. \n6. Risk management: Identifying and mitigating potential security risks and vulnerabilities. \n7. Physical security : Ensuring that medical devices and data are physically secure and \nprotected from theft or damage. \n8. Incident response: Developing and implementing a plan to detect, respond to, and recover \nfrom security incidents or breaches. \n \n 2.5 Attacks on IoMT \nThe IoMT  applications rely on a variety of technologies with their own set of security \nvulnerabilities. Due to the lack of fundamental security procedures, IoMT devices are susceptible \nto various types of cyber -attacks, which not only compromise patients' privacy b ut also cause \nfinancial and reputational damage  [22]. For example, the healthcare industry has lost more than \n$160 million since 2016 due to cyber-attacks, including ransomware attacks, which have increased \nby 94% between 2021 and 2022  [23]. In addition, att acks on brain implants have resulted in \nfatalities. Table 1 shows some of the potential IoMT attacks and their influence on the system’s \nsecurity requirements. \n  \n \n \n12 \n \nTable 1 Description of attacks and their effect on IoMT security requirements [4] \nAttack Brief Description Effects \nSide-channel \nattack \nThe information is obtained from the side \nchannels of the encryption device. \nConfidentiality, \nIntegrity \nTampering \ndevices \nThe IoMT device is physically accessed to \nmodify the data (modification in a device \nusing RFID or communication link). \nConfidentiality, \nIntegrity \nTag cloning An attacker might exploit data obtained \nthrough a successful side -channel attack or \nreplicate data from a  previously used tag. \nThe cloned tag, for example, might be used \nto gain access to an unlawful facility or data, \nsuch as medical data (Using simple \ntechnologies, attackers may clone RFIDs). \nConfidentiality, \nAuthorization, \nIntegrity \nSensor tracking This form of attack invades patients’ \nprivacy. Attackers might obtain access to \npatients’ whereabouts or fake GPS  data by \nusing unsecured equipment. Other sensor s, \nsuch as those used in fall detection, \nwheelchair management,  and remote \nmonitoring systems, can also be utilized to \ndivulge sensitive data about patients. \nConfidentiality, \nAuthorization, \nIntegrity, \nPrivacy \nEavesdropping An attacker intercepts and tracks the \nnecessary hardware and communication to \ncapture data. Data obtained in this manner \n(unlawfully) can be utilized in a variety of \nways. \nConfidentiality, \nNon-\nrepudiation, \nPrivacy \nReplay An attacker can use an authentication \nmessage that was previously transmitted \nbetween two legitimate users. In this \nsituation, an attacker can intercept a signed \npacket and send it back to target multiple \ntimes. \nAuthorization \nMan-in-the-middle It’s a cyber -attack that targets two IoMT \ndevices’ communication and gains access to \ntheir private data. The attacker can listen in \non or monitor the communication between \nthe two devices in this attack. The attacker \ncan alter the intercepted data before they are \ntransmitted to their intended destination. \nConfidentiality, \nAuthorization \n13 \n \nRogue access A fake gateway is placed inside the wireless \nnetwork range in this attack to give genuine \nusers access and intercept traffic. \n \nDoS/DDoS Unlike DoS attacks, which are carried out \nby a single node, a DDoS attack is carried \nout by several sources, flooding a specified \ntarget with messages or connection requests \nwith the purpose of rendering the service \ninaccessible to legitimate users. \nAvailability \nSinkhole A malicious node attracts traffic in this \nattack by offering a better connection \nquality. Once the attack is successful, other \nattacks (such as eavesdropping or selective \nforwarding) can be launched, in which the \nmalicious node isolates specific nodes by \ndiscarding packets that pass through them. \n \nSniffing Data transferred between two nodes is \npassively intercepted by sniffing attacks. \nSince the attacker can observe the data \npassed between the system’s layers. \nConfidentiality \nSelective \nForwarding \nA malicious node may simply change, drop, \nor selectively forward some messages to \nother nodes in the network. As a result, the \ninformation received by the destination is \nincomplete. \nAll \nBrute Force The attackers usually use automated tools to \ncreate multiple password combinations \nuntil they succeed. The dictionary attack is \nan example of a serious vulnerability for \nIoMT devices. \nConfidentiality, \nIntegrity \nSQL injection An SQL injection attack involves \nintroducing a faulty SQL statement into the \napplication’s backend database.  A \nsuccessful SQL injection attack can \ncompromise or change sensitive patient \ndata. \nAll \n14 \n \nAccount \nhijacking \nAt the network level, many IoT devices \ncommunicate in transparent text or with \ninsecure encryption. Intercepting the packet \nwhen an end user is authenticating allows an \nattacker to undertake account hijacking. \nConfidentiality, \nIntegrity \nRansomware Ransomware encrypts important \ninformation and demands a large fee to \nunlock it. In return for money, attackers can \nencrypt sensitive data such as healt h \ninformation and keep the decryption key. \nIntegrity, \nAvailability \n \n2.6 IoMT Device Strengths and Weaknesses \nIoMT devices can transfer data within seconds via internet.  These data can be electronic \nmedical records, physical check results, electronic prescriptions, diagnostic reports, medical \nreference invitations, personal daily health information, etc. This is the biggest advantage of IoMT \ndevice. But every device has its own advantages and disadvantages. Some of the advantages of \nIoMT devices are- \n1. Portability: Most of the IoMT devices are small and portable.  These devices are cell \nphone, smart watch, or any other wearable devices. People can travel with those devices \nand checks heart rate, oxygen saturation, BMI, number of steps taken etc.  while \ntravelling.  \n2. Many applications: IoMT devices have many applications like patients’ making online \nappointments and consulting with doctors, paperless medical records, preserving whole \nmedical history in the server etc. But the most import application  now a days is data \nmining. Through data mining, we can obtain deeper cognition of the reason why disease \nhappens to someone. For example, by using association rules, researchers can know the \npotential relationship among genes of disease. \n3. New job for experts in medical industry: The development of Internet Medical Things \nwould create new positions. With the Internet of Medical Things, the number of \noutpatient doctors will decrease, while special data analysis experts will be increasingly \nneeded. \n \n15 \n \nSome of the disadvantages of IoMT devices are- \n1. Battery Life: The portable facility relies on a battery as a power source. Therefore, the \nweakness of the battery turns into one of the weaknesses of the Internet of Medical Things. \nWith the development of technology, the battery can work under a secure circuit so that \nthe battery can continuously output a stable current until the voltage drops to the minimum \neven there is some unstable current. \n2. Monitoring Liability:  Data taken by the IoMT devices are not mo nitored by any \nhealthcare professional. So, the liability of the accuracy of those data are compromised.  \n3. Data Security: IoMT system transfers a vast amount of medical data. Ensuring the data \nprivacy and security of those data is the biggest challenge and disadvantage of IoMT \ndevice. \n2.7 IoMT Device Trends 2023 \nEvery year IoMT devices used in healthcare organizations are revolutionizing medical care \nin unique ways. Improving healthcare outcomes and the evolution of high -speed networking \ntechnologies are some of the promising areas of market advancement in IoMT devices. IoMT \ndevices evolve with time.  \n• Consumer Health Wearable s: Consumer health wearables are consumer -grade \ndevices for personal fitness, such as activity trackers, bands, wristbands, sports \nwatches, and smart garments. Most of these devices are not regulated by health \nauthorities but may be suggested by experts for specific health applications. \nCompanies operating in this space include Misfit (Fossil Group), Fitbit, and Samsung \nMedical. \n• In-clinic Segment: The in-clinic segment includes IoMT devices that are used for \nadministrative or clinical functions. Instead of th e care provider physically using a \ndevice, the provider can be located remotely while a device is used by qualified staff. \nExamples include Rijuven’s Clinic in a Bag, which is a cloud -based examination \nplatform for clinicians to assess patients at any poin t of care; ThinkLabs’ digital \nstethoscope; and Tytocare’s comprehensive telehealth patient examination device for \nthe heart, lungs, ears, skin, throat, and abdomen, which also can measure \ntemperature. \n16 \n \n• Personal Emergency Response System (PERS):  PERS is a me dical alert system \nthat integrates wearable device/relay units and a live medical call center service to \nincrease self-reliance for homebound or limited-mobility seniors. A PERS has three \ncomponents: a small radio transmitter, a console connected to the te lephone, and an \nemergency response center that monitors calls. The package allows users to quickly \ncommunicate and receive emergency medical care [5]. \n• Remote Patient Monitoring (RPM) Device: RPM comprises all home monitoring \ndevices and sensors used for chronic disease management, which involves \ncontinuous monitoring of physiological parameters to support long -term care in a \npatient’s home. For continuous observation of discharged patients to  accelerate \nrecovery time and prevent re -hospitalization; and medication management acute \nhome monitoring is very important. It can also provide users with medication \nreminders and dosing information [6]. \n• Innovative Devices: Some of the innovative devices in 2023 are Zoll’s wearable \ndefibrillator, which continuously monitors patients at risk of ventricular tachycardia \nor f ibrillation; Stanley Healthcare’s hand hygiene compliance system, which \nincorporates an occupancy sensor and a real -time location system receiver to track \nthe identity of employees using the dispenser and uses analytics to determine whether \nemployees are following hygiene protocol; and Boston Children’s Hospital’s GPS -\nbased MyWay app, which guides visitors to their destination using the quickest route \n[7]. \n2.8 IoMT System Security Techniques \nThere are several different techniques to secure IoMT systems. These techniques can be \ndivided into three main categories: 1) symmetric; 2) asymmetric; and 3) keyless, as shown \nin Figure 3 . Symmetric and asymmetric techniques rely on cryptographic algorithms, while \nkeyless techniques are noncryptographic [3].  \n17 \n \n \nFigure 3 IoMT security techniques [3] \n2.8.1 Cryptography \nCryptography is the art of keeping information secret and safe by transforming it into a \nform that unintended recipients cannot understand. Three basic cryptographic techniques are \nsymmetric key, asymmetric key, and hash function. Symmetric key cryptographic algorithm is \nbased on a single key to encrypt and decrypt data between two nodes trying to communicate. The \nkey is to be generated and distributed prior to using the algorithm. Symmetric key encryption \ntechnique is illustrated in figure 4.  \n \nFigure 4 Symmetric key Encryption [3] \n \nThe public and private key pairs are used in the asymmetric key technique. Key distribution \nis not required in this algorithm.  Public key available to everyone. But the private key is secured \n\n18 \n \nto the receiver only.  So, any data encrypted with the public can only be decrypted by the actual \nreceiver. The illustration of asymmetric encryption is presented in figure 5. On the other hand, the \nhash function algorithm generates a hash value with a fixed length based on the transmitted data. \n \nFigure 5 Symmetric key Encryption [3] \n \n2.8.2 Advanced Encryption Standard (AES) 256 \nThe AES 256 Encryption algorithm (also known as the Rijndael algorithm)  is a secure \nsymmetric encryption algorithm using a 256-bit key. It transforms plain text into a cipher through \nmultiple rounds including round key addition, byte substitution, row shifting, and column mixing. \nAES 256 reduces the risk of a data breach in case of a security breach, making the encrypted data \nunreadable without the proper key [8]. The National Institute of Standards and Technology \nselected three “flavors” of AES: 128-bit, 192-bit, and 256-bit. Each type uses 128-bit blocks. The \ndifference lies in the length of the key. As the longest, the 256-bit key provides the strongest level \nof encryption. With a 256-bit key, a hacker would need to try 2256 different combinations to ensure \nthe right one is included [8]. AES has several steps through which the data is encrypted. Figure 6 \nshows all the steps sequentially. \n The steps in AES are- \n \n• Divide Information into Blocks : The first step of AES 256 encryption is dividing the \ninformation into blocks. Assuming AES has a 128 - bits block size, it divides the \ninformation into 4x4 columns of 16 bytes. \n\n19 \n \n• Key Expansion : The next step of AES 256 encryption involves the AES algorithm \nrecreating multiple round keys from the first key using Rijndael’s key schedule. \n• Adding the Round Key: In round key addition, the AES algorithm adds the initial round \nkey to the data that has been subdivided into 4x4 blocks. \n• Bite Substitution: In this step, each byte of data is substituted with another byte of data. \n• Shifting Rows: The AES algorithm then proceeds to shift rows of the 4x4 arrays. Bytes on \nthe 2nd row are shifted one space to the left, those on the third are shifted two spaces, and \nso on. \n• Mixing Columns: The AES algorithm uses a pre -established matrix to mix the 4x4 \ncolumns of the data array. \n• Another Round Key Addition: The AES algorithm then repeats the second step, adding \naround key once again, then does this process all over again. \n \nFigure 6 Steps of AES algorithm [9] \n \nThe main features of AES are [9]: \n• Key Expansion: A single key is taken during the first stage, which is later expanded to \nmultiple keys used in individual rounds \n• Byte Data: The AES encryption algorithm does operations on byte data instead of bit \ndata. \n\n20 \n \n• Key Length: The number of rounds to be carried out depends on the length of the key \nbeing used to encrypt data. The 128-bit key size has ten rounds, the 192-bit key size has \n12 rounds, and the 256-bit key size has 14 rounds. \n2.8.3 Steganography \nSteganography is the process of concealing information in another medium (audio, video, \nimage, etc.).It is generally known as invisible communication. It hides the transmitted data inside \na cover object (carrier object). In this process cover object and st ego-object (carrying hidden \ninformation object) looks indifferent[10]. In this project we have used image as a cover object. So, \nwe are discussing about image steganography.  \n \nImage steganographic techniques can be divided into two domains. Spatial domain and \ntransform domain. Ther e are many versions of spatial steganography. All directly change some \nbits in the image pixel values in hiding data. Spatial steganographic techniques can be classified \ninto- \n• Least significant bit (LSB)  \n• Pixel value differencing (PVD)  \n•  Edges-based data embedding method (EBE)  \n• Random pixel embedding method (RPE)  \n• Mapping pixel to hidden data method  \n• Labeling or connectivity method  \n• Pixel intensity-based method  \n• Texture based method  \n• Histogram shifting method  \n \nTransform domain is a more complex way of hiding information in an image. Various algorithms \nand transformations are used on the image to hide information in it. Transform domain techniques \nare classified into- \n• Discrete Fourier transformation technique (DFT).  \n• Discrete cosine transformation technique (DCT).  \n• Discrete Wavelet transformation technique (DWT).  \n21 \n \n• Lossless or reversible method (DCT)  \n• Embedding in coefficient bits  \n2.8.4 Least Significant Bit (LSB) \n LSB based steganography is one of the simplest techniques that hides a secret message in \nthe least significant bit of pixel values without introducing many perceptible distortions. Change \nin the value of the LSB are imperceptible for human eyes. In LSB method the change in byte is \n0.000002%. An illustration of LSB method is shown in Figure 7. \n \n \nFigure 7 LSB method \n \n• Advantages of spatial domain LSB technique are:  \n• There is less chance for degradation of the original image.  \n• More information can be stored in an image.  \n• Disadvantages of the LSB technique are:  \n• Less robust, the hidden data can be lost with image manipulation.  \n \n \n \n \n\n22 \n \n3. RELATED WORK \nWe have designed our use case inspired from two papers. Their work is closely related to our \nwork. The first paper named “Image Steganography embedded with Advance Encryption Standard \n(AES) securing with SHA -256” combines image steganography, AES , and SHA 256. The \nproposed method of this paper secures the weaker section which is the key in Advance Encryption \nStandard using hashing technique. The y enhanced the level of concealment of information from \nunauthorized access and for covert information exchange by encrypting the data and hiding it into \na multimedia file known as image. The Secure Hash Algorithm 256 generates a hash key of 256 \nbits which is an irreversible hashing technique , after that the key is used in the process of \nencrypting the text with Advance Encryption Standard 256 . The cipher text is embedded into a \ntarget image using LSB method [11]. The flow chart of the proposed system taken from the paper \nis shown in Figure 8 and the summary of the paper is presented in Table 2 below. \n \nFigure 8 Flow chart of the proposed system from the paper named “Image Steganography \nembedded with Advance Encryption Standard (AES) securing with SHA-256” [11] \n\n23 \n \nTable 2 Summary of the paper named “Image Steganography embedded with Advance \nEncryption Standard (AES) securing with SHA-256” \nAuthors Method Observation \nVikas Singhal, Yash \nKumar Shukla, \nNavin Prakash \nSHA-256, \nAES-256, \nLSB  \nThe proposed system successfully  \nhides the data using image \nsteganography via LSB and surplus \nthe protection of data using  \ncryptographic technique AES-256. \n \nThe second paper named “ IoMT Security: SHA3-512, AES -256, RSA and  LSB \nSteganography” combines image steganography, AES, RSA, and SHA 256.  This paper proposes \nan information security scheme for IoMT  that utilizes AES -256, RSA, SHA3 -512 and LSB \nembedding in medical scans or images [12]. The summary of the paper is presented in Table 3 and \nthe result comparison of the proposed method is illustrated from the paper in Figure 9. \n \nTable 3 Summary of the paper named “IoMT Security: SHA3-512, AES-256, RSA and LSB \nSteganography” \nAuthors Method Observation \nWassim Alexan, \nAhmed Ashraf, \nEyad Mamdouh, \nSarah Mohamed, \nMohamed Moustafa, \nSHA-512, \nAES-256, \nRSA \nLSB  \nThe proposed scheme  \nguarantees the secure  \ntransmission of medical data  \nthrough insecure channels or \nnetworks. \n \n24 \n \n \n \nFigure 9 The histograms to compare results of the proposed method taken from the paper \nnamed “IoMT Security: SHA3-512, AES-256, RSA and LSB Steganography” [12] \n \n \n\n25 \n \n4. CASE STUDY \nIn the case study, we have designed a hybrid encryption & decryption technique for the \ntransmitter and receiver respectively. It is a combination of Advanced Encryption Standard (AES) \n256 and Least Significant Bit (LSB). We have downloaded the dataset from Kaggle [5]. \n \nThe transmitter and receiver sides have separate implementation steps. On the transmitter side, \nfirst sensitive medical data is then encrypted with a symmetric key (k) utilizing the AES -256 \nalgorithm. This encrypted data is transformed into a bitstream. Then bitstream is embedded into a \ncover medical image using the Least Significant Bit (LSB) method which results in a stego-image. \nOn the receiver side, all the steps are performed in reverse sequence. First, the extraction of the \nLSB-embedded data is done. Then the extracted data is decrypted using the symmetric key (k).  \nFigure 10 depicts the flow diagram of the implemented case study. \n \n \nFigure 10 Case study process diagram \n \n \n \n\n26 \n \n4.1 Case Study Implementation \nPython script is designed for steganography, the practice of concealing information within \nother non -secret data, specifically within image files. The code is divided into two primary \ncomponents: an Encryption Module and a Decryption Module. \nThe Encryption Module performs the following tasks: \nI. Imports essential libraries, such as os, sys, hashlib, Image (from PIL), base64, AES \n(from Cryptodome.Cipher), and pandas, providing a range of functionalities required \nfor the encryption and encoding process. \nII. The AES_encryptor function is responsible for encrypting the input message using the \nAdvanced Encryption Standard (AES) algorithm in Galois/Counter Mode (GCM). It \ngenerates a random salt and derives a private_key from the password, employing the \nScrypt Key Derivation Function (KDF). T he function outputs a dictionary containing \nthe encrypted cipher_text, salt, nonce, and tag, all encoded in base64 format. \nIII. Several auxiliary functions are defined to facilitate the encoding process:  \na) str_to_bin: Transforms a string into its binary representation.  \nb) genData: Generates binary data from the encrypted dictionary with a length \nprefix for each value.  \nc) modPixEncode: Alters the image's pixel values based on the binary data, \nembedding the encrypted information within the pixels.  \nd) encode_image: Accept s an image and an encrypted dictionary as inputs, \nprocesses the image data, encodes the binary data into the image, and creates a \nnew image with modified pixel values. \nIV. The image_encode function reads an input image file, obtains a secret key (password) \nfrom the user, encrypts the input data using the AES_encryptor function, and calls the \nencode_image function to embed the encrypted message within the image. The \nencoded image is saved in the \"Encoded Image\" folder with an '_encoded' suffix. \nV. The script extracts data from an Excel file, converts it into a string, and calls the \nimage_encode function to encode the message into an image. \n \n \n \n27 \n \nThe Decryption Module performs the following tasks: \n \nI. Imports essential libraries, similar to the Encryption Module. \nII. The AES_decryptor function is responsible for decrypting the encrypted data using \nthe AES algorithm in GCM. It derives a private_key from the password, employing \nthe Scrypt KDF. The function outputs the decrypted plain_text. \nIII. Several auxiliary functions are defined to facilitate the decoding process:  \na) binary_to_str: Transforms binary data back into a string.  \nb) modPixDecode: Extracts the binary data concealed within the image's pixel \nvalues. \nc) decode_image: Accepts an image file path as input, processes the image, \nextracts the binary data, and reconstructs the encrypted dictionary. \nIV. The image_decode function reads an encoded image file, obtains a secret key \n(password) from the user, calls the decode_image function to extract the encrypted \ndictionary from the image, and then calls the AES_decryptor function to decrypt \nthe message. It returns the decrypted data. \nV. In the __main__ block, the script prompts the user to input the encoded image file \nname and th e secret key. It then calls the image_decode function to extract and \ndecrypt the hidden message from the image, subsequently printing the decoded data \nto the console. \n \nThe Python script of use case demonstrates an effective steganographic method to securel y \nconceal information within image files. This implementation employs a combination of Advanced \nEncryption Standard (AES) encryption in Galois/Counter Mode (GCM) and image manipulation \ntechniques, ensuring a high level of security for the hidden data. The script is divided into two \nprimary modules—Encryption and Decryption—each containing a series of functions designed to \nhandle the encryption, encoding, decryption, and decoding processes. This approach offers a \npractical and secure solution for embedding s ensitive information within images, which can be \nuseful in a wide range of applications, such as secure communication, data storage, and digital \nwatermarking. \n28 \n \n4.2 Results \nIn order to assess the effectiveness of the steganographic technique employed, we have \nconducted a thorough analysis comparing the histograms of the original image and the stego image. \nThe histograms serve as a visual representation of the distribution of pixel values in an image, and \ntheir identical nature demonstrates that both images are remarkably similar, thereby ensuring that \nany potential hackers would be unable to distinguish between the two. This evaluation was \nperformed by superimposing one histogram on top of the other, allowing for a direct comparison \nbetween the two sets of data. \n \nA wide variety of cover imag es were utilized for this experiment, encompassing grayscale \nimages, color images, images of differing resolutions, and images of various types. The results \nconsistently showed that the steganographic technique performed exceptionally well, with the \nhistograms of the encrypted images appearing virtually indistinguishable from those of the original \nimages. \n \nIn Figure 11, we observe a comparison between grayscale images and their corresponding \nhistograms. It is evident from this comparison that both histogram s are virtually identical, which \nserves as a strong indication of the effectiveness of the steganographic technique in preserving the \noriginal image's characteristics. Similarly, in Figure 12, we examine a comparison between color \nimages and their respective histograms. Here, we can also observe that the RGB histograms closely \nresemble one another, further reinforcing the notion that the employed steganographic method is \nboth robust and reliable in ensuring that the encrypted images remain undetectable to  potential \nhackers. \n29 \n \n \nFigure 11 Cover grayscale image, encrypted stego image and their corresponding histograms \n \n \n \nFigure 12 Cover color image, encrypted stego image and their corresponding histogram \n\n30 \n \n5. DISCUSSION & FUTURE WORK \nThis project aims to explore IoMT device structure and security protocols. Also designs a \nhybrid cryptographic and steganographic technique to ensure communication security in IoMT \ndevices for medical data transmission. There are lots of techniques of secure data transmission. \nBut AES is the most popular one now a days. As AES is unbreakable until today, it’s used in all \nmeans of secure data communications. So , in today’s perspective it might look unnecessary to \ncombine any other encryption technique with AES. But the computation al power is increasing \nrapidly. So, in near fu ture in the era of quantum computing  it might not be impossible  to break \nAES. Lots of research are going on to design more secure algorithms. Our project was a small part \nof it.  \n \nIn this design we have combined two techniques. The constraints of this design are- \n \n• Separate cover image is required , which will increase the data size that needs to be \ntransmitted. \n• If the secret key that is used in AES needs to be shared with some one, in what \nencryption technique that will be transmitted? \n \nSo, to solve the transmission problem of the secret key  of AES, we will design an asymmetric \nencryption technique like Rivest-Shamir-Adleman (RSA) public key private key system in future \nand combine with our process. In that process, secret key will be shared encrypting with the public \nkey so that only authorized users can unlock it with private key. \n \n \n \n \n31 \n \n6. CONCLUSION \nThe use of IoMT is a reality of today’s world. Many hospital systems are adopting it or in the \nprocess of adopting it. The majo rity of current research activities focuses on how medical and \nhealth-monitoring technologies can help reduce healthcare costs while improving patient health. \nThis is also an objective of many developed hospitals and medical facilities. As a result, protecting \nthis technology has become critical, as the IoMT is vulnerable to a variety of attacks due to its \nreliance on wireless communications. These attacks have the potential to compromise the system \nand breach patient’s privacy, as well as compromise the confidentiality, integrity, and availability \nof medical services. We have discussed IoMT security requirements and the techniques of securing \nthis domain and their related assets. Securing healthcare data will increase the effectiveness of \nIoMT devices benefiting the patients and developing a smart healthcare system. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n33 \n \nAPPENDIX B. CODE \n \nEncoder code \n \n # import the library module \nimport os \nimport sys \nimport hashlib \nfrom PIL import Image \nfrom base64 import b64encode, b64decode \nimport hashlib \nfrom Cryptodome.Cipher import AES \nimport os \nfrom Cryptodome.Random import get_random_bytes \nimport pandas as pd \n \ndef AES_encryptor(plain_text, password): \n    # generate a random salt \n    salt = get_random_bytes(AES.block_size) \n    # use the Scrypt KDF to get a private key from the password \n    private_key = hashlib.scrypt(password.encode(), salt=salt, n=2**14, r=8, p=1, \ndklen=32) \n    #print(private_key) \n    # create cipher config \n    cipher_config = AES.new(private_key, AES.MODE_GCM) \n \n    # return a dictionary with the encrypted text \n    cipher_text, tag = cipher_config.encrypt_and_digest(bytes(plain_text, 'utf-\n8')) \n    #print(b64encode(cipher_text).decode('utf-8')) \n    print(b64encode(cipher_config.nonce).decode('utf-8')) \n    return { \n        'cipher_text': b64encode(cipher_text).decode('utf-8'), \n        'salt': b64encode(salt).decode('utf-8'), \n        'nonce': b64encode(cipher_config.nonce).decode('utf-8'), \n        'tag': b64encode(tag).decode('utf-8') \n    } \n \n \n \n# Encode data into image \n34 \n \n \ndef genData(modValue): \n    newd = [] \n    for value in modValue.values(): \n        for char in value: \n            newd.append(format(ord(char), '08b')) \n    return newd \n \n \n# Pixels are modified according to the \n# 8-bit binary data and finally returned \ndef modPix(pix, value): \n  \n    datalist = genData(value) \n    lendata = len(datalist) \n    imdata = iter(pix) \n  \n    for i in range(lendata): \n  \n        # Extracting 3 pixels at a time \n        pix = [value for value in imdata.__next__()[:3] + \n                                imdata.__next__()[:3] + \n                                imdata.__next__()[:3]] \n  \n        # Pixel value should be made \n        # odd for 1 and even for 0 \n        for j in range(0, 8): \n            if (datalist[i][j] == '0' and pix[j]% 2 != 0): \n                pix[j] -= 1 \n  \n            elif (datalist[i][j] == '1' and pix[j] % 2 == 0): \n                if(pix[j] != 0): \n                    pix[j] -= 1 \n                else: \n                    pix[j] += 1 \n                # pix[j] -= 1 \n                 \n                 \n        # Eighth pixel of every set tells \n        # whether to stop ot read further. \n        # 0 means keep reading; 1 means thec \n        # message is over. \n        if (i == lendata - 1): \n            if (pix[-1] % 2 == 0): \n35 \n \n                if(pix[-1] != 0): \n                    pix[-1] -= 1 \n                else: \n                    pix[-1] += 1 \n  \n        else: \n            if (pix[-1] % 2 != 0): \n                pix[-1] -= 1 \n  \n        pix = tuple(pix) \n        yield pix[0:3] \n        yield pix[3:6] \n        yield pix[6:9] \n         \n \ndef encode_enc(newimg, data_dict): \n    w = newimg.size[0] \n    (x, y) = (0, 0) \n  \n    for pixel in modPix(newimg.getdata(), data_dict): \n        newimg.putpixel((x, y), pixel) \n        if (x == w - 1): \n            x = 0 \n            y += 1 \n        else: \n            x += 1 \n \ndef image_encode(data): \n    import re \n \n    # remove non-printable characters from input \n    #data = re.sub('[^\\x20-\\x7E]', '', data) \n \n    img_path = os.path.join(\"Original Image\", input(\"Enter image file name: \")) \n    image = Image.open(img_path, 'r') \n    secret_key = input(\"Enter Secret Key: \") \n     \n    AES_data = AES_encryptor(data, secret_key) \n    salt = AES_data['salt'] \n    cipher_text = AES_data['cipher_text'] \n    nonce = AES_data['nonce'] \n    tag = AES_data['tag'] \n36 \n \n    value = {'salt': salt, 'cipher_text': cipher_text, 'nonce': nonce, 'tag': \ntag} \n \n    newimg = image.copy() \n    encode_enc(newimg, value) \n \n    # Save the encoded image in the same format as the original image \n    format = image.format \n    if format == 'JPEG': \n        extension = 'jpg' \n    else: \n        extension = format.lower() \n    new_img_path = os.path.join(\"Encoded Image\", \nos.path.splitext(os.path.basename(img_path))[0] + '_encoded.' + extension) \n    newimg.save(new_img_path, format) \n \n     \nexcel_file = \"data set.xlsx\" \nsheet_name = \"Dataset\" \ndf = pd.read_excel(excel_file, sheet_name=sheet_name, header=None) \narr = df.iloc[5].values \ndata = \"\".join(map(str, arr)) \nprint (data) \n#secret_key = input(\"Enter Secret Key: \") \nimage_encode(data) \n \n \n \n \n \n \n \n \n \n \n \n \n37 \n \nDecoder code \n \n# import the library module \nimport os \nimport sys \nimport hashlib \nfrom PIL import Image \nfrom base64 import b64encode, b64decode \nimport hashlib \nfrom Cryptodome.Cipher import AES \n \ndef AES_decryptor(cipher_text, password, salt, nonce, tag): \n    # use the Scrypt KDF to get a private key from the password \n    private_key = hashlib.scrypt(password.encode(), salt=b64decode(salt), \nn=2**14, r=8, p=1, dklen=32) \n    # create cipher config \n    cipher_config = AES.new(private_key, AES.MODE_GCM, nonce=b64decode(nonce)) \n    # return the decrypted text \n    decrypted_text = cipher_config.decrypt_and_verify(b64decode(cipher_text), \nb64decode(tag)) \n    return decrypted_text.decode('utf-8') \n \ndef decode_data(imdata): \n    binary_data = '' \n    for pixel in imdata: \n        for value in pixel: \n            binary_data += str(value % 2) \n    # split by 8-bit chunks \n    chunks = [binary_data[i:i+8] for i in range(0, len(binary_data), 8)] \n    # convert each chunk to its ASCII equivalent \n    message = ''.join([chr(int(chunk, 2)) for chunk in chunks]) \n    # remove padding at end of message \n    end_index = message.find('\\x00') \n    if end_index != -1: \n        message = message[:end_index] \n    return message \n \ndef image_decode(secret_key): \n    img_path = os.path.join(\"Encoded Image\", input(\"Enter encoded image file \nname: \")) \n    image = Image.open(img_path, 'r') \n \n38 \n \n    imdata = list(image.getdata()) \n \n    salt = '' \n    cipher_text = '' \n    nonce = '' \n    tag = '' \n \n    # read the hidden values from the image \n    for i in range(4): \n        value = '' \n        for j in range(8): \n            value += str(imdata[i*3+j][0] % 2) \n        if i == 0: \n            salt = value \n        elif i == 1: \n            cipher_text = value \n        elif i == 2: \n            nonce = value \n        else: \n            tag = value \n \n    decrypted_text = AES_decryptor(cipher_text, secret_key, salt, nonce, tag) \n    print('Decrypted message:', decrypted_text) \n \nsecret_key = input(\"Enter Secret Key: \") \nimage_decode(secret_key) \n \n \n \n \n \n \n \n \n \n \n \n39 \n \nREFERENCES \n \n[1] Fabio Duarte, “Number of IoT Devices (2023),” explodingtopics.com/, 2023. \nhttps://explodingtopics.com/blog/number-of-iot-devices (accessed Apr. 18, 2023). \n[2] MarketWatch, “Internet of Medical Things (IoMT) Market Global Trends, Market Share, \nIndustry Size, Growth, Opportunities and Market Forecast 2028,” marketwatch.com, 2023. \nhttps://www.marketwatch.com/press-release/internet-of-medical-things-iomt-market-\nglobal-trends-market-share-industry-size-growth-opportunities-and-market-forecast-2028-\n2023-03-29 (accessed Apr. 28, 2023). \n[3] A. Ghubaish, T. Salman, M. Zolanvari, D. Unal, A. Al-Ali, and R. Jain, “Recent Advances \nin the Internet-of-Medical-Things (IoMT) Systems Security,” IEEE Internet Things J., vol. \n8, no. 11, pp. 8707–8718, 2021, doi: 10.1109/JIOT.2020.3045653. \n[4] R. Hireche, H. Mansouri, and A. -S. K. Pathan, “Security and Privacy Management in \nInternet of Medical Things (IoMT): A Synthesis,” J. Cybersecurity Priv., vol. 2, no. 3, pp. \n640–661, 2022, doi: 10.3390/jcp2030033. \n[5] Mayor Muriel Bowser, “Personal Emergency Response System (PERS),” \nodr.dc.gov/book/path/PERS. https://odr.dc.gov/book/path/PERS (accessed Apr. 17, 2023). \n[6] L. P. Malasinghe, N. Ramzan, and K. Dahal, “Remote patient monitoring: a comprehensive \nstudy,” J. Ambient Intell. Humaniz. Comput. , vol. 10, no. 1, pp. 57 –76, 2019, doi: \n10.1007/s12652-017-0598-x. \n[7] Satavisa Pati, “Top 10 IoMT Trends and Use Cases in Healthcare for 2023,” \nwww.analyticsinsight.net, 2022. https://www.analytic sinsight.net/top-10-iomt-trends-and-\nuse-cases-in-healthcare-for-2023/ (accessed Apr. 17, 2023). \n[8] N-able, “Understanding AES 256 Encryption,” www.n-able.com, 2019. https://www.n -\nable.com/blog/aes-256-encryption-algorithm (accessed Feb. 23, 2023). \n[9] Baivab Kumar Jena, “What Is AES Encryption and How Does It Work?,” \nwww.simplilearn.com, 2023. https://www.simplilearn.com/tutorials/cryptography -\ntutorial/aes-encryption (accessed Mar. 02, 2023). \n[10] M. Hussain, “A Survey of Image Steganography Techniques A S urvey of Image \nSteganography Techniques Mehdi Hussain and Mureed Hussain,” no. February, 2015. \n[11] M. V. Singhal*, M. Y. K. Shukla, and D. N. Prakash, “Image Steganography embedded \n40 \n \nwith Advance Encryption Standard (AES) securing with SHA-256,” Int. J. Innov. Technol. \nExplor. Eng., vol. 9, no. 8, pp. 641–648, 2020, doi: 10.35940/ijitee.h6442.069820. \n[12] W. Alexan, A. Ashraf, E. Mamdouh, S. Mohamed, and M. Moustafa, “IoMT Security: \nSHA3-512, AES-256, RSA and LSB Steganography,” Proc. - 2021 8th NAFOSTED Conf. \nInf. Comput. Sci. NICS 2021 , no. December, pp. 177 –181, 2021, doi: \n10.1109/NICS54270.2021.9701567.  \n[13] Ferguson, J.E.; Redish, A.D. Wireless communication with implanted medical devices \nusing the conductive properties of the body. Expert Rev. Med. Devices 2011, 8, 427–433. \n[14] Kos, A.; Milutinovi´c, V.; Umek, A. Challenges in wireless communication for connected \nsensors and wearable devices used in sport biofeedback applications. Future Gener. \nComput. Syst. 2019, 92, 582–592. \n[15] Lone, T.A.; Rashid, A.; Gupta, S.; Gupta, S.K.;  Rao, D.S.; Najim, M.; Srivastava, A.; \nKumar, A.; Umrao, L.S.; Singhal, A. Securing communication by attribute -based \nauthentication in hetnet used for medical applications. EURASIP J. Wirel. Commun. Netw. \n2020, 146, 146. \n[16] Toscano, E.; Bello, L.L. Comp arative assessments of IEEE 802.15. 4/ZigBee and \n6LoWPAN for low-power industrial WSNs in realistic scenarios. In Proceedings of the 9th \nIEEE International Workshop on Factory Communication Systems, Lemgo, Germany, 21–\n24 May 2012. \n[17] Navya, V.; Deepalakshmi, P. Threshold-based energy-efficient routing for transmission of \ncritical physiological  parameters in a wireless body area network under emergency \nscenarios. Int. J. Comput. Appl. 2021, 43, 367–376. \n[18]  Sundaresan, S.; Doss, R.; Zhou, W. RFID in h ealthcare–current trends and the future. In \nSpringer Series in Bio -/Neuroinformatics; Kasabov, N., Ed.; Springer: Berlin/Heidelberg, \nGermany, 2015; Volume 5, pp. 839–870. \n[19] Chen, X.; Zhu, H.; Geng, D.; Liu, W.; Yang, R.; Li, S. Merging RFID and blockch ain \ntechnologies to accelerate big data medical research based on physiological signals. J. \nHealthc. Eng. 2020, 2020, 2452683. \n[20] P. Kasyoka, M. Kimwele and S. Mbandu Angolo, \"Certificateless pairing -free \nauthentication scheme for wireless body area network in healthcare management system\", J. \nMed. Eng. Technol., vol. 44, no. 1, pp. 12-19, 2020. \n41 \n \n[21] T. Belkhouja, S. Sorour and M. S. Hefeida, \"Role -based hierarchical medical data \nencryption for implantable medical devices\", Proc. IEEE Global Commun. Conf. \n(GLOBECOM), pp. 1-6, Dec. 2019. \n[22] Sun, Y.; Lo, F.P.-W.; Lo, B. Security and privacy for the internet of medical things enabled \nhealthcare systems: A survey. IEEE Access 2019, 7, 183339–183355. \n[23] Davis, J. Ransomware Attacks Cost Healthcare Sector at Least $160M Since 2016. Health \nIT Security. Available online: https: //healthitsecurity.com/ \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n42 \n \nPOSTER PRESENTATION \n \nThis work is presented in  2023 Student Research and Creative Endeavor Symposium on 24 \nMarch 2023 at Purdue University Fort Wayne. \n "
  },
  {
    "source": "ECG_Analysis.pdf",
    "content": "AnalysisofECGSignalsUsingXResNet1d\nRiyazShaik\nIntroduction\nTheelectrocardiogram(abbreviatedasECGorEKG)representsanelectricaltracingoftheheart\nandis recordednon-invasivelyfromthesurfaceof thebody.[1]Thewaywemonitorand\ndiagnosecardiovasculardiseaseshaschangeddramaticallyasa resultofthedevelopmentof\nhealthcaretechnologies.Oftheseadvancements,theexaminationofElectrocardiogram(ECG)\ndataisonethatisessentialforidentifyingandassessinganumberofdisordersassociatedwith\ntheheart.Animportantresourceforthecreationandverificationofalgorithmstargetedat\naccurateECGanalysisisthePTBXLdataset[2], avastcollectionofECGrecordings.\nThisreportexploresECGsignalrecognition,analysis,andexplorationusingthelargePTBXL\ndataset.Thisdatasetprovidesa solidbasisfortrainingandassessingDeepLearningmodelsto\nidentifyminutepatternsandabnormalitiesinECGsignals.It consistsofannotatedrecordings\nfromindividualswitha rangeofheartdiseases.It seekstoclarifythemethods,strategies,and\nresourcesusedto makeuseofthisenormousdatabaseinordertocomprehendcardiachealth\nindicators,spotanomalies,andfurtherourknowledgeofcardiovasculardisorders.\nProblemStatement:\nTo implementa deeplearningmodelfordetectingandanalyzingECGsignalsandclassifythe\ngivenECGgraphintooneofthefiveclassesfromPTBXLdataset.To checktheflexibility\nwhiletestingdifferentleadsinputordataaswellprovidinghowmuchaccuracyisachievedby\nthemodel.\nDiscussion:\nThisreportpresentstheassigneddiscussion,successfulintegrationofPTBXLdataset[2] and\ndeeplearning,whichhasenormouspotentialforclinicalcardiologyapplications.DeepLearning\nmodelsforautomatedECGanalysishavethepotentialtoacceleratethediagnosisprocess,help\nidentifycardiacissuesearly,andsupportmedicalprofessionalsingivingtimelytherapies.\nThePTB-XLECGdatasetisasizablecollectionof21799clinical12-leadECGs,eachlasting10\nseconds,from18869patients.Upto twocardiologistsannotatedtherawwaveformdata,\nsometimesaddingmanyECGcommentstoeachrecord.[2]TheSCP-ECGstandardisfollowed\nbythe71distinctECGstatements,whichincludediagnostic,form,andrhythmstatements.\nWhilediscussingourmodel,theDatasetplaysadominantroletowhichwepresent5-classesto\nbegroupedinaccordancewiththeGraphicalstructureoftheECGs.The5-Classeswere;\n● NormalECG:\nTheheartbeatsbetween60and100beatsperminute(82bpm)inasteadysinusrhythm\n[5],whichisaccountedforbyNormalECG.\nAgain,Normalsinusrhythm,orNSR,isdefinedasthepresenceofaPwavewithasinus\nmorphologyprecedingeachQRScomplex.AnuprightPwaveinleadIIandabiphasic\n(upanddown)PwaveinleadV1areexamplesofsinusmorphology.\nFig-1.0:Normaladult12-leadECG[6]\n● MyocardialInfarction:\nReducedor stoppedbloodflowto a sectionof themyocardiumcausesmyocardial\ninfarction(MI),alsoreferredtoasa \"heartattack.\"Heartattackcanbe\"silent,\"going\nunnoticed,orit canbea devastatingincidentthatresultsinhemodynamicdeclineand\nabruptdeath.[8]\n\nFig-1.1:ECGsinAcuteMyocardialInfarction[7]\n● ST/TChange:\nChangesintheSTandTwavescouldbeasignofheartdiseaseoratypicalvariation.As\naresult,theclinicalsettingandtheexistenceofcomparableresultsonearlierECGsmust\nbetakenintoconsiderationwheninterpretingtheresults.\nAmonghypertensionpatients,nonspecificalterationsin theST-segmentandT-wave\n(ST-T)areamongthemostcommonECGabnormalities.[9]\nFig-1.2:NonspecificSTsegmentandTwavechanges[10]\nIt is considered,changesintheSTandT wavescouldbea signofheartdiseaseora\ntypicalvariation.\n● ConductionDisturbance:\nAconductiondisturbanceaffectstheelectricalsystemthatregulatestherhythmandpace\nofyourheart.Itissometimesreferredtoasheartblock.Thecardiacconductionsystemis\nthenamegiventothismechanism.[12]\n\nFig-1.3:OnesortofConductionabnormalities[11]\n● Hypertrophy:\nA thickerandsometimesinefficientlypumpingleftpumpingchamberin theheartis\nknownas left ventricularhypertrophy, or LVH.Theheartmuscleis occasionally\noverworkedbyconditionslikeaorticstenosisorhighbloodpressure.Theinnerwallsof\ntheheartmaythickenasa reactionto thispressureoverload.Theleftventriclemay\nbecomeweaker, stiffer, andlesselasticasa resultofthesethickerwalls,whichcould\ninhibitnormalbloodflow.[14]\nFig-1.4(a):LeftVentricularHypertrophy[15]\nRightventricularhypertrophyistheresultofthethickeningoftherightventricularwall\nasaresultofprolongedpressureoverload.WhenleadV1exhibitsanR/Sratiolargerthan\n1 andnoothercontributingfactorsarepresent,orwhentheleadV1Rwavemeasures\nmorethan7 millimetersinheight,RVHisdiagnosed.Thestrainpatternappearswhen\nthereissignificantpressureandarelativelythickrightventricularwall.[13]\n\nFig-1.4(b):RightVentricularHypertrophy(RVH)ECG[16]\nWitheachclasshavinganimmenserole,theECGgraphswereanalyzedwithcarefultraining\nandvalidation.Themodelistoestablishacertainaspectofdifferenceintheclassificationof\n\ntheseclasseswhereit aimsto differentiatetheNormalECGswiththeirregularandmost\ncommonpotentialfibrillations.\nDesignandImplementation\nThedesignwasimpliedbasedontheresnet1dmodelwithseverallayers(explainedindetailin\nstepsfor implementation).Theentirecodeis dividedinto5 parts;namelyutils,stratify,\nbasic_conv1d,xresnet1d,fastai_model_xresnet1d101.For a successfuland hasslefree\nimplementation,acertaincriteriaischosenandgivenbelow:\n● DataPreprocessing\n● FeatureExtraction\n● DeepLearningModelSelection\n● TrainingandValidation(whichisincludedwhilediscussing“ModelSelection”)\n● ModelEvaluationMetrics(whichisincludedwhilediscussing“ModelSelection”)\n● InterpretabilityandVisualization(showcasedinthe“Result”section)\nAlltheabovecriteriawereoptimallyaimedandworkedforimplementation.\nPlatform:VisualStudioCode\nModeofuseandsupport:Python,OpenCVconcepts,Numpy, Matplot,Torch,Fastai\nNow, asmentionedearlier, thecodeisdividedinto5partsinplaceofrobustimplementation.The\nstepsare discussedin an intertwinedwaywitheachstephavinga detailedstep-by-step\nprocedure.\nDataPreprocessing(stratify):\nFirstly, therawdatafromPTBXLdataset[2] needstobepreprocessed.Thisprocessingisdone\nbasedonstratifyingtechniquepresentedbytheinstitutionthatprovidedthedataset.Overall\nstrategyisasfollows:\n● StratifybasedonpatientIDstoensurepatientsarenotspreadacrossfolds\n● BalancelabelsincludingSCPcodes,gender,andagebucketsacrossfolds\n● Optionallyreservesomefoldsforhigherqualitydata\nImplementationdetails:\n1. Computea\"quality\"scoreforeachECGbasedonhumanvalidation\n2. ExtractalluniquepatientIDs\n3. ForeachpatientID:\na. Aggregatealltheirlabels(SCPcodes,gender, agebucket)\nb. TracknumberofECGsperpatient\n4. Definetargetratiosforfolds(usuallyuniform1/n_folds)\n5. Stratificationmainlogic:\na. IteratethrougheachpatientID\nb. Identifywhichfoldhasmostspaceforthispatient'slabels\nc. Assignpatient'sECGstothatfold\nd. Updatecountsandavailablespaceperfoldandperlabel\n6. Return:\na. Listoffolds,eachcontainingindexesofstratifiedpatientIDs\nb. Listoffolds,eachcontainingstratifiedpatientdata\nThestratificationloop,whichiterativelyselectspatientsandplacestheminthefoldwiththe\ngreatestamountof spaceleftfortheirsetoflabels,containstheessentiallogic.Thisis an\napplicationofk-foldsforstratifiedgroups.Distributionscanbebalancedbycountingthenumber\nofECGsperpatientandthetrackingspaceperlabel.Somefoldsmayprovidecleanerdatawith\nqualityscores.\nFeatureExtraction(utils):\nAllthefeatureextractionandotherimportantmulti-labelclassificationonECGsignalsisdonein\nthe“utils”partofthecode.LoadsthePTB-XLdataset'slabeldata(Y)andECGsignaldata(X).\nextractsCSVfileswithSCPcodesforeverysignalaswellastherawECGsignals.Belowgiven\narethedetailedexplanationofimplementedkeyfunctions:\n1. load_dataset()\na. Inputs:\ni. path:BasefolderforPTB-XLdataset\nii. sampling_rate:100Hzor500HzECGdata\nb. ReadsPTBXLdatabasecsvfile\ni. Containsmetadatalikefilename,age,sex,scp_codes\nc. Parsesscp_codesstringintodictionaryusingast.literal_eval\nd. LoadsrawECGsignaldatafor eachfileusingWFDB(WaveformDatabase\nSoftwarePackage)\ni. Loopsthroughfilenamesandreadssignal+metadata\ne. Cachesrawsignalstospeedupnextrun\nf. Returns:\ni. X:numpyarrayofrawsignaldataforeachECGrecording\nii. Y:dataframewithmetadataforeachrecording\n2. CombinesthespecificSCPcodestoprovidehigher-levellabelsforrhythm,form,and\nothercriteria.addsnewlabelcolumnstothedataframe.\ncompute_label_aggregations()\na. Goalistocreatelabelsatdifferentsemanticlevels\nb. Inputs:\ni. df:dataframewithscp_codesforeachrecording\nii. ctype:typeoflabelstocreate,e.g.\"diagnostic\",\"rhythm\"\nc. Loadsaggregationmappingfile\nd. Appliesfunctionstoaggregatescpcodesintoparticulargroups\ne. Createsnewcolumnsindfforeachlabeltype\nf. Returnsupdateddf\n3. Filtersthedatasetbasedona chosenlabeltype.Alsobinsthemultilabellabelsinto\nmulti-hotencodedlabels.Savestheencoder.\nselect_data()\na. Filterssignalsandlabelsbychosenlabeltype\nb. Onlykeepsclasseswithsufficientsamples\nc. FitsMultiLabelBinarizer()tohot-encodelabels\nd. Splitsmultilabellabelsintomulti-hot\ne. Savesthebinarizerforfutureinference\nf. Returns:\ni. X:selectedsignals\nii. Y:selectedmetadata\niii. y:multi-hotencodedlabels\niv. mlb:fittedencodingmodel\n4. StandardizestheECGsignalstohave0meanandunitvariance.Fitsscalerontraindata\nandappliesittoallsplits.\npreprocess_signals()\na. Standardizessignalstohave0mean,unitvariance\nb. Fitsscalerontrainingdata\nc. Appliessametransformationtoallsplits\nd. Savesscalerforfutureinference\ne. Returnsstandardizedsignaldataforeachsplit\n5. Findsthedecisionthresholdformodelprobabilitiesthatoptimizesa metric,suchas\nsensitivity-specificity, usingthefind_optimal_cutoff_threshold()function.\n6. Aggregatessavedfindingsfromvarioustestsintoa tabletoprovidea summaryofthe\nmodel'sperformanceusing“generate_ptbxl_summary_table()”.\nModelSelection(basic_conv1d,xresnet1d,fastai_model):\n1. BasicConv1d:\nTheprimarymodelclassthatdescribesthearchitectureofa 1Dconvolutionalneural\nnetwork.Enablestheconfigurationofaclassificationhead,batchnorm,squeeze-excite,\ndropout,numberandsizeofconvolutionlayers,activationfunctions,andpooling.\n● Layers:\na. _conv1d:A 1Dconvolutionlayerwithbatchnorm,activations,and\ndropout\nb. _fc:Alinearfullyconnectedlayerwithbatchnormandactivations\nc. AdaptiveConcatPool1d:concatenatestheoutputafterperformingadaptive\nmaxandaveragepooling.utilizedtocreatethecategorizationhead.\nd. SqueezeExcite1d:Increasesmodelaccuracybycombiningexcitationand\nsqueeze.\ne. create_head1d:After the conv layers,this functiongeneratesa\nclassificationheadwithcompletelyconnectedlayers.\n● Functions:\na. weight_init:Setsthelayers'initialweightsaccordingtotheirkind.\nb. Convandlinearlayersarereturnedindependentlybyget_layer_groups.\nbeneficialforvaryingratesoflearning.\nc. Thefinallinearclassificationlayerisreturnedbyget_output_layer.\nd. Thelastlinearclassificationlayeris configuredusingset_output_layer.\nHelpfulforfine-tuningtheparameters.\n2. XResNet1D:\nTheResidualNeuralNetwork(ResNet)architecture'sXResNet1Dmodelisa variation\ncreatedespeciallyfor one-dimensionaldata, like time series signalslike\nelectrocardiograms(ECGs)orothersequentialdata.Itsmainobjectiveistoefficiently\nlearnfeaturesfromsequentialdatafor taskssuchas anomalydetection,signal\nclassification,andpredictionbyutilizingdeeplayertopologiesandresidualconnections.\n● ResBlockClass:Describesatypeofresidualblockthatisutilizedonanetwork.\nThisblockincludesseverallayersofconvolution,layersofnormalization,and\noptionalpartslikeSelf-Attention(SA)mechanismsandSqueeze-and-Excitation\n(SE)Modules.\n● XResNet1DClass:Outlinestheprimaryarchitectureandputsinplaceaseriesof\nlayersforclassificationandregressiontasks,suchasstemlayers,ResBlocks,\npoolinglayers,andafullylinkedhead.\n● ConvClass:Outlinesa convolutionallayerthathaschoicesforvariouskernel\nsizes,stride,activationfunctions,andnormalizationstrategies.\n● AdaptiveAvgPool:Buildsa layerofadaptiveaveragepoolingforvaryinginput\nsizes.\n● MaxPoolandAvgPool:Functionsthatproducethelayersforaveragepooling\nandmaximumpooling,respectively.\n● ResBlock:Buildsaresidualblockwithidentitypathways,optionalSAmethods,\nandConvLayers.\nThe codeprovidesflexibilityin customizingvariouscomponents,activationfunctions,\nnormalizationtechniques,and the overallnetworkarchitecturedesignedfor processing\none-dimensionalsequentialdatasuchas ECGsignalanalysis.It alsoservesto definethe\nstructureandoperationsoftheXResNet1dmodel.\n3. Fastai_Model:\nUsingtheFastailibrary, thefastai_modelclasscanbeusedasa modeltoconstructa\nmachinelearningmodelfortimeseriesclassificationorregressionapplications.Offering\nanadaptableframeworkforbuildingandhoningmachinelearningmodelswithFastaiis\ntheaimofthisproject.\n● ImportingLibrariesandDependencies:\nThecodestartsbyimportinganumberoflibraries,includingPyTorch,Fastai,and\notherutilitymodulesliketimeseries_utils.Additionally, it importsparticular\nmethodsandclassesneededforvisualizing,evaluating,andtrainingmodels.\n● ClassDefinition- ClassificationModel:Thefastai_modelparticularmodelis\nbasedon thisclass.Forthe purposeof fittingthe modelandgenerating\npredictions,subclassesareanticipatedtooverrideitsfit()andpredict()functions.\n● InitializationandConfiguration:\nAnumberofattributes,suchasthemodelname,numberofclasses,inputsize,\nchannels,lossfunction,learningrate,etc.,areinitializedby theconstructor\n__init__.Timeseriesdatacategorizationandregressiontasksarethefocusofthis\nclass.\n● ‘fit()’Method:\nThisfunctionfirstpreparesthe data,thenusesTimeseriesDatasetCropsto\nconstructdatasetsandtrainthemodel.Basedonthesuppliedconfigurations,it\ndefinesthemetricsandlossfunctionsandbuildsthemodelarchitecture.\n● ‘predict()’Method:\nLoadsthetrainedmodel,preparesthedata,andprovidespredictionsbasedonthe\nsupplieddata.Thepredictionsthathavebeencombinedoverseveralinputdata\nsegmentsarereturned.\n● ModelSelectionandTraining:\na. Fastai'sLearnerclassisusedtoinitializethemodelarchitecture,whichis\nchosenbasedontheresnet1d101.\nb. TheLearnerobjectcontainsconfigurationsformetrics,lossfunctions,and\notherfeatures.\nc. To discoveranappropriatelearningrate,themodelistrainedusingthe\nfit_one_cycle()methodandthelearningratefinder(lr_find_plot()).\nThesemodelscontaintrainingandvalidationsectionsthatbasicallysplitsthedatasetinto\ntraining,validationandtestingdata.Thefoldernamed“data”consistsofallthenecessarydata\nutilizedinbuildingthismodel.Thenforth,thetrainingprocessisdonebyselectingavarying\nlearningratetoachievebetteraccuracy.\nResultsandPerformanceEvaluation\nThemodelperformedtheclassificationwithatrainingaccuracyof92.5%, allthedatapredicted\nandtheresultsgivenbythemodelin‘.csv’formatarepresentintheresultsfolder.\nTheplottedgraphofvaryinglearningrateisshownbelowinFig2.0,whichfocusesontheless\nrateachievedincaseoflossandtheninitializestrainingthemodel.\nFig-2.0:Findinglearningrate\nAgain,theplotoflossescorrespondingtothenumberofbatchesisshowninFig2.1.Thistells\nusabouttheprocessandlossfactorsputtingouttheefficiencyofit.\n\nFig2.1:Lossescorrespondingtobatchesprocessedontrainingandvalidationdata\nTheaccuracyofboththetrainingandvalidationdatacorrespondingtothenumberofepochsis\nshownin Fig2.2.It canbeobservedfromtheaccuracygraph,theaccuracyofvalidationis\ndecreasedbyasmallamountwhencomparedtotraining.\nWethoughtoftwoprobablereasonsforthiscase.Firstly, I thinkthemodelalsolearntthenoise\nandrandomfluctuationsthatarespecifictothetrainingdataset.Asaresult,theaccuracyinthe\nvalidationdatasetmightbecomparativelylesshigherthanwhatwasanticipatedinthemodel.\nSecondly, theregularizationmethodsappliedforthetrainingdatasetmightnotbeasparticularly\nactiveduringvalidation.\n\nFig2.2:Accuracyintrainingandvalidationdataset\nAllthepredictionsofallthefiveclassesaresavedin‘.npy’format.Alsothemacro_aucofeach\ndata(train,validation,test)aresavedin‘.csv’format.Macro_aucisbasicallytheaverageArea\nUnderCurve(AUC)calculatedacrossdifferentclasses.\nFromtheresultsobserved,themodelcouldsuccessfullyclassifythe5followingclasses:Normal\nECG,MyocardialInfarction,ST/TChange,ConductionDisturbance,Hypertrophy. Wearestill\nhopingtodomore,butconsideringwhatweexpectedtheresultswereprettywellderivedand\noutperformedwhatwasthoughtofthemodel’soutcome.\n\nConclusion\nAnECGcanbe usedto diagnosecoronaryheartdisease,whichincludesanginaandheart\nattacks,aswellasirregularcardiacrhythms[17]. Whileconsideringtheabovecases,adetailed\nstudywasdonetothepre-existingmodels.FromtheredataclassificationandtheECGconcept\nwere illustratedprovidingwell acknowledgedresultspreviewedfrom the existing\nliteratures[3][4]. Thosewereourmotivationto comeupwiththeprovidedmodelandhence\ntrainingadatasetwithECGs.\nMovingforwardtoourwork,wefocusedtoillustrateasmuchclarificationsofhowthedatais\nclassified.Thetrainingisdonewithmentionedclasses(5classes)andbasedonthecomparisons\nofECGgraphstheresultswereanalyzedonhowcloseitwastotheclassifieddataset.Theresults\nhelpedunderstandhoweachclassificationhasworked,basedonmatchingthecriteriaspecified\ntothedataandthisgivesoutaproperoutlooktoagoodprecision.Eveniftheconceptledtoa\ndifferentLeadgroup,suchas ourveryownreadings(smartwatch,applewatch,etc.)will\ncompareandclassifytowhichclassourECGfallsupon.\nNevertheless,afterclassificationwetendto researchmoreontoanactiveformofworkin\nanalyzingmoreclassestoourmodel.We arekeepinginmindastowhathastobefocusedin\nfutureworkstobetterourbuiltmodel.\nReference:\n1. Y. SattarandL.Chhabra,“Electrocardiogram,”2023.\nAvailable:Electrocardiogram[Accessed:10-Dec-2023]\n2. P. Wagner, N.Strodthoff, R.-D.Bousseljot,W. Samek,andT. Schaeffter, “PTB-XL,a\nlargepubliclyavailableelectrocardiographydataset.”PhysioNet,09-Nov-2022.\nAvailable:PTB-XL,a large publiclyavailableelectrocardiographydatasetv1.0.3\n[Accessed:6-Dec-2023]\n3. S. Aziz,S. Ahmed,andM.-S.Alouini,“ECG-basedmachine-learningalgorithmsfor\nheartbeatclassification,”Sci.Rep.,vol.11,no.1,pp.1–14,2021.\nAvailable:ECG-basedmachine-learningalgorithmsfor heartbeatclassification|\nScientificReports[Accessed:7-Dec-2023]\n4. N.D.Gai,“ECGbeatclassificationusingmachinelearningandpre-trainedconvolutional\nneuralnetworks,”arXiv[eess.SP],2022.\nAvailable:[2207.06408]ECGbeatclassificationusingmachinelearningandpre-trained\nconvolutionalneuralnetworks[Accessed:7-Dec-2023]\n5. “NormalECG,”Queensu.ca.[Online].\nAvailable:NormalECG. [Accessed:10-Dec-2023].\n6. D.JenkinsandS.Gerred,“Ecglibrary.com:Normaladult12-leadECG,”Ecglibrary.com.\n[Online].Available:https://ecglibrary.com/norm.php.[Accessed:11-Dec-2023].\n7. “ECGsinacutemyocardialinfarction,”ACLSMedicalTraining,28-Oct-2019.[Online].\nAvailable:ECGsinAcuteMyocardialInfarction-ACLSMedicalTraining.\n[Accessed:10-Dec-2023].\n8. N.OjhaandA.S.Dhamoon,MyocardialInfarction.StatPearlsPublishing,2023.\nAvailable:MyocardialInfarction-StatPearls-NCBIBookshelf.\n[Accessed:9-Dec-2023].\n9. H.Baoetal.,“NonspecificST-Tchangesassociatedwithunsatisfactorybloodpressure\ncontrolamongadultswithhypertensionin China:EvidencefromtheCSPPTstudy,”\nMedicine(Baltimore),vol.96,no.13,p.e6423,2017.\nAvailable:NonspecificST-T changesassociatedwithunsatisfactorybloodpressure\ncontrolamongadultswithhypertensioninChina-PMC. [Accessed:9-Dec-2023].\n10.Shade,“NonspecificST segmentand T wavechanges,”Manualof Medicine,\n18-Apr-2022.[Online].\nAvailable:NonspecificST segmentand T wavechanges- Manualof Medicine.\n[Accessed:11-Dec-2023].\n11.“Conductionabnormalities,”Default.[Online].\nAvailable:ConductionAbnormalities| CDEM. [Accessed:11-Dec-2023].\n12.“Conductiondisorders,”NHLBI,NIH.[Online].Available:Arrhythmias-Conduction\nDisorders| NHLBI,NIH. [Accessed:12-Dec-2023].\n13.L.Steven,“Rightventricularhypertrophy(RVH)ECGreview,”Healio,29-Apr-2015.\n[Online].\nAvailable:RightVentricularHypertrophy(RVH)ECGReview| Learnthe Heart.\n[Accessed:12-Dec-2023].\n14.“Leftventricularhypertrophy,”ClevelandClinic.[Online].\nAvailable:LeftVentricularHypertrophy(LVH):Causes,SymptomsandTreatment.\n[Accessed:12-Dec-2023].\n15.“Leftventricularhypertrophy,”Queensu.ca.[Online].\nAvailable:Leftventricularhypertrophy. [Accessed:10-Dec-2023].\n16.L.Steven,“Rightventricularhypertrophy(RVH)ECGreview,”Healio,29-Apr-2015.\n[Online].\nAvailable:RightVentricularHypertrophy(RVH)ECGReview| Learnthe Heart.\n[Accessed:10-Dec-2023].\n17.“ECGtest,”Gov.au.[Online].\nAvailable:ECGtest-BetterHealthChannel. [Accessed:12-Dec-2023]."
  },
  {
    "source": "2309.03897v1.pdf",
    "content": "ProPainter: Improving Propagation and Transformer for Video Inpainting\nShangchen Zhou Chongyi Li Kelvin C.K. Chan Chen Change Loy\nS-Lab, Nanyang Technological University\n{s200094, chongyi.li, chan0899, ccloy}@ntu.edu.sg\nhttps://shangchenzhou.com/projects/ProPainter\n… …\nProPainter(ours)\nE2FGVI(CVPR22)\nSTTN(ECCV20)\nFuseFormer(CVPR21)\nFGT(ECCV22)ISVI(CVPR22)\nFGVC(ECCV20)CPNet(ICCV19)STTN-480p(ECCV20)\nE2FGVI-480p(CVPR22)\nProPainter-480p(ours) 5G10G15G20GMemory\n……\nImageProp.(global)\nFeatureProp.(local)\n(a)Dual-domainPropagation (c)PerformanceGain(b)Mask-guidedSparseVideoTransformer\nSparseQuerySpace(maskedwindow)SparseKey/ValueSpace(temporal stridedwindow)\nT\n(d)Inputwithobjectmask(e)FuseFormer(f)FGT (g)E2FGVI(h)ProPainter(ours)\nFaster Slower\nFigure 1: (a) Dual-domain propagation enables more effective propagation due to its global and reliable nature. (b) Mask-\nguided sparse video Transformer achieves high efficiency by discarding unnecessary and redundant windows. (c) ProPainter\noutperforms prior methods while maintaining efficiency. (d-h) In visual comparisons with FuseFormer [22], FGT [42], and\nE2FGVI [19], our ProPainter exhibits superiority in filling complete and rich textures.\nAbstract\nFlow-based propagation and spatiotemporal Transformer\nare two mainstream mechanisms in video inpainting (VI).\nDespite the effectiveness of these components, they still suffer\nfrom some limitations that affect their performance. Previous\npropagation-based approaches are performed separately ei-\nther in the image or feature domain. Global image propaga-\ntion isolated from learning may cause spatial misalignment\ndue to inaccurate optical flow. Moreover, memory or compu-\ntational constraints limit the temporal range of feature prop-\nagation and video Transformer, preventing exploration of\ncorrespondence information from distant frames. To address\nthese issues, we propose an improved framework, called\nProPainter, which involves enhancedProPagation and an ef-\nficient Transformer. Specifically, we introduce dual-domain\npropagation that combines the advantages of image and fea-\nture warping, exploiting global correspondences reliably.\nWe also propose a mask-guided sparse video Transformer,\nwhich achieves high efficiency by discarding unnecessary\nand redundant tokens. With these components, ProPainter\noutperforms prior arts by a large margin of 1.46 dB in PSNR\nwhile maintaining appealing efficiency.\n1. Introduction\nVideo inpainting (VI) aims to fill gaps or missing re-\ngions in a video with visually consistent content while\nensuring spatial and temporal coherence. This technique\nhas broad applications, including video completion [ 10],\nobject removal [9, 37], video restoration [ 31], watermark,\nand logo removal [ 19]. VI is challenging because it re-\nquires establishing accurate correspondence across distant\nframes for information aggregation. To address this chal-\nlenge, various mechanisms have been explored, such as\n3D CNN [ 6, 11], video internal learning [ 41, 27], flow-\nguided propagation [ 37, 10, 43, 42, 19], and video Trans-\nformer [22, 42, 19]. Among these mechanisms, flow-guided\npropagation and video Transformer have become mainstream\nchoices for VI due to their promising performance.\narXiv:2309.03897v1  [cs.CV]  7 Sep 2023\nPropagation-based methods in VI can be divided into\ntwo categories: image propagation and feature propagation.\nThe former employs bidirectional global propagation in the\nimage domain with a pre-completed flow field. While this\napproach can fill the majority of holes in a corrupted video,\nit requires an additional image or video inpainting network\nafter propagation to hallucinate the remaining missing re-\ngions. This isolated two-step process can result in unpleasant\nartifacts and texture misalignment due to inaccurate flow, as\nshown in Figure 1(f). To address this issue, a recent ap-\nproach called E2FGVI [19] implements propagation in the\nfeature domain, incorporating flow completion and content\nhallucination modules in an end-to-end framework. With the\nlearnable warping module, the feature propagation module\nrelieves the pressure of having inaccurate flow. However,\nE2FGVI employs a downsampled flow field to match the\nspatial size of the feature domain, limiting the precision of\nspatial warping and the efficacy of propagation, potentially\nresulting in blurry results. Moreover, feature propagation can\nonly be performed within a short range of video sequences\ndue to memory and computational constraints, hindering\npropagation from distant frames and leading to missing tex-\nture, as shown in Figure 1(g).\nBoth image- and feature-based propagation have their\npros and cons. In this study, we carefully revisit the VI prob-\nlem and investigate the possibility of combining the strengths\nof both techniques. We demonstrate that with systematic\nredesigns and adaptation of best practices in the literature,\nwe can achieve dual-domain propagation, as illustrated in\nFigure 1(a). To achieve reliable and efficient information\npropagation across a video, we identify several essential\ncomponents: i) Efficient GPU-based propagation with relia-\nbility check – Unlike previous methods that rely on complex\nand time-consuming CPU-centric operations, such as index-\ning flow trajectories, we perform global image propagation\non GPU with flow consistency check. This implementation\ncan be inserted at the beginning of the inpainting network\nand jointly trained with the other modules. Thus, subse-\nquent modules are able to correct any propagation errors\nand benefit from the long-range correspondence information\nprovided by the global propagation, resulting in a significant\nperformance improvement. ii) Improved feature propagation\n– Our implementation of feature propagation leverages flow-\nbased deformable alignment [3], which improves robustness\nto occlusion and inaccurate flow completion compared to\nE2FGVI [19]. iii) Efficient flow completion – We design\na highly efficient recurrent network to complete flows for\ndual-domain propagation, which is over 40 times ( ∼192\nfps1) faster than SOTA method [43] while maintaining com-\nparable performance. We demonstrate that these designs are\nessential to achieve efficient propagation of global and local\ninformation without texture misalignment or blurring in the\n1Tested on a single NVIDIA Tesla V100 GPU (32G).\nfilling results. An example is shown in Figure 1(h).\nIn addition to dual-domain propagation, we introduce an\nefficient mask-guided sparse video Transformer tailored\nfor the VI task. The classic spatiotemporal Transformer\nis computationally intensive due to the quadratic number\nof interactions between video tokens, making it intractable\nfor high-resolution and long temporal-length videos. For\ninstance, contemporary Transformer-based methods, Fuse-\nFormer [22] and FGT [42], are unable to handle 480p videos\nwith a 32G GPU1 due to excessive memory demands. How-\never, we observe that the inpainting mask usually covers\nonly a small local region, such as the object area2. Moreover,\nadjacent frames contain highly redundant textures. These\nobservations suggest that spatiotemporal attention is unneces-\nsary for most unmasked areas, and it is adequate to consider\nonly alternating interval frames in attention computation.\nMotivated by these observations, we redesign the Trans-\nformer by discarding unnecessary and redundant windows\nin the query and key/value space, respectively, significantly\nreducing computational complexity and memory without\ncompromising inpainting performance.\nThe main contribution of this work is to provide a system-\natic study into the core problem of VI and offer a practical\nsolution that is both effective and efficient. Propagating in-\nformation in two distinct image and feature domains and\ncombining them in a unified framework with fast GPU im-\nplementation is new for VI task. The mask-guided sparse\nvideo Transformer also offers practical insights into design-\ning efficient spatiotemporal attention for VI task. Compared\nto the state-of-the-art methods, our model achieves superior\nperformance with a large margin of 1.46 dB in PSNR, while\nalso significantly reducing memory consumption.\n2. Related Work\nNumerous deep networks with different modules and\npropagation strategies have achieved significant success in\nvideo inpainting. These approaches can be broadly catego-\nrized into four categories:\n3D convolution. Earlier video inpainting networks typically\nemployed 3D CNNs [6, 33, 11] or temporal shift [7] to aggre-\ngate spatiotemporal information. These methods often suffer\nfrom limited receptive fields in both temporal and spatial\ndimensions and misalignment between adjacent frames. As\na result, they are less effective in exploring distant content.\nInternal learning. To fully exploit content of a video, some\nstudies [41, 27, 30] adopt internal learning to encode and\nmemorize the appearance and motion of the video through\ndeep networks. However, these methods require individual\ntraining for each test video, limiting their practical use.\nFlow-guided propagation. Optical flow [13, 18, 46] and\nhomography [17, 1] are commonly used in video inpainting\n2Object regions account for only 13.6% of the DA VIS [28] dataset.\nnetworks to align neighboring reference frames to enhance\ntemporal coherence and aggregation. However, incomplete\noptical flow may not provide valid propagation for complet-\ning missing regions. To address this issue, recent flow-based\nmethods [37, 10, 12, 43, 42] focus on first completing the\nflow and then use it as a guidance for pixel-domain propa-\ngation. This approach simplifies RGB pixel inpainting by\ncompleting a less complex flow field. However, this of-\nfline propagation is independent of the subsequent learnable\nrefinement module, making it difficult to correct content\ndistortion caused by inaccurate propagation. Inspired by\nflow-guided recurrent networks [2, 3], Li et al. [19] proposed\nan end-to-end framework that jointly learns flow completion\nand feature propagation in the downsampled feature domain.\nHowever, downsampled flow reduces its ability to provide\nspatially precise warping. To overcome this limitation, we\npropose more faithful propagation by performing both pixel\nand feature propagation with flow consistency checks.\nVideo Transformer. Attention [17, 26, 11, 18] and Trans-\nformer [40, 21, 22, 1, 19, 42] blocks adopt spatiotemporal\nattention to explore recurrent textures in a video. This en-\nables them to retrieve and aggregate tokens with similar tex-\nture or context for filling in missing regions. Liu et al. [22]\npresent a fine-grained fusion Transformer based on the soft\nsplit and composition operations, which further boosts video\ninpainting performance. However, these methods are com-\nputationally and memory intensive. To address this issue,\nsome Transformers [21, 1, 42] decouple the spatiotempo-\nral attention by performing spatial and temporal attention\nalternately, while others [19, 42] adopt window-based Trans-\nformers [23, 38] to reduce the spatial range for efficient video\nattention. However, these approaches still involve redundant\nor unnecessary tokens. Inspired by token pruning for adap-\ntive attention [ 29, 39, 25, 20, 15] in high-level tasks, our\nstudy proposes a more efficient and faster video Transformer\nwith sparse spatiotemporal attention and a largely reduced\ntoken space while maintaining inpainting performance.\nRecent studies [18, 19, 42] have demonstrated the effec-\ntiveness of combining flow-guided propagation with Trans-\nformer in VI. However, the high memory requirement of\nthe Transformer limits the propagation range during both\ntraining and inference, severely hindering the ability to prop-\nagate temporally distant content. In this paper, we also adopt\nthis combination strategy but propose a reliable propagation\nscheme, along with an efficient Transformer model that fully\nexploits the benefits of long-range propagation and atten-\ntion. This results in superior inpainting performance while\nmaintaining computational efficiency.\n3. Methodology\nGiven a masked video sequence X = {Xt ∈\nRH×W×3}T\nt=1, which has a sequence length of T, along\nwith corresponding mask sequence M = {Mt ∈\nRH×W×1}T\nt=1, the objective of video inpainting is to gen-\nerate visually consistent and coherent content within the\ncorrupted or missing regions. ProPainter, as shown in Fig-\nure 2, is composed of three key components: Recurrent Flow\nCompletion (RFC), Dual-Domain Propagation (DDP), and\nMask-guided Sparse Video Transformer (MSVT). Before\nfeeding the sequence into ProPainter, we extract the for-\nward and backward optical flows, denoted as Ff = {Ff\nt =\nFt→t+1 ∈ RH×W×2}T−1\nt=1 and Fb = {Fb\nt = Ft+1→t ∈\nRH×W×2}T−1\nt=1 from a given video X. We first use RFC\nto complete the corrupted flow fields. Guided by the com-\npleted flows, we then perform global image propagation\nand local feature propagation sequentially. Finally, we\nemploy multiple MSVT blocks to refine propagation fea-\ntures and a decoder to reconstruct the final video sequence\nˆY = {ˆYt ∈ RH×W×3}T\nt=1. We introduce the specific design\nof each component below.\n3.1. Recurrent Flow Completion\nPre-trained flow completion modules are commonly used\nin video inpainting networks [37, 10, 43, 42]. The rationale\nbehind this approach is that it is simpler to complete missing\nflow than to directly fill in complex RGB content [37]. Fur-\nthermore, using completed flow to propagate pixels reduces\nthe pressure of video inpainting and better maintains tem-\nporal coherence. E 2FGVI [19] proposes to insert the flow\ncompletion module into an end-to-end framework, which\nsimplifies the inpainting pipeline. However, flow completion\nmodules that are learned together with inpainting-oriented\nlosses can result in a suboptimal learning process and less\naccurate completed flow. Moreover, the downsampled flow\nmay limit the precision of spatial warping and the efficacy\nof propagation, which can result in blurred and incomplete\nfilling content, as shown in Figure 1(g). Therefore, an in-\ndependent flow completion model is not only important but\nalso necessary for video inpainting.\nTo maintain temporal coherence while completing flows,\nprevious methods [37, 42] adopt sliding-window-based net-\nworks to aggregate optical flow information from adjacent\nframes, which are highly correlated. However, these meth-\nods can be computationally expensive as repeated inferences\nare required in the overlapping frames. To improve efficiency\nand enhance flow coherence further, we adopt a recurrent\nnetwork [2, 3] for flow completion, which provides precise\noptical flow fields for subsequent propagation modules.\nWe complete forward and backward flows using the same\nprocess, thus we denote Ff and Fb as F for simplicity.\nWe first encode the flows Ft into a downsampled feature\nft with a downsampling ratio of 8. Next, we employ de-\nformable alignment [ 3] that is based on deformable con-\nvolution (DCN) [ 8, 45], to bidirectionally propagate the\nflow information from nearby frames for flow completion.\nFor simplicity, we only describe the backward propagation\nRecurrentFlowCompletion\n……\nMaskedFlows\n……\nCompletedFlows\n……\nImageProp.(global)\n……\nMaskedFrames\nEncoder Decoder\nFeatureProp.(local)\nMasks\nMSVTBlocks×(\nOutputFrames(Sec.3.2)(Sec.3.2)\n(Sec.3.3)\n(Sec.3.1)\n……\nFigure 2: ProPainter comprises three key components: recurrent flow completion, dual-domain propagation, and mask-guided\nsparse Transformer. First, we employ a highly efficient recurrent flow completion network to complete the corrupted flow\nfields. We then perform propagation in both image and feature domains, which are jointly trained. This approach enables us to\nexplore correspondences from both global and local temporal frames, resulting in more reliable and effective propagation. The\nsubsequent mask-guided sparse Transformer blocks refine the propagated features using spatiotemporal attention, aided by a\nsparse strategy that considers only a subset of the tokens. This enhances efficiency and reduces memory consumption, while\nmaintaining performance.\nprocess here. Taking the concatenated feature c(ft, ˆft+1),\nwhere ˆft+1 is the propagation feature of the t+1-th frame,\nas input a lightweight network with a stack of convolutions\nis employed to compute DCN offsets ot→t+1 and modula-\ntion masks mt→t+1. DCN alignment propagation can be\nexpressed as:\nˆft = R\n\u0000\nD( ˆft+1; ot→t+1, mt→t+1), ft\n\u0001\n, (1)\nwhere D(·) denotes deformable convolution, and R(·) de-\nnotes the convolution layers that fuse the aligned and current\nfeatures. In this way, information of ( t + 1)-th flow can\nbe adaptively transferred to the current t-th flow. Finally, a\ndecoder is used to reconstruct the completed flows ˆFt. For\nclarity, an illustration of deformable alignment is provided\nin the supplementary material.\n3.2. Dual-domain Propagation\nAfter completing the flow, we perform global and local\npropagation in the image and feature domains, respectively.\nWe employ distinct alignment operations and strategies for\neach domain. Both domains involve bidirectional propa-\ngation in the forward and backward directions. Here, we\nelaborate on the backward propagation since the forward\npropagation follows the same process.\nImage propagation. To maintain efficiency and simplicity,\nwe adopt flow-based warping for image propagation, along\nwith a simple reliability check strategy. This process does\nnot involve any learnable operation. In the case of a video\nsequence X with binary masksM (a pixel with value 1 repre-\nsents masked region) and completed flows ˆF, we first check\nthe validity of completed flow based on forward-backward\nconsistency error [37, 10]:\nEt→t+1\n\u0000\np\n\u0001\n=\n\r\r\r ˆFt→t+1\n\u0000\np\n\u0001\n+ ˆFt+1→t\n\u0000\np + ˆFt→t+1(p)\n\u0001\r\r\r\n2\n2\n,\n(2)\nwhere p denotes a pixel position of the current frame. Only\npixels with a small consistency error will be propagated,\ni.e., C1 : Et→t+1(p) < ϵ, where ϵ is a threshold and set to\n5. Furthermore, we only consider the masked areas of the\ncurrent frame Xt that needs to be filled, i.e.,C2 : Mt(p) = 1,\nand we only propagate the unmasked areas from neighboring\nframe Xt+1, i.e., C3 : Mt+1(p + ˆFt→t+1(p)) = 0. By\nenforcing the three constraints, a reliable propagation area\nAr is identified as:\nAr\n\u0000\np\n\u0001\n=\n(\n1 if p ∈ C1 ∩ C2 ∩ C3,\n0 otherwise. (3)\nThe process of image propagation is expressed as:\nˆXt = W\n\u0000\nXt+1, ˆFt→t+1\n\u0001\n∗ Ar + Xt ∗\n\u0000\n1 − Ar\n\u0001\n, (4)\nwhere W(·) denotes warping operation. To ensure contin-\nuous propagation, we promptly update the mask Mt of the\ncurrent frame and convert the propagated area to the un-\nmasked status by updating masks via ˆMt = Mt − Ar. After\nglobal image propagation, we obtain a partially filled video\nsequence ˆX, which greatly eases the learning process for\nsubsequent modules.\nFeature propagation. We use an image encoder with\nthe same structure as previous works [ 22, 19] to extract\nfeatures from a local sequence ˆXTl\nt=1, denoted as {et ∈\nR\nH\n4 ×W\n4 ×C}Tl\nt=1. Similar to E 2FGVI [19], we also adopt\nflow-guided deformable alignment module [ 3] for feature\npropagation, which has demonstrated remarkable benefits\nDCNMasksDCNOffsets\nWarpConvLayers\nDCNConvLayers\n!! !\"\"↓\"\"↓\n#\"$%→\" $%\"$%→\"↓ $(̂'!\"#)\nĉ!! ̂)\"$%\nPropagationFlowConditionPool\nFigure 3: Flow-guided deformable alignment is effective\nby taking reliable completed flows and mask-aware condi-\ntions. We concatenate the validated flow map, original mask,\nand updated mask into conditions to produce DCN offsets\n(residue to optical flow). A DCN is then applied to align the\npropagation feature from the previous frame. Finally, a CNN\nblock is employed to fuse the current and aligned features,\nachieving the propagation feature of the current frame.\nin various low-level video tasks [ 5, 4, 44]. Unlike the de-\nformable alignment used in Sec. 3.1 that directly learns DCN\noffsets, flow-guided deformable alignment employs the com-\npleted flow as a base offset and refines it by learning off-\nset residue. However, our design differs from E 2FGVI in\nthat we offer richer conditions for learning DCN offsets.\nAs illustrated in Figure 3, apart from the current feature\net, warped propagation feature W(ˆet+1, ˆF↓\nt→t+1), and com-\npleted flows ˆF↓\nt→t+1, we additionally introduce the flow\nvalid map Vt+1→t calculated by consistency check (Eq. 2),\nas well as the original mask M↓\nt , and updated mask ˆM↓\nt\nafter image propagation. With these conditions, a stack of\nconvolutions is employed to predict the DCN offset residue\neot→t+1 and modulation masks mt→t+1. The flow-guided\nDCN alignment propagation is expressed as:\nˆet = R\n\u0000\nD(ˆet+1; ˆF↓\nt→t+1 + eot→t+1, mt→t+1), ft\n\u0001\n, (5)\nwhere ↓ denotes downsampling. The improved reliability\nof flow and the additional awareness of mask as a condition\nmake our flow-guided deformable alignment module more\nstable to learn than previous designs [ 3, 19]. The current\nstep is able to focus more on truly challenging regions where\nflow is invalid and former image propagation is unreliable.\n3.3. Mask-Guided Sparse Video Transformer\nWhile video Transformers have achieved excellent per-\nformance in video inpainting, they can be computationally\nand memory intensive, posing a challenge to their practical\napplication. E 2FGVI and FGT have addressed this issue\nby using window-based Transformer blocks, but they still\nhave some efficiency limitations. To overcome this, we pro-\npose a novel sparse video Transformer that builds on the\nwindow-based approach. Given a video sequence feature\n0\n11110 0000000000\n!\nT\n\"!SparseMask\nMask ⁄T2\n#Feature\nT\nT\nWindowSparseTemporalSparse\nWindowExpend\nDownPooling\nSparseKey/ValueSpace\nSparseQuerySpaceQueryTokens\nKey/ValueTokens\nlocaltokens(w/expendwindow)\nglobaltokens\n…Flatten\n…\nMulti-healSelfAttention\n$\n%,'\nFigure 4: Mask-guided sparse video Transformer. To reduce\ncomputational complexity and memory usage, our mask-\nguided sparse Transformer filters out unnecessary and redun-\ndant windows in the query and key/value space, respectively,\nbefore applying self-attention. To enlarge spatial interrela-\ntion range, we also adopt the window expand strategy [38]\nand pooling global tokens [42, 19].\nEl ∈ RTl×H\n4 ×W\n4 ×C, we use the soft split operation [22] to\ngenerate patch embeddings Z ∈ RTl×M×N×Cz . We parti-\ntion Z into m × n non-overlapping windows, resulting in\npartitioned features Zw ∈ RTl×m×n×h×w×Cz , where m×n\nand h × w are the number and size of the windows, respec-\ntively. We obtain the query Q, key K, and value V from Zw\nthrough linear layers. We design sparse strategies for both\nquery and key/value spaces separately. Note that we also\napply the window expand strategy [22] and integrate global\ntokens [42] into key and value, enabling us to use a small\nwindow size of 5 × 9 in our experiments. We omit them\nfrom the following discussion since they do not affect our\nsparse strategy designs.\nSparse Query Space. We observe that mask regions of-\nten occupy only a small area of the video, such as in the\ncase of object removal in the DA VIS [28] dataset, where\nthe proportion of object regions is only 13.6%. This indi-\ncates that spatiotemporal attention may not be necessary\nfor all query windows. To exploit this observation, we se-\nlectively apply attention to query windows that intersect\nwith the mask regions. Specifically, we first use nearest\nneighbor interpolation to downsample the mask sequence\nM ∈ RTl×H×W to M↓ ∈ RTl×m×n, where m × n is the\nnumber of non-overlapping windows after partitioning. We\nthen sum it up in the temporal dimension and obtain sparse\nmask SQ ∈ Rm×n for query cubes following the equation:\nSQ = Clip\n\u0010XTl\nt=1\nM↓\nt , 1\n\u0011\n, (6)\nwhere Clip represents a clipping function that set SQ to 1 ifPTl\nt=1 M↓\nt > 0. In other words, if the query cube at a win-\ndow (i, j) has never contained any mask region in the past\nframes, then SQ(i, j) = 0, indicating that spatiotemporal\nattention within this window can be skipped.\nTable 1: Quantitative comparisons on YouTube-VOS [36] and DA VIS [28] datasets. The best and second performances are\nmarked in red and blue, respectively. E∗\nwarp denotes Ewarp (×10−3). All methods are evaluated following their default\nsettings. Since DFVI, FGVC, ISVI, and FGT involve several CPU processes, their FLOPs cannot be accurately projected.\nAccuracy Efficiency\nYouTube-VOS DA VIS FLOPs Runtime\nModels PSNR↑ SSIM↑ VFID↓ E∗warp ↓ PSNR↑ SSIM↑ VFID↓ E∗warp ↓ (10 frames) (s/frame)\nDFVI [37] 29.16 0.9429 0.066 1.651 28.81 0.9404 0.187 1.596 - 0.837\nCPNet [17] 31.58 0.9607 0.071 1.622 30.28 0.9521 0.182 1.521 1407G 0.316\nFGVC [10] 29.67 0.9403 0.064 1.163 30.80 0.9497 0.165 1.571 - 1.795\nSTTN [40] 32.34 0.9655 0.053 1.061 30.61 0.9560 0.149 1.438 1315G 0.051\nTSAM [46] 30.22 0.9468 0.070 1.014 30.67 0.9548 0.146 1.235 1001G 0.068\nFuseFormer [22] 33.32 0.9681 0.053 1.053 32.59 0.9701 0.137 1.349 1025G 0.114\nISVI [43] 30.34 0.9458 0.077 1.008 32.17 0.9588 0.189 1.291 - 1.594\nFGT [42] 32.17 0.9599 0.054 1.025 32.86 0.9650 0.129 1.323 - 1.828\nE2FGVI [19] 33.71 0.9700 0.046 1.013 33.01 0.9721 0.116 1.289 986G 0.085\nProPainter (Ours) 34.43 0.9735 0.042 0.974 34.47 0.9776 0.098 1.187 808G 0.083\nSparse Key/Value Space. Due to the highly redundant and\nrepetitive textures in adjacent frames, it is unnecessary to\ninclude all frames as key/value tokens in each Transformer\nblock. Instead, we will only include strided temporal frames\nalternately, with a temporal stride of 2 in our design. That is,\nin each odd-numbered Transformer block, only odd-number\nframes are activated to participate in self-attention with their\nkey and value, while even-number blocks include only even-\nnumber frames. By doing so, the key and value space is\nreduced by half, effectively reducing the computation and\nmemory cost of the Transformer module. After filtering out\nunnecessary and redundant windows based on our sparse\nstrategy, we perform self-attention on the remaining win-\ndows to extract refined features. These features are then\ngathered using a soft composition operation [22] for subse-\nquent modules. Experimental results suggest that our design\nsignificantly reduces the computational cost of video Trans-\nformers while maintaining performance for video inpainting.\n3.4. Training Objectives\nFlow Completion. We utilize L1 loss as the reconstruction\nloss and a second-order smoothness constraint on the flow\nfield [24] to promote the collinearity of neighboring flows\nand thus enhance the smoothness of the completed flow field.\nVideo Inpainting. We adopt L1 loss as the reconstruction\nloss for all pixels. To enhance the realistic and temporal\nconsistency of video inpainting results, we also employ an\nadversarial loss that is measured using a T-PatchGAN [ 6]\ndiscriminator. The details and formulation of these losses\nare provided in the supplementary material.\n4. Experiments\nDatasets. We use the training set of YouTube-VOS [36] with\n3471 video sequences to train our networks. Two widely-\nused test sets are adopted for evaluation: YouTube-VOS [36]\nand DA VIS [28], which consist of 508 and 90 video se-\nquences, respectively. For the DA VIS test set, following\nFuseFormer [22] and E2FGVI [19], we use 50 video clips\nfor evaluations. During training, we follow [13, 17, 22, 19]\nand generate stationary and object masks in a random fash-\nion to simulate the masks in video completion and object\nremoval tasks. As for evaluation, we adopt the stationary\nmasks provided in [19] to calculate quantitative scores, and\nthe object masks are extracted from their segmentation la-\nbels for qualitative comparisons. Video frames are sized to\n432 × 240 for training and evaluation.\nTraining Details and Metrics. We use RAFT [32] to ex-\ntract optical flow in our approach. For training the RFC\nnetwork, we set the flow sequence length to 10 and perform\ndeformable propagation on feature maps that are downsam-\npled by a factor of 8 for faster processing. We adopt 8\nTransformer blocks for the inpainting modules and use a\nlocal video sequence of length 10. The Transformer window\nsize is 5×9, and the extended size is half of the window size.\nWe train both the RFC and inpainting modules using the\nAdam [14] optimizer with a batch size of 8, setting the initial\nlearning rate to 10−4 and running 700k iterations3 for each.\nWe implement our method using the PyTorch framework and\ntrain it on 8 NVIDIA Tesla V100 (32G) GPUs.\nWe employ the widely used PSNR and SSIM metrics [35]\nto evaluate the reconstruction performance and VFID [34]\nscores to measure the perceptual similarity between input\nvideos and outputs, as used in recent video inpainting stud-\nies [22, 19]. Additionally, we report the flow warping error\nEwarp [16] to assess the temporal consistency and smooth-\nness of the resulting video sequences.\n3We set 450k training iterations for ablation study.\nMaskedFramesFuseFormerFGT E2FGVIProPainter(Ours)\nFigure 5: Qualitative comparisons on both video completion and object removal. Our ProPainter exhibits superiority in\nproducing complete and faithful textures, resulting in enhanced spatiotemporal coherence for video inpainting.\n4.1. Comparisons\nQuantitative Evaluation. We compare ProPainter with nine\nstate-of-the-art methods including DFVI [37], CPNet [17],\nFGVC [ 10], STTN [ 40], TSAM [ 46], Fuseformer [ 22],\nISVI [43], FGT [42], and E2FGVI [19] on both YouTube-\nVOS [36] and DA VIS [28]. Thanks to the efficient design,\nProPainter uses a temporal length of 20 for inference. Ta-\nble 1 shows that ProPainter outperforms other methods in all\nquantitative metrics, especially on the DA VIS dataset, where\nour method surpasses the state-of-the-art method by 1.14 dB\nin PSNR. The results suggest that our method has superior\ninpainting capability, enabling it to produce higher-quality,\nfaithful, and seamless videos.\nQualitative Evaluation. For the visual comparison, we\ncompare our method with FuseFormer [ 22], FGT [ 42],\nand E 2FGVI [ 19], which are representative methods of\nTransformer-, image propagation-, and feature propagation-\nbased approaches, respectively. Figure 5 presents four com-\nparison results for video completion and object removal. Our\nmethod uses dual-domain propagation to ensure reliable and\nlong-range propagation. It completes missing regions with\ncoherence and clear contents, while other compared methods\ntend to fail or produce unpleasant inpainting results such as\ntexture distortions and black hazy region in FGT [42] results,\nas well as artifacts in FuseFormer [22] and E2FGVI [19].\nEfficiency Comparison. Table 1 presents the efficiency\ncomparisons between all methods in terms of FLOPs and\nrunning time. The FLOPs of all methods are computed\nbased on a temporal length of 10. We consider all learn-\nTable 2: Comparisons of flow completion networks. Our net-\nwork offers a dual benefit with high accuracy and efficiency.\nEPE↓ DFVI [37] FGVC [10] FGT [42] ISVI [43] Ours\nYouTube-VOS 0.046 0.032 0.021 0.019 0.020\nDA VIS 0.107 0.082 0.052 0.051 0.051\nRuntime (s/frame) 0.130 1.125 0.312 0.231 0.005\nable modules (including the recurrent flow completion) in\nour ProPainter to calculate the FLOPs. The running time\nrecords the time of all processes in each method, including\ninpainting, as well as flow calculation and flow completion\nif involved. To keep efficiency, we use only five iterations of\nthe RAFT network to calculate optical flow.\nFlow Completion Comparisons. We compare our recurrent\nflow network with previous approaches [37, 10, 43] on both\nYouTube-VOS and DA VIS datasets. Table 2 presents the\nend-point-error (EPE) of flow completion and running time\nof each method. Our recurrent network offers a dual benefit\nwith high accuracy and efficiency. Compared to previous\nmethods, our network is approximately 40 times faster while\nmaintaining a comparable flow completion accuracy to the\nstate-of-the-art methods.\n4.2. Ablation Study\nEffectiveness of Image Propagation. Table 3 shows that\nExp. (a) experiences a significant performance drop when\nimage propagation is removed. Moreover, the model’s prop-\nagation ability is reduced without image propagation, as\npresented in Figure 7, causing it to fail to complete missing\nTable 3: Ablation study of dual-main propagation and sparse Transformer.\nExp. (a) w/o Img Prop. (b) w/ Img Prop. in FGVC (c) w/o Feat Prop. (d) w/ Feat Prop. in E2FGVI (f) Full TokensProPainter\nPSNR 33.05 32.91 33.17 33.94 34.18 34.15\nSSIM 0.9724 0.9687 0.9732 0.9756 0.9765 0.9764\nImgProp.ofFGVC ImgProp.(Ours)MaskedFrame w/ImgProp.ofFGVC(Exp.b) ProPainter(Ours)\nFigure 6: Visual comparison on image propagation methods of FGVC [10] and ours.\nMaskedFramesw/oImgProp.(Exp.a) w/ImgProp.\nFigure 7: Comparison of w/ and w/o image propagation.\ncontent with details. To verify the effectiveness of our relia-\nbility check strategy in image propagation, we replaced our\ndesign with the FGVC image propagation module in Exp.\n(b) (without retraining), resulting in a noticeable decrease in\nPSNR. This is because the FGVC image propagation method\nis prone to being affected by incorrect optical flow, leading\nto severe texture distortion that subsequent modules cannot\ncorrect. Our model can effectively aware and stop unreliable\npropagation areas using a simple reliability check via Eq.2,\nand generate more faithful inpainting results.\nEffectiveness of Feature Propagation. Similarly, we ob-\nserve a slight decrease in performance by either removing\nfeature propagation, i.e., Exp. (c), or replacing it with the\nFeature propagation of E 2FGVI, i.e., Exp. (d), indicating\nthe effectiveness of the feature propagation modules and our\nreliability mask-aware conditions. This suggests that our\ndesign, which learns reliable DCN offsets in the feature do-\nmain, can further complement and enhance the propagation\nability in the image domain.\nEffectiveness of Sparse Transformer.In theory, our strat-\negy of using masks to guide sparsity only eliminates redun-\ndant and unnecessary tokens (windows), while preserving\nessential information. This means that there should be no ad-\nverse effect on performance. To confirm this, we conducted\nExp. (d), comparing our approach to standard self-attention\nwithout sparse filtering. Our results indicate that our sparse\nTransformer block performs almost as well as the standard\none, indicating that it can achieve high efficiency without\nsacrificing performance.\n1020304050600\n300\n600\n900\nTemporal Length\nFLOPs (G)\nFuseFormer [22]FGT [42]E2FGVI [19]Ours\n240p480p720p960p05001,0001,5002,000\nSpatial Resolution\nFLOPs (G)\nFGT [42]E2FGVI [19]Ours\nFigure 8: FLOPs cures of different Transformer blocks.\nEfficiency of Sparse Transformer.In Figure 8, we compare\nthe FLOPs of different Transformer blocks with respect to\ntemporal length and spatial resolution, including those used\nin FuseFormer [22], FGT [42], and E2FGVI [19]. We use\na mask with a missing region ratio of 1/6 (higher than the\naverage object ratio of 13.6% in DA VIS) to calculate the\nFLOPs of our mask-guided sparse Transformer. The curves\nindicate that the efficiency advantage of our sparse Trans-\nformer becomes more prominent as the temporal length and\nvideo resolution increase, indicating great potential for de-\nveloping longer-range spatiotemporal attention and applying\nit to larger resolution videos.\n5. Conclusion\nThis study introduces a novel and improved video in-\npainting framework called ProPainter. It incorporates an\nenhanced dual-domain propagation and an efficient mask-\nguided sparse video Transformer. Thanks to the two mod-\nules, our ProPainter exhibits reliable and precise propagation\ncapabilities over long distances, significantly improving the\nperformance of video inpainting while maintaining high effi-\nciency in terms of running time and computational complex-\nity. We believe that the designs in ProPainter will provide\nvaluable insights to the video inpainting community.\nAcknowledgement. This study is supported under the\nRIE2020 Industry Alignment Fund Industry Collaboration\nProjects (IAF-ICP) Funding Initiative, as well as cash and\nin-kind contribution from the industry partner(s).\nReferences\n[1] Jiayin Cai, Changlin Li, Xin Tao, Chun Yuan, and Yu-Wing\nTai. DeViT: Deformed vision transformers in video inpainting.\nIn ACM MM, 2022. 2, 3\n[2] Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and\nChen Change Loy. BasicVSR: The search for essential com-\nponents in video super-resolution and beyond. InCVPR, 2021.\n3\n[3] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and\nChen Change Loy. BasicVSR++: Improving video super-\nresolution with enhanced propagation and alignment. In\nCVPR, 2022. 2, 3, 4, 5\n[4] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and\nChen Change Loy. Investigating tradeoffs in real-world video\nsuper-resolution. In CVPR, 2022. 5\n[5] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and\nChen Change Loy. On the generalization of BasicVSR++\nto video deblurring and denoising. arXiv preprint\narXiv:2204.05308, 2022. 5\n[6] Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Winston\nHsu. Free-form video inpainting with 3d gated convolution\nand temporal patchgan. In ICCV, 2019. 1, 2, 6, 12\n[7] Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, and Winston\nHsu. Learnable gated temporal shift module for deep video\ninpainting. In BMVC, 2019. 2\n[8] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang,\nHan Hu, and Yichen Wei. Deformable convolutional networks.\nIn ICCV, 2017. 3\n[9] Mounira Ebdelli, Olivier Le Meur, and Christine Guillemot.\nVideo inpainting with short-term windows: application to\nobject removal and error concealment. IEEE TIP. 1\n[10] Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf.\nFlow-edge guided video completion. In ECCV, 2020. 1, 3, 4,\n6, 7, 8, 12, 14\n[11] Yuan-Ting Hu, Heng Wang, Nicolas Ballas, Kristen Grauman,\nand Alexander G Schwing. Proposal-based video completion.\nIn ECCV, 2020. 1, 2, 3\n[12] Lei Ke, Yu-Wing Tai, and Chi-Keung Tang. Occlusion-aware\nvideo object inpainting. In ICCV, 2021. 3\n[13] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So\nKweon. Deep video inpainting. In CVPR, 2019. 2, 6\n[14] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2014. 6\n[15] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei\nNiu, Mengshu Sun, Bin Ren, Minghai Qin, Hao Tang, and\nYanzhi Wang. SpViT: Enabling faster vision transformers via\nsoft token pruning. In ECCV, 2022. 3\n[16] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman,\nErsin Yumer, and Ming-Hsuan Yang. Learning blind video\ntemporal consistency. In ECCV, 2018. 6\n[17] Sungho Lee, Seoung Wug Oh, DaeYeun Won, and Seon Joo\nKim. Copy-and-paste networks for deep video inpainting. In\nICCV, 2019. 2, 3, 6, 7\n[18] Ang Li, Shanshan Zhao, Xingjun Ma, Mingming Gong,\nJianzhong Qi, Rui Zhang, Dacheng Tao, and Ramamoha-\nnarao Kotagiri. Short-term and long-term context aggregation\nnetwork for video inpainting. In ECCV, 2020. 2, 3\n[19] Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and Ming-\nMing Cheng. Towards an end-to-end framework for flow-\nguided video inpainting. In CVPR, 2022. 1, 2, 3, 4, 5, 6, 7, 8,\n13, 15\n[20] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue\nWang, and Pengtao Xie. Not all patches are what you need:\nExpediting vision transformers via token reorganizations. In\nICLR, 2022. 3\n[21] Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei\nLu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hong-\nsheng Li. Decoupled spatial-temporal transformer for video\ninpainting. arXiv preprint arXiv:2104.06637, 2021. 3\n[22] Rui Liu, Hanming Deng, Yangyi Huang, Xiaoyu Shi, Lewei\nLu, Wenxiu Sun, Xiaogang Wang, Jifeng Dai, and Hongsheng\nLi. Fuseformer: Fusing fine-grained information in transform-\ners for video inpainting. In ICCV, 2021. 1, 2, 3, 4, 5, 6, 7, 8,\n13, 15\n[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin Transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 3\n[24] Simon Meister, Junhwa Hur, and Stefan Roth. Unflow: Unsu-\npervised learning of optical flow with a bidirectional census\nloss. In AAAI, 2018. 6, 11\n[25] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan,\nZuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. AdaViT:\nAdaptive vision transformers for efficient image recognition.\nIn CVPR, 2022. 3\n[26] Seoung Wug Oh, Sungho Lee, Joon-Young Lee, and Seon Joo\nKim. Onion-peel networks for deep video completion. In\nICCV, 2019. 3\n[27] Hao Ouyang, Tengfei Wang, and Qifeng Chen. Internal video\ninpainting by implicit long-range propagation. In ICCV, 2021.\n1, 2\n[28] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc\nVan Gool, Markus Gross, and Alexander Sorkine-Hornung.\nA benchmark dataset and evaluation methodology for video\nobject segmentation. In CVPR, 2016. 2, 5, 6, 7, 12, 13, 15, 16\n[29] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie\nZhou, and Cho-Jui Hsieh. DynamicViT: Efficient vision\ntransformers with dynamic token sparsification. In NeurIPS,\n2021. 3\n[30] Jingjing Ren, Qingqing Zheng, Yuanyuan Zhao, Xuemiao\nXu, and Chen Li. DLFormer: Discrete latent transformer for\nvideo inpainting. In CVPR, 2022. 2\n[31] Nick C Tang, Chiou-Ting Hsu, Chih-Wen Su, Timothy K Shih,\nand Hong-Yuan Mark Liao. Video inpainting on digitized\nvintage films via maintaining spatiotemporal continuity.IEEE\nTMM. 1\n[32] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow. In ECCV, 2020. 6\n[33] Chuan Wang, Haibin Huang, Xiaoguang Han, and Jue Wang.\nVideo inpainting by jointly learning temporal structure and\nspatial details. In AAAI, 2019. 2\n[34] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu,\nAndrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-video\nsynthesis. In NeurIPS, 2018. 6\n[35] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. TIP, 2004. 6\n[36] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang,\nDingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen,\nand Thomas Huang. YouTube-VOS: Sequence-to-sequence\nvideo object segmentation. In ECCV, 2018. 6, 7, 13, 15\n[37] Rui Xu, Xiaoxiao Li, Bolei Zhou, and Chen Change Loy.\nDeep flow-guided video inpainting. In CVPR, 2019. 1, 3, 4,\n6, 7\n[38] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai,\nBin Xiao, Lu Yuan, and Jianfeng Gao. Focal attention for\nlong-range interactions in vision transformers. In NeurIPS,\n2021. 3, 5\n[39] Hongxu Yin, Arash Vahdat, Jose M Alvarez, Arun Mallya,\nJan Kautz, and Pavlo Molchanov. A-ViT: Adaptive tokens for\nefficient vision transformer. In CVPR, 2022. 3\n[40] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning\njoint spatial-temporal transformations for video inpainting. In\nECCV, 2020. 3, 6, 7, 13\n[41] Haotian Zhang, Long Mai, Ning Xu, Zhaowen Wang, John\nCollomosse, and Hailin Jin. An internal learning approach to\nvideo inpainting. In ICCV, 2019. 1, 2\n[42] Kaidong Zhang, Jingjing Fu, and Dong Liu. Flow-guided\ntransformer for video inpainting. In ECCV, 2022. 1, 2, 3, 5,\n6, 7, 8, 12, 13, 14, 15\n[43] Kaidong Zhang, Jingjing Fu, and Dong Liu. Inertia-guided\nflow completion and style fusion for video inpainting. In\nCVPR, 2022. 1, 2, 3, 6, 7, 12, 14\n[44] Shangchen Zhou, Jiawei Zhang, Jinshan Pan, Haozhe Xie,\nWangmeng Zuo, and Jimmy Ren. Spatio-temporal filter adap-\ntive network for video deblurring. In ICCV, 2019. 5\n[45] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. De-\nformable convnets v2: More deformable, better results. In\nCVPR, 2019. 3\n[46] Xueyan Zou, Linjie Yang, Ding Liu, and Yong Jae Lee. Pro-\ngressive temporal feature alignment network for video in-\npainting. In CVPR, 2021. 2, 6, 7, 13\nProPainter: Improving Propagation and Transformer for Video Inpainting\n– Supplementary Materials –\nIn this supplementary materials, we provide additional details, further discussions, and more results to supplement the main\npaper. In Sec. A, we present the architecture details and loss functions of our proposed ProPainter. In Sec. B, we provide\nin-depth analysis of the performance improvement achieved by our method and highlight its advantages. Sec. C contains more\nquantitative evaluations and visual comparisons.\nA. Architecture and Loss Details\nA.1. Architecture\nOur network adopts two distinct deformable alignment modules in the recurrent flow completion network (RFC) and feature\npropagation, respectively. To provide further clarity, we present a detailed illustration of the former alignment module (w/o\nflow guided) in Figure 9, which can be easily compared with the latter alignment module (w/ flow guided) depicted in Figure 3\nof the main paper. There are two main differences between the two modules: 1) different condition pools were employed to\npredict the parameters of the deformable convolutional networks (DCN); 2) the former predicts the DCN offsets directly, while\nthe latter uses optical flow as the base offset of DCN and predicts the residual offsets to the flow fields.\nDCNMasksDCNOffsets\nConvLayers\nDCNConvLayersc\nConditionPool\n!!\n !\"!\"#\n!\"! !\"!\"#\nFigure 9: An illustration of deformable alignment module that is adopted in the recurrent flow completion network.\nA.2. Loss Functions\nLoss Functions of RFC network. To train the recurrent flow completion network (RFC), we utilize two losses. The first one\nis the reconstruction loss that is applied to both valid and invalid regions, as depicted in the following equation:\nLflow\nrec =\n\r\r\rMt ⊙ ( ˆFt − Ft)\n\r\r\r\n1\n∥Mt∥1\n+\n\r\r\r(1 − Mt) ⊙ ( ˆFt − Ft)\n\r\r\r\n1\n∥1 − Mt∥1\n, (7)\nwhere ⊙ denotes the dot product. The second one is second-order smooth loss [24] that encourages the smooth and coherent\ncompleted flow fields, which is a critical property for the subsequent propagation modules. The loss can be expressed as:\nLflow\nsmooth =\n\r\r\r△ ˆFt\n\r\r\r\n1\n, (8)\nwhere △ denotes the divergence operator. The overall loss function of RFC is: Lflow = α1Lflow\nrec + α2Lflow\nsmooth, where we set\nα1 = 1, α2 = 0.5 in our experiments.\nLoss Functions of ProPainter. Our ProPainter is trained using two types of loss. For reconstruction loss, we use L1 loss to\nmeasure the distance between output video sequence ˆY and ground-truth one Y :\nLrec =\n\r\r\rˆYt − Yt\n\r\r\r\n1\n. (9)\nFurthermore, we introduce an adversarial training procedure with a T-PatchGAN based discriminatorD [6] to enhance the\nquality and coherence of generated videos by differentiating between real and reconstructed videos:\nLD = EY\nh\nlogD(Y)\ni\n+ EˆY\nh\n1 − logD(ˆY)\ni\n. (10)\nFor the generator, the GAN loss is formulated as:\nLG = −EˆY\nh\nlogD(ˆY)\ni\n. (11)\nThus, the objective of ProPainter learning is: Linpaint = λ1Lrec + λ2LG, where we set λ1 = 1, λ2 = 0.01.\nB. More Discussions\nAs indicated in Table 1 of the main paper, our proposed ProPainter outperforms the state-of-the-art networks by a large\nmargin on all quantitative metrics, especially on the DA VIS [28] dataset. In this section, we explore the primary factor that\ncontributes to these remarkable performance gains and discuss the situations in which our approach has a competitive edge.\nB.1. Factor Behind Improved Performance\nOur proposed ProPainter benefits greatly from global image propagation, which significantly reduces the difficulty of\nlearning for subsequent modules. As shown in Figure 10, image propagation has filled the majority of the masks and even\nentirely completed masked regions. This means that modules following image propagation only need to refine and complement\nthe completed contents of image propagation instead of learning the entire inpainting process. Our method differs from\nprevious approaches [10, 42, 43] in several aspects: 1) In contrast to earlier image propagation methods that are independent of\nnetwork training, which prevent the network from correcting propagation errors, our proposed image propagation is involved\nin the model training, enabling subsequent models to fix any texture misalignment or artifacts caused by image propagation; 2)\nWe employ a more reliable propagation strategy, which is compared in Figure 6 of the main paper; 3) Unlike previous methods\nthat are implemented on the CPU and involved some complex and time-consuming processes, such as indexing pixel-wise\nflow trajectories and Poisson blending, we implement a more efficient image propagation with GPU acceleration.\nT\nInputImg.Prop.\nInputImg.Prop.\nFigure 10: The initial results and updated masks after our global image propagation. Image propagation shows effective to fill\nmost or entire masked regions, which significantly alleviates the learning difficulty experienced by video inpainting networks.\nB.2. Motion Distribution\nIn the main paper, Table 1 shows that ProPainter’s performance improvement is more noticeable on the DA VIS [28] dataset\nthan on the YouTube-VOS [36] dataset. Our ablation study and analysis in the main paper attribute the performance gains\nprimarily to the design of dual-domain propagation, which relies on motion flow fields to propagate information across videos.\nHowever, we have observed that many videos in the YouTube-VOS dataset have almost stationary scenes without motion,\nwhich limits the effectiveness of our dual-domain propagation module. Moreover, we have analyzed the motion magnitude\ndistribution on both datasets and found that the YouTube-VOS dataset contains a greater proportion of regions with small\nmotion, as presented in Figure 11.\n0 2 4 6 8 10 12 14\nMotion Magnitude\nYouTube-VOS\nDAVIS\nFigure 11: Motion magnitude distribution on YouTube-VOS [36] and DA VIS [28] datasets.\nC. More Results\nC.1. Quantitative Evaluation on 480p Videos\nTable 4 presents a quantitative comparison on the DA VIS [28] dataset with 480p (864 × 480) videos. The comparison only\nincludes STTN [40] and E2FGVI [19], since other methods require memory demands exceeding 32G (such as TSAM [46],\nFuseFormer [22], and FGT [ 42]) or excessively long time for inference on a 480p video. Runtimes are measured on an\nNVIDIA Tesla V100 (32G) GPU. This comparison suggests that our method exhibits benefits in terms of both accuracy and\nefficiency even at a high resolution.\nTable 4: Quantitative comparisons on DA VIS [28] dataset with 480p (864 × 480) videos.\nPSNR↑ SSIM↑ VFID↓ Runtime (s/frame)↓\nSTTN [40] 30.72 0.9534 0.055 0.262\nE2FGVI [19] 32.98 0.9693 0.041 0.332\nProPainter (Ours) 33.81 0.9739 0.035 0.249\nC.2. Qualitative Comparisons on Flow Completion\nIn Figure 12, we provide a visual comparison of flow completion performance between our recurrent flow completion\nnetwork and previous methods, including FGVC [ 10], FGT [42], and ISVI [43]. The results show that our recurrent flow\ncompletion network outperforms other methods in producing complete and accurate flow fields. As a result, the subsequent\ndual-domain propagation module can rely more on accurate optical flows, leading to a more reliable and precise propagation\nin later stages.\nMaskedFlowFGVCFGT ISVI RFC(Ours)\nFigure 12: Qualitative comparisons of flow completion. Our recurrent flow completion network exhibits superiority in\ngenerating complete and faithful flow fields, thereby facilitating more precise and reliable propagation for ProPainter.\nC.3. Qualitative Comparisons\nIn this section, we provide additional visual comparisons of our method with the state-of-the-art methods, including\nFuseFormer [22], FGT [42], and E2FGVI [19]. Figures 13 and 14 present the comparisons of video completion performance\non the YouTube-VOS [36] and DA VIS [28] datasets, respectively.\nMaskedFramesFuseFormerFGTE2FGVIProPainter(Ours)\nReduced\nFigure 13: Qualitative comparisons on YouTube-VOS [36] dataset. Our ProPainter exhibits superiority in producing complete\nand faithful textures, resulting in enhanced spatiotemporal coherence for video inpainting. (Zoom in for best view.)\nMaskedFramesFuseFormerFGTE2FGVIProPainter(Ours)\nReduced\nFigure 14: Qualitative comparisons on DA VIS [28] dataset. Our ProPainter exhibits superiority in producing complete and\nfaithful textures, resulting in enhanced spatiotemporal coherence for video inpainting. (Zoom in for best view.)\nFurthermore, our [project page] provides a video demo that showcases some results of object removal, along with an\ninteractive demo using ProPainter. This demo incorporates a video instance segmentation network and enables users to select\nand remove specific objects from the video. A screenshot of this demo is presented in Figure 15.\nFigure 15: A screenshot of the interactive ProPainter demo."
  },
  {
    "source": "2407.13078v1.pdf",
    "content": "Enhancing Temporal Action Localization:\nAdvanced S6 Modeling with Recurrent Mechanism\nSangyoun Lee1, Juho Jung2, Changdae Oh3, Sunghee Yun4\nSogang University1, Sungkyunkwan University2, University of Wisconsin–Madison3, Erudio Bio4\nleesy0882@sogang.ac.kr1, jhjeon9@g.skku.edu2, changdae.oh@wisc.edu3, sunghee.yun@erudio.bio4\nAbstract\nTemporal Action Localization (TAL) is a critical task in\nvideo analysis, identifying precise start and end times of\nactions. Existing methods like CNNs, RNNs, GCNs, and\nTransformers have limitations in capturing long-range de-\npendencies and temporal causality. To address these chal-\nlenges, we propose a novel TAL architecture leveraging\nthe Selective State Space Model (S6). Our approach in-\ntegrates the Feature Aggregated Bi-S6 block, Dual Bi-S6\nstructure, and a recurrent mechanism to enhance tempo-\nral and channel-wise dependency modeling without increas-\ning parameter complexity. Extensive experiments on bench-\nmark datasets demonstrate state-of-the-art results with mAP\nscores of 74.2% on THUMOS-14, 42.9% on ActivityNet,\n29.6% on FineAction, and 45.8% on HACS. Ablation stud-\nies validate our method’s effectiveness, showing that the\nDual structure in the Stem module and the recurrent mech-\nanism outperform traditional approaches. Our findings\ndemonstrate the potential of S6-based models in TAL tasks,\npaving the way for future research. Our code is available at\nhttps://github.com/lsy0882/RDFA-S6.\n1. Introduction\nTemporal Action Localization (TAL) is a crucial video\nanalysis task that identifies the precise start and end times\nof actions in videos. As video content becomes increasingly\ncomplex and abundant, accurate TAL methods are essential\nfor effectively capturing and analyzing meaningful actions\nin applications like sports analytics, surveillance, and inter-\nactive media [4, 15, 23, 43]. However, significant challenges\nremain in TAL, particularly in effectively capturing long-\nrange dependencies and temporal causality in video data.\nTraditional approaches to TAL, including CNNs, RNNs,\nGCNs, and Transformers, each bring unique strengths but\nalso have inherent limitations. CNNs are effective at captur-\ning spatial features but struggle with long-range dependen-\ncies due to limited receptive fields [31]. RNNs can model\ntemporal sequences but face challenges such as vanishing\ngradients, which hinder their ability to capture long-term\ndependencies [10, 26]. GCNs are powerful for relational\ndata but are not inherently designed for sequential temporal\ndata [17]. Transformers have revolutionized TAL with their\nability to model global context using self-attention mecha-\nnisms [2, 32]. However, their reliance on attention scores\nto capture relationships within a sequence does not inher-\nently account for the temporal causality and history of visual\nelements over time. This limitation makes Transformers\nless optimal for tasks requiring precise temporal causality,\nsuch as TAL, where understanding the sequential nature and\ndependencies of actions is crucial [11, 13].\nThe State Space Model (SSM) [11, 13] has emerged as a\npromising alternative for sequence modeling by addressing\nthe limitations of traditional methods, especially in capturing\ntemporal causality. Within the SSM framework, the Selec-\ntive State Space Model (S6) [11] stands out for TAL tasks\ndue to its ability to maintain and leverage historical context\nthrough its selection mechanism and gating operation. These\nproperties enable S6 to dynamically adjust the influence\nof new inputs—specifically, the spatiotemporal feature vec-\ntors extracted from the current video clip—ensuring that the\nmodel retains and utilizes critical temporal information while\nintegrating new data. This dynamic adjustment and selec-\ntive retention enable S6 to capture long-range dependencies\nand temporal causality effectively, providing understanding\nof action sequences essential for accurately pinpointing the\nstart and end times of actions in TAL.\nActionMamba [7], an S6-based TAL method, has demon-\nstrated that S6-based method can surpass Transformers\nin sequence modeling by replacing Transformer blocks\nwith S6 blocks. ActionMamba simply substitutes the\nTransformer-based blocks for sequence modeling in the Ac-\ntionFormer [42] architecture with S6-based blocks. The S6\nblocks use a bi-directional processing approach [45] and\nincorporate weight sharing between networks operating in\neach direction. However, this study lacks a thoughtful de-\nsign focused on effective TAL methods, instead offering\na straightforward replacement of Transformer blocks with\n1\narXiv:2407.13078v1  [cs.CV]  18 Jul 2024\nslightly enhanced S6 ones. While ActionMamba highlights\nthe potential for S6-based sequence modeling to outperform\nTransformer-based approaches, it falls short of fully explor-\ning this potential or providing clear guidelines for leveraging\nS6 effectively in TAL tasks.\nOur research aims to explore the potential of S6-based\nTAL methods by building on insights from previous studies\non CNNs, RNNs, GCNs, and Transformers [10,17,26,31,32].\nWe propose a novel architecture that leverages the strengths\nof these traditional models while capitalizing on the unique\ncapabilities of S6.\nThis paper makes the following contributions to the field\nof TAL:\n1. Advanced Dependency Modeling with S6: We con-\nduct a pioneering exploration of S6’s potential in TAL\ntasks, particularly focusing on its dependency modeling\ncapabilities. By introducing an advanced dependency\nmodeling technique based on the Feature Aggregated\nBi-S6 (FA-Bi-S6) block design and the Dual Bi-S6\nstructure, we enable robust and effective modeling of\ndependencies within video sequences. The FA-Bi-S6\nblock employs multiple Conv1D layers with different\nkernel sizes to capture various granularities of tempo-\nral and channel-wise features, while the Dual Bi-S6\nstructure processes features along both the temporal\nand channel dimensions to enhance the integration of\nspatiotemporal dependencies. This approach provides\ndirection for TAL modeling, enabling more effective\nutilization of S6 in this domain.\n2. Efficiency through Recurrent Mechanism: Our study\nreveals that using a recurrent mechanism to repeatedly\napply a single S6-based model outperforms the tradi-\ntional approach of stacking multiple blocks. This re-\ncurrent application enhances the model’s performance\nwithout increasing the number of parameters, providing\nan effective solution for improving TAL models.\n3. State-of-the-Art Performance : We achieve state-\nof-the-art (SOTA) results across multiple benchmark\ndatasets, including THUMOS-14 [15], ActivityNet [4],\nFineAction [23], and HACS [43]. Our ablation stud-\nies analyze the effectiveness of each component of our\nproposed architecture, confirming the performance im-\nprovements brought by our method.\n2. Related works\nConvolutional Neural Networks (CNNs) Early TAL re-\nsearch used 2D CNNs to process spatial information, with\ninitial attempts like FV-DTF [25] combining spatial and\ntemporal data but handling them separately. The introduc-\ntion of 3D CNNs, as seen in CDC [29], marked a signifi-\ncant advancement by capturing spatiotemporal features with\nthree-dimensional convolution kernels. However, temporal\nresolution loss inherent in traditional 3D CNNs was still\na challenge to conquer. To cope with this, methods such\nas TPC [37] and FSN [38] aimed to balance spatial and\ntemporal feature processing. GTAN [24] and PBRNet [20]\nfurther optimized temporal intervals and hierarchical fea-\nture extraction. TPC maintained temporal receptive fields\nwhile downsampling spatial fields, and FSN captured finer-\ngrained dependencies by sequentially processing spatial and\ntemporal features.\nOur FA-Bi-S6 block builds on these advances by incorpo-\nrating multiple Conv1D layers with varying kernel sizes in\nparallel to capture a wide range of local contexts. The result-\ning feature map is processed bi-directionally by the Bi-S6\nnetwork, enhancing the model’s ability to capture complex\ndynamics effectively.\nRecurrent Neural Networks (RNNs) To address the tem-\nporal challenges that CNNs alone couldn’t solve, RNNs\nwere integrated into TAL frameworks. Early efforts like\nPSDF [40] and advancements such as AS [1] used RNNs to\nenhance temporal context modeling from dense trajectories\nand refine spatial features for detailed analysis. More sophis-\nticated integrations followed, such as GRU-Split [16], which\nemployed GRUs to refine action boundaries and probabili-\nties. However, RNNs introduced challenges like managing\nlong sequences and vanishing gradients. RCL [34] addressed\nthese issues by using a recurrent module to dynamically ad-\njust action segment predictions.\nOur research transcends the limitations of CNNs and\nRNNs by incorporating a recurrent mechanism within our\nS6-based architecture. This mechanism, integrated with\nour Backbone’s Stem module, enhances temporal context\nmodeling using the efficiency and precision of state space\nmodels.\nGraph Convolutional Networks (GCNs) The limitations\nof RNNs led to the exploration of GCNs in the TAL domain.\nGCNs structure video data as graphs, with nodes represent-\ning spatiotemporal features and edges defining their relation-\nships, allowing for more comprehensive modeling of tem-\nporal dependencies. A notable advancement, P-GCN [41],\nexpanded the range of dependencies that could be modeled\nbut faced challenges in scalability and efficiency due to com-\nputational overhead. G-TAD [36] addressed these issues\nwith a dual-stream graph convolution framework, efficiently\ncapturing both fixed and adaptive temporal dependencies.\nBuilding on GCN insights, we developed the Dual Bi-S6\nstructure, integrating the TFA-Bi-S6 and CFA-Bi-S6 blocks.\nTFA-Bi-S6 captures temporal dependencies, while CFA-Bi-\nS6 handles dependencies between spatiotemporal features by\nfocusing on the channel dimension. This combined approach\nenhances the robustness and accuracy of TAL by effectively\nmodeling both temporal and channel-wise contexts.\nTransformers The limitations of GCNs in handling ex-\ntensive temporal dependencies led to the adoption of\n2\nNeckd\nNeck2\nNeck1\nHeadsd\nHeads2\nHeads1\n𝑶𝒖𝒕𝒑𝒖𝒕𝒅\n𝑩𝒓𝒂𝒏\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝑶𝒖𝒕𝒑𝒖𝒕𝒅\n𝑵𝒆𝒄𝒌\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\nPretrained\nvideo \nencoder\nBackbone\n𝑿\nሾ 𝐵,𝐶௜௡,𝐿ሿ\n𝑪𝒍𝒊𝒑𝒔\nሾ 𝐿,𝑇,𝐻,𝑊ሿ\nFreeze\nInternVideo2\n6B / 1B\n𝑺𝒄𝒐𝒓𝒆𝒔 𝒄𝒍𝒂𝒔𝒔\n  𝒑𝒓𝒆𝒅\n㏝𝑻 𝒔𝒕𝒂𝒓𝒕,𝑻 𝒆𝒏𝒅㏞\n𝑺𝒄𝒐𝒓𝒆𝒔 𝒄𝒍𝒂𝒔𝒔\n  𝒑𝒓𝒆𝒅\n㏝𝑻 𝒔𝒕𝒂𝒓𝒕,𝑻 𝒆𝒏𝒅㏞\n𝑺𝒄𝒐𝒓𝒆𝒔 𝒄𝒍𝒂𝒔𝒔\n  𝒑𝒓𝒆𝒅\n㏝𝑻 𝒔𝒕𝒂𝒓𝒕,𝑻 𝒆𝒏𝒅㏞\nEmbedding\nmodule\nStem\nmodule\nBranch\nmodule\nLayerNorm\n㏙C㏚\nConv1D\n(𝑪𝒆𝒎𝒃,𝑪𝒄𝒍𝒂𝒔𝒔㏚\nConv1D\n(𝑪𝒆𝒎𝒃,𝟐㏚\nConv1D\n(𝑪𝒆𝒎𝒃,𝑪𝒆𝒎𝒃㏚\nConv1D \n(𝑪𝒆𝒎𝒃,𝑪𝒆𝒎𝒃㏚\nx2\nx2\nScale \nReLU\nLN㏙C㏚ \nReLU\nLN㏙C㏚ \nReLU\nNM\nS\n(a)\n𝑪𝒍𝒊𝒑𝟏\nሾ 𝑇,𝐻,𝑊ሿ\n𝑻\n𝑪𝒍𝒊𝒑𝟐\nሾ 𝑇,𝐻,𝑊ሿ\n𝑪𝒍𝒊𝒑𝑳\nሾ 𝑇,𝐻,𝑊ሿ\nPretrained\nvideo \nencoder\nPretrained\nvideo \nencoder\nPretrained\nvideo \nencoder\n𝑪𝒊𝒏 𝑪𝒆𝒎𝒃\nSpatio㎿tem\nporal\nfeature extract\n㏙ 𝑪𝒊𝒏\n㏚\nEm\nbedding\nm\nodule\n𝑪𝒆𝒎𝒃 𝑪𝒆𝒎𝒃\n𝑪𝒆𝒎𝒃\nFeature\nAggregation\nDual㎿path\nProcessing\nVideo\nSequence\nTemporal\nBi㎿S6\n㏙Inter clip㏚\n𝑪𝒆𝒎𝒃\n𝑪𝒆𝒎𝒃\n𝑪𝒆𝒎𝒃\nChannel\nBi㎿S6\n㏙Intra clip㏚\nShared\nShared\n𝑪𝒆𝒎𝒃\nAdd\nFeature\nAggregation\nRepeat 㐱 𝒓㐲\nStem module\n(b)\nFigure 1. Illustration of the proposed architecture and its components. (a) The architecture overview, which consists of four main\nparts: Pretrained video encoder, Backbone, Neck, and Heads (Action classification head and Temporal boundary regression head). (b) The\noverview of the proposed methods, highlighting the Stem module with an orange shaded area. The Stem module consists of three parts:\nDual-path processing (Dual Bi-S6 Structure), Feature Aggregation & Temporal/Channel Bi-S6 (Feature Aggregated Bi-S6 Block Design),\nand the repeat processing with shared networks (Recurrent Mechanism).\nTransformer-based models in TAL. Transformers use self-\nattention to extend temporal dependencies beyond GCN\nconstraints. TRA [44] used variable temporal boundary pro-\nposals with multi-head self-attention for flexible temporal\nmodeling, though it faced challenges in maintaining tempo-\nral causality over long sequences. ActionFormer [42] im-\nproved on this by using local self-attention and a multiscale\nfeature pyramid to capture various temporal resolutions, but\nit still struggled with capturing long-range dependencies and\nmaintaining precise temporal causality.\nTo address these issues, we introduced the S6 network\ninto our TAL system. The S6 network uses selective mecha-\nnisms and gating functions to modulate the impact of each\ntime step’s spatiotemporal features. This approach allows\nS6 to preserve critical historical information while integrat-\ning new spatiotemporal features, effectively capturing long-\nrange dependencies and temporal causality. By leveraging\nthese capabilities, S6 enhances the accuracy of feature ex-\ntraction and action localization, addressing the limitations of\nTransformer-based models in TAL.\n3. Proposed Methods\nWe introduce our approach, emphasizing advanced de-\npendency modeling for TAL by integrating the S6 model to\nimprove long-range dependency handling. Our key compo-\nnents include the Feature Aggregated Bi-S6 Block Design,\nDual Bi-S6 Structure, and Recurrent Mechanism.\n3.1. Preliminary: Selective Space State Model (S6)\nOur architecture uses the S6 model with selective mech-\nanisms and gating operations to capture complex temporal\ndynamics and capture long-range dependencies effectively.\nThe S6 model operates with parameters (∆t, A, B, C),\n3\ndiscretized to manage sequence transformations:\nht = Aht−1 + Bxt, y t = Cht\nHere, xt represents the input at time step t, which, in the\ncase of TAL, is the spatiotemporal feature vector extracted\nfrom single clip. The hidden state at time step t, ht, captures\nthe temporal context of the sequence. The output at time\nstep t, yt, represents the processed feature. The state matrix\nA determines how the previous hidden state ht−1 and the\nhistorical information from all previous steps influence the\ncurrent hidden state ht [12], contributing to precise action\nlocalization. The input matrix B defines how the input xt\naffects the hidden state ht. Finally, the output matrix C\ntranslates the hidden state ht into the output yt.\nThe process starts with the input xt being projected to\nderive B, C, and ∆t. This step transforms raw input fea-\ntures into suitable representations for state-space modeling.\nSpecifically, the projection functions apply linear transfor-\nmations to the input xt:\nB = Linear(xt), C = Linear(xt)\nTo dynamically manage information flow, the S6 model\nemploys selection mechanism and gating function. The dy-\nnamically adjusted parameter ∆t controls the discretization\nof the state-space model based on the relevance of the input\nxt, functioning similarly to a gating mechanism in RNNs.\nThe projection function s∆(xt), which includes learnable\nparameters, projects the input xt to one dimension before\nbroadcasting it across channels:\n∆t = softplus(s∆(xt))\nNext, the discretization step adjusts the parameters A and\nB for the current time step t, ensuring that the parameters\nare appropriately scaled for discrete-time processing:\nAt = exp(∆tA)\nBt = (∆tA)−1(exp(∆tA) − I) · ∆tB\nThe hidden state ht is updated using At and Bt, and the\noutput yt is generated using Ct = C:\nht = Atht−1 + Btxt, y t = Ctht\nThe selective update of the hidden state can be understood\nas:\nht = (1 − ∆t)ht−1 + ∆txt\nwhere ∆t functions similarly to the gating function gt in\nRNNs, determining the influence of the input xt on the\nhidden state ht. This dynamic adjustment helps the model\nfocus on relevant portions of the input, ensuring effective\nhandling of long-range dependencies.\nS6 is particularly effective in TAL tasks due to its abil-\nity to maintain and refine temporal context over extended\nsequences. By dynamically adjusting ∆t, the model can\nselectively retain important temporal features.\n3.2. Overview\nOur architecture, inspired by ActionFormer [42] and Ac-\ntionMamba [7], consists of four primary components: a\nPretrained video encoder, a Backbone, a Neck, and Heads.\nThe overview of architecture is depicted in Figure 1a.\nPretrained Video Encoder The Pretrained video encoder\nextracts spatiotemporal attributes from video clips. Trained\non diverse datasets such as UCF, Kinetics, Something-\nSomething, and vision-language multi-modal datasets like\nWebVid and InternVid, it leverages the vast training data\nfrom InterVideo2-6B/1B [35]. The pretrained video en-\ncoder’s example of receiving each clip and extracting spa-\ntiotemporal features is shown in Appendix A.\nBackbone The Backbone captures dependencies and ex-\ntracts features at various temporal resolutions from the se-\nquence data. As illustrated in Figure 1a, it consists of three\nmain modules:\n• Embedding Module: This module captures the coarse\nlocal context of spatiotemporal features. As shown\nin Figure 2a, the sequence is first passed through a\nConv1D to increase the dimensionality from Cin to\nCemb, followed by Layer Normalization (LN) and\nReLU activation. This process is followed by Be se-\nquential Conv1D with dimensions Cemb to Cemb, each\nfollowed by LN and ReLU activation, resulting in an\nembedded sequence of shape [B, Cemb, L].\n• Stem Module: This core component processes the em-\nbedded sequences to capture long-range dependency\nusing the Dual Bi-S6 Structure. As shown in Figure\n2b, it applies two main blocks in parallel: the Temporal\nFeature Aggregated Bi-S6 (TFA-Bi-S6) block and the\nChannel Feature Aggregated Bi-S6 (CFA-Bi-S6) block,\nwhich focus on capturing temporal and channel-wise\ndependencies, respectively. Each of these blocks is\nstacked Bs times. The TFA-Bi-S6 block handles input\nsequences reshaped from [B, Cemb, L] to [B, L, Cemb]\nand outputs back to [B, Cemb, L]. The CFA-Bi-S6\nblock processes the temporal-pooled output of TFA-\nBi-S6 with shape [B, Cemb, 1] and scales it using a\nsigmoid activation. The outputs from these blocks are\ncombined through point-wise multiplication with the\nTFA-Bi-S6 output. This combined output then goes\nthrough an affine transformation with a drop path and\nskip connection, followed by LN to enhance capacity.\nThis process uses a Recurrent Mechanism, repeating\nr times, with a weight-shared network applied at each\nrepetition to refine temporal dependency modeling.\n• Branch Module: This module handles temporal multi-\nscale dependencies. As shown in Figure 2c, each branch\napplies the Temporal Bi-S6 (T-Bi-S6) block, which is\n4\n· σ\nTFA㎿Bi㎿S6\n CFA㎿Bi㎿S6\n𝐈𝐧𝐩𝐮𝐭𝒓\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒓\n𝒓: Weight shared\nrecurrent index\nൈ𝑩𝒔 ൈ𝑩𝒔\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕  ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ Avg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nDrop path ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nConv1D㏙𝐶௘௠௕,𝐶௘௠௕㏚\nConv1D㏙𝐶௜௡,𝐶௘௠௕㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௜௡,𝐿 ሿ\nൈ𝑩𝒆\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\n T㎿Bi㎿S6\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒅\n𝒅: Weight unshared\ndownsample index\n𝐈𝐧𝐩𝐮𝐭𝒅\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nൈ𝑩𝒃\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nDrop path\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nMax pool\nLN㏙C㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗ⁄  ሿ\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nc\nS6\n S6\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nSiLUSiLU\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n· ·\nSum\nSum\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nLinear㏙𝐿௔, 2𝐿௔㏚\nc· ·\nS6\n S6\nLinear㏙2𝐿௔, 𝐿௔㏚\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,1  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nSiLUSiLU\nAvg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nSum\n Sum\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nc· ·\nS6\n S6\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗିଵሿ\n: Shared\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ Chunk\nSiLUSiLU\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n(a) Embedding module\n· σ\nTFA㎿Bi㎿S6\n CFA㎿Bi㎿S6\n𝐈𝐧𝐩𝐮𝐭𝒓\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒓\n𝒓: Weight shared\nrecurrent index\nൈ𝑩𝒔 ൈ𝑩𝒔\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕  ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ Avg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nDrop path ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nConv1D㏙𝐶௘௠௕,𝐶௘௠௕㏚\nConv1D㏙𝐶௜௡,𝐶௘௠௕㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௜௡,𝐿 ሿ\nൈ𝑩𝒆\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\n T㎿Bi㎿S6\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒅\n𝒅: Weight unshared\ndownsample index\n𝐈𝐧𝐩𝐮𝐭𝒅\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nൈ𝑩𝒃\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nDrop path\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nMax pool\nLN㏙C㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗ⁄  ሿ\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nc\nS6\n S6\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nSiLUSiLU\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n· ·\nSum\nSum\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nLinear㏙𝐿௔, 2𝐿௔㏚\nc· ·\nS6\n S6\nLinear㏙2𝐿௔, 𝐿௔㏚\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,1  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nSiLUSiLU\nAvg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nSum\n Sum\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nc· ·\nS6\n S6\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗିଵሿ\n: Shared\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ Chunk\nSiLUSiLU\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ (b) Stem module\n· σ\nTFA㎿Bi㎿S6\n CFA㎿Bi㎿S6\n𝐈𝐧𝐩𝐮𝐭𝒓\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒓\n𝒓: Weight shared\nrecurrent index\nൈ𝑩𝒔 ൈ𝑩𝒔\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕  ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ Avg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nDrop path ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nConv1D㏙𝐶௘௠௕,𝐶௘௠௕㏚\nConv1D㏙𝐶௜௡,𝐶௘௠௕㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௜௡,𝐿 ሿ\nൈ𝑩𝒆\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\n T㎿Bi㎿S6\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒅\n𝒅: Weight unshared\ndownsample index\n𝐈𝐧𝐩𝐮𝐭𝒅\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nൈ𝑩𝒃\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nDrop path\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nMax pool\nLN㏙C㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗ⁄  ሿ\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nc\nS6\n S6\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nSiLUSiLU\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n· ·\nSum\nSum\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nLinear㏙𝐿௔, 2𝐿௔㏚\nc· ·\nS6\n S6\nLinear㏙2𝐿௔, 𝐿௔㏚\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,1  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nSiLUSiLU\nAvg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nSum\n Sum\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nc· ·\nS6\n S6\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗିଵሿ\n: Shared\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ Chunk\nSiLUSiLU\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ (c) Branch module\nFigure 2. Diagrams of the Embedding, Stem, and Branch modules. (a) Embedding module. (b) Stem module. (c) Branch module.\na modified version of the Bi-S6 block used in Action-\nMamba [7], followed by an affine drop path and resid-\nual connection. After this, the output undergoes LN\nand max pooling along the temporal dimension, effec-\ntively obtaining various temporal resolutions. The T-Bi-\nS6 block processes the input sequence reshaped from\n[B, Cemb, L/2d−1] to [B, L/2d−1, Cemb] and outputs\nback to [B, Cemb, L/2d−1]. This process is repeated\nfor each downsampling index (d = 1, 2, ...,5), where\nthe output shape becomes [B, Cemb, L/2d].\nNeck and Heads The Neck is designed with simplicity and\nefficiency in mind, utilizing layer normalization for channel-\nwise normalization, which is the same as the LN used in the\nBranch module. This step ensures that the temporal multi-\nscale sequences reflecting precise temporal dependencies\nprocessed by the Backbone are normalized and ready for\nsubsequent processing.\nThe Heads leverage the normalized features from the\nNeck to carry out two primary tasks: action classification and\ntemporal boundary regression. The action classification head\ngenerates channels equal to the number of action categories,\npredicting class scores for each category. Simultaneously,\nthe temporal boundary regression head outputs two channels\nto predict the frame indices marking the start and end of\nan action. This dual-head design ensures that the model\ncan accurately classify actions and determine their temporal\nboundaries within the video segments.\n3.3. Advanced Dependency Modeling for TAL\nFeature Aggregated Bi-S6 (FA-Bi-S6) Block Design The\nFA-Bi-S6 block design is one of our contributions, enabling\nrobust and effective modeling of dependencies within video\nsequences. This block design incorporates multiple Conv1D\nlayers, each with different kernel sizes, operating sequen-\ntially within two main blocks: the TFA-Bi-S6 block and the\nCFA-Bi-S6 block, as shown in Figure 3a and 3b.\nIn the TFA-Bi-S6 block, the input sequence of shape\n[B, Cemb, L] is first passed through a linear layer that ad-\njusts the dimensions from [B, Cemb, L] to [B, L,2Cemb].\nThe sequence is then divided into two chunks, and one of\nthese chunks is flipped. These chunks are processed through\nmultiple Conv1D layers with varying kernel sizes (2, 3, 4),\neach capturing different granularities of temporal features.\nThe outputs from these Conv1D layers are summed to create\nan aggregated feature map, which is then processed through a\nS6 network focusing on temporal dependencies. The output\nof the S6 blocks is then multiplied pointwise with the orig-\ninal chunked input processed through the SiLU activation.\nThe results from each chunk are concatenated, which handle\nbi-directional temporal dependencies. The final output is\nobtained by combining the results, which are then processed\nthrough a linear layer and reshaped back to [B, Cemb, L].\nIn the CFA-Bi-S6 block, the process is similar to the TFA-\nBi-S6 block with adaptations for channel-wise dependency\nmodeling. The input sequence is first adaptively pooled\nto [B, Cemb, La] before the linear layer processing. The\nConv1D layers in this block have varying kernel sizes (2, 4,\n8) to capture different scales of channel-wise dependencies.\nAfter processing through the S6 blocks and linear layer, the\nfinal output is average pooled to [B, Cemb, 1]. These adjust-\nments enable the CFA-Bi-S6 block to focus on capturing\ndiverse channel-wise dependencies and enhance the over-\nall capacity to model complex spatiotemporal interactions\nwithin video sequences.\nBy integrating the Bi-S6 block with the aggregated fea-\nture map, our design leverages the strengths of both multi-\nscale feature extraction and bi-directional processing. The\ncombined architecture allows the model to effectively cap-\n5\n· σ\nTFA㎿Bi㎿S6\n CFA㎿Bi㎿S6\n𝐈𝐧𝐩𝐮𝐭𝒓\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒓\n𝒓: Weight shared\nrecurrent index\nൈ𝑩𝒔 ൈ𝑩𝒔\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕  ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ Avg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nDrop path ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nConv1D㏙𝐶௘௠௕,𝐶௘௠௕㏚\nConv1D㏙𝐶௜௡,𝐶௘௠௕㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௜௡,𝐿 ሿ\nൈ𝑩𝒆\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\n T㎿Bi㎿S6\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒅\n𝒅: Weight unshared\ndownsample index\n𝐈𝐧𝐩𝐮𝐭𝒅\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nൈ𝑩𝒃\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nDrop path\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nMax pool\nLN㏙C㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗ⁄  ሿ\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nc\nS6\n S6\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nSiLUSiLU\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n· ·\nSum\nSum\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nLinear㏙𝐿௔, 2𝐿௔㏚\nc· ·\nS6\n S6\nLinear㏙2𝐿௔, 𝐿௔㏚\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,1  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nSiLUSiLU\nAvg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nSum\n Sum\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nc· ·\nS6\n S6\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗିଵሿ\n: Shared\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ Chunk\nSiLUSiLU\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n(a) TFA-Bi-S6\n· σ\nTFA㎿Bi㎿S6\n CFA㎿Bi㎿S6\n𝐈𝐧𝐩𝐮𝐭𝒓\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒓\n𝒓: Weight shared\nrecurrent index\nൈ𝑩𝒔 ൈ𝑩𝒔\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕  ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ Avg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nDrop path ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nConv1D㏙𝐶௘௠௕,𝐶௘௠௕㏚\nConv1D㏙𝐶௜௡,𝐶௘௠௕㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௜௡,𝐿 ሿ\nൈ𝑩𝒆\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\n T㎿Bi㎿S6\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒅\n𝒅: Weight unshared\ndownsample index\n𝐈𝐧𝐩𝐮𝐭𝒅\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nൈ𝑩𝒃\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nDrop path\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nMax pool\nLN㏙C㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗ⁄  ሿ\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nc\nS6\n S6\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nSiLUSiLU\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n· ·\nSum\nSum\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nLinear㏙𝐿௔, 2𝐿௔㏚\nc· ·\nS6\n S6\nLinear㏙2𝐿௔, 𝐿௔㏚\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,1  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nSiLUSiLU\nAvg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nSum\n Sum\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nc· ·\nS6\n S6\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗିଵሿ\n: Shared\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ Chunk\nSiLUSiLU\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ (b) CFA-Bi-S6\n· σ\nTFA㎿Bi㎿S6\n CFA㎿Bi㎿S6\n𝐈𝐧𝐩𝐮𝐭𝒓\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒓\n𝒓: Weight shared\nrecurrent index\nൈ𝑩𝒔 ൈ𝑩𝒔\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕  ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ Avg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nDrop path ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nConv1D㏙𝐶௘௠௕,𝐶௘௠௕㏚\nConv1D㏙𝐶௜௡,𝐶௘௠௕㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௜௡,𝐿 ሿ\nൈ𝑩𝒆\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nLN㏙C㏚\nReLU\n T㎿Bi㎿S6\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗሿ\n𝐎𝐮𝐭𝐩𝐮𝐭𝒅\n𝒅: Weight unshared\ndownsample index\n𝐈𝐧𝐩𝐮𝐭𝒅\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nൈ𝑩𝒃\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nDrop path\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\nMax pool\nLN㏙C㏚\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗ⁄  ሿ\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nc\nS6\n S6\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\nConv1D㏙𝐶௘௠௕㏚,4\nConv1D㏙𝐶௘௠௕㏚,3\nConv1D㏙𝐶௘௠௕㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nBCL㛳BLCሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ 𝐵,𝐿,𝐶௘௠௕ ሿ\nSiLUSiLU\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\n· ·\nSum\nSum\nሾ 𝐵,𝐿,𝐶௘௠௕ ሿሾ  𝐵,𝐿,𝐶௘௠௕ ሿ\nLinear㏙𝐿௔, 2𝐿௔㏚\nc· ·\nS6\n S6\nLinear㏙2𝐿௔, 𝐿௔㏚\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\nConv1D㏙𝐿௔㏚,8\nConv1D㏙𝐿௔㏚,4\nConv1D㏙𝐿௔㏚,2\n: Shared\nChunk\nሾ 𝐵,𝐶௘௠௕,1  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿 ሿ\nAdap pool ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿ\nSiLUSiLU\nAvg pool ሾ 𝐵,𝐶௘௠௕,1  ሿ\nSum\n Sum\nሾ 𝐵,𝐶௘௠௕,𝐿௔ ሿሾ  𝐵,𝐶௘௠௕,𝐿௔ ሿ\nc· ·\nS6\n S6\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙2C௘௠௕, 𝐶௘௠௕㏚\nConv1D㏙𝐶௘௠௕㏚,4\nLinear㏙𝐶௘௠௕, 2𝐶௘௠௕㏚\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\n𝐈𝐧𝐩𝐮𝐭\nሾ 𝐵,𝐶௘௠௕,𝐿/2ௗିଵሿ\n: Shared\nሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ\n𝐎𝐮𝐭𝐩𝐮𝐭\nBCL㛳BLCሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nFlip㏙𝐿㏚ ሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ Chunk\nSiLUSiLU\nሾ 𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿሾ  𝐵,𝐿 2ௗିଵ⁄ ,𝐶௘௠௕ ሿ\nBLC㛳BCL ሾ 𝐵,𝐶௘௠௕,𝐿 2ௗିଵ⁄  ሿ (c) T-Bi-S6\nFigure 3. Diagrams of the Feature aggregated Bi-S6 block design. (a) TFA-Bi-S6 model. (b) CFA-Bi-S6 model. (c) T-Bi-S6 model.\nture and utilize spatiotemporal features across a wide range\nof context, addressing the limitations of traditional single\nconvolutional approaches. This design is particularly ad-\nvantageous for TAL tasks, where actions may occur over\nvarying temporal spans, and the local context provided by\nsurrounding frames is crucial for accurate localization.\nDual Bi-S6 Structure The Dual Bi-S6 structure is a novel\ncomponent of our proposed architecture, designed to en-\nhance the modeling of spatiotemporal dependencies by pro-\ncessing features along both the temporal and channel dimen-\nsions. This dual-path approach ensures that the model can\ncapture and integrate the rich contextual information present\nin video sequences, thereby improving the accuracy of TAL.\nAs shown in Figure 2b, the Dual Bi-S6 structure consists\nof two parallel paths: the TFA-Bi-S6 and the CFA-Bi-S6.\nEach path processes the input sequence differently to ex-\ntract complementary information. The TFA-Bi-S6 reflects\ntemporal dynamics within the video sequence, providing\na detailed temporal analysis of the input. Simultaneously,\nthe CFA-Bi-S6 captures the interactions between different\nspatiotemporal features, and its output is then scaled using a\nsigmoid function to transform the values into a range suitable\nfor modulation.\nAfter processing the input through both paths, the out-\nputs of the TFA-Bi-S6 and CFA-Bi-S6 are combined using\npoint-wise multiplication. This fusion step integrates the\ntemporal dependencies captured by the TFA-Bi-S6 with the\nchannel-wise dependencies modeled by the CFA-Bi-S6. The\npoint-wise multiplication ensures that the combined features\nreflect both types of dependencies, with the TFA-Bi-S6 han-\ndling global dependencies between clips and the CFA-Bi-S6\naddressing local dependencies between spatiotemporal fea-\ntures within clips. The design intention behind this structure\nis to leverage the strengths of both paths: the TFA-Bi-S6\ncaptures temporal dependencies and dynamics, while the\nCFA-Bi-S6 emphasizes the relationships between spatiotem-\nporal features. By scaling the output of the CFA-Bi-S6 and\nmultiplying it with the TFA-Bi-S6 output, the model effec-\ntively combines temporal analysis with channel-wise context,\nleading to a more comprehensive understanding of the video.\nRecurrent Mechanism This mechanism, integrated with\nour Stem module in the Backbone, enhances the accuracy\nof temporal context modeling by leveraging the efficiency\nand precision of state space models. As shown in Figure 2b,\nthe process begins by passing the input sequence through\nthe Stem module to capture initial temporal dependencies.\nThe output is combined with the original input sequence\nand reprocessed by the Stem module, repeating this process\nr times. Each iteration refines the temporal dependencies\nfurther, enhancing the model’s ability to capture long-range\ndependencies and intricate temporal patterns. This recurrent\nmechanism provides a robust framework for refining tempo-\nral context, allowing the model to improve its understanding\nof temporal dependencies dynamically.\nThe effectiveness of this recurrent mechanism in speech\nseparation tasks highlights its potential for TAL tasks as well.\nIn speech separation, recurrent mechanisms have proven to\nexcel in capturing long-range dependencies and intricate\ntemporal patterns [6, 14]. This iterative refinement process,\nwhich involves passing the input sequence through a module\nmultiple times to capture and refine temporal dependencies,\nallows models to handle complex long-range dependencies\nwith greater precision. Such capabilities are directly applica-\nble to TAL tasks, where identifying precise segments within\na video also requires understanding temporal dependencies\nover extended periods.\n6\nBase System\nmAP (%)\n@.3 @.4 @.5 @.6 @.7 Avg\nCNN CDC [29] 40.1 29.4 23.3 13.1 7.9 22.8\nTAL-Net [5] 53.2 48.5 42.8 33.8 20.8 39.8\nPBRNet [20] 58.5 54.6 51.3 41.8 29.5 47.1\nRNN AS [1] 51.8 42.4 30.8 20.2 11.1 31.3\nRCL [34] 70.1 62.3 52.9 42.7 30.7 51.7\nGCN G-TAD [36] 66.4 60.4 51.6 37.6 22.9 47.8\nTransformer TallFormer [9] 76.0 71.5 63.2 50.9 34.5 59.2\nActionFormer [42] 82.1 77.8 71.0 59.4 43.9 66.8\nTriDet [28] 83.6 80.1 72.9 62.4 47.4 69.3\nS6 ActionMamba [7] 86.9 83.1 76.9 65.1 50.8 72.7\nOurs 88.7 84.6 78.2 66.6 51.9 74.2\n(a)\nBase System\nmAP (%)\n@.5 @.75 @.95 Avg\nCNN BSN [19] 46.5 30.0 8.0 30.0\nDCAN [8] 51.8 36.0 9.5 35.4\nRNN DeepAct [30] 37.8 24.8 10.0 24.0\nGCN G-TAD [36] 50.4 34.6 9.0 34.1\nA VFusion [3] 54.3 37.7 8.9 36.8\nTransformer ActionFormer [42] 54.7 37.8 8.4 36.6\nTriDet [28] 54.7 38.0 8.4 36.8\nTCANet [27] 54.3 39.1 8.4 37.6\nAdaTAD [21] 61.7 43.4 10.9 41.9\nS6 ActionMamba [7] 62.4 43.5 10.2 42.0\nOurs 64.1 44.0 10.6 42.9\n(b)\nBase System\nmAP (%)\n@.5 @.75 @.95 Avg\nCNN DBG [18] 10.7 6.4 2.5 6.8\nGCN G-TAD [36] 13.7 8.8 3.1 9.1\nTransformer VideoMAE-v2 [33] 29.1 17.7 5.1 18.2\nS6 ActionMamba [7] 45.4 28.8 6.8 29.0\nOurs 46.4 29.5 7.6 29.6\n(c)\nBase System\nmAP (%)\n@.5 @.75 @.95 Avg\nCNN DyFADet [39] 64.0 44.8 14.1 44.3\nTransformer TadTR [22] 47.1 32.1 10.9 32.1\nTriDet [28] 62.4 44.1 13.1 43.1\nS6 ActionMamba [7] 64.0 45.7 13.3 44.6\nOurs 66.4 47.2 14.3 45.8\n(d)\nTable 1. Results of temporal action localization on benchmark datasets. (a) THUMOS-14 [15], (b) ActivityNet [4], (c) FineAction [23],\n(d) HACS [43]. The metric used is mean Average Precision (mAP) evaluated at multiple tIoU thresholds.\n4. Experiments\nWe provide a comprehensive evaluation of our TAL\nmethod through extensive experiments. We demonstrate its\neffectiveness using various benchmark datasets and conduct\nablation studies to assess the impact of various components\nof our proposed approach.\n4.1. Evaluation on Benchmarks\nTo evaluate the effectiveness of the proposed method for\nTAL, we utilized the benchmark datasets THUMOS-14 [15],\nActivityNet [4], FineAction [23], and HACS [43]. Detailed\ndescriptions of each benchmark can be found in Appendix B.\nTable 1a presents experimental results on THUMOS-14.\nWe compared our method with various approaches, includ-\ning CNNs, RNNs, GCNs, Transformers-based, and the latest\nSOTA S6-based model. Our method achieved an average\nmAP of 74.2%, surpassing the previous SOTA by 1.5%. In\nTable 1b, we summarize our performance on ActivityNet.\nDespite its larger scale and variety of classes, which gener-\nally result in lower scores, our method achieved an average\nmAP of 42.9%, surpassing the previous SOTA by 0.9%.\nThe outcomes on FineAction are presented in Table 1c.\nThis benchmark, being relatively new, lacked RNN-based\nstudies for comparison. Therefore, we included studies uti-\nlizing CNN, GCN, Transformer, and S6 models. FineAc-\ntion’s high class variety relative to its size makes it par-\nticularly challenging, generally resulting in lower mAP\nscores. Nonetheless, our approach achieved an average mAP\nof 29.6%, which is 0.6% higher than the previous SOTA.\nFinally, Table 1d displays our experimental performance\non HACS. Most studies focused on Transformer-based ap-\nproaches due to the dataset’s large scale. Despite this, our\nproposed method achieved an average mAP of 45.8%, ex-\nceeding the previous SOTA by 1.2%.\n4.2. Ablation Studies\nStem module structure and Block quantities We investi-\ngated the impact of varying the structure of the Stem mod-\nule and the number of blocksin the Embedding, Stem, and\nBranch modules to understand their effect on performance.\nThe results, presented in Table 2a, demonstrate the su-\nperiority of the Dual structure in the Stem module, which\nutilizes both temporal and channel blocks, consistently out-\nperforming the Single structure that only uses the temporal\nblock. This finding suggests that addressing both temporal\nand channel-wise dependencies provides a more compre-\nhensive understanding for TAL. Additionally, using a single\nblock in each module often yielded better performance than\nmultiple blocks, indicating that simpler, less complex model\nstructures help prevent overfitting and effectively capture es-\nsential spatiotemporal features. Notably, omitting the Stem\nmodule (Bs = 0) results in a significant performance drop,\nhighlighting its importance in sequence interpretation.\nKernel sizes and Aggregation methods We evaluated the\nperformance impact of differentkernel sizecombinations for\nTFA-Bi-S6 and CFA-Bi-S6 blocks and variousaggregation\nmethods using the Dual structure. This analysis, detailed in\nTable 2b, explores how different configurations influence the\nmodel’s ability to capture temporal and channel-wise local\ncontext.\n7\nStructure (Be,Bs,Bb) Params Avg mAP\n(M) (%)\nSingle (1,0,1) 16.0 69.4\n(1,1,1) 18.8 72.2\n(2,1,1) 19.6 72.0\n(1,2,1) 21.6 71.7\n(1,1,2) 33.0 71.0\n(2,2,1) 22.5 71.8\n(2,1,2) 33.8 71.1\n(1,2,2) 35.9 71.3\n(2,2,2) 36.6 71.5\n(1,4,1) 27.3 71.3\n(1,8,1) 38.6 70.7\nDual (1,1,1) 21.7 72.8\n(1,2,1) 28.5 72.5\n(a)\nKTFA KCFA Aggregate Params Avg mAP\n(M) (%)\nX X Sum 20.5 72.1\n(4) X Sum 21.6 72.6\nX (4) Sum 20.6 72.3\n(4) (4) Sum 21.7 72.8\n(2,4) (4) Sum 22.1 73.0\n(4) (2,4) Sum 21.7 72.9\n(2,4) (2,4) Sum 22.2 73.1\n(2,3,4) (2,3,4) Sum 23.0 73.4\n(2,3,4,8) (2,3,4,8) Sum 25.2 73.2\n(2,4,8) (2,3,4) Sum 24.3 73.4\n(2,3,4) (2,4,8) Sum 23.1 73.5\n(2,3,4) (2,4,8) Concat 31.6 72.8\n(2,4,8) (2,4,8) Sum 24.5 73.4\n(b)\nBs r Params Avg mAP\n(M) (%)\n1 1 23.1 73.5\n2 23.1 73.6\n4 23.1 73.7\n8 23.1 73.9\n16 23.1 74.2\n32 23.1 74.0\n2 1 31.3 73.1\n16 31.3 73.4\n32 31.3 73.2\n4 1 47.8 72.8\n16 47.8 72.6\n32 47.8 72.1\n8 1 80.9 72.3\n(c)\nTable 2. Ablation studies on the proposed methods. (a) Performance comparison with varying numbers of blocks in the Embedding, Stem,\nand Branch modules (Be, Bs, Bb) and different structures (Structure) using only single Conv1D layer without Feature Aggregation. In this\ncontext, “Single” refers to using only the temporal block in the Stem module, while “Dual” refers to using both the temporal and channel\nblocks in the Stem module. (b) Performance comparison with different kernel size combinations for TFA-Bi-S6 and CFA-Bi-S6 blocks\n(KTFA and KCFA ) and different aggregation methods (Aggregate) using the Dual structure. (c) Performance comparison with varying\niterations (r) of applying residual connections in the recurrent Dual S6 structure in the Stem module and different numbers of blocks in the\nStem module (Bs), with both Dual structure and Feature Aggregation applied. All results are from the THUMOS-14 dataset.\nThe results show that using multiple kernel sizes for\nConv1D layers in both TFA-Bi-S6 and CFA-Bi-S6 blocks\nimproves performance, demonstrating the benefit of captur-\ning a diverse range of local contexts at multiple scales for\nTAL. However, configurations with four or more kernel sizes\nper block resulted in decreased performance, likely due to\noverfitting, as the increased model complexity led to learning\nnoise and less relevant patterns.\nThe absence of Conv1D layers led to reduced perfor-\nmance, underscoring the importance of capturing temporal\nand channel-wise local context through these layers. Further-\nmore, the Sum aggregation method outperformed the Concat\nmethod, indicating that summing feature maps effectively\nintegrates information across different scales without adding\nexcessive complexity.\nRecurrent mechanism iterations We examined the im-\npact of varying the number of iterations r in the recurrent\nmechanism, along with the Dual structure and Feature Ag-\ngregation. This analysis, detailed in Table 2c, assesses how\niterative refinement of temporal dependencies affects model\nperformance compared to increasing the number of Stem\nblocks (Bs).\nThe results show that increasing the number of recur-\nrent iterations r generally improves performance up to a\ncertain point. Beyond this point, however, additional itera-\ntions resulted in a slight performance drop, likely due to an\nimbalance in temporal dependency. This suggests that there\nis an optimal number of iterations after which the benefits\nbegin to diminish. In contrast, increasing the number of\nStem blocks (Bs) while keeping r fixed at 1 led to a decrease\nin performance, indicating that simply adding more Stem\nblocks is not effective for improving TAL.\nThis comparison shows that adopting a recurrent ap-\nproach, with Bs set to 1 and increasing r, is more efficient\nand effective than stacking additional blocks. The recurrent\nmechanism improves temporal precision and long-range de-\npendency modeling while optimizing memory usage, crucial\nfor accurately understanding extended actions in video se-\nquences and boosting performance, making it a practical\nstrategy for TAL tasks using the S6-based model.\n5. Conclusion\nIn this paper, we introduced a novel architecture leverag-\ning S6 to provide effective solutions for TAL tasks based\non insights from previous studies. By integrating the Fea-\nture Aggregated Bi-S6 block and the Dual Bi-S6 structure,\nour approach captures multi-scale temporal and channel-\nwise dependencies. The recurrent mechanism further refines\ntemporal context modeling, enhancing performance with-\nout increasing parameter complexity. Consequently, our ap-\nproach achieves state-of-the-art results on various benchmark\ndatasets, with average mAP scores of 74.2% on THUMOS-\n14, 42.9% on ActivityNet, 29.6% on FineAction, and 45.8%\non HACS. Additionally, ablation studies confirm the advan-\ntages of our design, demonstrating that the Dual structure\nin the Stem module outperforms the Single structure, the\nrecurrent mechanism is more effective than merely stacking\nadditional blocks, and Temporal Aggregation further boosts\nperformance. These findings pave the way for future re-\nsearch to further explore the potential of state space models\nin TAL tasks.\n8\nReferences\n[1] Humam Alwassel, Fabian Caba Heilbron, and Bernard\nGhanem. Action search: Spotting actions in videos and its\napplication to temporal action localization. In Proceedings\nof the European Conference on Computer Vision (ECCV) ,\nSeptember 2018. 2, 7\n[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun,\nMario Luˇci´c, and Cordelia Schmid. Vivit: A video vision\ntransformer. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pages 6836–6846,\nOctober 2021. 1\n[3] Anurag Bagchi, Jazib Mahmood, Dolton Fernandes, and\nRavi Kiran Sarvadevabhatla. Hear me out: Fusional ap-\nproaches for audio augmented temporal action localization.\narXiv preprint arXiv:2106.14118, 2021. 7\n[4] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and\nJuan Carlos Niebles. Activitynet: A large-scale video bench-\nmark for human activity understanding. In Proceedings of the\nieee conference on computer vision and pattern recognition,\npages 961–970, 2015. 1, 2, 7\n[5] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold,\nDavid A Ross, Jia Deng, and Rahul Sukthankar. Rethinking\nthe faster r-cnn architecture for temporal action localization.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 1130–1139, 2018. 7\n[6] Chen Chen, Chao-Han Huck Yang, Kai Li, Yuchen Hu,\nPin-Jui Ku, and Eng Siong Chng. A neural state-space\nmodel approach to efficient speech separation. arXiv preprint\narXiv:2305.16932, 2023. 6\n[7] Guo Chen, Yifei Huang, Jilan Xu, Baoqi Pei, Zhe Chen,\nZhiqi Li, Jiahao Wang, Kunchang Li, Tong Lu, and Limin\nWang. Video mamba suite: State space model as a ver-\nsatile alternative for video understanding. arXiv preprint\narXiv:2403.09626, 2024. 1, 4, 5, 7\n[8] Guo Chen, Yin-Dong Zheng, Limin Wang, and Tong Lu.\nDcan: improving temporal action detection via dual context\naggregation. In Proceedings of the AAAI conference on artifi-\ncial intelligence, volume 36, pages 248–257, 2022. 7\n[9] Feng Cheng and Gedas Bertasius. Tallformer: Temporal\naction localization with a long-memory transformer. In\nEuropean Conference on Computer Vision, pages 503–521.\nSpringer, 2022. 7\n[10] Felix A Gers, J ¨urgen Schmidhuber, and Fred Cummins.\nLearning to forget: Continual prediction with lstm. Neural\ncomputation, 12(10):2451–2471, 2000. 1, 2\n[11] Albert Gu and Tri Dao. Mamba: Linear-time sequence\nmodeling with selective state spaces. arXiv preprint\narXiv:2312.00752, 2023. 1\n[12] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christo-\npher R´e. Hippo: Recurrent memory with optimal polynomial\nprojections. Advances in neural information processing sys-\ntems, 33:1474–1487, 2020. 4\n[13] Albert Gu, Karan Goel, and Christopher R´e. Efficiently mod-\neling long sequences with structured state spaces. arXiv\npreprint arXiv:2111.00396, 2021. 1\n[14] Xiaolin Hu, Kai Li, Weiyi Zhang, Yi Luo, Jean-Marie\nLemercier, and Timo Gerkmann. Speech separation using\nan asynchronous fully recurrent convolutional neural net-\nwork. Advances in Neural Information Processing Systems,\n34:22509–22522, 2021. 6\n[15] Haroon Idrees, Amir R Zamir, Yu-Gang Jiang, Alex Gorban,\nIvan Laptev, Rahul Sukthankar, and Mubarak Shah. The\nthumos challenge on action recognition for videos “in the\nwild”. Computer Vision and Image Understanding, 155:1–23,\n2017. 1, 2, 7\n[16] Hassan Keshvarikhojasteh, Hoda Mohammadzade, and\nHamid Behroozi. Temporal action localization using gated re-\ncurrent units. The Visual Computer, 39(7):2823–2834, 2023.\n2\n[17] Thomas N Kipf and Max Welling. Semi-supervised classi-\nfication with graph convolutional networks. arXiv preprint\narXiv:1609.02907, 2016. 1, 2\n[18] Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao\nLuo, Zhipeng Cui, Chengjie Wang, Jilin Li, Feiyue Huang,\nand Rongrong Ji. Fast learning of temporal action proposal\nvia dense boundary generator. In Proceedings of the AAAI\nconference on artificial intelligence, volume 34, pages 11499–\n11506, 2020. 7\n[19] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and\nMing Yang. Bsn: Boundary sensitive network for temporal\naction proposal generation. In Proceedings of the European\nconference on computer vision (ECCV), pages 3–19, 2018. 7\n[20] Qinying Liu and Zilei Wang. Progressive boundary refine-\nment network for temporal action detection. In Proceedings\nof the AAAI conference on artificial intelligence, volume 34,\npages 11612–11619, 2020. 2, 7\n[21] Shuming Liu, Chen-Lin Zhang, Chen Zhao, and Bernard\nGhanem. End-to-end temporal action detection with 1b\nparameters across 1000 frames. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18591–18601, 2024. 7\n[22] Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Shiwei\nZhang, Song Bai, and Xiang Bai. End-to-end temporal action\ndetection with transformer. IEEE Transactions on Image\nProcessing, 31:5427–5441, 2022. 7\n[23] Yi Liu, Limin Wang, Yali Wang, Xiao Ma, and Yu Qiao.\nFineaction: A fine-grained video dataset for temporal ac-\ntion localization. IEEE transactions on image processing ,\n31:6937–6950, 2022. 1, 2, 7\n[24] Fuchen Long, Ting Yao, Zhaofan Qiu, Xinmei Tian, Jiebo\nLuo, and Tao Mei. Gaussian temporal awareness networks\nfor action localization. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages\n344–353, 2019. 2\n[25] Dan Oneata, Jakob Verbeek, and Cordelia Schmid. The lear\nsubmission at thumos 2014. 2014. 2\n[26] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On\nthe difficulty of training recurrent neural networks. In Inter-\nnational conference on machine learning, pages 1310–1318.\nPmlr, 2013. 1, 2\n[27] Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang,\nWei Wu, Xiang Wang, Yu Qiao, Junjie Yan, Changxin Gao,\nand Nong Sang. Temporal context aggregation network for\ntemporal action proposal refinement. In Proceedings of the\n9\nIEEE/CVF conference on computer vision and pattern recog-\nnition, pages 485–494, 2021. 7\n[28] Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, and\nDacheng Tao. Tridet: Temporal action detection with relative\nboundary modeling. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18857–18866, 2023. 7\n[29] Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki\nMiyazawa, and Shih-Fu Chang. Cdc: Convolutional-de-\nconvolutional networks for precise temporal action local-\nization in untrimmed videos. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n5734–5743, 2017. 2, 7\n[30] Yeongtaek Song and Incheol Kim. Deepact: a deep neural\nnetwork model for activity detection in untrimmed videos.\nJournal of Information Processing Systems, 14(1):150–161,\n2018. 7\n[31] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,\nand Manohar Paluri. Learning spatiotemporal features with\n3d convolutional networks. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 4489–4497,\n2015. 1, 2\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 1, 2\n[33] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan\nHe, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling\nvideo masked autoencoders with dual masking. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 14549–14560, 2023. 7\n[34] Qiang Wang, Yanhao Zhang, Yun Zheng, and Pan Pan. Rcl:\nRecurrent continuous localization for temporal action detec-\ntion. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition , pages 13566–13575,\n2022. 2, 7\n[35] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He,\nGuo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang,\net al. Internvideo2: Scaling video foundation models for multi-\nmodal video understanding. arXiv preprint arXiv:2403.15377,\n2024. 4\n[36] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and\nBernard Ghanem. G-tad: Sub-graph localization for temporal\naction detection. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , pages 10156–\n10165, 2020. 2, 7\n[37] Ke Yang, Peng Qiao, Dongsheng Li, Shaohe Lv, and Yong\nDou. Exploring temporal preservation networks for precise\ntemporal action localization. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 32, 2018. 2\n[38] Ke Yang, Xiaolong Shen, Peng Qiao, Shijie Li, Dongsheng\nLi, and Yong Dou. Exploring frame segmentation networks\nfor temporal action localization. Journal of Visual Communi-\ncation and Image Representation, 61:296–302, 2019. 2\n[39] Le Yang, Ziwei Zheng, Yizeng Han, Hao Cheng, Shiji Song,\nGao Huang, and Fan Li. Dyfadet: Dynamic feature ag-\ngregation for temporal action detection. arXiv preprint\narXiv:2407.03197, 2024. 7\n[40] Jun Yuan, Bingbing Ni, Xiaokang Yang, and Ashraf A. Kas-\nsim. Temporal action localization with pyramid of score\ndistribution features. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June\n2016. 2\n[41] Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin\nZhao, Junzhou Huang, and Chuang Gan. Graph convolutional\nnetworks for temporal action localization. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 7094–7103, 2019. 2\n[42] Chen-Lin Zhang, Jianxin Wu, and Yin Li. Actionformer: Lo-\ncalizing moments of actions with transformers. In European\nConference on Computer Vision, pages 492–510. Springer,\n2022. 1, 3, 4, 7\n[43] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and\nZhicheng Yan. Hacs: Human action clips and segments\ndataset for recognition and temporal localization. In Proceed-\nings of the IEEE/CVF International Conference on Computer\nVision, pages 8668–8678, 2019. 1, 2, 7\n[44] Yibo Zhao, Hua Zhang, Zan Gao, Weili Guan, Jie Nie, Anan\nLiu, Meng Wang, and Shengyong Chen. A temporal-aware\nrelation and attention network for temporal action localiza-\ntion. IEEE Transactions on Image Processing, 31:4746–4760,\n2022. 3\n[45] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,\nWenyu Liu, and Xinggang Wang. Vision mamba: Efficient\nvisual representation learning with bidirectional state space\nmodel. arXiv preprint arXiv:2401.09417, 2024. 1\n10\nSampled video\n㏝3, 16, 224, 224㏞\nW㚍224\nH㚍224\nPer frames\n㏙x16㏚\nProjection\n㏙588,3200㏚\nT㚍256\nC㚍588\nPatchify\n㏙1,14,14㏚\nFlatten\n㏙3,14,14㏚\n14\n14\nPatch embed.\n㏝16, 256,3200㏞\nT㚍256\nC㚍3200\nPosition embed.\n㏝4096, 3200㏞\nT㚍4096\nC㚍3200\nFlatten\n㏙16,256㏚\n3D sine㎿\ncosine init\nMHSA㏙3200㏚\nMLP㏙12800㏚\nMasking\n㏙1㚊ρ㏚\n㏙          ㏚\nx48\nContextual embed.\n㏝4096㏙1㎿ρ㏚, 3200㏞\nT㚍4096㏙1㚊ρ㏚\nC㚍3200\nEncoded feat.\n㏝1, 3200㏞\nL㚍1\nC㚍3200\nMHSA\n㏙3200㏚\nMean pool\n㏙4096㏙1㚊ρ㏚㏚\nColor mapping: ㏙Temporal, Token,Channel, Spatial㏚\nFigure 4. Process of extracting spatiotemporal features using the pretrained video encoder. The encoder processes 30fps RGB video\nframes, groups them into 16-frame clips, and applies patchification, positional embedding, and multi-head self-attention to produce encoded\nfeature vectors.\nAppendix / supplemental material\nThis appendix is organized as follows:\n• Appendix A provides a detailed explanation of the process in-\nvolved in extracting spatiotemporal features from video clips\nusing a pretrained video encoder. It covers the technical steps\nand methodology used, including patchification, positional\nembedding, and multi-head self-attention mechanisms.\n• Appendix B describes the benchmark datasets used for eval-\nuating Temporal Action Localization (TAL) methods. It in-\ncludes detailed descriptions of datasets such as THUMOS-14,\nActivityNet, FineAction, and HACS, along with their key\ncharacteristics and evaluation metrics.\nA. Example of Pretrained Video Encoder\nExtracting Spatiotemporal Features from\nEach Clip\nTo understand the design intention of the Dual Bi-S6\nstructure, it is crucial to explain how the Pretrained video\nencoder extracts spatiotemporal features from each clip, clar-\nifying the information contained in the sequence.\nFor instance, when processing the THUMOS dataset us-\ning the same Pretrained video encoder as ActionMamba,\nwe start with RGB videos at 30 fps and a spatial resolu-\ntion of 224x224. We segment 16 frames into a single clip,\nsetting a frame interval of 4 (stride=4) between clips, yield-\ning multiple clips from each video, each clip measuring\n[3, 16, 224, 224]. Within each frame, patches of size 14x14\nare generated, producing 256 patch tokens per frame. Each\npatch token, representing spatial information and RGB chan-\nnels, is flattened to a dimension of [256, 588]. These spatial\ntokens are projected to a channel size of 3200, forming\npatch embedding tokens with dimensions [16, 256, 3200].\nAdding 3D sine-cosine positional embeddings to both the\npatch and frame dimensions, and then flattening these dimen-\nsions, results in position-embedded tokens with dimensions\n[4096, 3200]. Next, a proportion ρ of tokens is masked,\nand the channels are projected to 3200, followed by multi-\nhead self-attention and a feedforward layer with a hidden\nchannel size of 12800, repeated 48 times to incorporate spa-\ntiotemporal context, resulting in contextual embedded tokens\nwith dimensions [4096(1 − ρ), 3200]. Finally, multi-head\nself-attention and mean pooling are applied along the token\ndimension to produce an encoded feature vector with dimen-\nsions [1, 3200] for each clip. This process is repeated for all\nclips, stacking the encoded feature vectors sequentially over\ntime to generate the sequence data, excluding the first and\nlast two clips, which may lack video information, as shown\nin Figure 4.\nB. Benchmark Datasets for Temporal Action\nLocalization\nTo provide a comprehensive evaluation of TAL methods,\nwe employ several benchmark datasets that vary in size, com-\nplexity, and focus. Here, we describe the key characteristics\nand evaluation metrics of the datasets utilized in this study:\nTHUMOS-14: This large-scale dataset is specifically\ndesigned for video action recognition and includes detailed\ntemporal frame index annotations for 20 action classes. The\nprimary evaluation metric for THUMOS-14 is mean Average\nPrecision (mAP), which is calculated at various temporal\nIntersection over Union (tIoU) thresholds of 0.3, 0.4, 0.5,\n0.6, and 0.7. This allows for a thorough assessment of the\nmodel’s performance across different levels of temporal pre-\ncision.\nActivityNet: Significantly larger and more complex than\nTHUMOS-14, ActivityNet comprises approximately 20,000\nvideos spanning 200 action classes. The diverse range of\nclasses in ActivityNet presents a more challenging scenario\nfor TAL models. The mAP evaluation metric is also em-\nployed here, with tIoU thresholds set at 0.5, 0.75, and 0.95,\nproviding a stringent test for action localization performance.\nFineAction: Consisting of around 16,000 videos fea-\nturing 106 action classes, FineAction emphasizes everyday\nactivities and sports. The high variety of classes relative to\n11\nits size makes it a particularly challenging dataset. Evalua-\ntion methods are akin to those used for ActivityNet, utilizing\nmAP scores at multiple tIoU thresholds.\nHACS (Human Action Clips and Segments): This ex-\ntensive dataset includes approximately 50,000 videos cover-\ning 200 action classes, primarily capturing various actions\nfrom everyday life. Evaluation of the HACS dataset is con-\nducted using the same methodology as ActivityNet, ensuring\na consistent benchmark for comparing TAL model perfor-\nmance across different datasets.\nThese detailed descriptions of the datasets underscore the\ndiverse and comprehensive nature of the benchmarks used\nin this study, providing a robust framework for evaluating\nthe effectiveness of TAL methods.\n12"
  },
  {
    "source": "2407.07071v2.pdf",
    "content": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\nLarge Language Models Using Only Attention Maps\nYung-Sung Chuang† Linlu Qiu† Cheng-Yu Hsieh‡ Ranjay Krishna‡\nYoon Kim† James Glass†\nMassachusetts Institute of Technology† University of Washington‡\nyungsung@mit.edu\nAbstract\nWhen asked to summarize articles or answer\nquestions given a passage, large language mod-\nels (LLMs) can hallucinate details and respond\nwith unsubstantiated answers that are inaccu-\nrate with respect to the input context. This pa-\nper describes a simple approach for detecting\nsuch contextual hallucinations. We hypothe-\nsize that contextual hallucinations are related\nto the extent to which an LLM attends to in-\nformation in the provided context versus its\nown generations. Based on this intuition, we\npropose a simple hallucination detection model\nwhose input features are given by the ratio of\nattention weights on the context versus newly\ngenerated tokens (for each attention head). We\nfind that a linear classifier based on these look-\nback ratio features is as effective as a richer\ndetector that utilizes the entire hidden states\nof an LLM or a text-based entailment model.\nThe lookback ratio-based detector—Lookback\nLens—is found to transfer across tasks and\neven models, allowing a detector that is trained\non a 7B model to be applied (without retrain-\ning) to a larger 13B model. We further apply\nthis detector to mitigate contextual hallucina-\ntions, and find that a simple classifier-guided\ndecoding approach is able to reduce the amount\nof hallucination, for example by 9.6% in the\nXSum summarization task.1\n1 Introduction\nDespite the utility and impressive capabilities of\nlarge language models (LLMs), their tendency to\ngenerate hallucinations, i.e., content that deviates\nfrom facts or contextually relevant information (Ji\net al., 2023), presents a significant challenge in\ntheir deployment. In this work, we focus on the\nscenarios where the model is provided with the cor-\nrect facts within the input context but still fails to\ngenerate accurate outputs, a phenomenon we term\ncontextual hallucination. Despite the simplicity of\n1Source code: github.com/voidism/Lookback-Lens\nthis setup, LLMs struggle with contextual halluci-\nnations, frequently producing errors in tasks such\nas summarization and document-based question an-\nswering (e.g., Table 1), which can cause serious\nissues in applications such as retrieval-augmented\ngeneration (RAG) (Lewis et al., 2020), even when\ncorrect documents are retrieved.\nMost prior studies that propose methods to com-\nbat hallucination focus on the scenario without any\ninput context, where the hallucinations arise from\nthe LLMs’ parametric knowledge. These works\ndetect and mitigate hallucinations by generally us-\ning the LLM’s representations, such as hidden\nstates (Burns et al., 2023; Azaria and Mitchell,\n2023), MLP outputs (Zhang et al., 2024; Simhi\net al., 2024), attention block outputs (Zhang et al.,\n2024; Simhi et al., 2024) and attention head out-\nputs (Li et al., 2024; Chen et al., 2024b; Simhi\net al., 2024). In contrast, the provided contex-\ntual information plays a key role in detecting con-\ntextual hallucinations. Insofar as attention (more\nso than other model internals) provides a human-\nmeaningful measure of how much weight is given\nto the context during generation, this motivates the\nuse of signals from the attention maps for halluci-\nnation detection and mitigation.\nTo leverage signals from attention maps, we start\nby hypothesizing that contextual hallucinations are\nrelated to the extent to which an LLM attends to\nthe provided contextual information. Concretely,\nwe propose a simple feature called lookback ratio,\nwhich is computed as the ratio of attention weights\non the given context versus the newly generated to-\nkens. At each time step, we calculate this lookback\nratio for each attention head, and train a linear clas-\nsifier, which we call the Lookback Lens, to detect\ncontextual hallucinations based on the lookback\nratio features, as illustrated in Figure 1. The Look-\nback Lens performs on par with, and sometimes\neven surpasses, more complex feature-based detec-\ntors that utilize hidden states from LLMs or text-\n1\narXiv:2407.07071v2  [cs.CL]  3 Oct 2024\nx1 x2 x3 … xN… y1 y2 y3 yt-2yt-1\nAttention Weights\naverage\nover Y\nLookback Ratio\n+ =\nH heads\nL layers\nvt~vt+T-1\nLinear\nClassiﬁer P(Factual)\nFeed\nForward\nAdd & Norm\nMulti-Head\nAttention\nAdd & Norm\nN×\nTransformer\nDocument: [...] Summary: …\nAttention Map\nLookback Lens\nT tokens\nin a span T\naverage \nover T\naverage\nover X\nxN-1 …\nH x L\nFigure 1: An illustration of the Lookback Lens. We extract attention weights and calculate the lookback ratios for all\nlayers and all heads. We train a linear classifier on the concatenated features to predict truthfulness of the generation.\nbased entailment models trained on extensively an-\nnotated datasets. We can further integrate this de-\ntector during decoding to derive a Lookback Lens\nGuided Decoding strategy which can reduce con-\ntextual hallucinations by 9.6% from LLaMA-2-\n7B-Chat in the XSum summarization task. Fur-\nthermore, our use of “higher level” attention map\nfeatures makes it possible to transfer the detec-\ntor across models without retraining, allowing a\nLLaMA2-13B-Chat model to use the same detec-\ntor that has been trained on LLaMA-2-7B-Chat,\nand still reduce hallucinations by 3.2% in XSum.\nThese results collectively highlight the potential of\ncombating contextual hallucination by leveraging\nthe information from attention maps.\n2 Contextual Hallucinations Detection\n2.1 Lookback Lens\nTo detect contextual hallucinations in LLMs, we\nintroduce a lookback ratio, a measure based on\nthe attention distribution of a transformer model.\nGiven a transformer with L layers, each with H\nheads, the model processes an input sequence of\ncontext tokens X = {x1, x2, . . . , xN } of length\nN followed by a set of newly generated tokens\nY = {y1, y2, . . . , yt−1} to generate the next token\nyt. For time step t, and for each head, we calcu-\nlate the ratio of attention weights focused on the\ncontext tokens versus the newly generated tokens.\nFormally, for each head h in layer l, we define:\nAl,h\nt (context) = 1\nN\nNX\ni=1\nαl\nh,i,\nAl,h\nt (new) = 1\nt − 1\nN+t−1X\nj=N+1\nαl\nh,j,\nwhere αl\nh,i and αl\nh,j are softmax-ed attention\nweights assigned to context tokens X and new to-\nDataset Examples Correct\nCNN/DM 1000 49.6%\nNQ 2655 67.8%\nTable 1: Dataset statistics and GPT-4o evaluation results\non responses greedy decoded by LLaMA-2-7B-chat.\nkens Y respectively. The lookback ratio LRl,h\nt for\nhead h in layer l at time step t is then calculated as:\nLRl,h\nt = Al,h\nt (context)\nAl,h\nt (context) + Al,h\nt (new)\n.\nTo utilize these lookback ratios as input fea-\ntures in detecting hallucinations, we concatenate\nthe lookback ratios across all heads and layers into\na feature vector for the time step t:\nvt = [LR1,1\nt , LR1,2\nt , . . . ,LRL,H\nt ].\nGiven a text span of interest{yt, yt+1, ..., yt+T−1},\nwe average the corresponding lookback ratio vec-\ntors {vt, vt+1, ...,vt+T−1} into a single vector ¯v.\nWe then employ a logistic regression classifier F\nto predict if the span is factual (1) or hallucinated\n(0) based on the averaged lookback ratio vector.\nP(y = 1|¯ v) = F(¯ v) = σ(w⊤¯ v+ b),\nwhere σ denotes the sigmoid function, w is the\nweight vector, andb is the bias term of the classifier.\nDefining Span The Lookback Lens predicts the\nprobability of hallucinations over spans. We con-\nsider two ways to obtain spans for a given sequence:\npredefined spans or sliding window.\n1) Predefined Spans: When the hallucinated\nand non-hallucinated span annotations are avail-\nable, we directly train the classifier to differentiate\nbetween them. This is a clean setting where all\nspans are either hallucinated or non-hallucinated.\n2\n2) Sliding Window: In practice, we do not have\nany predefined spans during decoding, thus we\nneed a sliding window setup that iterates over all\npossible spans. Specifically, we process the sen-\ntences into fixed-sized chunks and train the classi-\nfier to predict a label of 0 if any hallucinated con-\ntent exists within a chunk, and 1 otherwise. Here,\nthe annotated data is only used for creating labels,\nnot for the span segmentation. This is more real-\nistic for classifier-guided decoding, but it presents\ngreater challenges because a chunk can contain\nboth hallucinated and non-hallucinated content.\n2.2 Experimental Setup\nData Training the Lookback Lens requires labels\nfor hallucinated and non-hallucinated examples. To\nobtain these examples, we first prompt LLaMA-\n2-7B-Chat (Touvron et al., 2023) to greedy de-\ncode responses for 1,000 summarization exam-\nples from the CNN/DM dataset (See et al., 2017)\nand 2,655 QA examples from the Natural Ques-\ntions (Kwiatkowski et al., 2019) following the setup\nof Liu et al. (2024). More details are shown in\nAppendix A. Although being prompted to gener-\nate correct responses, the decoded responses will\ncontain both hallucinated and non-hallucinated in-\nformation as the LLaMA model is still not perfect.\nThen, we employed GPT-4o (OpenAI, 2024) to ver-\nify the truthfulness of these responses and provide\nspan-level annotations on hallucinated segments\n(detailed prompts in Appendix B.1).\nAdditionally, we performed a pilot study of hu-\nman annotation on a subset of 70 examples of the\nsummarization task (details in Appendix B.2), con-\nfirming a 97% consistency rate between GPT-4o\nannotations and human judgments, and validating\nthe reliability of the automated annotations. We\nshow LLaMA-2-7B-Chat’s results on both tasks, as\nevaluated by GPT-4o, in Table 1. The results show\nthat the generated summaries from LLaMA-2-7B-\nChat still exhibit hallucinations about half of the\ntime, highlighting the challenge of summarization\ntasks.\nBaselines We compare our detection method\nagainst several baselines: 1) Text-based entail-\nment classifier: We fine-tune the DeBERTa-v3-\nbase (He et al., 2021) model on the same dataset of\nCNN/DM and NQ as a natural language entailment\n(NLI) task. Additionally, we include the results\nfrom a state-of-the-art entailment model (Vectara,\n2023) trained on a huge amount of annotated NLI\ndata (see details in Appendix C.1).\n2) Hidden states-based classifier: We train clas-\nsifiers using the same setting as the Lookback Lens\nbut used input features from the hidden states of\nLLaMA-2-7B-Chat from its 24th, 28th, and 32nd\nlayers instead of the lookback ratio. This baseline\nresembles a broad range of existing methods in the\nliterature (Azaria and Mitchell, 2023; Simhi et al.,\n2024). Our selection of layers followed the find-\nings outlined in Azaria and Mitchell (2023), which\nused layers 32, 28, 24, and 20 of a 32-layer LLM\nfor detecting hallucinations. They find that layers\nnear the 28th layer are most effective (see Table 3\nand 4 in Azaria and Mitchell (2023)).\nWe include additional experiments for leverag-\ning multiple layers or all layers in predicting con-\ntextual hallucinations in Appendix D.2, but the\nresults are not significantly better than using the\n28th layer. Some papers suggest attention block\noutputs could be more useful for detecting hallu-\ncinations (Campbell et al., 2023; Li et al., 2024),\nwe include the additional comparative experiments\nin Appendix D.3, but the difference between hid-\nden states and attention block outputs is relatively\nsmall.\n2.3 Results\nOur results are presented in Table 2. We consider\nboth predefined span segmentation and sliding win-\ndow with a window size of 8. We include the two-\nfold validation setting on the source task and the\nout-of-domain transfer setting on the target task,\nwith the tasks either question answering (QA) or\nsummarization (Sum.). We find that the Lookback\nLens achieves slightly better performance than the\nhidden states-based classifier and significantly out-\nperforms the NLI models (SoTA and our impl.).\nThe advantage of the Lookback Lens over the hid-\nden states-based classifier is more significant in the\nsliding window settings, as shown in the right-hand\nside of Table 2.\nAdditionally, we observe that the hidden states-\nbased classifier tends to overfit the training sets\nduring the two-fold validation, and present a sub-\nstantial performance drop when transferred to out-\nof-domain tasks. In contrast, Lookback Lens, while\nnot always fitting the training set perfectly, consis-\ntently exhibits better performance when applied to\nout-of-domain tasks. This contrast highlights the\neffectiveness and generalizability of the lookback\nratio features we extract from the attention maps.\n3\nPredefined Span Sliding Window = 8\nMethod Source Target Source − − − →Target Source − − − →Target\nTrain Test Transfer Train Test Transfer\nText based NLI\nSoTA NLI – Sum. – – 76.6 – – 57.1\nSoTA NLI – QA – – 58.6 – – 61.8\nNLI (our impl.) QA Sum. – – 55.1 – – 53.0\nNLI (our impl.) Sum. QA – – 71.0 – – 64.9\nHidden states based\n32nd Layer QA Sum. 100.0 89.6 79.4 99.0 97.1 56.1\n32nd Layer Sum. QA 100.0 82.5 81.8 97.0 94.8 59.4\n28th Layer QA Sum. 100.0 91.4 83.6 99.2 97.3 57.7\n28th Layer Sum. QA 100.0 83.3 84.7 97.2 95.2 58.8\n24th Layer QA Sum. 100.0 92.0 81.3 99.2 97.4 58.3\n24th Layer Sum. QA 100.0 83.1 83.0 99.2 97.4 58.3\nAttention maps based (Ours)\nLookback Lens QA Sum. 98.3 91.2 85.3 88.3 87.1 66.1\nLookback Lens Sum. QA 97.7 88.8 82.0 86.2 85.3 66.0\nTable 2: AUROC of the classification tasks using predefined span segmentation and sliding window (size = 8) on\nNQ (QA) and CNN/DM (Sum.). The source task scores (Train/Test) are averaged over two-fold validation.\nPrevious\nChunk\nNew Chunk\nCandidates\nF(v1)=0.1\nF(v2)=0.3\nF(v3)=0.9\nF(v4)=0.6\n✔\nLinear\nClassiﬁer… …\nSample\nLookback Lens Scores\nConcatenate New Chunk\nto Previous Chunks\n(...repeat until EOS)\nExtract Averaged \nLookback Ratios\nv1\nv2\nv3\nv4\n_\n_\n_\n_\n_\n_\n_\n_\nFigure 2: Lookback Lens Guided Decoding: sample multiple chunk candidates, compute lookback ratios from\nattention maps to be scored by Lookback Lens, and select the best candidate that is less likely to be hallucinations.\n3 Contextual Hallucinations Mitigation\n3.1 Lookback Lens Guided Decoding\nTo mitigate the impact of contextual hallucinations\nidentified by the Lookback Lens, we introduce a\nclassifier-guided decoding strategy to guide the gen-\neration toward more contextually accurate outputs.\nThis approach serves as a robustness test of the\nLookback Lens’ ability to handle various text gener-\nation scenarios. While prior studies on controllable\ntext generation adjust the output probabilities using\nclassifiers based on the output tokens (Yang and\nKlein, 2021), our method fundamentally differs by\nnot using the tokens themselves but rather their\nattention maps during generation.\nWe propose Lookback Lens Guided Decoding,\nwhich incorporates Lookback Lens (F) into the de-\ncoding process. Since all tokens in the vocabulary\nshare the same attention pattern during one decod-\ning step, F cannot directly influence one-step to-\nken choice. Instead, F can evaluate multiple-token\nchunks, as each chunk causes different attention\npatterns in multiple decoding steps.\nGiven the context and partially generated text,\nwe independently sample a set of k candidate\nchunks {C1, C2, . . . , Ck} at the same decoding\nstep t. For each chunk Cj, the associated lookback\nratios are averaged to form a feature vector ¯ vj. As\nshown in Figure 2, we select the best candidate C∗\npredicted by F and append to the generation,\nC∗ = arg max\nCj∈{C1,C2,...,Ck}\nF(¯ vj).\nWe repeat this process until it generates the EOS\ntoken or reaches the maximum length.\n3.2 Experimental Setup\nWe evaluateLookback Lens Guided Decoding on\nthree tasks that involve generating texts condi-\ntioned on given contexts, including summariza-\ntion with XSum (Narayan et al., 2018), QA with\nNQ (Kwiatkowski et al., 2019), and multi-turn con-\nversations with MT-bench (Zheng et al., 2024).\nFor testing the generalization ability of the Look-\nback Lens, we only train it with the CNN/DM sum-\n4\nmarization dataset from the detection task in Sec-\ntion 2.2. Thus, only the XSum dataset will be the\nsame-task transfer setting, while NQ and MT-bench\nwill be cross-task transfer setting.\nXSum To test the Lookback Lens’s effectiveness\nat transferring across data distributions for the same\ntask (summarization), we use 1,000 examples sam-\npled from the testing set of XSum. Prior stud-\nies (Maynez et al., 2020) indicate that traditional\nevaluation metrics such as ROUGE (Lin, 2004) or\nBERTScore (Zhang et al., 2019a) correlated poorly\nwith human evaluation on faithfulness and factu-\nality. Recent studies (Chiang and Lee, 2023; Liu\net al., 2023) also show a strong correlation between\nGPT-4 (OpenAI, 2023) evaluation and human eval-\nuation. Thus, we report the averaged accuracy from\nthe binary judgments of GPT-4o, with the prompts\nin Appendix B.1. We also conduct a pilot study\nfor human evaluation on GPT-4o’s judgment in\nAppendix B.2, finding that 97% of the GPT-4o\njudgments are consistent with human judgment.\nNatural Questions We use the NQ data from\nthe setup of Liu et al. (2024) we describe in Ap-\npendix C.2 and evaluate the best span exact match\nfollowing Kandpal et al. (2023); Mallen et al.\n(2023).\nMT-Bench We consider a multi-turn conversa-\ntions setup where the model needs to follow previ-\nous chat history. We use MT-bench (Zheng et al.,\n2024), a multi-turn instruction-following bench-\nmark covering eight categories. We focus exclu-\nsively on generating responses for the second turn\nand use GPT-3.5’s responses as the default for the\nfirst turn. We use GPT-4 to score the model’s an-\nswers on a scale of 1 to 10 based on various factors,\nincluding helpfulness, relevance, accuracy, depth,\ncreativity, and level of detail of the response.\nAdditionally, since we are particularly interested\nin mitigating contextual hallucinations, we further\nexclude math questions and evaluate the remaining\n50 general questions. We specifically instruct GPT-\n4o to focus on whether the responses are faithful to\nthe chat history (see prompt in Appendix B.1). We\nrefer to this setup as MT-Bench (hallu.).\nBaselines To evaluate the performance of our pro-\nposed method, we compared it against the follow-\ning baselines: 1) Greedy Decoding: generating re-\nsponses using the LLaMA-2-7B-Chat model (Tou-\nvron et al., 2023) through greedy decoding. 2)\nOther Classifier-Guided Decoding: using exactly\nMethod XSum NQ MT-Bench\nHallu. Ori.\nGreedy Decoding 49.0 71.2 6.08 5.10\nText-based classifier guided decoding\nSoTA NLI† 59.0 74.2 6.12 5.03\nNLI (our impl.) 44.1 72.5 5.72 4.99\nHidden states based classifier guided decoding\n32nd layer 48.3 73.9 5.49 4.91\n28th layer 48.9 73.0 5.71 5.06\n24th layer 47.5 73.9 5.65 5.16\nLookback Lens guided decoding\nOurs 58.6 74.2 6.27 5.10\nTable 3: Decoding results using 8 candidates per chunk\nin a chunk size of 8. We compare our methods with\ngreedy decoding and classifier-guided decoding using\nthe NLI models, and hidden state representations of\ndifferent layers. †The SoTA NLI is trained on 731k\nexamples so it may not be directly comparable.\nthe same setting but with different classifiers intro-\nduced in Section 2.2, including text-based entail-\nment classifiers and hidden states-based classifiers.\n3.3 Main Results\nWe show our results using eight candidates per\nchunk in a chunk size of eight in Table 3, and\nthe ablation with different chunk sizes is shown\nin Table 6. Lookback Lens Guided Decoding can\nimprove the performance on both in-domain task\n(XSum, by 9.6%) and out-of-domain tasks (NQ,\nby 3%). The original greedy decoding results on\nXSum achieved 49.0% correct which means 510\nexamples were hallucinated. Our decoding method\nsignificantly reduced the number of hallucinated\nexamples from 510 to 414, resulting in an 18.8%\nreduction in the hallucinated examples. This re-\nsult is on par with using SoTA NLI to guide the\ndecoding, where SoTA NLI is trained on roughly\n731k annotated summarization examples, which is\n700× larger compared to our 1k training set. (See\nAppendix C.1.) In contrast, decoding guided by\nhidden states-based or the NLI (our implementa-\ntion) classifiers, both trained on the same data of\nour method, can only slightly improve the perfor-\nmance on NQ, but not for XSum, probably due\nto the issue of distribution shift, highlighting the\nadvantages of Lookback Lens in generalization abil-\nity.\nFor MT-bench, we evaluate both settings: the\noriginal setting (ori.) and the setting that is specifi-\ncally for judging contextual hallucinations (hallu.).\n5\nSource Target Predefined Sliding\nSpan Window\nLookback Lens: Train 13B → Test 13B\nQA Sum. 84.0 60.4\nSum. QA 84.3 60.8\nQA-train QA 93.3 63.7\nLookback Lens: Train 7B → Test 13B\nQA Sum. 73.5 58.8\nSum. QA 78.2 60.5\nQA-train QA 80.6 62.4\nTable 4: Cross model transfer results on detection tasks.\nWe do not expect our method can improve on the\noriginal setting, because it evaluates many factors\nsuch as helpfulness, relevance, etc. But we expect\nto see an improvement on the hallucination setting.\nThe results shown in Table 3 suggest that our de-\ncoding method can boost the performance on the\nhallucination setting while maintaining the same\nperformance in the original setting, which shows\nthat our decoding method is effective in reducing\nhallucinations without compromising the overall\ngeneration quality.\n4 Cross-model Transfer\nOne benefit of using the lookback ratio to capture\nhigher-level model patterns for hallucination detec-\ntion is its potential to better transfer across models.\nA classifier trained with one model’s lookback ra-\ntio could potentially be applied to another model\nwithout retraining, provided correlation between\nthe target model’s attention pattern and that of the\noriginal model. Here, we show that we can transfer\na Lookback Lens trained on attention maps from\nLLaMA-2-7B-Chat to LLaMA-2-13B-Chat with-\nout any retraining.\nSince the total numbers of attention heads are\ndifferent in 7B and 13B models, and there is no\nobvious one-to-one mapping between the heads,\nwe use a linear regression model to map the heads\nfrom the 13B model to the heads in 7B model.\nConcretely, we have 1024 heads in 7B and 1600\nheads in 13B. We extract the averaged lookback\nratio per head for all the |D| training examples,\nresulting in a 1024 × |D| matrix and a 1600 × |D|\nmatrix.2 We then fit a linear regression model to\nmap the heads to reconstruct the 7B heads from\n13B heads. After applying the linear transformation\nto the lookback ratio from 13B, the transformed\n2To ensure that two models are generating the same content\nwhen extracting lookback ratio, we decode from 7B and run\nthe 13B model on the 7B outputs.\nMethod XSum NQ\nGreedy 52.9 74.0\nText-based classifier guided decoding\nSoTA NLI† 59.6 74.4\nMethod CNN/DM NQ-train CNN/DM\n→XSum →NQ →NQ\nLookback Lens guided decoding\n13B → 13B 57.9 75.6 74.8\n7B → 13B 56.1 76.4 73.7\nTable 5: Cross model transfer from LLaMA-2-7B-chat\nto LLaMA-2-13B-chat using greedy decoding and clas-\nsifier guided sampling methods with chunk size 8.\nheads can be directly used by 7B’s classifiers. See\ndetails in Appendix C.1.\nThe detection results are shown in Table 4. We\nfirst show the same-model (13B →13B) + cross-\ntask transfer result, and the cross-model (7B→13B)\n+ cross-task transfer result. Although cross-model\ntransfer yields slightly worse results compared to\nsame-model transfer, the AUROC scores are still\nnon-trivially high. Consider that doing cross-model\n+ cross-task transfer at the same time may be tough\nto Lookback Lens, we also include one more setting\nthat does training on 2.5K examples of the NQ\ntraining set3 and then transfer to the NQ testing set.\nWe see the cross-model same-task transfer results\nare even closer to the same-model transfer results.\nGiven promising results on detection tasks,\nwe apply cross-model transfer to Lookback Lens\nGuided Decoding . We conduct the same-task\ntransfer setting: NQ-train (7B) to NQ (13B), and\nCNN/DM (7B) to XSum (13B). In Table 5, we ob-\nserve a performance improvement similar to same-\nmodel transfer using 13B itself, or using the SoTA\nNLI model applied on the 13B decoding. How-\never, on cross-task + cross-model transfer settings:\nCNN/DM (7B) to NQ (13B), we do not observe\nsignificant improvements where we attribute to the\nlarger distribution shift. We leave this challenging\nsetting for future work.\n5 Discussions and Ablations\nIn this section, we further conduct various experi-\nments and ablation studies on the Lookback Lens\nand its corresponding classifier guided decoding.\nEffect of Chunk Size In Section 3.3 (Table 3),\nwe experiment with chunk size = 8. Here, we study\n3The NQ-train 2.5K data is annotated in the same method\nto annotate NQ testing set, as described in Section 2.2.\n6\nthe effect of varying chunk sizes, from 4, 8, to 16.\nWe see that there is a slight trend that Lookback\nLens guided decoding prefers shorter chunk size\nfor NQ and longer chunk size for XSum. However,\nin general the improvements are consistent across\ndifferent chunk sizes, thus reducing the need to\noptimize for chunk sizes.\nMethod NQ XSum\nChunk size= 4 8 16 4 8 16\nGreedy 71.2 49.0\nText-based classifier guided decoding\nSoTA NLI† 73.7 74.2 74.4 57.3 59.0 62.1\nHidden states based classifier guided decoding\n32nd layer 72.6 73.9 72.7 48.9 48.3 48.3\n28th layer 72.9 73.0 74.1 47.2 48.9 47.1\n24th layer 75.0 73.9 72.5 47.6 47.5 51.2\nLookback Lens guided decoding\nOurs 75.4 74.2 74.3 53.2 58.6 57.7\nTable 6: Performance comparison on various datasets\nusing different methods and chunk sizes.\nMethod\nPredefined Span\nQA →Sum. Sum. →QA\nAll heads 85.3 82.0\nTop-k heads only\nwith k = 10 50 100 10 50 100\nLargest mag. 71.2 82.3 82.8 79.2 80.3 81.1\nMost positive 65.1 74.9 75.4 66.3 70.3 74.4\nMost negative 59.5 67.5 74.4 66.4 70.2 73.0\nTable 7: Cross-task transfer AUROC using top- k at-\ntention heads selected according to: coefficients with\nthe largest magnitude (largest mag.), most positive, and\nmost negative. We consider k = 10, 50, and 100.\nPredictive Power of Different Heads In the\naforementioned experiments, we utilize all atten-\ntion heads to train the Lookback Lens. We are thus\ninterested in how the predictive power is distributed\namong different heads in making predictions. That\nis, how much performance can we recover if we\nonly utilize a subset of heads? To answer this, we\nuse the coefficients in the linear classifier of the\nLookback Lens (in Section 2) to estimate the impor-\ntance of each head in detecting hallucinations.\nIn Table 7, we show the results on detection tasks\nachieved by different detectors trained using only a\nsubset of top-k heads with the largest magnitude of\ncoefficients in the original Lookback Lens trained\nwill all heads. The results show that the predic-\nLayers\nPredefined Span\nQA → Sum. Sum. → QA\nLayer 1-4 69.6 64.0\nLayer 5-8 75.6 70.1\nLayer 9-12 75.4 68.3\nLayer 13-16 81.2 78.2\nLayer 17-20 80.8 78.2\nLayer 21-24 64.4 73.1\nLayer 25-28 66.0 74.4\nLayer 29-32 66.4 71.4\nLayer 1-32 85.3 82.0\nTable 8: Cross-task transfer AUROC among layers.\ntive power is not concentrated only on a subset of\nheads. Using only top-10 heads is worse than using\nall heads, and increasing k consistently improves\nperformance and top-100 heads largely recover the\nmodel’s performance using all heads.\nMore interestingly, we also include the results\nthat only select the top-k heads among the heads\nwith most positive/negative coefficients, which are\npositive/negatively correlated to factuality. On the\nheads with positive coefficients, higher lookback\nratio (i.e., when the heads attend at the context\nmore) indicates higher factuality and less halluci-\nnation; conversely, heads with negative coefficients\nsuggest a lower lookback ratio (i.e., attending to\ngenerated tokens more) is more likely to be truth-\nful. Table 7 shows that none of positive or negative\nheads alone can be on par with using the top- k\nlargest magnitude heads. This result implies that\nboth positive and negative heads are critical for a\nmodel to generate factual responses. We conjecture\nthat the positive heads may specialize at context\ngrounding, and thus higher lookback ratio on these\nheads leads to more factual response. On the other\nhand, the negative heads may be critical at ensuring\nconsistency in its own generation, and thus should\nattend to the generated tokens more. We leave\nfurther investigation on this interesting balance for\nfuture work. Meanwhile, we visualize the lookback\nratio of positive/negative heads in Appendix D.1.\nReducing Number of Layers We experiment\nwith using only a subset of layers for Lookback\nLens, as shown in Table 8. We can see that the\npredictive power is not concentrated in any subset\nof layers, as none of them can recover the perfor-\nmance of the full model that uses all layers. How-\never, we observe that the middle layers (13-16, 17-\n20) are slightly more useful than other layers.\n7\nShe had the \"biggest year of her career\", playing 95 shows, bringing in an average $2.4m (£1.5m) per city, according to Forbes.\nEndorsement deals with companies like Pepsi and H&M, along with the surprise album she released in December 2013, helped her to the top spot.\nThe self-titled album was released on iTunes with no prior promotion.\nBeyonce has just announced she'll be releasing a platinum version of that album later this month, which will include new songs and a concert video.\nSource: Forbes\nTaylor Swift came in second on the Forbes list, taking home an estimated $64m (£40m).   …[truncated]\nDocument\n \n Beyonce topped the Forbes list of highest-paid female musicians, earning an estimated $100m (£64m) in the past year.\nSummary: Greedy Decoding\nAccording to Forbes, Beyonce had the \nbiggest year of her career in terms (0.99)\nSummary: Lookback Lens Guided Decoding\nAccording to Forbes, Beyonce came in ﬁrst \nplace on their list of highest (0.97)\nAccording to Forbes, Beyonce earned an \nestimated $30m (£ (0.65)\n⃛⃛\nAccording to Forbes, Beyonce had the \nbiggest year of her career in terms of \nearnings, bringing in an average of \n$2.4m per city\n⃛⃛\n, thanks to endorsement deals and her \nsurprise album. Taylor Swift came (0.94)\nand earning an estimated $80m overall. \nEndorsement deals (0.12)\nand earning an estimated $100m overall. \n(0.05)\n⃛⃛\nFigure 3: Qualitative example on XSum using the LLaMA-2-7B-Chat model with greedy decoding and Lookback\nLens Guided Decoding. The numbers in the parenthesis show the predicted scores from the Lookback Lens.\nQualitative Study We show qualitative exam-\nples from XSum in Figure 3 to illustrate how Look-\nback Lens guided decoding improves performance.\nGreedy decoding from LLaMA-2-7B-Chat results\nin a hallucination, i.e. $100m (£64m), that does not\nexist in the input document. However, the Look-\nback Lens is able to assign low scores for the chunk\ncandidates that have contextual hallucinations (as\nmarked in red). Therefore, Lookback Lens Guided\nDecoding is able to help the model generate a sum-\nmary that is factual to the given context.\n6 Related Work\nHallucinations in LLMs Simhi et al. (2024) de-\nfined close-book hallucination vs open-book hal-\nlucination for settings of relying on parametric\nknowledge vs knowledge in context. We termopen-\nbook hallucination as contextual hallucination for\nbetter clarity. Previous studies in hallucinations pri-\nmarily focus on close-book hallucinations (Chen\net al., 2023; Min et al., 2023; Chern et al., 2023) and\ntheir detection (Azaria and Mitchell, 2023; Simhi\net al., 2024) and mitigation (Li et al., 2024; Chuang\net al., 2024; Chen et al., 2024a; Zhang et al., 2024).\nMost of the studies focus on leveraging LLM’s in-\nternal representations, such as hidden states (Burns\net al., 2023; Azaria and Mitchell, 2023), MLP out-\nputs (Zhang et al., 2024; Simhi et al., 2024), at-\ntention block outputs (Zhang et al., 2024; Simhi\net al., 2024) and attention head outputs (Li et al.,\n2024; Chen et al., 2024b; Simhi et al., 2024). Our\nwork, however, focuses on contextual hallucina-\ntions, where models produce content inconsistent\nwith the provided context (Maynez et al., 2020;\nFabbri et al., 2021; Shi et al., 2023). Thus, differ-\nent from prior studies, we focus on the attention\nmaps instead of internal representations, as we be-\nlieve that the attention maps patterns record how\nthe LLM process the given contextual information.\nMost of the prior studies treat detection and miti-\ngation as two separate tasks, expect for Simhi et al.\n(2024); Chen et al. (2024a). Our work focuses not\nonly on detection, but also tries to incorporate the\ndetector into the decoding process to further miti-\ngate the contextual hallucinations. Recently, Simhi\net al. (2024) also explored detecting and mitigat-\ning both close-book and open-book hallucinations.\nHowever, their open-book hallucination setting is\nlimited to DisentQA (Neeman et al., 2023), which\ncreates knowledge conflicts between parametric\nknowledge and given context. In contrast, we focus\non LLaMA-2’s naturally generated responses to\ncapture general cases where LLMs fail to follow\nthe context, not just due to knowledge conflicts.\nClassifier Guided Generation Classifier guided\ngeneration aims to control attributes like topic or\nsentiment in text generation. PPLM (Dathathri\net al., 2019) uses gradient ascent to adjust LM prob-\nabilities via attribute classifiers. FUDGE (Yang and\nKlein, 2021) uses an attribute predictor on partial\nsequences to modify LM probabilities. Our method\nuniquely guides generation using classifiers on at-\ntention maps, setting it apart from prior approaches.\n8\nSelf-attention and Model Behavior The atten-\ntion mechanism, initially introduced in RNN-\nbased encoder-decoder for neural machine trans-\nlation (Bahdanau et al., 2015; Luong et al., 2015),\nwas later adopted in the Transformer model’s\nself-attention module (Vaswani et al., 2017), en-\nabling greater parallelization. Self-attention’s in-\nterpretability has led researchers to use it for un-\nderstanding model behaviors (Clark et al., 2019;\nHao et al., 2021; Vashishth et al., 2019). Our\nwork demonstrates that attention maps in LLMs\nare effective for detecting contextual hallucinations,\nproviding a lightweight and interpretable solution\ncompared to complex hidden representation meth-\nods (Zhang et al., 2024; Chen et al., 2024b).\n7 Conclusion\nWe introduce theLookback Lens, a lightweight clas-\nsifier designed to detect contextual hallucinations\nby utilizing the lookback ratio, which is computed\nsolely from attention weights. This classifier not\nonly effectively identifies contextual hallucinations\nbut also mitigates them through Lookback Lens\nGuided Decoding from the LLM. Remarkably, the\nmethod is transferable across various tasks, and\neven across models after mapping their attention\nheads. This research opens up new possibilities\nfor leveraging attention map information to combat\nhallucinations in large language models.\nLimitations\nDespite the effectiveness of the Lookback Lens and\nits decoding, there are several limitations to con-\nsider.\n• First, the performance upper bound of Look-\nback Lens Guided Decoding is limited by the\nsampling capabilities of the LLM itself. If the\nLLM fails to sample the correct chunk among\nthe eight candidates, the Lookback Lens can-\nnot correct the error.\n• Second, although the Lookback Lens is a\nlightweight classifier with negligible inference\ntime, the requirement to sample multiple can-\ndidates from the LLM increases the total in-\nference time. We argue that Lookback Lens\nGuided Decoding is a preliminary approach\nthat demonstrates the feasibility of integrating\nthe Lookback Lens into the decoding process,\nas well as a robustness test for the Lookback\nLens to handle various text generation scenar-\nios. However, other options, such as inter-\nvening in the attention map mechanism based\non Lookback Lens signals, could potentially\nachieve faster inference, and we leave this for\nfuture work.\n• Lastly, the Lookback Lens relies on annotated\nexamples of around 1k-2k to train the classi-\nfier. While other end-to-end methods (Chuang\net al., 2024) can mitigate close-book halluci-\nnations without training data, they lack inter-\npretability due to the absence of a detection\nstep. Nevertheless, we believe that requiring\n1,000 annotated examples is a feasible setting.\nAcknowledgement\nWe sincerely thank Philip Schroeder, Huirong Wen,\nAndrew Rouditchenko, Nishad Gothoskar, Ani\nNrusimha, Howard Chen, Weijia Shi, and Nour\nJedidi for their discussion and help in this project.\nThis research was sponsored by the United States\nAir Force Research Laboratory and the United\nStates Air Force Artificial Intelligence Accelerator\nand was accomplished under Cooperative Agree-\nment Number FA8750-19-2-1000. The views and\nconclusions contained in this document are those\nof the authors and should not be interpreted as rep-\nresenting the official policies, either expressed or\nimplied, of the United States Air Force or the U.S.\nGovernment. The U.S. Government is authorized\nto reproduce and distribute reprints for Government\npurposes notwithstanding any copyright notation\nherein. Linlu and Yoon were supported in part by\nMIT-IBM Watson AI Lab.\nEthics Statement\nIn this research, we used publicly available datasets\nand we did not collect any personal information.\nAll datasets and models are used in accordance\nwith their intended use and licenses. Our method\nis designed to improve the factuality of large lan-\nguage models (LLMs), which can have a positive\nimpact on various applications, such as question-\nanswering systems, summarization systems, and\nother applications that rely on LLMs. When de-\nployed, however, our approach still carries the is-\nsues stemming from LLMs, which means that there\nis a risk that the LLM can produce biased, harmful,\nor offensive output. Therefore, caution should be\nexercised before implementing similar approaches\nin real-world applications.\n9\nReferences\nAmos Azaria and Tom Mitchell. 2023. The internal\nstate of an llm knows when it’s lying. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2023, pages 967–976.\nDzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, ICLR\n2015.\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Stein-\nhardt. 2023. Discovering latent knowledge in lan-\nguage models without supervision. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nJames Campbell, Richard Ren, and Phillip Guo.\n2023. Localizing lying in llama: Understanding in-\nstructed dishonesty on true-false questions through\nprompting, probing, and patching. arXiv preprint\narXiv:2311.15131.\nJifan Chen, Grace Kim, Aniruddh Sriram, Greg Durrett,\nand Eunsol Choi. 2023. Complex claim verification\nwith evidence retrieved in the wild. arXiv preprint\narXiv:2305.11859.\nShiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu,\nTeng Xiao, Siyang Gao, and Junxian He. 2024a.\nIn-context sharpness as alerts: An inner represen-\ntation perspective for hallucination mitigation. arXiv\npreprint arXiv:2403.01548.\nZhongzhi Chen, Xingwu Sun, Xianfeng Jiao, Fengzong\nLian, Zhanhui Kang, Di Wang, and Chengzhong Xu.\n2024b. Truth forest: Toward multi-scale truthful-\nness in large language models through intervention\nwithout tuning. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 38, pages\n20967–20974.\nI Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua\nFeng, Chunting Zhou, Junxian He, Graham Neubig,\nPengfei Liu, et al. 2023. Factool: Factuality detec-\ntion in generative ai–a tool augmented framework\nfor multi-task and multi-domain scenarios. arXiv\npreprint arXiv:2307.13528.\nCheng-Han Chiang and Hung-Yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions? In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 15607–15631.\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon\nKim, James R Glass, and Pengcheng He. 2024. Dola:\nDecoding by contrasting layers improves factuality in\nlarge language models. In The Twelfth International\nConference on Learning Representations.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does bert look\nat? an analysis of bert’s attention. arXiv preprint\narXiv:1906.04341.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language models:\nA simple approach to controlled text generation. In\nInternational Conference on Learning Representa-\ntions.\nAlexander R Fabbri, Chien-Sheng Wu, Wenhao Liu,\nand Caiming Xiong. 2021. Qafacteval: Improved\nqa-based factual consistency evaluation for summa-\nrization. arXiv preprint arXiv:2112.08542.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2021. Self-\nattention attribution: Interpreting information interac-\ntions inside transformer. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 35,\npages 12963–12971.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. Preprint, arXiv:2111.09543.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. True: Re-evaluating factual\nconsistency evaluation. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3905–3920.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2023. Large language\nmodels struggle to learn long-tail knowledge. In In-\nternational Conference on Machine Learning, pages\n15696–15707. PMLR.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:453–\n466.\nPhilippe Laban, Tobias Schnabel, Paul N Bennett, and\nMarti A Hearst. 2022. Summac: Re-visiting nli-\nbased models for inconsistency detection in summa-\nrization. Transactions of the Association for Compu-\ntational Linguistics, 10:163–177.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\n10\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\nNie, and Ji-Rong Wen. 2023. Halueval: A large-\nscale hallucination evaluation benchmark for large\nlanguage models. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6449–6464.\nKenneth Li, Oam Patel, Fernanda Viégas, Hanspeter\nPfister, and Martin Wattenberg. 2024. Inference-\ntime intervention: Eliciting truthful answers from\na language model. Advances in Neural Information\nProcessing Systems, 36.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024. Lost in the middle: How language mod-\nels use long contexts. Transactions of the Association\nfor Computational Linguistics, 12:157–173.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2511–2522.\nMinh-Thang Luong, Hieu Pham, and Christopher D\nManning. 2015. Effective approaches to attention-\nbased neural machine translation. In Proceedings\nof the 2015 Conference on Empirical Methods in\nNatural Language Processing, pages 1412–1421.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 9802–9822.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. arXiv preprint\narXiv:2005.00661.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. arXiv preprint\narXiv:2305.14251.\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807.\nElla Neeman, Roee Aharoni, Or Honovich, Leshem\nChoshen, Idan Szpektor, and Omri Abend. 2023.\nDisentqa: Disentangling parametric and contextual\nknowledge with counterfactual question answering.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 10056–10070.\nOpenAI. 2022. Introducing chatgpt.\nOpenAI. 2023. Gpt-4 technical report.\nOpenAI. 2024. Hello gpt-4o.\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\nGet your vitamin C! robust fact verification with\ncontrastive evidence. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 624–643, Online. As-\nsociation for Computational Linguistics.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073–\n1083.\nWeijia Shi, Xiaochuang Han, Mike Lewis, Yulia\nTsvetkov, Luke Zettlemoyer, and Scott Wen-tau\nYih. 2023. Trusting your evidence: Hallucinate\nless with context-aware decoding. arXiv preprint\narXiv:2305.14739.\nAdi Simhi, Jonathan Herzig, Idan Szpektor, and Yonatan\nBelinkov. 2024. Constructing benchmarks and inter-\nventions for combating hallucinations in llms. arXiv\npreprint arXiv:2404.09971.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction and\nVERification. In NAACL-HLT.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nShikhar Vashishth, Shyam Upadhyay, Gaurav Singh\nTomar, and Manaal Faruqui. 2019. Attention in-\nterpretability across nlp tasks. arXiv preprint\narXiv:1909.11218.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nVectara. 2023. vectarahallucination_valuation_model.\nhttps://huggingface.co/vectara/hallucina\ntion_evaluation_model. Accessed: 2024-06-12.\n11\nKevin Yang and Dan Klein. 2021. Fudge: Controlled\ntext generation with future discriminators. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n3511–3535.\nShaolei Zhang, Tian Yu, and Yang Feng. 2024.\nTruthx: Alleviating hallucinations by editing large\nlanguage models in truthful space. arXiv preprint\narXiv:2402.17811.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019a. Bertscore: Evaluating\ntext generation with bert. In International Confer-\nence on Learning Representations.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019b.\nPAWS: Paraphrase Adversaries from Word Scram-\nbling. In Proc. of NAACL.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2024.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. Advances in Neural Information Processing\nSystems, 36.\nA Data Creation for Lookback Lens\nOur experimental setup aims to evaluate the ability\nof Lookback Lens to detect hallucinations in large\nlanguage models with attention maps. We consider\nthe summarization task and question-answering\n(QA) task in data creation.\nFor the summarization task, we sampled 1,000\nexamples from the CNN/DM dataset (See et al.,\n2017). For QA, we use 2,655 examples from the\nNatural Questions (Kwiatkowski et al., 2019) from\nthe setup of Liu et al. (2024) to mix the gold docu-\nment with irrelevant documents. To keep our focus\nmore on LLM hallucinations rather than being dis-\ntracted by assessing LLMs’ long-context utilization\nability, we limited context to three documents per\nquestion where the gold document containing the\nanswer was placed in the middle, surrounded by\ntwo irrelevant documents.\nWe prompt LLaMA-2-7B-Chat (Touvron et al.,\n2023) to generate correct responses by greedy de-\ncoding for both tasks to ensure that both halluci-\nnated and non-hallucinated examples derive from\nthe same source distribution. The max length of\ngeneration is set to 256 tokens, or until the EOS\ntoken is generated.\nAfter the annotation was collected, we extract\nhallucinated and non-hallucinated spans, as well\nas the corresponding attention map lookback ratio,\nfrom the LLaMA-2-7B-Chat model, to train the\nLookback Lens classifiers.\nIn the predefined span setting, three types of\nspans are considered as non-hallucinated spans: 1)\nthe text segment before the first hallucinated span\nin the response 2) the text segment after the last\nhallucinated span in the response 3) the response\nannotated as non-hallucinated. All the annotated\nhallucinated spans are used as negative data to train\nthe Lookback Lens.\nIn the sliding window setting, we consider all the\npossible fixed sized chunk with size = 8. If a chunk\nis overlapping with any of the annotated halluci-\nnated spans, then it is considered as hallucinated,\notherwise it is non-hallucinated.\nWhy not use existing data? Initially, we consid-\nered using the HaluEval dataset (Li et al., 2023),\nwhich was created by prompting GPT-3.5 (OpenAI,\n2022) to generate “hallucinated examples” against\nhuman-annotated non-hallucinated responses, on\nsummarization, QA, and dialogue tasks. However,\nwe have concerns that their method introduces a\nbias by creating fundamentally different data distri-\n12\nbutions between hallucinated and non-hallucinated\nexamples. This discrepancy could potentially lead\nthe classifier to learn to distinguish the sources of\nresponses rather than accurately detecting halluci-\nnations.\nAdditionally, we argue that the LLM’s attention\nweight will be more meaningful if the text is gen-\nerated by the same LLM itself, not from external\nsources and teacher forcing to obtain the attention\nweights. To ensure an unbiased and controlled eval-\nuation environment, we generated our own dataset\non summarization and QA tasks.\nB Evaluation Details\nB.1 Evaluation Prompt for GPT-4o\nWe show the templates used to prompt GPT-4o\n(gpt-4o-2024-05-13) in annotating the truthful-\nness of a response and the span-level hallucination\nsegment prediction in Table 9 and Table 10, respec-\ntively for CNN/DM and Natural Questions.\nThis prompt is used for 1) collecting the data to\ntrain the Lookback Lens in Table 1, and 2) evalu-\nating the XSum summarization task in Sections 3,\n4, and 5. We also provide the approximate cost of\nGPT-4o calls (in USD):\n• 1000 examples from XSum is around $8.\n• 1000 examples from CNN/DM is around $12.\n• 2655 examples from NQ is around $16.\nB.2 Human Evaluation on GPT-4o Evaluation\nSummarization To assess the quality of GPT-\n4o’s evaluations, we initially conducted a pilot\nstudy using 70 XSum dataset examples, with na-\ntive English-speaking authors and colleagues as\nevaluators. Evaluators received the document,\nground truth summary, LLaMA-2-7B-Chat’s sum-\nmary, and GPT-4o’s judgment to provide a binary\njudgment on GPT-4o’s accuracy. Our interface is\ndepicted in Appendix B.1 (see Figure 4). This ini-\ntial evaluation affirmed the correctness of GPT-4o’s\njudgments in 68 out of 70 cases. To further verify\nthese results, we expanded our evaluation through\nAmazon MTurk, adding two additional annotations\nper example. Across all 210 evaluations (70 initial\n+ 140 MTurk), only 9 annotations were marked\nincorrect, and in only 2 cases did a majority of\nannotators deem the judgment incorrect (marked\nincorrect by at least two annotators). With a fi-\nnal accuracy of 97.1%, and high intra-annotator\nFigure 4: Screenshot of human annotation interface.\nagreement, the comprehensive evaluation supports\nGPT-4o’s use as an automatic evaluator for the en-\ntire dataset.\nQuestion Answering We expand the human eval-\nuation to Natural Questions dataset using Amazon\nMTurk. The evaluation interface is copied from the\nsummarization setup, but changing “summary” to\n“answer”, as well as adding the “question” field.\nWe take 50 examples and assign each example to\nthree different annotators. There are 7 annotations\nmarked incorrect out of the 150 annotations. In\ntotal, 3 of the examples are marked incorrect by at\nleast two annotators. If applying a majority vote,\n47 out of 50 examples are correct, resulting in a\n94.0% accuracy. This suggests that it is generally\nsufficient to use GPT-4o to verify the generated\nresponses on the question-answering task.\n13\nYou will be provided with a document and a proposed summary. Your task is to determine if the\nproposed summary can be directly inferred from the document. If the summary contains any information\nnot found in the document, it is considered false. Even if the summary is different from a ground\ntruth summary, it might still be true, as long as it doesn’t contain false information.\nFor each proposed summary, explain why it is true or false based on the information from the\ndocument. Focus only on the original document’s content, disregarding any external context.\nAfter your explanation, give your final conclusion as Conclusion: True if the proposed summary is\ncompletely accurate based on the document, or Conclusion: False if it contains any incorrect or\nunsupported information. If your conclusion is ’False’, identify the exact phrases or name entities\nfrom the summary that is incorrect by stating Problematic Spans: [the inaccurate text spans from\nthe summary, in Python list of strings format].\n#Document#: {document}\n#Ground Truth Summary#: {ground_truth_summary}\n#Proposed Summary#: {response}\nWrite your explanation first, and then give your final conclusion as Conclusion: True if\nthe proposed summary is completely accurate based on the document, or Conclusion: False if it\ncontains any incorrect or unsupported information. Add Problematic Spans: [the exact inaccurate\ntext spans from the summary, in a list of strings] if your conclusion is ’False’.\nTable 9: Prompt template for GPT-4o in annotating the truthfulness and predicting span-level hallucinations on\nsummarization tasks. Used for CNN/DM and XSum.\nYou will be provided with a document and a proposed answer to a question. Your task is to determine\nif the proposed answer can be directly inferred from the document. If the answer contains any\ninformation not found in the document, it is considered false. Even if the answer is different from\na ground truth answer, it might still be true, as long as it doesn’t contain false information.\nFor each proposed answer, explain why it is true or false based on the information from the document.\nFocus only on the original document’s content, disregarding any external context.\nAfter your explanation, give your final conclusion as Conclusion: True if the proposed answer is\ncompletely accurate based on the document, or Conclusion: False if it contains any incorrect or\nunsupported information. If your conclusion is ’False’, identify the exact phrases or name entities\nfrom the answer that is incorrect by stating Problematic Spans: [the inaccurate text spans from the\nanswer, in Python list of strings format].\n#Document#: {document}\n#Ground Truth Answers (a list of valid answers)#: {ground_truth_answers}\n#Proposed Answer#: {response}\nWrite your explanation first, and then give your final conclusion as Conclusion: True if\nthe proposed answer is completely accurate based on the document, or Conclusion: False if it\ncontains any incorrect or unsupported information. Add Problematic Spans: [the exact inaccurate\ntext spans from the answer, in a list of strings] if your conclusion is ’False’.\nTable 10: Prompt template for GPT-4o in annotating the truthfulness and predicting span-level hallucinations on\nquestion-answering tasks. Used for Natural Questions.\nB.3 Evaluation Prompt for MT-Bench\nWe show the evaluation prompt for MT-Bench\n(hallucination) in Table 11. We follow stan-\ndard practice for MT-Bench (original) evaluation4\nand show evaluation prompts in Table 12. We\nevaluate MT-bench (original) with their default\nGPT-4 model gpt-4-0613 and our proposed MT-\nBench (hallucination) with the latest GPT-4o model\n(gpt-4o-2024-05-13).\n4https://github.com/lm-sys/FastChat/tree/main/\nfastchat/llm_judge.\nC Experiment Details\nC.1 Model Details\nState-of-the-art NLI Model We give further de-\ntail on the pretrained SoTA NLI model 5 used as\nour topline hallucination detector. Specifically,\nthe model is based on DeBERTa-V3-base (He\net al., 2021) and further finetuned on a range\nof NLI and summarization datasets with exam-\n5https://huggingface.co/vectara/hallucination\n_evaluation_model\n14\nPlease act as an impartial judge and evaluate the faithfulness and consistency of the response\nprovided by an AI assistant to the user question displayed below. Your evaluation should consider\nwhether the assistant’s answer to the second user question is faithful and consistent to the chat\nhistory. If the answer contains any misinformation not found or not supported by the chat history,\nit is considered a hallucination. You evaluation should focus on the assistant’s answer to the\nsecond user question. Begin your evaluation by providing a short explanation. Be as objective as\npossible. After providing your explanation, you must rate the response on a scale of 1 to 10 by\nstrictly following this format: “[[rating]]\", for example: “Rating: [[5]]\".\n<|The Start of Assistant A’s Conversation with User|>\n### User:\n{question_1}\n### Assistant A:\n{answer_1}\n### User:\n{question_2}\n### Assistant A:\n{answer_2}\n<|The End of Assistant A’s Conversation with User|>\nTable 11: GPT-4o evaluation prompt for MT-bench (hallucination).\nples annotated with factual consistency, including\nFEVER (Thorne et al., 2018), Vitamin C (Schus-\nter et al., 2021) and PAWS (Zhang et al., 2019b).\nRoughly 731k data examples can be collected from\nthe training set of the above three datasets. The\nmodel is reported to have superior performance\nwhen evaluated on TRUE (Honovich et al., 2022)\nSummaC Benchmark (Laban et al., 2022) and\nAnyScale Ranking Test for Hallucinations 6.\nOther Model Details and License\n• Llama-2-7B-Chat: A 7B parameter model\nthat is instruction fine-tuned. HuggingFace\nID: meta-llama/Llama-2-7b-chat-hf.\n• Llama-2-13B-Chat: A 13B parameter model\nthat is instruction fine-tuned. HuggingFace\nID: meta-llama/Llama-2-13b-chat-hf.\n• hallucination_evaluation_model:\nBased on microsoft/deberta-v3-base\nwhich has 86M parameters. HuggingFace ID:\nvectara/hallucination_evaluation_model.\n• DeBERTa-V3-Base: a 86M parameters en-\ncoder based model. HuggingFace ID:\nmicrosoft/deberta-v3-base.\nThe above models have the following licenses.\n6https://www.anyscale.com/blog/llama-2-is-abo\nut-as-factually-accurate-as-gpt-4-for-summaries\n-and-is-30x-cheaper\n• Llama-2-7B-Chatis under the Llama 2 Com-\nmunity License Agreement.\n• Llama-2-13B-Chat is under the Llama 2\nCommunity License Agreement.\n• vectara/hallucination_evaluation_model\nis under the Apache 2.0 License.\n• DeBERTa-V3-Baseis under MIT License.\nInference Details We run all the models on\nNVIDIA A6000 (48GB) and V100 (32GB) GPUs.\nWe do not train the model, but only run the in-\nference part. Each of the examples takes around\n20-30 seconds for 7B model, 40-60 seconds for\n13B model to generate responses using our Look-\nback Lens Guided Decoding . Please check Ap-\npendix C.2 to estimate the total running time on\neach of the datasets, as it depends on number of\nexamples.\nAll the inferences are run with either greedy\ndecoding or sampling using temperature 0.9 and\ntop-p sampling with p = 0.95. The implementation\nis based on Huggingface Transformers packages.7\nAll the scores in the paper are from a single run due\nto the limited computation for the large models.\nClassifier Training Details We use Scikit-Learn\nsklearn.linear_model.LogisticRegression8\n7https://github.com/huggingface/transformers\n8https://scikit-learn.org/stable/modules/gene\nrated/sklearn.linear_model.LogisticRegression.ht\nml\n15\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI\nassistant to the user question displayed below. Your evaluation should consider factors such as\nthe helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. You\nevaluation should focus on the assistant’s answer to the second user question. Begin your evaluation\nby providing a short explanation. Be as objective as possible. After providing your explanation,\nyou must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\",\nfor example: \"Rating: [[5]]\".\n<|The Start of Assistant A’s Conversation with User|>\n### User:\n{question_1}\n### Assistant A:\n{answer_1}\n### User:\n{question_2}\n### Assistant A:\n{answer_2}\n<|The End of Assistant A’s Conversation with User|>\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant\nto the user question. Your evaluation should consider correctness and helpfulness. You will be\ngiven a reference answer and the assistant’s answer. You evaluation should focus on the assistant’s\nanswer to the second question. Begin your evaluation by comparing the assistant’s answer with the\nreference answer. Identify and correct any mistakes. Be as objective as possible. After providing\nyour explanation, you must rate the response on a scale of 1 to 10 by strictly following this format:\n\"[[rating]]\", for example: \"Rating: [[5]]\".\n<|The Start of Reference Answer|>\n### User:\n{question_1}\n### Reference answer:\n{ref_answer_1}\n### User:\n{question_2}\n### Reference answer:\n{ref_answer_2}\n<|The End of Reference Answer|>\n<|The Start of Assistant A’s Conversation with User|>\n### User:\n{question_1}\n### Assistant A:\n{answer_1}\n### User:\n{question_2}\n### Assistant A:\n{answer_2}\n<|The End of Assistant A’s Conversation with User|>\nTable 12: GPT-4 evaluation prompt for general questions (top) and math questions (bottom) on MT-bench (original).\n16\nto train the classifiers of Lookback Lens on CPU\nmachine. We use all the default hyperparameters,\nsuch as L2 penalty, etc, but we change the\nmax_iter to 1000 to ensure it is converged.\nHeads Mapping Details We use Scikit-Learn\nsklearn.linear_model.LinearRegression9 in\nSection 4, to fit a linear transformation from\nLLaMA-2-13B-Chat’s attention heads to LLaMA-\n2-7B-Chat’s attention heads. It is computed to\nsolve the close-form Ordinary Least Squares opti-\nmization problem, without gradient descent. We\nuse all the default hyperparameters and run it on\nour CPU machine.\nC.2 Dataset Details\nThe datasets we used in the paper have the follow-\ning details:\n• CNN/DM: sampled 1000 examples from the\ntesting set. Apache-2.0 license. https://hu\nggingface.co/datasets/abisee/cnn_dai\nlymail\n• Natural Questions: Apache-2.0 license. Test-\ning set: 2655 examples from https://gith\nub.com/nelson-liu/lost-in-the-middl\ne. NQ-train: sampled 2499 examples from\nits training set, using the positive document\nprovided by https://github.com/faceboo\nkresearch/DPR\n• XSum: 1000 examples sampled from the test-\ning set. MIT license. https://github.com\n/EdinburghNLP/XSum\n• MT-bench: 80 examples. Apache-2.0 license.\nhttps://github.com/lm-sys/FastChat/\ntree/main/fastchat/llm_judge\nD Additional Results\nD.1 Visualization\nWe visualize the lookback ratio of the top-10 most\npositive/negative heads when LLaMA-2-7B-Chat\ndecodes the answer for an NQ example. The top-10\nmost positive/negative heads are selected with the\nmost positive/negative coefficients from the clas-\nsifier. The green rectangle frames the part that\ncontains the hallucinations, i.e. and in Germany in\nthe 14th century. We can see that during the gener-\nation of the hallucinated span, the positive heads,\n9https://scikit-learn.org/stable/modules/gene\nrated/sklearn.linear_model.LinearRegression.html\nFigure 5: Top-10 positive/negative heads ranked from\ntop to the bottom by the magnitude of their coefficients\nin the Lookback Lens classifier.\nespecially for the top-1 heads (topmost), show a\nlower lookback ratio (in blue), while the negative\nheads show a slightly higher lookback ratio (in red).\nHowever, the behavior ofLookback Lens still needs\nto be determined by the collective behavior of all\nheads and the weight and bias of the classifier.\nD.2 Using Multiple or All Layers for Hidden\nStates\nMultiple Layer We follow the prior\nstudy (Azaria and Mitchell, 2023) to use\nthe layers with the best predictive power in\nhallucination detection: 32nd/28th/24th/20th\nlayers. We concatenate the 4 layer features\ninto a huge feature. Please note that the hidden\ndimension of LLaMA-7B is 4096, so combining 4\nlayers would result in a 16384-dim feature vector.\nIn contrast, our Lookback Lens feature for the 7B\nmodel is only 1024-dim. Thus, the big classifier\nusing 16384 input features is supposed to be more\neffective given that it uses 10x more features.\nHowever, the result shown in Table 13 indicates\nthat concatenating 4 layers is still less effective\ncompared to our Lookback Lens.\nAll Layers We also try to use the hidden states\nfrom all layers, but concatenating them all will re-\nsult in a huge feature vector with dimensions of\nmore than 100k and make the classifier extremely\nslow in training. Thus, we perform max/average\npooling for the features across different layers, re-\nsulting in 4096-dim feature vectors as the classifier\ninputs. The results shown in the table below are\nstill worse than our Lookback Lens results.\nThe two experiments above indicate that using\n17\nMethod\nAUROC (sliding window = 8)\nNQ → Sum. Sum. → NQ\nResidual outputs (hidden states)\nLayer 32 56.1 59.4\nLayer 28 57.7 58.8\nLayer 24 58.3 58.3\nLayer 20 57.6 59.5\nConcatenate above 4 layers 58.8 59.2\nMax pooling all 32 layers 56.7 59.2\nAverage pooling all 32 layers 57.3 59.2\nOurs: Lookback Lens 66.1 66.0\nTable 13: AUROC results for different methods of uti-\nlizing hidden states.\nMethod\nAUROC (sliding window = 8)\nNQ → Sum. Sum. → NQ\nAttention block outputs\nLayer 32 57.6 60.7\nLayer 28 58.5 57.2\nLayer 24 56.3 57.2\nResidual outputs (hidden states)\nLayer 32 56.1 59.4\nLayer 28 57.7 58.8\nLayer 24 58.3 58.3\nOurs: Lookback Lens 66.1 66.0\nTable 14: AUROC results for different layers and out-\nputs.\nmultiple or all layers may not be the key to making\nthe classifier accurate. Instead, by designing good\nfeatures like lookback ratio, the compact 1024-dim\nfeature can be even more effective compared to the\n10x bigger high-dimensional hidden state features.\nD.3 Comparing Attention Outputs with\nHidden States\nSome papers mention that attention block out-\nputs could be more useful for detecting halluci-\nnations (Campbell et al., 2023; Li et al., 2024),\nwhile our main experiments only consider the hid-\nden states as input features for detecting contextual\nhallucinations. Here we include additional experi-\nment results that use attention block outputs instead.\nIn Table 14, we show that there is no significant\ndifference when switching to attention block out-\nputs, and our Lookback Lens still outperforms these\nbaselines.\n18"
  },
  {
    "source": "2012.07177v2.pdf",
    "content": "Simple Copy-Paste is a Strong Data Augmentation Method\nfor Instance Segmentation\nGolnaz Ghiasi* 1 Yin Cui* 1 Aravind Srinivas* † 1,2\nRui Qian† 1,3 Tsung-Yi Lin1 Ekin D. Cubuk1 Quoc V . Le1 Barret Zoph1\n1Google Research, Brain Team 2 UC Berkeley 3 Cornell University\nAbstract\nBuilding instance segmentation models that are data-\nefﬁcient and can handle rare object categories is an\nimportant challenge in computer vision. Leveraging data\naugmentations is a promising direction towards addressing\nthis challenge. Here, we perform a systematic study of\nthe Copy-Paste augmentation ( e.g., [13, 12]) for instance\nsegmentation where we randomly paste objects onto an\nimage. Prior studies on Copy-Paste relied on modeling the\nsurrounding visual context for pasting the objects. How-\never, we ﬁnd that the simple mechanism of pasting objects\nrandomly is good enough and can provide solid gains on\ntop of strong baselines. Furthermore, we show Copy-Paste\nis additive with semi-supervised methods that leverage\nextra data through pseudo labeling ( e.g. self-training).\nOn COCO instance segmentation, we achieve 49.1 mask\nAP and 57.3 box AP , an improvement of +0.6 mask AP\nand +1.5 box AP over the previous state-of-the-art. We\nfurther demonstrate that Copy-Paste can lead to signiﬁcant\nimprovements on the LVIS benchmark. Our baseline model\noutperforms the LVIS 2020 Challenge winning entry by\n+3.6 mask AP on rare categories. 1\n1. Introduction\nInstance segmentation [22, 10] is an important task in\ncomputer vision with many real world applications. In-\nstance segmentation models based on state-of-the-art con-\nvolutional networks [11, 57, 67] are often data-hungry.\nAt the same time, annotating large datasets for instance\nsegmentation [40, 21] is usually expensive and time-\nconsuming. For example, 22 worker hours were spent per\n*Equal contribution. Correspondence to: golnazg@google.com.\n†Work done during an internship at Google Research.\n1Code and checkpoints for our models are available at https:\n//github.com/tensorflow/tpu/tree/master/models/\nofficial/detection/projects/copy_paste\n0.1 0.25 0.5 0.75 1.0\nFraction of COCO Dataset\n20\n25\n30\n35\n40\n45\n50COCO Box AP\nStandard Aug.\nStrong Aug.\nStrong Aug. + Copy-Paste\nFigure 1. Data-efﬁciency on the COCO benchmark: Combining\nthe Copy-Paste augmentation along with Strong Aug. (large scale\njittering) allows us to train models that are up to 2 × more data-\nefﬁcient than Standard Aug. (standard scale jittering). The aug-\nmentations are highly effective and provide gains of +10 AP in\nthe low data regime (10% of data) while still being effective in the\nhigh data regime with a gain of +5 AP. Results are for Mask R-\nCNN EfﬁcientNet-B7 FPN trained on an image size of 640×640.\n1000 instance masks for COCO [40]. It is therefore impera-\ntive to develop new methods to improve the data-efﬁciency\nof state-of-the-art instance segmentation models.\nHere, we focus on data augmentation [50] as a simple\nway to signiﬁcantly improve the data-efﬁciency of instance\nsegmentation models. Although many augmentation meth-\nods such as scale jittering and random resizing have been\nwidely used [26, 25, 20], they are more general-purpose\nin nature and have not been designed speciﬁcally for in-\nstance segmentation. An augmentation procedure that is\nmore object-aware, both in terms of category and shape,\nis likely to be useful for instance segmentation. The Copy-\nPaste augmentation [13, 12, 15] is well suited for this need.\nBy pasting diverse objects of various scales to new back-\nground images, Copy-Paste has the potential to create chal-\nlenging and novel training data for free.\narXiv:2012.07177v2  [cs.CV]  23 Jun 2021\nFigure 2. We use a simple copy and paste method to create new images for training instance segmentation models. We apply random scale\njittering on two random training images and then randomly select a subset of instances from one image to paste onto the other image.\nThe key idea behind the Copy-Paste augmentation is to\npaste objects from one image to another image. This can\nlead to a combinatorial number of new training data, with\nmultiple possibilities for: (1) choices of the pair of source\nimage from which instances are copied, and the target im-\nage on which they are pasted; (2) choices of object instances\nto copy from the source image; (3) choices of where to paste\nthe copied instances on the target image. The large variety\nof options when utilizing this data augmentation method al-\nlows for lots of exploration on how to use the technique\nmost effectively. Prior work [12, 15] adopts methods for de-\nciding where to paste the additional objects by modeling the\nsurrounding visual context. In contrast, we ﬁnd that a sim-\nple strategy of randomly picking objects and pasting them at\nrandom locations on the target image provides a signiﬁcant\nboost on top of baselines across multiple settings. Specif-\nically, it gives solid improvements across a wide range of\nsettings with variability in backbone architecture, extent of\nscale jittering, training schedule and image size.\nIn combination with large scale jittering, we show that\nthe Copy-Paste augmentation results in signiﬁcant gains in\nthe data-efﬁciency on COCO (Figure 1). In particular, we\nsee a data-efﬁciency improvement of 2 × over the com-\nmonly used standard scale jittering data augmentation. We\nalso observe a gain of +10 Box AP on the low-data regime\nwhen using only 10% of the COCO training data.\nWe then show that the Copy-Paste augmentation strategy\nprovides additional gains with self-training [44, 73] wherein\nwe extract instances from ground-truth data and paste them\nonto unlabeled data annotated with pseudo-labels. Using\nan EfﬁcientNet-B7 [56] backbone and NAS-FPN [17] ar-\nchitecture, we achieve 57.3 Box AP and 49.1 Mask AP on\nCOCO test-dev without test-time augmentations. This\nresult surpasses the previous state-of-the-art instance seg-\nmentation models such as SpineNet [11] (46.3 mask AP)\nand DetectoRS ResNeXt-101-64x4d with test time aug-\nmentation [43] (48.5 mask AP). The performance also sur-\npasses state-of-the-art bounding box detection results of\nEfﬁcientDet-D7x-1536 [57] (55.1 box AP) and YOLOv4-\nP7-1536 [61] (55.8 box AP) despite using a smaller image\nsize of 1280 instead of 1536.\nFinally, we show that the Copy-Paste augmentation re-\nsults in better features for the two-stage training procedure\ntypically used in the LVIS benchmark [21]. Using Copy-\nPaste we get improvements of 6.1 and 3.7 mask AP on the\nrare and common categories, respectively.\nThe Copy-Paste augmentation strategy is easy to plug\ninto any instance segmentation codebase, can utilize un-\nlabeled images effectively and does not create training or\ninference overheads. For example, our experiments with\nMask-RCNN show that we can drop Copy-Paste into its\ntraining, and without any changes, the results can be eas-\nily improved, e.g., by +1.0 AP for 48 epochs.\n2. Related Work\nData Augmentations. Compared to the volume of work\non backbone architectures [35, 51, 53, 27, 56] and detec-\ntion/segmentation frameworks [19, 18, 47, 38, 26, 39], rel-\natively less attention is paid to data augmentations [50]\nin the computer vision community. Data augmentations\nsuch as random crop [36, 35, 51, 53], color jittering [53],\nAuto/RandAugment [6, 7] have played a big role in achiev-\ning state-of-the-art results on image classiﬁcation [27, 56],\nself-supervised learning [28, 24, 5] and semi-supervised\nlearning [64] on the ImageNet [48] benchmark. These\naugmentations are more general purpose in nature and are\nmainly used for encoding invariances to data transforma-\ntions, a principle well suited for image classiﬁcation [48].\nMixing Image Augmentations. In contrast to augmenta-\ntions that encode invariances to data transformations, there\nexists a class of augmentations that mix the information\ncontained in different images with appropriate changes to\ngroundtruth labels. A classic example is the mixup data\naugmentation [66] method which creates new data points\nfor free from convex combinations of the input pixels and\nthe output labels. There have been adaptations of mixup\nsuch as CutMix [65] that pastes rectangular crops of an im-\nage instead of mixing all pixels. There have also been appli-\ncations of mixup and CutMix to object detection [69]. The\nMosaic data augmentation method employed in YOLO-\nv4 [1] is related to CutMix in the sense that one creates a\nnew compound image that is a rectangular grid of multi-\nple individual images along with their ground truths. While\nmixup, CutMix and Mosaic are useful in combining multi-\nple images or their cropped versions to create new training\ndata, they are still not object-aware and have not been de-\nsigned speciﬁcally for the task of instance segmentation.\nCopy-Paste Augmentation. A simple way to combine in-\nformation from multiple images in an object-aware manner\nis to copy instances of objects from one image and paste\nthem onto another image. Copy-Paste is akin to mixup and\nCutMix but only copying the exact pixels corresponding to\nan object as opposed to all pixels in the object’s bounding\nbox. One key difference in our work compared to Con-\ntextual Copy-Paste [12] and InstaBoost [15] is that we do\nnot need to model surrounding visual context to place the\ncopied object instances. A simple random placement strat-\negy works well and yields solid improvements on strong\nbaseline models. Instaboost [15] differs from prior work on\nCopy-Paste [12] by not pasting instances from other images\nbut rather by jiterring instances that already exist on the im-\nage. Cut-Paste-and-Learn [13] proposes to extract object in-\nstances, blend and paste them on diverse backgrounds and\ntrain on the augmented images in addition to the original\ndataset. Our work uses the same method with some differ-\nences: (1) We do not use geometric transformations ( e.g.\nrotation), and ﬁnd Gaussian blurring of the pasted instances\nnot beneﬁcial; (2) We study Copy-Paste in the context of\npasting objects contained in one image into another image\nalready populated with instances where [13] studies Copy-\nPaste in the context of having a bank of object instances and\nbackground scenes to improve performance; (3) We study\nthe efﬁcacy of Copy-Paste in the semi-supervised learning\nsetting by using it in conjunction with self-training. (4) We\nbenchmark and thoroughly study Copy-Paste on the widely\nused COCO and LVIS datasets while Cut-Paste-and-Learn\nuses the GMU dataset [16]. A key contribution is that our\npaper shows the use of Copy-Paste in improving state-of-\nthe-art instance segmentation models on COCO and LVIS.\nInstance Segmentation. Instance segmentation [22, 23] is\na challenging computer vision problem that attempts to both\ndetect object instances and segment the pixels correspond-\ning to each instance. Mask-RCNN [26] is a widely used\nframework with most state-of-the-art methods [67, 11, 43]\nadopting that approach. The COCO dataset is the widely\nused benchmark for measuring progress. We report state-\nof-the-art2 results on the COCO benchmark surpassing\nSpineNet [11] by 2.8 AP and DetectoRS [43] by 0.6 AP.3\nCopy and paste approach is also used for weakly su-\npervised instance segmentation. Remez et al. [45] intro-\nduce an adversarial approach where it uses a generator net-\nwork to predict the segmentation mask of an object within a\ngiven bounding box. Given the generated mask, the object\nis blended on another background and then a discrimina-\ntor network is used to make sure the generated mask/image\nlooks realistic. Different from this work, we use Copy-Paste\nas an augmentation method.\nLong-Tail Visual Recognition. Recently, the computer vi-\nsion community has begun to focus on the long-tail na-\nture of object categories present in natural images [59, 21],\nwhere many of the different object categories have very few\nlabeled images. Modern approaches for addressing long-\ntail data when training deep networks can be mainly divided\ninto two groups: data re-sampling [41, 21, 62] and loss re-\nweighting [30, 8, 3, 54, 37, 46]. Other more complicated\nlearning methods ( e.g., meta-learning [63, 29, 32], causal\ninference [58], Bayesian methods [34],etc.) are also used to\ndeal with long-tail data. Recent work [9, 3, 33, 71, 37] has\npointed out the effectiveness of two-stage training strate-\ngies by separating the feature learning and the re-balancing\nstage, as end-to-end training with re-balancing strategies\ncould be detrimental to feature learning. A more compre-\nhensive summary of data imbalance in object detection can\nbe found in Oksuz et al. [42]. Our work demonstrates sim-\nple Copy-Paste data augmentation yields signiﬁcant gains in\nboth single-stage and two-stage training on the LVIS bench-\nmark, especially for rare object categories.\n3. Method\nOur approach for generating new data using Copy-Paste\nis very simple. We randomly select two images and ap-\nply random scale jittering and random horizontal ﬂipping\non each of them. Then we select a random subset of objects\nfrom one of the images and paste them onto the other image.\nLastly, we adjust the ground-truth annotations accordingly:\nwe remove fully occluded objects and update the masks and\nbounding boxes of partially occluded objects.\nUnlike [15, 12], we do not model the surrounding con-\ntext and, as a result, generated images can look very dif-\nferent from real images in terms of co-occurrences of ob-\njects or related scales of objects. For example, giraffes and\nsoccer players with very different scales can appear next to\neach other (see Figure 2).\n2Based on the entries in https://paperswithcode.com/\nsota/instance-segmentation-on-coco .\n3We note that better mask / box AP on COCO have been reported\nin COCO competitions in 2019 - https://cocodataset.org/\nworkshop/coco-mapillary-iccv-2019.html .\n(a) Standard Scale Jittering (SSJ)\n (b) Large Scale Jittering (LSJ)\nFigure 3. Notation and visualization of the two scale jittering augmentation methods used throughout the paper. Standard Scale Jittering\n(SSJ) resizes and crops an image with a resize range of 0.8 to 1.25 of the original image size. The resize range in Large Scale Jittering\n(LSJ) is from 0.1 to 2.0 of the original image size. If images are made smaller than their original size, then the images are padded with\ngray pixel values. Both scale jittering methods also use horizontal ﬂips.\nBlending Pasted Objects. For composing new objects into\nan image, we compute the binary mask (α) of pasted objects\nusing ground-truth annotations and compute the new image\nas I1 × α+ I2 × (1 − α) where I1 is the pasted image and\nI2 is the main image. To smooth out the edges of the pasted\nobjects we apply a Gaussian ﬁlter toαsimilar to “blending”\nin [13]. But unlike [13], we also found that simply compos-\ning without any blending has similar performance.\nLarge Scale Jittering. We use two different types of\naugmentation methods in conjunction with Copy-Paste\nthroughout the text: standard scale jittering (SSJ) and large\nscale jittering (LSJ). These methods randomly resize and\ncrop images. See Figure 3 for a graphical illustration of the\ntwo methods. In our experiments we observe that the large\nscale jittering yields signiﬁcant performance improvements\nover the standard scale jittering used in most prior works.\nSelf-training Copy-Paste. In addition to studying Copy-\nPaste on supervised data, we also experiment with it as a\nway of incorporating additional unlabeled images. Our self-\ntraining Copy-Paste procedure is as follows: (1) train a su-\npervised model with Copy-Paste augmentation on labeled\ndata, (2) generate pseudo labels on unlabeled data, (3) paste\nground-truth instances into pseudo labeled and supervised\nlabeled images and train a model on this new data.\n4. Experiments\n4.1. Experimental Settings\nArchitecture. We use Mask R-CNN [26] with Efﬁcient-\nNet [56] or ResNet [27] as the backbone architecture. We\nalso employ feature pyramid networks [38] for multi-scale\nfeature fusion. We use pyramid levels from P2 to P6, with\nan anchor size of 8 × 2l and 3 anchors per pixel. Our\nstrongest model uses Cascade R-CNN [2], EfﬁcientNet-B7\nas the backbone and NAS-FPN [17] as the feature pyramid\nwith levels from P3 to P7. The anchor size is 4 × 2l and\nwe have 9 anchors per pixel. Our NAS-FPN model uses 5\nrepeats and we replace convolution layers with ResNet bot-\ntleneck blocks [27].\nTraining Parameters. All models are trained using syn-\nchronous batch normalization [31, 20] using a batch size of\n256 and weight decay of 4e-5. We use a learning rate of\n0.32 and a step learning rate decay [25]. At the beginning\nof training the learning rate is linearly increased over the\nﬁrst 1000 steps from 0.0032 to 0.32. We decay the learn-\ning rate at 0.9, 0.95 and 0.975 fractions of the total number\nof training steps. We initialize the backbone of our largest\nmodel from an ImageNet checkpoint pre-trained with self-\ntraining [64] to speed up the training. All other results are\nfrom models with random initialization unless otherwise\nstated. Also, we use large scale jittering augmentation for\ntraining the models unless otherwise stated. For all differ-\nent augmentations and dataset sizes in our experiments we\nallow each model to train until it converges ( i.e., the vali-\ndation set performance no longer improves). For example,\ntraining a model from scratch with large scale jittering and\nCopy-Paste augmentation requires 576 epochs while train-\ning with only standard scale jittering takes 96 epochs. For\nthe self-training experiments we double the batch size to\n512 while we keep all the other hyper-parameters the same\nwith the exception of our largest model where we retain the\nbatch size of 256 due to memory constraints.\nDataset. We use the COCO dataset [40] which has 118k\ntraining images. For self-training experiments, we use\nthe unlabeled COCO dataset (120k images) and the Ob-\njects365 dataset [49] (610k images) as unlabeled images.\nFor transfer learning experiments, we pre-train our models\non the COCO dataset and then ﬁne-tune on the Pascal VOC\ndataset [14]. For semantic segmentation, we train our mod-\nels on the train set (1.5k images) of the PASCAL VOC\n2012 segmentation dataset. For detection, we train on the\ntrainval set of PASCAL VOC 2007 and PASCAL VOC\n25 50 75 100 125 150 175 200\nEpochs\n36\n38\n40\n42\n44\n46\n48COCO Box AP\nRand init, w/ SSJ\nRand init, w/ SSJ, w/ Copy-Paste\nImageNet init, w/ SSJ\nImageNet init, w/ SSJ, w/ Copy-Paste\n0 100 200 300 400 500 600\nEpochs\n36\n38\n40\n42\n44\n46\n48COCO Box AP\nRand init, w/ SSJ\nRand init, w/ SSJ, w/ Copy-Paste\nRand init, w/ LSJ\nRand init, w/ LSJ, w/ Copy-Paste\nFigure 4. Copy-Paste provides gains that are robust to training conﬁgurations. We train Mask R-CNN (ResNet-50 FPN) on 1024 ×1024\nimage size for varying numbers of epochs. Left Figure: Copy-Paste with and without initializing the backbone by ImageNet pre-training.\nRight Figure: Copy-Paste with standard and large scale jittering. Across all of the conﬁgurations training with Copy-Paste is helpful.\n2012. We also benchmark Copy-Paste on LVIS v1.0 (100k\ntraining images) and report results on LVIS v1.0 val (20k\nimages). LVIS has 1203 classes to simulate the long-tail\ndistribution of classes in natural images.\n4.2. Copy-Paste is robust to training conﬁgurations\nIn this section we show that Copy-Paste is a strong data\naugmentation method that is robust across a variety of train-\ning iterations, models and training hyperparameters.\nRobustness to backbone initialization. Common prac-\ntice for training Mask R-CNN is to initialize the back-\nbone with an ImageNet pre-trained checkpoint. However\nHe et al . [25] and Zoph et al . [73] show that a model\ntrained from random initialization has similar or better per-\nformance with longer training. Training models from Ima-\ngeNet pre-training with strong data-augmentation (i.e. Ran-\ndAugment [7]) was shown to hurt the performance by up\nto 1 AP on COCO. Figure 4 (left) demonstrates that Copy-\nPaste is additive in both setups and we get the best result\nusing Copy-Paste augmentation and random initialization.\nRobustness to training schedules. A typical training\nschedule for Mask R-CNN in the literature is only 24 (2 ×)\nor 36 epochs (3×) [25, 26, 15]. However, recent work with\nstate-of-the-art results show that longer training is helpful\nin training object detection models on COCO [73, 57, 11].\nFigure 4 shows that we get gains from Copy-Paste for the\ntypical training schedule of 2 × or 3× and as we increase\ntraining epochs the gain increases. This shows that Copy-\nPaste is a very practical data augmentation since we do not\nneed a longer training schedule to see the beneﬁt.\nCopy-Paste is additive to large scale jittering augmen-\ntation. Random scale jittering is a powerful data augmen-\ntation that has been used widely in training computer vi-\nsion models. The standard range of scale jittering in the\nliterature is 0.8 to 1.25 [39, 25, 6, 15]. However, augment-\ning data with larger scale jittering with a range of 0.1 to\nModel FLOPs Box AP Mask AP\nRes-50 FPN (1024) 431 B 47.2 41.8\nw/ Copy-Paste 431 B (+1.0) 48.2 (+0.6) 42.4\nRes-101 FPN (1024) 509 B 48.4 42.8\nw/ Copy-Paste 509 B (+1.4) 49.8 (+0.8) 43.6\nRes-101 FPN (1280) 693 B 49.1 43.1\nw/ Copy-Paste 693 B (+1.2) 50.3 (+1.1) 44.2\nEff-B7 FPN (640) 286 B 48.5 42.7\nw/ Copy-Paste 286 B (+1.5) 50.0 (+1.0) 43.7\nEff-B7 FPN (1024) 447 B 50.8 44.7\nw/ Copy-Paste 447 B (+1.1) 51.9 (+0.5) 45.2\nEff-B7 FPN (1280) 595 B 51.1 44.8\nw/ Copy-Paste 595 B (+1.5) 52.6 (+1.1) 45.9\nCascade Eff-B7 FPN(1280) 854 B 52.9 45.6\nw/ Copy-Paste 854 B (+1.1) 54.0 (+0.7) 46.3\nTable 1. Copy-paste works well across a variety of different model\narchitectures, model sizes and image resolutions. See table 13 in\nthe Appendix for benchmark results on different object sizes.\n2.0 [57, 11] and longer training signiﬁcantly improves per-\nformance (see Figure 4, right plot). Figure 5 demonstrates\nthat Copy-Paste is additive to both standard and large scale\njittering augmentation and we get a higher boost on top of\nstandard scale jittering. On the other hand, as it is shown in\nFigure 5, mixup [66, 69] data augmentation does not help\nwhen it is used with large scale jittering.\nCopy-Paste works across backbone architectures and\nimage sizes. Finally, we demonstrate Copy-Paste helps\nmodels with standard backbone architecture of ResNet [27]\nas well the more recent architecture of EfﬁcientNet [56].\nWe train models with these backbones on the image size of\n640×640, 1024×1024 or 1280×1280. Table 1 shows that\nwe get signiﬁcant improvements over the strong baselines\ntrained with large scale jittering for all the models. Across\n7 models with different backbones and images sizes Copy-\nPaste gives on average a 1.3 box AP and 0.8 mask AP im-\nprovement on top of large scale jittering.\n0.2 0.4 0.6 0.8 1.0\nFraction of COCO Dataset\n20\n25\n30\n35\n40\n45\n50COCO AP\nw/ SSJ\nw/ SSJ w/ mixup\nw/ SSJ w/ Copy-Paste\n0.2 0.4 0.6 0.8 1.0\nFraction of COCO Dataset\n20\n25\n30\n35\n40\n45\n50COCO AP\nw/ LSJ\nw/ LSJ w/ mixup\nw/ LSJ w/ Copy-Paste\nFigure 5. Copy-Paste is additive to large scale jittering augmentation. Improvement from mixup and Copy-Paste data augmentation on top\nof standard scale jittering(Left Figure) and large scale jittering(Right Figure). All results are from training Mask R-CNN EfﬁcientNetB7-\nFPN on the image size of 640×640.\nSetup Box AP Mask AP\nEff-B7 FPN (640) 48.5 42.7\nw/ self-training (+1.5) 50.0 (+1.3) 44.0\nw/ Copy-Paste (+1.5) 50.0 (+1.0) 43.7\nw/ self-training Copy-Paste (+2.9) 51.4 (+2.3) 45.0\nTable 2. Copy-Paste and self-training are additive for utilizing ex-\ntra unlabeled data. We get signiﬁcant improvement of 2.9 box AP\nand 2.3 mask AP by combining self-training and Copy-Paste.\n4.3. Copy-Paste helps data-efﬁciency\nIn this section, we show Copy-Paste is helpful across\na variety of dataset sizes and helps data efﬁciency. Fig-\nure 5 reveals that Copy-Paste augmentation is always help-\nful across all fractions of COCO. Copy-Paste is most help-\nful in the low data regime (10% of COCO) yielding a 6.9\nbox AP improvement on top of SSJ and a 4.8 box AP im-\nprovement on top of LSJ. On the other hand, mixup is only\nhelpful in a low data regime. Copy-Paste also greatly helps\nwith data-efﬁciency: a model trained on75% of COCO with\nCopy-Paste and LSJ has a similar AP to a model trained on\n100% of COCO with LSJ.\n4.4. Copy-Paste and self-training are additive\nIn this section, we demonstrate that a standard self-\ntraining method similar to [64, 73] and Copy-Paste can be\ncombined together to leverage unlabeled data. Copy-Paste\nand self-training individually have similar gains of 1.5 box\nAP over the baseline with 48.5 Box AP (see Table 2).\nTo combine self-training and Copy-Paste we ﬁrst use a\nsupervised teacher model trained with Copy-Paste to gener-\nate pseudo labels on unlabeled data. Next we take ground\ntruth objects from COCO and paste them into pseudo la-\nbeled images and COCO images. Finally, we train the stu-\ndent model on all these images. With this setup we achieve\n51.4 box AP, an improvement of 2.9 AP over the baseline.\nSetup Pasting into Box AP Mask AP\nself-training - 50.0 44.0\n+Copy-Paste COCO (+0.4) 50.4 44.0\n+Copy-Paste Pseudo data (+0.8) 50.8 (+0.5) 44.5\n+Copy-Paste COCO &\nPseudo data\n(+1.4) 51.4 (+1.0) 45.0\nTable 3. Pasting ground-truth COCO objects into both COCO and\npseudo labeled data gives higher gain in comparison to doing ei-\nther on its own.\nData to Paste on. In our self-training setup, half of the\nbatch is from supervised COCO data (120k images) and\nthe other half is from pseudo labeled data (110k images\nfrom unlabeled COCO and 610k from Objects365). Table 3\npresents results when we paste COCO instances on differ-\nent portions of the training images. Pasting into pseudo la-\nbeled data yields larger improvements compared to pasting\ninto COCO. Since the number of images in the pseudo la-\nbeled set is larger, using images with more variety as back-\nground helps Copy-Paste. We get the maximum gain over\nself-training (+1.4 box AP ) when we paste COCO instances\non both COCO and pseudo labeled images.\nData to Copy from. We also explore an alternative way to\nuse Copy-Paste to incorporate extra data by pasting pseudo\nlabeled objects from an unlabeled dataset directly into the\nCOCO labeled dataset. Unfortunately, this setup shows no\nadditional AP improvements.\n4.5. Copy-Paste improves COCO state-of-the-art\nNext we study if Copy-Paste can improve state-of-the-art\ninstance segmentation methods on COCO. Table 4 shows\nthe results of applying Copy-Paste on top of a strong 54.8\nbox AP COCO model. This table is meant to serve as\na reference for state-of-the-art performance. 4 For rigor-\n4https : / / paperswithcode . com / sota / object -\ndetection-on-coco\nModel FLOPs # Params AP val APtest-dev Mask APval Mask APtest-dev\nSpineNet-190 (1536) [11] 2076B 176M 52.2 52.5 46.1 46.3\nDetectoRS ResNeXt-101-64x4d [43] — — — 55.7 † — 48.5 †\nSpineNet-190 (1280) [11] 1885B 164M 52.6 52.8 — —\nSpineNet-190 (1280) w/ self-training [72] 1885B 164M 54.2 54.3 — —\nEfﬁcientDet-D7x (1536) [57] 410B 77M 54.4 55.1 — —\nYOLOv4-P7 (1536) [61] — — — 55.8 † — —\nCascade Eff-B7 NAS-FPN (1280) 1440B 185M 54.5 54.8 46.8 46.9\nw/ Copy-Paste 1440B 185M (+1.4) 55.9 (+1.2) 56.0 (+0.4) 47.2 (+0.5) 47.4\nw/ self-training Copy-Paste 1440B 185M (+2.5) 57.0 (+2.5) 57.3 (+2.1) 48.9 (+2.2) 49.1\nTable 4. Comparison with the state-of-the-art models on COCO object detection and instance segmentation. Parentheses next to the model\nname denote the input image size. † indicates results with test time augmentation.\nModel AP50 AP\nReﬁneDet512+ [68] 83.8 -\nSNIPER [52] 86.9 -\nCascade Eff-B7 NAS-FPN 88.6 75.0\nw/ Copy-Paste pre-training (+0.7) 89.3 (+1.5) 76.5\nTable 5. PASCAL VOC 2007 detection result on test set.\nWe present results of our EfﬁcientNet-B7 NAS-FPN model pre-\ntrained with and without Copy-Paste on COCO.\nous comparisons, we note that models need to be evalu-\nated with the same codebase, training data, and training\nsettings such as learning rate schedule, weight decay, data\npre-processing and augmentations, controlling for param-\neters and FLOPs, architectural regularization [60], train-\ning and inference speeds, etc. The goal of the table is to\nshow the beneﬁts of the Copy-Paste augmentation and its\nadditive gains with self-training. Our baseline model is a\nCascade Mask-RCNN with EfﬁcientNet-B7 backbone and\nNAS-FPN. We observe an improvement of +1.2 box AP\nand +0.5 mask AP using Copy-Paste. When combined\nwith self-training using unlabeled COCO and unlabeled\nObjects365 [49] for pseudo-labeling, we see a further im-\nprovement of 2.5 box AP and 2.2 mask AP, resulting in a\nmodel with a strong performance of 57.3 box AP and 49.1\nmask AP on COCO test-dev without test-time augmen-\ntations and model ensembling.\n4.6. Copy-Paste produces better representations for\nPASCAL detection and segmentation\nPreviously we have demonstrated the improved perfor-\nmance that the simple Copy-Paste augmentation provides\non instance segmentation. In this section we study the\ntransfer learning performance of the pre-trained instance\nsegmentation models that were trained with Copy-Paste on\nCOCO. Here we perform transfer learning experiments on\nthe PASCAL VOC 2007 dataset. Table 5 shows how the\nlearned Copy-Paste models transfer compared to baseline\nmodels on PASCAL detection. Table 6 shows the trans-\nfer learning results on PASCAL semantic segmentation as\nwell. On both PASCAL detection and PASCAL semantic\nModel mIOU\nDeepLabv3+ † [4] 84.6\nExFuse † [70] 85.8\nEff-B7 [73] 85.2\nEff-L2 [73] 88.7\nEff-B7 NAS-FPN 83.9\nw/ Copy-Paste pre-training (+2.7) 86.6\nTable 6. PASCAL VOC 2012 semantic segmentation results on\nval set. We present results of our EfﬁcientNet-B7 NAS-FPN\nmodel pre-trained with and without Copy-Paste on COCO. † in-\ndicates multi-scale/ﬂip ensembling inference.\nsegmentation we ﬁnd our models trained with Copy-Paste\ntransfer better for ﬁne-tuning than the baseline models.\n4.7. Copy-Paste provides strong gains on LVIS\nWe benchmark Copy-Paste on the LVIS dataset to see\nhow it performs on a dataset with a long-tail distribution of\n1203 classes. There are two different training paradigms\ntypically used for LVIS: (1) single-stage where a detector\nis trained directly on the LVIS dataset, (2) two-stage where\nthe model from the ﬁrst stage is ﬁne-tuned with class re-\nbalancing losses to help handle the class imbalance.\nCopy-Paste improves single-stage LVIS training. The\nsingle-stage training paradigm is quite similar to our Copy-\nPaste setup on COCO. In addition to the standard training\nsetup, certain methods are used to handle the class imbal-\nance problem on LVIS. One common method is Repeat Fac-\ntor Sampling (RFS) from [21], witht= 0.001. This method\naims at helping the large class imbalance problem on LVIS\nby over-sampling images that contain less frequent object\ncategories. For single-stage training on LVIS, we follow\nthe same training parameters on COCO to train our models\nfor 180k steps using a 256 batch size. As suggested by [21],\nwe increase the number of detections per image to 300 and\nreduce the score threshold to 0. Table 8 shows the results\nof applying Copy-Paste to a strong single-stage LVIS base-\nline of EfﬁcientNet-B7 FPN with 640 ×640 input size. We\nobserve that Copy-Paste augmentation outperforms RFS on\nAP, APc and APf, but under-performs on AP r (the AP for\nMask AP Mask AP r Mask APc Mask APf Box AP\ncRT (ResNeXt-101-32×8d) [33] 27.2 19.6 26.0 31.9 —\nLVIS Challenge 2020 Winner† [55] 38.8 28.5 39.5 42.7 41.1\nResNet-50 FPN (1024) 30.3 22.2 29.5 34.7 31.5\nw/ Copy-Paste (+2.0) 32.3 (+4.3) 26.5 (+2.3) 31.8 (+0.6) 35.3 (+2.8) 34.3\nResNet-101 FPN (1024) 31.9 24.7 30.5 36.3 33.3\nw/ Copy-Paste (+2.1) 34.0 (+2.7) 27.4 (+3.4) 33.9 (+0.9) 37.2 (+3.1) 36.4\nEfﬁcientNet-B7 FPN (1024) 33.7 26.4 33.1 37.6 35.5\nw/ Copy-Paste (+2.3) 36.0 (+3.3) 29.7 (+2.7) 35.8 (+1.3) 38.9 (+3.7) 39.2\nEfﬁcientNet-B7 NAS-FPN (1280) 34.7 26.0 33.4 39.8 37.2\nw/ Copy-Paste (+3.4) 38.1 (+6.1) 32.1 (+3.7) 37.1 (+2.1) 41.9 (+4.4) 41.6\nTable 7. Comparison with the state-of-the-art models on LVIS v1.0 object detection and instance segmentation. Parentheses next to our\nmodels denote the input image size. † We report the 2020 winning entry’s result without test-time augmentation.\nSetup (single-stage) AP APr APc APf\nEff-B7 FPN (640) 27.7 9.7 28.1 35.1\nw/ RFS 28.2 15.4 27.8 34.3\nw/ Copy-Paste 29.3 12.8 30.1 35.7\nw/ RFS w/ Copy-Paste 30.1 18.4 30.0 35.4\nTable 8. Single-stage training results (mask AP) on LVIS.\nrare classes). The best overall result comes from combin-\ning RFS and Copy-Paste augmentation, achieving a boost\nof +2.4 AP and +8.7 APr.\nCopy-Paste improves two-stage LVIS training. Two-\nstage training is widely adopted to address data imbal-\nance and obtain good performance on LVIS [37, 46, 55].\nWe aim to study the efﬁcacy of Copy-Paste in this two-\nstage setup. Our two-stage training is as follows: ﬁrst we\ntrain the object detector with standard training techniques\n(i.e., same as our single-stage training) and then we ﬁne-\ntune the model trained in the ﬁrst stage using the Class-\nBalanced Loss [8]. The weight for a class is calculated by\n(1 −β)/(1 −βn), where nis the number of instances of the\nclass and β = 0.999.5 During the second stage ﬁne-tuning,\nwe train the model with 3 × schedule and only update the\nﬁnal classiﬁcation layer in Mask R-CNN using the classiﬁ-\ncation loss only. From mask AP results in Table 9, we can\nsee models trained with Copy-Paste learn better features for\nlow-shot classes (+2.3 on AP r and +2.6 on AP c). Interest-\ningly, we ﬁnd RFS, which is quite helpful and additive with\nCopy-Paste in single-stage training, hurts the performance\nin two-stage training. A possible explanation for this ﬁnd-\ning is that features learned with RFS are worse than those\nlearned with the original LVIS dataset. We leave a more de-\ntailed investigation of the tradeoffs between RFS and data\naugmentations in two stage training for future work.\nComparison with the state-of-the-art. Furthermore, we\ncompare our two-stage models with state-of-the-art meth-\n5We scale class weights by dividing the mean and then clip their values\nto [0.01, 5], as suggested by [37].\nSetup (two-stage) AP APr APc APf\nEff-B7 FPN (640) 31.3 25.0 30.6 34.9\nw/ RFS 30.1 21.8 29.7 34.1\nw/ Copy-Paste 33.0 27.3 33.2 35.7\nw/ RFS w/ Copy-Paste 32.0 26.3 31.8 34.7\nTable 9. Two-stage training results (mask AP) on LVIS.\nods for LVIS6 in Table 7. Surprisingly, our smallest model,\nResNet-50 FPN, outperforms a strong baseline cRT [33]\nwith ResNeXt-101-32×8d backbone.\nEfﬁcientNet-B7 NAS-FPN model (without Cascade 7)\ntrained with Copy-Paste achieves comparable performance\nto LVIS challenge 2020 winner on overall Mask AP and\nBox AP without test-time augmentation. Also, it obtains\n32.1 mask APr for rare categories, outperforming the LVIS\nChallenge 2020 winning entry by +3.6 mask APr.\n5. Conclusion\nData augmentation is at the heart of many vision sys-\ntems. In this paper, we rigorously studied the Copy-Paste\ndata augmentation method, and found that it is very effec-\ntive and robust. Copy-Paste performs well across multi-\nple experimental settings and provides signiﬁcant improve-\nments on top of strong baselines, both on the COCO and\nLVIS instance segmentation benchmarks.\nThe Copy-Paste augmentation strategy is simple, easy to\nplug into any instance segmentation codebase, and does not\nincrease the training cost or inference time. We also showed\nthat Copy-Paste is useful for incorporating extra unlabeled\nimages during training and is additive on top of successful\nself-training techniques. We hope that the convincing em-\npirical evidence of its beneﬁts make Copy-Paste augmenta-\ntion a standard augmentation procedure when training in-\nstance segmentation models.\n6https://www.lvisdataset.org/challenge_2020\n7We ﬁnd using Cascade in our experiments improves APf but hurts APr.\nReferences\n[1] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\nYuan Mark Liao. Yolov4: Optimal speed and accuracy of\nobject detection. arXiv preprint arXiv:2004.10934, 2020.\n[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving\ninto high quality object detection. In CVPR, 2018.\n[3] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga,\nand Tengyu Ma. Learning imbalanced datasets with label-\ndistribution-aware margin loss. In NeurIPS, 2019.\n[4] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nECCV, 2018.\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML, 2020.\n[6] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\nvan, and Quoc V Le. Autoaugment: Learning augmentation\npolicies from data. In CVPR, 2019.\n[7] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In NeurIPS, 2020.\n[8] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge\nBelongie. Class-balanced loss based on effective number of\nsamples. In CVPR, 2019.\n[9] Yin Cui, Yang Song, Chen Sun, Andrew Howard, and\nSerge Belongie. Large scale ﬁne-grained categorization and\ndomain-speciﬁc transfer learning. In CVPR, 2018.\n[10] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se-\nmantic segmentation via multi-task network cascades. In\nCVPR, 2016.\n[11] Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi,\nMingxing Tan, Yin Cui, Quoc V Le, and Xiaodan Song.\nSpinenet: Learning scale-permuted backbone for recognition\nand localization. In CVPR, 2020.\n[12] Nikita Dvornik, Julien Mairal, and Cordelia Schmid. Mod-\neling visual context is key to augmenting object detection\ndatasets. In ECCV, 2018.\n[13] Debidatta Dwibedi, Ishan Misra, and Martial Hebert. Cut,\npaste and learn: Surprisingly easy synthesis for instance de-\ntection. In ICCV, 2017.\n[14] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. IJCV, 2010.\n[15] Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao\nGou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting\ninstance segmentation via probability map guided copy-\npasting. In ICCV, 2019.\n[16] Georgios Georgakis, Md Alimoor Reza, Arsalan Mousavian,\nPhi-Hung Le, and Jana Koˇseck´a. Multiview rgb-d dataset for\nobject instance detection. In 3DV, 2016.\n[17] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn:\nLearning scalable feature pyramid architecture for object de-\ntection. In CVPR, 2019.\n[18] Ross Girshick. Fast r-cnn. In ICCV, 2015.\n[19] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In CVPR, 2014.\n[20] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr\nDoll´ar, and Kaiming He. Detectron, 2018.\n[21] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\ndataset for large vocabulary instance segmentation. InCVPR,\n2019.\n[22] Bharath Hariharan, Pablo Arbel ´aez, Ross Girshick, and Ji-\ntendra Malik. Simultaneous detection and segmentation. In\nECCV, 2014.\n[23] Bharath Hariharan, Pablo Arbel ´aez, Ross Girshick, and Ji-\ntendra Malik. Hypercolumns for object segmentation and\nﬁne-grained localization. In CVPR, 2015.\n[24] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In CVPR, 2020.\n[25] Kaiming He, Ross Girshick, and Piotr Doll ´ar. Rethinking\nimagenet pre-training. In ICCV, 2019.\n[26] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017.\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016.\n[28] Olivier J H ´enaff, Aravind Srinivas, Jeffrey De Fauw, Ali\nRazavi, Carl Doersch, SM Eslami, and Aaron van den Oord.\nData-efﬁcient image recognition with contrastive predictive\ncoding. arXiv preprint arXiv:1905.09272, 2019.\n[29] Xinting Hu, Yi Jiang, Kaihua Tang, Jingyuan Chen, Chunyan\nMiao, and Hanwang Zhang. Learning to segment the tail. In\nCVPR, 2020.\n[30] Chen Huang, Yining Li, Chen Change Loy, and Xiaoou\nTang. Learning deep representation for imbalanced classi-\nﬁcation. In CVPR, 2016.\n[31] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In ICML, 2015.\n[32] Muhammad Abdullah Jamal, Matthew Brown, Ming-Hsuan\nYang, Liqiang Wang, and Boqing Gong. Rethinking class-\nbalanced methods for long-tailed visual recognition from a\ndomain adaptation perspective. In CVPR, 2020.\n[33] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan,\nAlbert Gordo, Jiashi Feng, and Yannis Kalantidis. Decou-\npling representation and classiﬁer for long-tailed recogni-\ntion. In ICLR, 2020.\n[34] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing\nShen, and Ling Shao. Striking the right balance with uncer-\ntainty. In CVPR, 2019.\n[35] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In NeurIPS, 2012.\n[36] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 1998.\n[37] Yu Li, Tao Wang, Bingyi Kang, Sheng Tang, Chunfeng\nWang, Jintao Li, and Jiashi Feng. Overcoming classiﬁer im-\nbalance for long-tail object detection with balanced group\nsoftmax. In CVPR, 2020.\n[38] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In CVPR, 2017.\n[39] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll´ar. Focal loss for dense object detection. In ICCV,\n2017.\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014.\n[41] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,\nKaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,\nand Laurens van der Maaten. Exploring the limits of weakly\nsupervised pretraining. In ECCV, 2018.\n[42] Kemal Oksuz, Baris Can Cam, Sinan Kalkan, and Emre Ak-\nbas. Imbalance problems in object detection: A review.\nTPAMI, 2020.\n[43] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors:\nDetecting objects with recursive feature pyramid and switch-\nable atrous convolution. arXiv preprint arXiv:2006.02334 ,\n2020.\n[44] Ilija Radosavovic, Piotr Doll ´ar, Ross Girshick, Georgia\nGkioxari, and Kaiming He. Data distillation: Towards omni-\nsupervised learning. In CVPR, 2018.\n[45] Tal Remez, Jonathan Huang, and Matthew Brown. Learning\nto segment via cut-and-paste. In ECCV, 2018.\n[46] Jiawei Ren, Cunjun Yu, Zhongang Cai, and Haiyu Zhao. Bal-\nanced activation for long-tailed visual recognition. In LVIS\nChallenge Workshop at ECCV, 2020.\n[47] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In NeurIPS, 2015.\n[48] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. IJCV, 2015.\n[49] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365:\nA large-scale, high-quality dataset for object detection. In\nICCV, 2019.\n[50] Connor Shorten and Taghi M Khoshgoftaar. A survey on\nimage data augmentation for deep learning. Journal of Big\nData, 2019.\n[51] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. InICLR,\n2015.\n[52] Bharat Singh, Mahyar Najibi, and Larry S Davis. Sniper:\nEfﬁcient multi-scale training. In NeurIPS, 2018.\n[53] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In CVPR, 2015.\n[54] Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, Wanli\nOuyang, Changqing Yin, and Junjie Yan. Equalization loss\nfor long-tailed object recognition. In CVPR, 2020.\n[55] Jingru Tan, Gang Zhang, Hanming Deng, Changbao Wang,\nLewei Lu, Quanquan Li, and Jifeng Dai. 1st place solution\nof lvis challenge 2020: A good box is not a guarantee of a\ngood mask. arXiv preprint arXiv:2009.01559, 2020.\n[56] Mingxing Tan and Quoc V Le. Efﬁcientnet: Rethinking\nmodel scaling for convolutional neural networks. In ICML,\n2019.\n[57] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efﬁcientdet:\nScalable and efﬁcient object detection. In CVPR, 2020.\n[58] Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-\ntailed classiﬁcation by keeping the good and removing the\nbad momentum causal effect. NeurIPS, 2020.\n[59] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,\nChen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and\nSerge Belongie. The inaturalist species classiﬁcation and de-\ntection dataset. In CVPR, 2018.\n[60] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and\nRob Fergus. Regularization of neural networks using drop-\nconnect. In ICML, 2013.\n[61] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\nYuan Mark Liao. Scaled-yolov4: Scaling cross stage partial\nnetwork. arXiv preprint arXiv:2011.08036, 2020.\n[62] Tao Wang, Yu Li, Bingyi Kang, Junnan Li, Junhao Liew,\nSheng Tang, Steven Hoi, and Jiashi Feng. The devil is in\nclassiﬁcation: A simple framework for long-tail object de-\ntection and instance segmentation. In ECCV, 2020.\n[63] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Meta-\nlearning to detect rare objects. In ICCV, 2019.\n[64] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V\nLe. Self-training with noisy student improves imagenet clas-\nsiﬁcation. In CVPR, 2020.\n[65] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classiﬁers with localizable\nfeatures. In ICCV, 2019.\n[66] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In ICLR, 2018.\n[67] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi\nZhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R.\nManmatha, Mu Li, and Alexander Smola. Resnest: Split-\nattention networks. In arXiv preprint arXiv:2004.08955 ,\n2020.\n[68] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and\nStan Z Li. Single-shot reﬁnement neural network for object\ndetection. In CVPR, 2018.\n[69] Zhi Zhang, Tong He, Hang Zhang, Zhongyue Zhang, Jun-\nyuan Xie, and Mu Li. Bag of freebies for training object de-\ntection neural networks. arXiv preprint arXiv:1902.04103 ,\n2019.\n[70] Zhenli Zhang, Xiangyu Zhang, Chao Peng, Xiangyang Xue,\nand Jian Sun. Exfuse: Enhancing feature fusion for semantic\nsegmentation. In ECCV, 2018.\n[71] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen.\nBbn: Bilateral-branch network with cumulative learning for\nlong-tailed visual recognition. In CVPR, 2020.\n[72] Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin,\nJonathon Shlens, and Quoc V Le. Learning data augmenta-\ntion strategies for object detection. In ECCV, 2020.\n[73] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao\nLiu, Ekin D. Cubuk, and Quoc V . Le. Rethinking pre-training\nand self-training. In NeurIPS, 2020.\nA. Ablation on the Copy-Paste method\nIn this section we present ablations for our Copy-Paste\nmethod. We use Mask R-CNN EfﬁcientNetB7-FPN archi-\ntecture and image size of 640×640 for our experiments.\nSubset of pasted objects. In our method, we paste a ran-\ndom subset of objects from one image onto another image.\nTable 10 shows that although we get improvements from\npasting only one random object or all the objects of one\nimage into another image, we get the best improvement by\npasting a random subset of objects. This shows that the\nadded randomness introduced from pasting a subset of ob-\njects is helpful.\nBlending. In our experiments, we smooth out the edges of\npasted objects using alpha blending (see Section 3). Ta-\nble 10 shows that this is not an important step and we get\nthe same results without any blending in contrast to [13]\nwho ﬁnd blending is crucial for strong performance.\nSetup Box AP Mask AP\nEfﬁcientNetB7-FPN (640) 48.5 42.7\nw/ Copy-Paste (one object) (-0.9) 49.1 (-0.6) 43.1\nw/ Copy-Paste (all objects) (-0.3) 49.7 (-0.4) 43.3\nw/ Copy-Paste (no blending) 50.0 43.7\nw/ Copy-Paste 50.0 43.7\nTable 10. Ablation studies for the Copy-Paste method on COCO.\nWe study the value of applying blending to pasted objects along\nwith how many objects to paste from one image to another.\nScale jittering. In this work, we show that by combining\nlarge scale jittering and Copy-Paste we obtain a signiﬁcant\nimprovement over the baseline with standard scale jittering\n(Figure 1). In the Copy-Paste method, we apply indepen-\ndent random scale jittering on both the pasted image (image\nthat pasted objects are being copied from) and the main im-\nage. In Table 11 we study the importance of large scale\njittering on both the main and the pasted images. Table 11\nshows that most of the improvement from large scale jit-\ntering is coming from applying it on the main image and\nwe only get slight improvement (0.3 box AP and 0.2 Mask\nAP) from increasing the scale jittering range for the pasted\nimage.\nMain Image Pasted Image Box AP Mask AP\nSSJ SSJ (-1.9) 48.1 (-1.6) 42.1\nSSJ LSJ (-2.3) 47.7 (-1.9) 41.8\nLSJ SSJ (-0.3) 49.7 (-0.2) 43.5\nLSJ LSJ 50.0 43.7\nTable 11. Ablation study on scale jittering methods for the main\nimage and the pasted image.\nB. Copy-Paste provides more gain on harder\ncategories of COCO\nFigure 6 shows the relative AP gain per category ob-\ntained from applying Copy-Paste on the COCO dataset.\nCopy-Paste improves the AP of all the classes except hair\ndrier. In Figure 6 classes are sorted based on the baseline AP\nper category. We observe most of the classes with the high-\nest improvement are on the left (lower baseline AP) which\nshows Copy-Paste helps the hardest classes the most.\nC. How likely objects are copied to an un-\nmatched scene?\nIn our method, we copy objects from a random image to\nanother random image without considering the context of\nthe images. In this section we compute the probability of\ncopying objects to an unmatched scene category (context)\nof indoor or outdoor.\nCOCO images do not have scene categories. But, we use\nCOCO-panoptic labels to assign the COCO images to in-\ndoor or outdoor scene categories. We found there are 42538\nindoor and 71017 outdoor images (we couldn’t estimate the\ncategory of the rest 4732 images). Table 12 shows the prob-\nability of copying objects from one scene category to an-\nother. Therefore, we copy objects to an unmatched scene in\nabout half (46.8%) of generated images.\nfrom\nto indoor outdoor\nindoor 14.1% 23.4%\noutdoor 23.4% 39.1%\nTable 12. Probability of copying objects from one scene category\nto another scene category for COCO dataset.\nD. Benchmark results on different object sizes\nIn the table 1 we report Copy-paste performance of vari-\nety of model architectures. In table 13 we provide additional\nbenchmarks on different object sizes.\nbook (18.2) (20.4)\nhair drier (20.4) (18.2)\nbackpack (20.5) (22.5)\nhandbag (21.9) (24.2)\napple (22.6) (25.8)\ncarrot (24.1) (25.1)\nknife (24.9) (30.3)\nbroccoli (25.2) (26.0)\nspoon (26.1) (29.1)\nbanana (29.3) (30.9)\ntraffic light (29.8) (30.2)\nskis (30.0) (32.6)\norange (30.2) (34.8)\nboat (30.7) (32.2)\nbench (31.8) (33.5)\ntoothbrush (32.6) (37.3)\ndining table (32.7) (34.8)\npotted plant (33.0) (33.5)\nbicycle (35.9) (38.0)\nchair (36.4) (38.9)\nbaseball bat (37.5) (43.7)\ntie (38.3) (41.3)\nremote (39.7) (43.2)\nbird (39.8) (41.9)\noven (40.7) (43.3)\nhot dog (40.9) (43.5)\nbaseball glove (41.1) (42.9)\nscissors (41.5) (45.4)\nsandwich (41.6) (44.6)\ncell phone (41.9) (44.6)\nwine glass (42.0) (43.1)\ntruck (42.4) (45.0)\nvase (43.0) (45.1)\nsink (43.4) (45.9)\nbottle (43.6) (45.0)\ncake (43.8) (47.1)\ntoaster (44.0) (45.7)\nfork (44.2) (49.3)\numbrella (44.5) (47.0)\nsurfboard (45.2) (46.6)\nsnowboard (45.3) (47.9)\nbowl (46.9) (47.0)\nsports ball (47.0) (48.3)\nkite (47.8) (47.9)\ncar (48.1) (49.0)\ncouch (48.1) (50.7)\ncup (48.4) (50.5)\nsuitcase (48.4) (49.3)\nparking meter (49.9) (51.0)\nbed (50.7) (54.6)\nmotorcycle (50.7) (51.7)\nclock (53.4) (53.9)\nteddy bear (53.9) (57.1)\ndonut (54.6) (56.9)\nkeyboard (56.3) (58.0)\npizza (56.4) (58.0)\ntennis racket (56.4) (58.9)\nsheep (57.8) (60.3)\nperson (58.3) (59.4)\nskateboard (58.6) (60.1)\ntv (61.1) (62.9)\nmicrowave (62.1) (66.6)\ncow (62.1) (64.3)\nhorse (64.0) (66.0)\nrefrigerator (64.3) (67.5)\nmouse (64.8) (64.7)\ntoilet (66.1) (69.5)\nlaptop (66.4) (71.0)\ndog (66.4) (69.7)\nzebra (67.5) (70.0)\nelephant (68.5) (70.9)\nstop sign (69.0) (71.7)\ngiraffe (69.1) (71.2)\nfrisbee (69.2) (71.6)\nairplane (69.7) (72.2)\nbus (69.7) (72.2)\nfire hydrant (70.6) (72.8)\ntrain (70.9) (73.2)\nbear (71.6) (76.7)\ncat (73.2) (76.3)\nClasses are sorted according to baseline AP per class\n10\n5\n0\n5\n10\n15\n20\n25\nAP% improvement over baseline\nFigure 6. Per category relative AP improvement from Copy-Paste on 80 classes of COCO dataset. Numbers in the parentheses show the\nAP per category of the baseline model (ﬁrst number) and the model trained with Copy-Paste (second number). Each number is the average\nover 5 runs. Classes are sorted based on the baseline AP per class.\nModel box AP box AP s box APm box APl mask AP mask AP s mask APm mask APl\nRes-50 FPN (1024) 47.2 28.5 49.6 64.6 41.8 23.0 44.3 60.1\nw/ Copy-Paste (+1.0) 48.2 (+0.4) 28.9 (+1.4) 51.0 (+1.8) 66.4 (+0.6) 42.4 (+0.2) 23.2 (+0.9) 45.2 (+1.1) 61.2\nRes-101 FPN (1024) 48.4 29.2 51.1 65.8 42.8 23.5 45.5 60.4\nw/ Copy-Paste (+1.4) 49.8 (+1.3) 30.5 (+1.8) 52.9 (+1.1) 66.9 (+0.8) 43.6 (+1.0) 24.5 (+1.4) 46.9 (+1.1) 61.5\nRes-101 FPN (1280) 49.1 30.4 51.9 66.6 43.1 24.5 46.0 61.6\nw/ Copy-Paste (+1.2) 50.3 (+1.3) 31.7 (+1.8) 53.7 (+0.6) 67.2 (+1.1) 44.2 (+1.2) 25.7 (+1.5) 47.5 (+0.2) 61.8\nEff-B7 FPN (1280) 51.1 33.3 53.9 67.9 44.8 26.6 47.9 62.7\nw/ Copy-Paste (+1.5) 52.6 (+1.0) 34.3 (+1.7) 55.6 (+2.3) 70.2 (+1.1) 45.9 (+0.9) 27.5 (+1.5) 49.4 (+1.8) 64.5\nTable 13. Box AP and Mask AP benchmark results on different object sizes for models trained with different backbones and image sizes."
  },
  {
    "source": "2501.01880v1.pdf",
    "content": "Long Context vs. RAG for LLMs: An Evaluation and Revisits\nXinze Li1, Yixin Cao2†, Yubo Ma1, Aixin Sun1†\n1 S-Lab, Nanyang Technological University\n2 School of Computer Science, Fudan University\n{xinze002, yubo001}@e.ntu.edu.sg axsun@ntu.edu.sg\nyxcao@fudan.edu.cn\nAbstract\nExtending context windows ( i.e., Long Con-\ntext, LC) and using retrievers to selectively\naccess relevant information ( i.e., Retrieval-\nAugmented Generation, RAG) are the two main\nstrategies to enable LLMs to incorporate ex-\ntremely long external contexts. This paper re-\nvisits recent studies on this topic, highlight-\ning their key insights and discrepancies. We\nthen provide a more comprehensive evalua-\ntion by filtering out questions answerable with-\nout external context, identifying the most ef-\nfective retrieval methods, and expanding the\ndatasets. We show that LC generally out-\nperforms RAG in question-answering bench-\nmarks, especially for Wikipedia-based ques-\ntions. Summarization-based retrieval performs\ncomparably to LC, while chunk-based retrieval\nlags behind. However, RAG has advantages in\ndialogue-based and general question queries.\nThese insights underscore the trade-offs be-\ntween RAG and LC strategies, offering guid-\nance for future optimization of LLMs with ex-\nternal knowledge sources. We also provide an\nin-depth discussion on this topic, highlighting\nthe overlooked importance of context relevance\nin existing studies.\n1 Introduction\nLarge Language Models (LLMs) (Brown et al.,\n2020) have demonstrated strong zero/few-shot ca-\npabilities in open-ended question answering (Yang\net al., 2019). However, they face challenges such as\nhallucinations (Shuster et al., 2021; Ji et al., 2023),\nlacking real-time information and domain-specific\nknowledge (Su et al., 2024; Zhang et al., 2024),\namong others. A common solution is to enhance\nLLMs with external memory to provide reliable\nand up-to-date data sources. Yet, incorporating\nadditional content is constrained by the limited\ncontext window of LLMs. To address this, two\nmain approaches are adopted: (i) building models\nwith long context windows to read in more infor-\nmation (LC) (Fei et al., 2024; Chen et al., 2023;\nWang et al., 2024c), and (ii) employing retriev-\ners to include text segments relevant to the query\n(RAG) (Jiang et al., 2023; Asai et al., 2024; Gao\net al., 2023).\nAs shown by the timeline in Figure 1a, there is a\nclear trend toward developing models that handle\nlonger context windows and combining LC with\nRAG methods. The chronological overview of re-\nlated studies highlights an increasing focus on both\nLC and RAG since mid-2023, as evidenced by a\ngrowing number of publications aimed at optimiz-\ning the efficient retrieval, and utilization of long\ncontexts. The development of models supporting\nlonger context windows underscores the growing\nimportance of handling extensive inputs effectively.\nDespite the broad consensus regarding the impor-\ntance of LC and RAG, there remain disagreements\nand contradictory insights from different studies,\nsummarized in Table 1. For example, while several\nstudies agree on the effectiveness of combining LC\nand RAG (Xu et al., 2024b; Jiang et al., 2024b),\nothers suggest that combining may not be benefi-\ncial (Bai et al., 2024a; Jin et al., 2024). Moreover,\nconflicting conclusions are reported regarding the\nbenefits of RAG versus LC. Some papers find RAG\nadvantageous in certain contexts (Xu et al., 2024a;\nYu et al., 2024), while others highlight superior\nresults from LC (Li et al., 2024; Xu et al., 2024b).\nThese divergent insights showcase the complexity\nand ongoing debates in the field, suggesting that\noptimal strategies may vary depending on specific\nmodel architectures and benchmark conditions.\nTo explore the underlying reasons, we conduct\nan in-depth investigation into the conditions that\nlead to disagreements among existing studies. Dur-\ning this process, we also identify key aspects\nthat may have been overlooked in earlier research.\nSpecifically, we revisit the evaluation process and\nimplement the following changes. First, we fil-\narXiv:2501.01880v1  [cs.CL]  27 Dec 2024\n2024\nLong B ench R et Meets LC LLM L ongRAG\nC hatQA2\nS elf-ROUTE O P-RAG\nSep Oct Jun Jul SepAugAug Oct\nL C  LLM M eets RAG\nNov Dec\nL C  RAG  P erformance\nLongBench V 2\n(Bai et al., 2024a) (Xu et al., 2024b) (Jiang et al., 2024b) (Li et al., 2024)\n(Xu et al., 2024a)\n(Y u et al., 2024)\n(Jin et al., 2024)\n(Leng et al., 2024)\n(Bai et al., 2024b)\n(a) Related work on LC and RAG, each paper is labeled by a char and one color. For instance, green and \"L\" represent\n\"LongRAG\".\n2023 2024\nJun Jul Aug Oct\nNemo-GPT -43B\nNov Mar Apr May Jun\n R \nGPT -3.5-T urbo B  P  \n C GPT -4- T urbo L   P  \nDeepSeek-V2-Chat L  \nGLM-4-9B-Chat V  \n M Gemma2-9B\n V  GLM4-Plus\nGPT -o1 P  \nLlama- 3.1-8B/70B-Instruct O  C   V   P  \nAug Sep\n C Qwen2-72B-Instruct P  Qwen2.5-72B-Instruct V  \nGemini- 1.5- flash P  \nJul\n O  L   P   S GP T - 4o V  \nGemini- 1.5-pro P   L   S  O  M \nChatGLM2-6B-32K B Claude2 C \nLlama2-70B R \nLlama2-7B-Chat B \nGPT -3.5-T urbo-16k S Claude-3-Haiku P  \nClaude-3-Sonnet L   P  \nClaude-3-Opus L   P  \nInternLM-7B-8K B \n B  R XGen-7B-8K LongChat-v1.5-7B-32K B \nV icuna-v1.5-7B-16K B \nMixtral-8x7B P  \nDBRX-Instruct P  \nLlama-3-ChatQA2-8B/70B C  \nMistral-NeMo-12B-Instruct M  \n(b) Chronological progress of key LLMs from 2023 to 2024. We focus on the models that publications in 1a use. We underline\nthe models that support context window length of ≥ 32K.\nE5-Mistral-7b\n1980s 2022 2023 2024\nFeb Mar Sep Oct Dec\n L   C  M Dra gon R  S  B \nBM25\nNov\nSentence W indow Retriever\nRAPT OR\nT ext-embedding- ada-002\nCon trie ver\n B  R  S \n B  R \n M BGE-Large L   O \nT ext-embedding- 3 Zhipu-embedding- 3 V  \nFeb\n P  \n(c) History of frequently used retrievers from the 1980s until 2024. Webold the retrievers that no existing publications in 1a uses.\nFigure 1: Chronological overview of the development of RAG and LC. The Sub-graphs respectively illustrate the\ntimelines for (a) publications related to LC and RAG, (b) long-context models, and (c) retrievers. We label before\neach model and retriever with the char and color block representing the publication that uses it.\nter out questions from existing datasets that can\nbe correctly answered without external context, re-\nmoving biases from the parametric knowledge of\nLLMs and focusing on questions requiring external\nknowledge. Second, we evaluate retrieval methods\nand baselines on a smaller filtered dataset (1,000+\nquestions) from 12 QA datasets to identify the best\nretriever. Third, we expand the dataset size by ap-\nproximately 10 times by collecting additional data\nfrom the original sources of the 12 datasets1. Lastly,\nwe compare the answers produced by the two set-\ntings, i.e., LC and RAG, and conduct an in-depth\nanalysis. Our results are based on the expanded\ndataset using the long-context setting and the best\nretrieval method identified earlier.\nOur key contributions in this paper are as follows:\n(i) Providing a comprehensive survey of existing\nstudies on LC and RAG, analyzing their implemen-\ntations and key insights. (ii) Proposing a fair and\nsystematic evaluation framework, and performing\ndetailed analyses to understand the strengths and\nlimitations of LC and RAG. (iii) Discussing chal-\n1The experiment code and expanded datasets are available\nat https://github.com/lixinze777/LC_VS_RAG\nlenges for comparing and combining LC and RAG,\nreflecting on the key points that researchers tend to\noverlook in this field. Evaluation results indicate\nthat LC models generally outperform RAG when\nprocessing self-contained information like stories,\nwhile RAG excels at handling fragmented infor-\nmation, particularly in dialogue-based contexts.\nThese experiments deepen our understanding of the\nstrengths and limitations of LC and RAG, offering\nvaluable insights into optimizing retrieval strate-\ngies and effectively integrating these approaches to\nenhance performance in open-domain question an-\nswering. These findings also based on a systematic\nsurvey of existing studies on this topic (see § 2).\nAdditionally, we discuss key aspects of comparing\nLC and RAG in § 6, highlighting areas that have\nbeen underexplored in prior research.\n2 Related Work\nOur primary focus is to evaluate and compare LC\nand RAG. To this end, we review papers with a\nsimilar focus, and provide a detailed analysis of the\nretrievers and long-context settings they employ.\n2.1 Retrievers\nRetrievers, as fundamental components of RAG\npipelines, focus on identifying and extracting con-\ntextually relevant segments of documents. We\ncategorize retrieval strategies into three main ap-\nproaches: chunk-based retrieval, which splits doc-\numents into smaller segments and then retrieves\nthose most relevant to a query; index-based re-\ntrieval, which builds specialized index structures\nto guide efficient and context-rich lookups; and\nsummarization-based retrieval, which leverages hi-\nerarchical summaries to capture a document’s key\ninformation at various levels of abstraction.\nChunk-based Retrieval can be broadly cat-\negorized into sparse retrievers and dense re-\ntrievers. Sparse retrievers, such as the classic\nBM25 (Robertson and Zaragoza, 2009), operate on\nterm frequency-based representations of text and\nrank chunks based on a similarity function, lever-\naging exact matches and term weighting. With\nthe advent of word embeddings, dense retrievers\nhave gained prominence. These models encode\nboth queries and document chunks into dense vec-\ntor representations and calculate relevance using\nsimilarity metrics, such as cosine similarity.\nSince text similarity is often defined by measur-\ning the distance between embeddings, the quality\nof these embeddings is particularly important. Con-\ntriever (Izacard et al., 2022) leverages contrastive\nlearning for training without supervision. By gen-\nerating synthetic queries and pre-training on un-\nlabeled data, Contriever provides robust retrieval\ncapabilities especially in cross-lingual applications.\nOn a larger scale, BGE-Large (Xiao et al., 2023)\nemploys diverse datasets and sophisticated training\nmethods to outperform previous models on compre-\nhensive benchmarks such as C-MTEB. E5Mistral-\n7b (Wang et al., 2024b) combines open-source,\ndecoder-only LLMs with synthetic data generation\npipelines. With minimal human annotations, the\nfine-tuning achieves SOTA performance on BEIR\nand MTEB. Dragon (Lin et al., 2023) also employs\ndata augmentation, including cropping and gener-\native queries, and integrates labels from multiple\nretrieval sources. This strategy ensures its effec-\ntiveness without increasing model complexity. An-\nother method of learning high-quality embeddings\nis through strong generalization ability from LLMs.\nFor instance, OpenAI embeddings draw upon the\nGPT-3.5/4 family while Zhipu-embedding-3 lever-\nages the GLM family (Zeng et al., 2024).\nIndex-based Retrieval requires pre-processing\non the documents with more complicated data struc-\ntures (Gupta et al., 2018). With the development\nof LLM, Llama-Index (Liu, 2022) was proposed to\nfacilitate interaction between the model and doc-\numents more conveniently. The index provides a\nflexible interface to construct various data struc-\ntures, known as “indices” that store, organize, and\nfacilitate quick retrieval of context. Once created,\nthese indices can be efficiently queried, guiding the\nLLM to the most relevant information, improving\nthe accuracy of responses. Some classic indexing\nmethods include tree index which constructs a hi-\nerarchical tree from nodes, and knowledge graph\nindex, which builds a knowledge graph with la-\nbeled nodes and relationships.\nSummarization-based Retrieval is built on top\nof chunk- and index-based approaches. It provides\ncomprehensive summaries for key points in a doc-\nument. These summaries available for retrieval.\nRAPTOR (Sarthi et al., 2024) improves retrieval\nby generating recursive summaries of text chunks\norganized in a tree structure. Instead of retrieving\nshort, contiguous text snippets, RAPTOR clusters\ntext segments, summarizes them at various levels,\nand forms a hierarchical tree that represents the\ndocument’s content at different levels of abstrac-\ntion. This allows retrieval models to extract context\nat varying levels of detail, improving the ability to\nhandle complex questions that require synthesizing\ninformation from multiple parts of the document.\nSuch a summarization-based retrieval method en-\nhances retrieval accuracy for tasks requiring long-\nrange or multi-step reasoning.\n2.2 Long-Context LLMs\nMany research efforts focus on extending input and\noutput windows to accommodate more context (see\nFigure 1b), enabling applications such as extended\ndialogues, large document processing, and complex\nmultimodal tasks. Thus, our analysis focuses on\ntwo dimensions: the model capabilities and the\ncontext length they can reach.\nModel Ability. While most of the models dis-\ncussed here excel at understanding long docu-\nments, many emphasize specialized capabilities.\nChatGLM2-6B-32K (Zeng et al., 2024) employs\nMulti-Query Attention to achieve high reason-\ning efficiency with low memory usage, mak-\ning it suitable for tasks requiring deep reason-\ning. XGen-7B-8K (Nijkamp et al., 2023) en-\nhances long-context conversational understanding\nand text summarization, enabling coherent and con-\ntextually rich dialogues. InternLM-7B-8k (Cai\net al., 2024) is optimized for knowledge under-\nstanding, reading comprehension, and multilingual\ntranslation, supporting diverse linguistic applica-\ntions. Models like DeepSeek-V2-Chat (DeepSeek-\nAI et al., 2024), Qwen2-72B-Instruct (Yang et al.,\n2024), Qwen2.5-72B-Instruct (Qwen et al., 2024),\nMixtral-7x8b (Jiang et al., 2024a), and DBRX-\nInstruct excel in mathematical computations, log-\nical reasoning, and coding, demonstrating strong\nperformance in technical and analytical tasks.\nAdditionally, Claude-3-Opus, Sonnet, Haiku,\nGemini-1.5-flash, and Gemini-1.5-pro (Reid et al.,\n2024) incorporate multi-modal capabilities, effec-\ntively handling both textual and visual informa-\ntion. GLM-4-9B-Chat (Zeng et al., 2024), Mistral-\n12b-Instruct, and Llama-3.1-Instruct (Dubey et al.,\n2024) offer robust multilingual abilities, strong\ninstruction-following and multi-turn dialogue ca-\npabilities, increasing their utility in a wide range\nof conversational scenarios. Finally, Claude-2 is\nnotable for low hallucination rate when processing\nextra-long documents, ensuring high accuracy and\nreliability in information retrieval and synthesis.\nContext Length. As shown in Figure 1b, there is\na clear trend of increasing context length in newly\nreleased models. Following the categorization ap-\nproach proposed by ChatQA2 (Xu et al., 2024a),\nwe classify these models into three categories based\non their supported context windows: short (up to\n4K), long (up to 32K), and ultra-long (more than\n32K) context models.\nShort context models, such as Llama2-70B and\nllama2-7B-chat-4k (Touvron et al., 2023), support\nup to 4K tokens and are typically employed as\nbaselines for retrieval and standard conversational\ntasks. Long context models, including XGen-7B-\n8K(Nijkamp et al., 2023), InternLM-7B-8k(Cai\net al., 2024), Mixtral-7x8b (Jiang et al., 2024a),\nDBRX-Instruct and Gemma2-9B (Mesnard et al.,\n2024), offer context windows ranging from 8K to\n32K tokens. These are ideal for extended con-\nversations, comprehensive text analysis, and de-\ntailed summarization tasks. Ultra-long context\nmodels extend beyond 32K tokens. For example,\nClaude-2 provides a 100K token window, while\nClaude-3-Opus, Sonnet, and Haiku handle up to\n200K tokens. GPT-4-Turbo(OpenAI et al., 2023),\nGPT-4o, and GPT-o1 all support 128K tokens, as\ndo DeepSeek-V2-Chat(DeepSeek-AI et al., 2024),\nQwen2-72B-Instruct(Yang et al., 2024), Qwen2.5-\n72B-Instruct (Qwen et al., 2024), GLM-4-9B-\nChat (Zeng et al., 2024), GLM-4-Plus, Mistral-12b-\nInstruct, and Llama-3.1-Instruct. Notably, Gemini-\n1.5-flahs and Gemini-1.5-pro(Reid et al., 2024)\nboth support up to an unprecedented 10M tokens.\nThese ultra long-context models enable the process-\ning of exceptionally large documents, complex mul-\ntimodal tasks, and extensive multi-turn dialogues.\n2.3 Comparing & Combining LC and RAG\nSince the increase in LLMs’ context window\nlengths, some models can contain the entire docu-\nment, reducing the need to retrieve on documents.\nHence, more studies have begun comparing the\nperformance of long-context LLMs and RAG, as\nwell as investigating ways to combine them. Long-\nBench (Bai et al., 2024a) conducts early compari-\nson experiments on a 4K model with RAG and a\n32K model. Xu et al. (2024b) systematically com-\npare LC LLMs and RAG, and proposes their combi-\nnation. LongRAG (Jiang et al., 2024b) introduces\nlong retrievers and long readers, a successful appli-\ncation of long retrieval units to RAG. ChatQA2 (Xu\net al., 2024a) instruction-tunes long-context LLMs\nto a 128K context window and tests their ability\nwith long-context retrievers. Self-ROUTE (Li et al.,\n2024) enables the model to select either RAG or\nLC based on self-reflection to reduce costs. OP-\nRAG (Yu et al., 2024) preserves the original order\nof retrieved chunks, and LC LLM meets RAG (Jin\net al., 2024) investigates long-context LLMs in\nRAG systems, proposing retrieval reordering meth-\nods. LC RAG Performance of LLM (Leng et al.,\n2024) evaluates the effectiveness of RAG on long-\ncontext LLMs across context lengths from 2K to\n2M tokens. Very recently, LongBench is updated\nto LongBench V2 (Bai et al., 2024b), which tests\nLLMs on long context comprehension and reason-\ning with a more realistic and challenging setting.\nWe summarize the key insights from these pa-\npers into three categories: (1) general insights\nsuch as chunking strategies, (2) combining the two\nstrategies, and (3) comparing the performance be-\ntween LC and RAG (see Table 1).\nSome papers reach consensus on chunking strat-\negy that, retrieval units should be longer (Jiang\net al., 2024b) and the number of chunks should\nbe kept low (Yu et al., 2024). According to (Xu\net al., 2024b), selecting the top 5 to 10 chunks typ-\nically yields strong performance, while retrieving\nPaper Type Findings\nLongBench (B) ● Retrieval helps 4k model, but not 16k/32k models.\n(Bai et al., 2024a) + Models benefit from continuous training on long contexts.\n+ Splitting context into shorter and more chunks is better.\nRet-LC LLM(R) ⋆ LC is better for multi-hop benchmarks than 4k RAG.\n(Xu et al., 2024b) ○ RAG improves on 70B/43B models on all context lengths.\n+ For LC model, best results are obtained from top-5 or top-10.\nLongRAG (L) ○ Retrieval benefits from long retrieval units.\n(Jiang et al., 2024b)\nChatQA2 (C) ☆ For sequence lengths up to 32K, RAG outperforms LC.\n(Xu et al., 2024a) ○ From 3K to 24K, greater context window benefits RAG.\nSelf-ROUTE (S) ⋆ LC consistently outperforms RAG, but RAG has lower cost.\n(Li et al., 2024)\nOP-RAG (O) ☆ Efficient retrieval can outperform brute-force LC.\n(Yu et al., 2024) + Too many chunks in RAG harms performance.\n+ Preserving the original order is better than ordering by score.\nLC LLM-RAG(M) ● Retrieve more passages first improves performance then drops.\n(Jin et al., 2024) + Ordering higher score information to front and back helps.\nLC RAG ○ Most close models’ RAG improves up to 100k tokens.\nPerformance (P) ● Most open models’ RAG peak at 16k-32k then performance drops.\n(Leng et al., 2024)\nLongBench v2(V) ☆ GPT-4o performs better at 128k without RAG.\n(Bai et al., 2024b) ○ GPT-4o performance keeps increasing to 128k RAG context.\n● Qwen2.5 & GLM-4-Plus drop with >32k RAG contexts.\nTable 1: Important findings from existing studies that compare or combine LC with RAG (label in brackets). We\ngroup the insights into three categories: 1) General strategies that improve performance marked by +. 2) Combining\nLC and RAG, where ○indicates combining is good, and ●for combining is not helpful, and 3) Comparing LC and\nRAG, where ☆indicates RAG outperforms LC, and ⋆for LC outperforms RAG.\nmore than 20 chunks leads to diminished results.\nLongBench (Bai et al., 2024a) presents a different\nfinding, suggesting that splitting a long context into\nshorter and more numerous chunks is better. How-\never, at the time of its publication, LLMs generally\nexhibited weaker long-context capabilities, and the\nstudy did not incorporate very long retrieval units\n(>1000 tokens). Consequently, LongBench’s find-\nings are not at odds with the broader consensus.\nNonetheless, these papers present disagreement\nregarding performance of retrieval on long-context\nLLMs. For instance, LongBench (Bai et al., 2024a)\nfinds that retrieval helps short-context models but\nnot 7B long-context models. In contrast, Xu et al.\n(2024b) suggest that RAG improves 70B models\nacross all context lengths, attributing the discrep-\nancy to the difference between model sizes. Sim-\nilarly, ChatQA2 (Xu et al., 2024a) observes that\nincreasing the context window from 3K to 24K\ntokens consistently benefits RAG. Notably, Long-\nBench V2 (Bai et al., 2024b) shows that GPT-4o\ncontinues to improve in RAG performance even\nat 128K input, whereas Qwen2.5 and GLM-4-Plus\nshow performance deterioration beyond 32K input.\nThe observations align with findings from (Leng\net al., 2024) that RAG for close-source models can\nimprove up to 100K input, whereas performance\nfor some open-source models peaks around 16K\ntokens. Hence, the varying behaviors might be due\nto different model size and architecture.\nThere are even greater discrepancies in the direct\ncomparisons between the two methods. Xu et al.\n(2024b) claims that long-context models outper-\nform retrieval with short-context models in multi-\nhop benchmarks. In contrast, ChatQA2 (Xu et al.,\n2024a) finds that RAG can outperform LC if a\nsufficient number of top-k chunks are used. Self-\nROUTE (Li et al., 2024) fully supports LC, arguing\nthat it outperforms RAG in all benchmarks. Mean-\nwhile, OP-RAG (Yu et al., 2024) defends RAG,\ndemonstrating that efficient retrieval strategies can\noutperform a brute-force approach of processing\nextremely long contexts.\nThe reasons for the differences among these stud-\nies are manifold. For instance, There are three\ncategories of retrieval methods (i.e., chunk-based,\nindex-based, and summarization-based retrieval),\nbut current studies rely predominantly on chunk-\nbased retrieval, leaving room for further optimiza-\ntion. Additionally, evaluation scores often repre-\nsent weighted averages across different datasets.\nBecause each dataset has distinct characteristics,\nplacing more emphasis on one dataset and less on\nanother can alter the final results. Finally, most ex-\nisting studies use only a few datasets with around\n200 questions each. This small sample size creates\ngreater room for variability and reduces the general\nreliability of these findings.\n3 Question Filtering and Expansion\nTo ensure a fair and comprehensive comparison,\nwe curate our evaluation dataset based on existing\ndatasets, and apply necessary filtering (§ 3.1) and\naugmentation (§ 3.2). We select 12 long-context\nQA datasets frequently used in studies comparing\nLC and RAG: Natural Questions (Kwiatkowski\net al., 2019), 2WikiMultihopQA (Ho et al., 2020),\nHotpotQA (Yang et al., 2018), MuSiQue (Trivedi\net al., 2022), MultiFieldQA (Bai et al., 2024a), Nar-\nrativeQA (Koˇciský et al., 2018), QASPER (Dasigi\net al., 2021), QuALTY (Pang et al., 2022), Cours-\nera, TOEFL-QA, and MultiDoc2Dial (An et al.,\n2024). We also include the NovelQA (Wang et al.,\n2024a) dataset, a high-quality, human-annotated re-\nsource derived from long-form novels. We present\nan overview of these datasets in Table 2, including\ntheir type, context type (single-doc or multi-doc),\ncontext source, average context length, and repre-\nsentative studies that have utilized each dataset.\n3.1 Question Filtering\nGiven the strong capabilities of modern LLMs,\nmany questions can be directly answered based on\nknowledge encoded in their parameters (Basmova\net al., 2024), reducing the need for external context\nin some cases. However, certain queries, such as\nthose related to private conversations, will always\nrequire additional context. To determine which ap-\nproach more effectively enhances an LLM’s perfor-\nmance with long documents, we filter the datasets\nto include only questions that the LLM cannot\nanswer correctly without external context. This\nensures that any correct answers obtained subse-\nquently must rely on external knowledge rather\nthan the model’s built-in knowledge.\nFor our implementation, we use GPT-4o for\nquestion filtering due to its strong capabilities. We\nemploy a strict exact-match scoring metric to en-\nsure that the model not only provides the correct\nanswer but also demonstrates a complete under-\nstanding of the required information.\n3.2 Question (and Context) Expansion\nRAG and LC produce identical answers for about\n60% of the questions in existing evaluations (Li\net al., 2024), leaving relatively few questions to\nhelp us understand the differences between the two.\nTo ensure robust statistical significance, we expand\nthe dataset size to approximately 20,000 questions\nby collecting additional samples.\nTo maintain a similar distribution as the origi-\nnal datasets, we follow two principles during data\ncollection. First, we collect questions only from\nthe original source of each dataset, avoiding arti-\nficially generated or LLM-augmented questions.\nSecond, we add distracting passages to the origi-\nnal context for each question to extend the context\nlength, following the implementation described in\nLongBench. For NovelQA, we use all its available\nquestions. For Coursera, MultiFieldQA, and Multi-\nDoc2Dial datasets, we do not further enlarge their\nsizes to avoid introducing artificial data.\nHereafter, we refer to the expanded dataset as the\nfull question setand the original, pre-expansion\ndataset as the sample question set.\n3.3 Dataset Statistics\nAfter expansion, we obtain 19,188 questions, of\nwhich 13,651 require context to be answered using\nthe filtering method from § 3.1, as listed in Table 3.\nNotably, questions grounded in factual knowledge,\nsuch as those from Coursera, show a high removal\nrate. Similarly, questions drawn from well-known\nbooks or requiring multi-hop reasoning often ex-\nhibit a higher likelihood of being directly answered\nby LLMs without context. Comparing the 12 indi-\nvidual datasets, we observe a similar filtering rate\nbetween the sample and the full question sets (see\nTables 2 and 3), indicating that both sets follow a\nsimilar distribution.\n4 Evaluation Methodology\n4.1 Evaluation Framework\nOur evaluation of RAG and LC is conducted in the\nfollowing three phases.\nPhase 1: Empirical Study on Retrievers. We\nevaluate five retrievers: BM25, Contriever, OpenAI\nEmbeddings, Llama-Index, and RAPTOR, on the\nsample question set. The retriever yielding the\nbest performance is then selected for subsequent\ncomparisons with LC on the full question set.\nPhase 2: Comparing RAG and LC. Using the\nbest retriever, RAG is compared with LC by an-\nDataset T Doc Source Avg Len Used by Papers # Q # Kept % Kept Mode\nNQ K multi Wikipedia 18,164.7 M, P 109 22 20 Open\nCoursera K multi Coursera 7,934.3 NIL (L-eval) 172 54 32 MCQ\nNovelQA C single books 67,000.0 NIL (NovelQA) 210 109 52 MCQ\n2WikiMHQA R multi Wikipedia 7,191.3 B, S, M 300 152 51 Open\nHotpotQA R multi Wikipedia 10,602.7 B, R, L, C, S, M 200 93 47 Open\nMuSiQue R multi Wikipedia 12,974.3 B, R, C, S 200 140 70 Open\nMultiFieldQA C single papers, reports 5,706.1 B, R, L, C, S 150 121 81 Open\nNarrativeQA C single books, films 25,274.2 B, R, S 200 171 86 Open\nQASPER C single papers 5,350.3 B, R, C 224 221 99 Open\nQuALTY C single stories 5,089.2 R, C 202 202 100 MCQ\nTOEFL-QA C single exams 729.1 NIL (L-eval) 121 121 100 MCQ\nMultiDoc2Dial C multi dialogue 3,076.9 NIL (L-eval) 158 158 100 Open\nTable 2: Overview of the original datasets (i.e., the pre-expanded sample question set) and their characteristics. The\ncolumn “T” represents dataset type with values “K” for “Knowledge”, “R” for “reasoning”, and “C” for “reading\ncomprehension”. For each dataset, we report the existing papers (with the label) about LC & RAG that use it. If no\npaper has used it, we report its source like L-eval (An et al., 2024). We also report number of questions in each set\n(# Q), number and percentage of questions retained after filtering (# Kept and % Kept) out questions needing no\ncontext, and mode of question.\nDataset # Questions # Kept Q % Kept Q\nCoursera 172 54 32\nNQ 1,109 373 34\nNovelQA 2,283 869 38\n2WikiMHQA 2,300 1,036 45\nHotpotQA 2,200 1,113 51\nMuSiQue 2,200 1,663 78\nMultiFieldQA 150 121 81\nNarrativeQA 2,211 1,880 85\nQASPER 2,718 2,674 98\nQuALTY 2,725 2,725 100\nTOEFL-QA 962 962 100\nMultiDoc2Dial 158 158 100\nTotal 19,188 13,628 71\nTable 3: Statistics of the full question set, ordered by\nincreasing percentage of questions kept after filtering\nout questions needing no context.\nswering questions on the full question set. Both\nmethods use the same underlying LLM for ques-\ntion answering. For RAG, relevant documents or\nchunks are fetched from the available context and\nprovided to the LLM as input to generate answers.\nIn contrast, for LC, the entire context available to\nthe question is given to the LLM, with truncation\nfrom the back of the context applied if the context\nexceeds the model’s context window. The evalua-\ntion metrics are explained in § 4.3.\nPhase 3: In-depth Analysis. We focus on 4 spe-\ncific subsets of questions: 1) those answered cor-\nrectly only by RAG, 2) those answered correctly\nonly by LC, 3) those RAG gives better answers, and\n4) those LC gives better answers. These subsets\nare analyzed to understand the types of questions\neach method excels at, providing insights into the\nstrengths and limitations of both approaches in dif-\nferent scenarios.\n4.2 Retriever Selection\nFigure 1 shows that existing studies primarily select\none or more chunk-based retrieval methods, while\nindex- and summarization-based retrievers are less\nfrequently evaluated. In our study, we evaluate\nvarious retrieval methods to ensure that RAG is\nsupported by the most effective retrievers.\nFor chunk-based retrieval, we use\nBM25 (Robertson and Zaragoza, 2009), Con-\ntriever (Izacard et al., 2022), and OpenAI’s\ntext-embedding-3-Small. BM25 serves as a\nclassic baseline, while Contriever and text-\nembedding-3-Small represent embeddings from\nwell-performing closed-source and open-source\nmodels, respectively.\nFor index-based retrieval, we employ Llama-\nindex and leverage two indexing methods that suit\nlong documents. Specifically, tree-index organizes\ndocuments into a hierarchical tree structure, en-\nabling efficient retrieval of context. The root node\ncontains a high-level summary, while subsequent\nchild nodes store progressively finer-grained repre-\nsentations. When queried, the retrieval process nav-\nigates through this hierarchy, starting from the top-\nlevel summary and moving down to more specific\nnodes as needed. Sentence Window Retriever\nfocuses on local, sentence-level context rather than\nentire documents or large text chunks. It creates\nsmaller “windows” of a few sentences each. When\na query arrives, the retriever searches these win-\ndows to identify segments most semantically simi-\nlar to the query. By working at a finer granularity,\nthe sentence window retriever provides more tar-\ngeted and contextually accurate snippets of text,\nQuestionsOnly RAGAnsweredCorrectly\nQuestionsOnly LCAnsweredCorrectly\nLC answersbetter(F1)\nRAG answersbetter (F1)\nQuestionsBothAnsweredWrongly\nQuestionsBothAnsweredCorrectly \nCorrectAnswersby LC(EM)\nCorrect Answersby RAG (EM)\nFigure 2: Evaluation Matrix for In-depth Analysis.\nimproving the model’s ability to answer specific\nquestions.\nFor summarization-based retrieval, we use\nRAPTOR (Sarthi et al., 2024). It constructs a hier-\narchical tree by recursively clustering text chunks\nbased on semantic similarity, summarizing each\ncluster into a parent node, and continuing this pro-\ncess until no further clustering is possible. After\nconstructing the tree, we apply the collapsed tree\ntraversal approach, as previous work has demon-\nstrated its superior performance. This approach\nflattens the hierarchical structure into a single layer\nand compares the query against all nodes across\nevery level simultaneously. The top-k most rele-\nvant nodes are then selected based on a predefined\ntoken limit, ensuring that the retrieved information\nmaintains the appropriate level of granularity.\nAlthough RAPTOR’s implementation appears\nsimilar to the Llama Tree Index, they differ in both\nconstruction and navigation. First, Llama Tree\nIndex groups consecutive nodes, while RAPTOR\nfreely clusters nodes from far positions, and even\nallows a single node to appear in multiple clusters.\nSecond, Llama Tree Index navigates down the hier-\narchy to retrieve only leaf nodes, while RAPTOR\nevaluates all nodes from all layers simultaneously.\nHence, RAPTOR can retrieve not only original\ntexts but also generated summaries.\n4.3 Evaluation Metric\nWe use a win-lose rate system to compare LC and\nRAG, as illustrated in Figure 2. The horizontal\nyellow block represents the questions that the LLM\nanswers correctly using LC, while the vertical blue\nblock represents the questions that the LLM an-\nswers correctly using RAG. Their overlap in the\ntop-left corner represents the questions that both\nmethods answer correctly. We apply an Exact\nMatch (EM) score strictly to all questions to de-\ntermine the correctness of the answers. Excluding\nthe overlap, the top right block indicates the ques-\ntions that only LCanswers correctly, and similarly,\nthe bottom left block indicates the questions that\nonly RAGanswers correctly.\nThe remaining gray block represents the ques-\ntions that both RAG and LC answer incorrectly, as\njudged by Exact Match. Since many questions in-\nvolve long open-ended responses, we calculate the\nF1 scores of the answers provided by both meth-\nods against the ground truth. If RAG achieves a\nhigher F1 score than LC, we consider RAG to have\nanswered the question better, and vice versa for LC.\nA detailed explanation of F1 score calculation is\nprovided in appendix A\nThe loose evaluation setting considers all cases\nin which one method outperforms the other, includ-\ning 1) when one method obtains the correct answer\nand the other is wrong under EM, and 2) when\none method achieves a higher F1 score. We adopt\nthis loose evaluation because references for some\ndatasets are long, open-ended answers, making it\nvery unlikely to match them exactly under EM. In\naddition, some short answers (about 5–6 words)\nmay differ slightly from the reference while still\nconveying the correct idea. Although these answers\nwould be marked incorrect by EM, they might at-\ntain a high F1 score. Hence, comparing F1 scores\nhelps compensate for the strictness of EM.\n5 Experiments\nTo obtain answers, we use the same prompt “From\nthe context: [context], answer the questions briefly\nwith no explanation. ”for both retrieval and long\ncontext settings. For MCQ questions, we add one\nsentence “Answer the question with the letters of\nthe correct options (e.g. A, BC, C, ACD, etc.) with-\nout including text”. These prompts ensure LLMs\nto directly answer the questions, which makes eval-\nuation more convenient.\n5.1 Phase 1: Retrievers\nEvaluated on the sample question set, Ta-\nble 5 reports the results of chunk-, index-, and\nsummarization-based retrievers. Among them,\nRAPTOR performs the best with a correct answer\nrate of 38.5%, while Index-based retrievers outper-\nform chunk-based retrievers. Within index-based\nretrievers, the “RAG Only” score for Tree Index\nis much lower than that for Window Parsing (82\nDataset # Questions LC Correct RAG Correct LC Only RAG Only LC Better RAG Better\nCoursera 54 26 20 10 4 10 4\n2WikiMHQA 1,036 594 431 242 79 265 107\nHotpotQA 1,113 876 723 212 59 231 67\nMultiFieldQA 121 63 60 14 11 44 21\nNQ 373 189 138 75 24 104 35\nNarrativeQA 1,880 558 405 276 123 685 281\nQASPER 2,674 884 863 517 496 1,011 762\nQuALITY 2,725 2,290 2,050 402 162 402 162\nTOEFL-QA 962 895 884 26 15 26 15\nMuiQue 1,663 821 663 344 186 426 225\nMultiDoc2Dial 158 14 38 5 29 65 58\nNovelQA 869 466 408 164 106 164 106\nOverall 13,628 7676 6,683 2,287 1,294 3,433 1,843\nTable 4: Performance of LC and RAG across different datasets. We report the number of questions answered\ncorrectly by each method, as well as the breakdown of questions where: only LC answers correctly (LC Only), only\nRAG answers correctly (RAG Only), LC outperforms RAG (LC Better), and RAG outperforms LC (RAG Better).\nType Retriever Correct (%) RAG Only RAG Better\nChunk BM25 319 (20.4) 50 141Contriever 315 (20.1) 43 143Text-emb-3-small 338 (21.6) 47 151\nIndex Tree Index 470 (30.1) 82 234Window Parsing 555 (35.5) 91 237\nSummarizationRAPTOR 602 (38.5)97 258\nTable 5: Comparison of different retrieval methods\nvs. 91), and their “RAG Better” scores are nearly\nidentical (234 vs. 237). This discrepancy suggests\nthat Tree Index may be undervalued in the “RAG\nOnly” metric but still contributes in open question\nscenarios that require long answers.\nWe further observe the questions and contexts\nthat each retriever exclusively answers correctly.\nRAPTOR shows stronger ability than other retriev-\ners, especially in scenarios that require an entire un-\nderstanding of the document, like research papers.\nChunk-based methods struggle when required in-\nformation is spread across multiple chunks. Index-\nbased retrievers are not as strong in overall under-\nstanding as RAPTOR, but they show good ability in\ninterpreting dialogues. Therefore, we select RAP-\nTOR as the primary retriever for evaluation on the\nfull question set.\n5.2 Phase 2: Comparing LC and RAG\nWe compare LC and RAG on the filtered, full ques-\ntion set. The results across 12 datasets are sum-\nmarized in Table 4. Overall, LC correctly answers\n56.3% of the questions, while RAG provides cor-\nrect answers to 49.0%. LC correctly answers more\nthan 2,000 questions that RAG misses, while RAG\nexclusively answers almost 1,300 questions. When\nlooking at the loose evaluation setting, LC answers\n3,433 questions better than RAG, and RAG an-\nswers 1,843 questions better than LC. The gap fur-\nther widens compared to strict setting, indicating\nlong-context LLM’s ability to answer questions\nwith open long answers is also strong.\nLooking at individual datasets, in Multi-\nDoc2Dial, RAG exhibits better performance than\nLC in strict evaluation (5 vs 29), but is surpassed by\nLC in loose evaluation (65 vs 58). In contrast, on\ndatasets like NarrativeQA and QuaLITY , LC shows\na strong lead not just in overall correctness but also\nin the number of questions that are answered better.\nCollectively, the results show that both methods\nhave unique strengths and limitations.\nAlthough LC shows better overall results than\nRAG, out of the 13,628 questions, almost 10% can\nbe only answered correctly by RAG, which is not\na small ratio. This shows that retrievers cannot be\nsimply replaced by long-context LLM in searching.\nThis also motivates us to further examine what kind\nof questions (and context) can be only answered\ncorrectly by RAG (or LC).\n5.3 Phase 3: In-Depth Analysis\nThe overall results are influenced by the combined\neffects of different scenarios, so we need to sepa-\nrately analyze each scenario to see if more detailed\nresults can be obtained. We analyze the perfor-\nmance of LC and RAG across different knowledge\nsources (Figure 3) and question types (Figures 4).\nHere, we use EM Scores only, for a strict evaluation\nstandard. We also report the results for loose evalu-\nation standard (i.e., EM Scores and F1 Scores) in\nappendix B, which shows similar trends.\nFrom Figure 3, it is evident that LC excels with\nknowledge sources such as Wikipedia and sto-\nries. However, the Wikipedia context is collected\n0 200 400 600 800 1000 1200\nWord Count\nWikiPedia\nStory\nPaper/Report\nDialogue\n873 348\n842 391\n531 507\n31 44 LC\nRAG\nFigure 3: Performance breakdown by knowledge source\nfor LC Only and RAG Only.\nby adding extensive noise to create long context,\nwhich generally makes the context less relevant\nto the question, with only a small portion being\nuseful. This synthetic context formation partially\nsimulates the RAG process and may introduce an\nunfair bias against the RAG pipeline. In addi-\ntion, summarization-based retrieval methods may\nsplit Wikipedia articles unnaturally, generating less\nmeaningful summaries. LC’s strong performance\ndemonstrates that long-context LLMs are robust to\nnoise in such forms of context.\nIn contrast, RAG performs better with dialogue-\nrelated sources and achieves comparable perfor-\nmance with papers or reports. The information in\nthese sources is naturally segmented, conversations\nhave turns, and papers and reports have clearly de-\nfined sections or subsections, making the retrieval\nof key segments easier.\nFigure 4 shows that LC performs better for fact-\nbased questions such as “Who”, “Where”, and\n“Which”. These questions often benefit from having\nall the relevant context available in a dense region\nclose to the answer. RAG, however, is largely com-\nparable to LC for more open-ended questions such\nas “How”, which often require synthesizing infor-\nmation from multiple sources and therefore benefit\nfrom retrieval-based approaches.\nFurthermore, RAG outperforms LC in the\n“Other” questions, which consist mainly of general\nquestions that can be answered with “Yes” or “No”.\nWe hypothesize that the reason could be due to the\ntraining data. Long-context LLMs are more famil-\niar with phrasing of common type questions than\ngeneral questions. Words like “Who” or “Where”\nact as keywords for long-context LLMs to search,\nwhile retrievers use these keywords not so well.\n5.4 Word Frequency Visualization\nTo better understand the scenarios that LC and\nRAG each excels at, we visualize the word fre-\nquencies by their TF-IDF scores, plotted in Fig-\nure 5. The TF-IDF scores were calculated from\n0 200 400 600 800 1000 1200\nWord Count\nOther\nHow\nWhy\nWhat\nWhich\nWhere\nWhen\nWho\n54 140\n333 280\n139 63\n919 468\n250 110\n142 59\n182 73\n268 101 LC\nRAG\nFigure 4: Performance breakdown by question type for\nLC Only and RAG Only.\ncountry\nsongfirstfilmnovelcity\npeopletimes\nperformer\ndataset\nnewonetimeearthmodel\nWords\n0.10\n0.15\n0.20\n0.25\n0.30TF-IDF Score\nLC\nRAG\nFigure 5: Top 15 Words based on TF-IDF Score for LC\nOnly vs. RAG Only.\nquestions in the datasets where either LC or RAG\nproduced correct answers exclusively. Specifically,\nall questions from each dataset are concatenated\nand treated as a single document for this analysis,\nmeaning that the TF-IDF scores primarily reflect\nthe term frequency within each dataset. Stopwords\nare removed and not shown in the plot.\nFigure 5 presents the top 15 words that appear\nmost frequently combined in both LC only and\nRAG only questions. Words such as ‘song’, ‘film’,\nand ‘novel’ have higher TF-IDF scores for LC,\nsuggesting that LC performs better with narrative\ntopics. Conversely, words like ‘country’, ‘dataset’,\nand ‘model’ have higher scores for RAG, indicating\nits strength in retrieving information on technical\nor data-oriented topics. This analysis underscores\nthe complementary strengths and limitations of LC\nand RAG in handling different types of questions.\n5.5 Impact of Generation Model in RAG\nWe now evaluate the impact of different generation\nmodels on RAG’s performance. Table 6 shows the\nresults of using GPT-4o and GPT-4-Turbo as the\ngenerator with three retrievers (BM25, Tree Index,\nRAPTOR), each of which represents one retriever\ntype. The results indicate that the performance of\ndifferent generation models remains largely con-\nRetrieverModel Correct (%) RAG Only RAG Better\nBM25 GPT-4o 319 (20.4) 50 141\nGPT-4-Turbo 310 (19.8) 51 152\nTree-IndexGPT-4o 470 (30.1) 82 234\nGPT-4-Turbo 458 (29.3) 81 229\nRAPTORGPT-4o 602 (38.5) 97 258\nGPT-4-Turbo589 (37.7) 99 295\nTable 6: Results of using different generation models\nsistent regardless of the retriever used. RAPTOR\nperforms the best across both generation models,\nthough there is a slight decrease in performance\nwhen using GPT-4-Turbo compared to GPT-4o.\nWhile GPT-4o slightly outperforms GPT-4-\nTurbo across all retrievers, the differences are\nmarginal. This implies that both generation models\nare capable of generating high-quality responses,\nand the choice between them may depend more\non other factors such as efficiency or resource\navailability. The consistency across retrievers also\ndemonstrates that the retrieval method plays a\nlarger role in determining overall performance than\nthe specific generation model used. We will report\nthe results from other models and the experiment\nis in progress.\n5.6 Case Study\nFor a deeper understanding of the difference be-\ntween LC and RAG, we conduct a case study to\nanalyze the frequent errors from each method, and\npresent them in Tables 7 and 8. We manually ex-\namine the questions that only RAG made mistakes,\nand those only LC made mistakes.\nThe most frequent mistake made by RAG is its\nfailure to retrieve the relevant context, leading to\nits refusal to answer the question. As shown in\nTable 7, the model correctly identifies that Anthony\nUpko was formerly involved in the government of\nNigeria but fails to retrieve the debt-to-GDP ratio as\npart of the context. This retrieval failure can arise\ndue to two possible reasons: the retriever might fail\nto locate the relevant sentences from documents,\nor the sentences may be split across two chunks,\nwith the debt-to-GDP ratio lacking a clear subject.\nInterestingly, when provided with the same prompt,\nLC rarely reports a lack of context, suggesting its\nrobustness in handling such cases.\nAnother error made by RAG is misinterpreting\npartial context. In the second example, where RAG\nincorrectly answered the birthday, the model re-\ntrieved May 8, 1940, instead of the correct date,\nJanuary 8, 1935. This occurred because the sen-\nQuestion: What is the debt-to-GDP\nratio of the country where Anthony\nUpko was formerly involved in the\ngovernment?\nWrong Answer: The context does not\nprovide the debt-to-GDP ratio for\nNigeria.\nGold: 11 percent\nRelevant Sents: 1. Nigeria is the\nworld’s 20th largest economy ... the\ndebt-to-GDP ratio is only 11 percent.\n2. Anthony Ukpo was Minister of\nInformation and Culture, and then\nGovernor of Rivers State, Nigeria.\nQuestion: When is the performer of\nsong Swing Down Sweet Chariot ’s\nbirthday?\nWrong Answer: May 8, 1940\nGold: January 8, 1935\nRelevant Sents: 1. Swing Down Sweet\nChariot is a traditional song ...\nrecorded by Elvis Presley.\n2. Elvis Aaron Presley (January 8,\n1935 - August 16, 1977), also known as\n...\nTable 7: Examples cases where RAG made mistakes\nQuestion: Do the tweets come from a\nspecific region?\nWrong Answer: Yes, the tweets come\nfrom 16 different countries.\nGold: No\nRelevant Sents: This helped us narrow\ndown our query space to 16 countries.\nQuestion: Where did Valancourt lose\nhis wealth?\nWrong Answer: In Gambling.\nGold: Paris\nRelevant Sents: Returning to her\naunt’s estate, Emily learns that\nValancourt has gone to Paris and lost\nhis wealth.\nTable 8: Examples representing common cases where\nonly RAG answers correctly\ntence ‘Swing Down Sweet Chariot is a traditional\nsong ... recorded by Elvis Presley’ spans too long,\ncreating ambiguity in linking the birthday to the cor-\nrect person. This type of retrieval failure highlights\na core limitation: RAG relies heavily on retriev-\ning continuous text spans, and any fragmentation\nor overly long context can lead to an incomplete\nunderstanding. In contrast, LC tends to provide\nmore holistic answers when processing longer con-\ntexts directly, as it bypasses the dependency on a\nretrieval module.\nWrong answers by LC are often caused by ques-\ntion misinterpretation. For instance, as shown in\nTable 8, when asked whether the tweets come from\na specific region, LC answers ‘yes’, referencing\nthat the tweets originate from 16 countries. It fails\nto interpret the relationship between ‘a specific\nregion’ and ‘16 different countries’. In another\nexample, when asked ‘where’ Valancourt lost his\nwealth, the model identifies the correct sentence\nbut answers ‘how’ instead of ‘where’. These exam-\nples highlight that LC sometimes struggles to align\nits semantic understanding with the required level\nof specificity or perspective, resulting in answers\nthat are related but not addressing the question’s in-\ntent. In both cases, the LLMs are able to locate the\nrelated texts from the documents, but the reasoning\nability might be affected by the noise.\n6 Discussion\n6.1 What is Long Context?\nAlthough we have reviewed 9 studies that either\ndirectly or implicitly compare or integrate RAG\nand Long Context, very few studies clearly define\nwhat Long Context is. To this end, we separately\ninterpret the two words ‘long’ and ‘context’.\nLong. Out of the 9 studies reviewed earlier, only\n2 studies, ChatQA2 and LongBench v2 explicitly\ndefine Long Context as greater than 32k and greater\nthan 8k tokens respectively. For other studies, we\ncan only infer their definitions of “long” based on\nthe models and datasets they use. It seems that\nthree studies consider 8k as a minimum require-\nment for long context, and another three studies set\nthis requirement at 16k. Lastly, OP-RAG regards\n128k as long context.\nIn short, each work defines ‘Long Context’ based\non its own criteria due to the lack of a clear stan-\ndard. Moreover, as the context windows of lan-\nguage models continue to expand, the terms ‘long’\nand ‘short’ are relative. For example, 4k tokens\nare not considered ‘long context’ in any of the re-\nviewed studies but are extremely long for BERT-\nbase models, which support only 512 tokens. As a\nresult, the definition of ‘long’ remains ambiguous,\nleading to inconsistent use of this concept among\nresearchers. In practice, the definition of ‘long’ is\ncomplicated, depending on the context length of\nlatest LLMs, and the length of the documents in\ntargeted domain.\nContext In the English dictionary, ‘context’ is\ndefined as “the situation within which something\nhappens, and that can help explain it”. By this\ndefinition, the context of a question is expected to\n“help explain it”, implying that the context should\nhave strong relevance to the question. However,\nlong-context datasets are not always constructed\nwith this principle in mind. The construction of\nlong-context datasets can generally be categorized\ninto two types:\nRealistic Long Texts: These datasets originate\nfrom sources such as novels, research papers, or\nother lengthy narratives, exemplified by datasets\nlike NovelQA. Such datasets typically pose chal-\nlenges that involve reading comprehension and re-\nquire models to process and synthesize dense infor-\nmation spread across a cohesive, extended text.\nSynthetic Long Texts: These datasets are often\ncreated by concatenating smaller, query-relevant\nsegments of text, such as Wikipedia-sourced\ndatasets in LongBench. This construction process\nmay involve stitching together Wikipedia excerpts,\ninjecting noise, or combining unrelated passages to\nsimulate a long document.\nA critical observation is that realistic long con-\ntexts align more closely with reading comprehen-\nsion tasks, where models primarily absorb and rea-\nson over information. Such datasets have high con-\ntextual relevance, since the questions are normally\nbased on the documents that users provided. In con-\ntrast, synthetic long contexts often resemble factual\nreasoning tasks, where models retrieve and verify\nknowledge. Such datasets inherently incorporate\na pre-processing step like a RAG pipeline. They\ncan assess the impact of information placement on\nmodel performance, such as the lost-in-the-middle\nphenomenon.\nOn the other hand, realistic and synthetic long\ntexts can only serve as proxies to reflect context\nrelevance to some extent. The scope of the context\nis question-dependent and difficult to define clearly.\n6.2 How to Compare or Combine LC &\nRAG?\nThe lack of a clear definition for long context also\nindicates the absence of a coherent framework for\ncomparing or combining LC and RAG. We pro-\npose such a framework by examining three key per-\nspectives: context length, context relevance, and\nexperiment design.\nContext Length. From the model’s perspective,\ncontext length refers to the maximum number of\ntokens a model can process. From the dataset’s per-\nspective, it denotes the amount of text provided\nwith a question. In synthetic datasets, context\nlength is flexible, but this introduces a trade-off\nbetween length and relevance. Adding irrelevant\ninformation as context may help to test a model’s\nrobustness to noise, but such testing may not rep-\nresent real-world use cases. Therefore, any frame-\nwork for comparing LC and RAG should clearly\ndefine what is considered ‘long’, while indicating\nwhether this length criterion originates from the\nmodel’s capabilities, the dataset’s design, or both.\nContext Relevance. An evaluation framework\nmust also address the relevance of the text pro-\nvided as input to the model. It is crucial to dis-\ntinguish between realistic long contexts and syn-\nthetic long contexts. When benchmarks include\nboth types, separate evaluations are necessary, as\nsynthetic contexts often have low relevance and\nmay not accurately reflect real-world scenarios.\nInterestingly, the construction of synthetic long\ncontexts often mirrors RAG pipelines. Providing an\nentire curated text to an LLM as context essentially\nrepresents a ‘long context RAG’ approach, given\nthat such text is assembled during dataset creation.\nFurther chunking can introduce biases against RAG\nby disrupting the continuity of information within\neach piece.\nAdditionally, many benchmarks categorize tasks\nas ‘single-doc’ or ‘multi-doc’ based on whether the\ntext originates from a single source or multiple doc-\numents. While convenient, this categorization does\nnot perfectly align with ‘realistic’ or ‘synthetic’\ncontexts. A single document may sometimes be\nartificially composed of smaller fragments, while a\nmulti-sourced document might involve highly rel-\nevant sources, such as a group of research papers\ndiscussing the same problem.\nThe key issue remains determining to what ex-\ntent the context provided as input to LLMs contains\nsufficient and relevant content to answer the ques-\ntion, without introducing unnecessary or unrelated\ninformation.\nExperiment Settings. When investigating LC\nand RAG, the experimental objectives can be\nbroadly grouped into two categories: comparison\nand combination.\nShort RAG v.s. Long Single Input : one might\ncompare a short-context RAG pipeline against a\nlong-context single-input setup, analyzing both per-\nformance and computational cost. This provides\ninsights into the trade-off between running an extra\nretrieval pipeline for shorter contexts versus allow-\ning the model to process a larger uninterrupted text.\nLong RAG v.s. Long Single Input: One may also\ncompare a long-context RAG pipeline with a long-\ncontext single-input approach. Here, the goal is\nto see whether chunking or filtering more relevant\ncontent through retrieval can outperform or com-\nplement a fully integrated long-context approach\nby truncating exceptionally long documents.\nIn the first setting, the retrieval pipeline naturally\nreduces the number of tokens. In the second set-\nting, the context length remains the same for both\nmethods, with the only difference being how the\ntext is processed.\nRAG over Increasing Context : Another possi-\nble goal is understanding how RAG performance\nchanges with increasing context lengths. In this\nscenario, the “LC” refers specifically to how many\ntokens a model can handle. This line of work can\nreveal how well RAG pipelines scale when models\nabsorb increasingly larger inputs.\nOn the other hand, findings from evaluations\noften serve as guidelines for settings that address\nreal-world problems. In this sense, RAG and LC\nmay complement each other in real-world settings,\ndepending on the characteristics of the data source\nand the types of questions to be answered.\n6.3 Revisiting All Studies\nBased on the earlier discussion, the exploration of\nLC and RAG methods in LLMs highlights some\ncritical challenges that researchers often overlook.\nTrade-off between Context Length and Rele-\nvance. Many studies hesitate between using flex-\nible synthetic context with noisy concatenated con-\ntexts, or realistic context with dense information\nbut less availability. Among the 9 studies, 6 se-\nlect synthetic context as part of the datasets. Our\nown evaluation has also selected synthetic context\ndatasets, but we consider the influence of synthetic\nlong context and separately evaluate their results\nby context source; e.g. a Wikipedia source with\nmanually added noises represents low context rele-\nvance.\nSeveral studies have attempted to address this\nchallenge. LongBench recently updated v2 which\ncollects only realistic data. Despite a smaller scale,\nLongBench v2 shows substantial improvement in\ncontext relevance compared to its first version. Lon-\ngRAG retrieves from a massive corpus for all ques-\ntions, instead of assigning one context to each ques-\ntion. This method avoids retrieving from a syn-\nthetic long context and is hence recommendable.\nDiversity in Retrieval Mechanisms.In the com-\nparison of RAG and LC, RAG is often under-\nrepresented due to an over-reliance on traditional\nretrieval strategies. Among the 9 studies, 5 ex-\nperiment with different retrievers, only 2 try dif-\nferent chunking sizes, and none consider any\nretrieval method beyond chunk-based retrievers.\nAlthough we experiment with index-based and\nsummarization-based retrievers, we cannot promise\nthat our selected method outperforms all retrieval\nstrategies.\nFor investigating RAG performance over increas-\ning context, some studies propose their own strate-\ngies for chunking and placing RAG. OP-RAG pro-\nposes preserving the original order of chunks from\nthe context, while LC LLM-RAG proposes plac-\ning higher-scored chunks at the front and back. In\naddition to more advanced retrievers, certain in-\nformation retrieval (IR) (Manning et al., 2008)\ntechniques like relevance feedback (Harman, 1992)\nor query expansion (Carpineto and Romano, 2012)\nmight further enhance RAG performance, yet these\nhave been overlooked in existing frameworks.\nComputational Cost. Most existing studies test\non 6 to 8 datasets, and it becomes increasingly\nexpensive to conduct experiments on too many\nmodels. This is especially the case when new long-\ncontext LLMs are being released at a very fast pace.\nHence, any work might be questioned because the\nexperiment results are only applicable to one or\na few models. Among all works, LC RAG Per-\nformance includes the largest number of models\n(20). While their efforts are remarkable, they only\nexperiment on 3 datasets. FinanceBench (Islam\net al., 2023) looks at finance domain, Databricks\nDocsQA is based on Databricks platform, and NQ\nas shown table 2 as a very low rate of requiring ex-\nternal knowledge. This is not meant as criticism but\nrather to show the trade-off between testing many\nmodels and having a comprehensive benchmark.\n7 Conclusion\nIn this paper, we survey existing studies compar-\ning or combining LC and RAG, analyzing why\ndifferent implementations may result in some con-\nflicts among their insights. Therefore, we present a\nthorough comparison of LC and RAG approaches\nby leveraging a diverse set of long context QA\ndatasets. We filtered out questions that could be an-\nswered from parametric knowledge, ensuring a fair\ncomparison by focusing on questions that required\nexternal context. Along these lines, we have devel-\noped a systematic filtering and evaluation process,\nidentified the best retrieval method, and expanded\nthe dataset to provide a statistically significant ba-\nsis for analysis. The results indicate that LC gen-\nerally outperforms RAG for tasks involving well-\nstructured, dense contexts—such as Wikipedia ar-\nticles and books—and is better at answering ques-\ntions requiring specific information. By contrast,\nRAG demonstrates advantages in handling frag-\nmented information, particularly in dialogue-based\nscenarios and for more general questions.\nBeyond merely presenting the experimental re-\nsults and findings, we delve deeper into the concept\nof long context and examine how LC and RAG\nshould be compared. Our discussion aims to en-\nsure that the insights gained are more impactful\nand applicable to real-world scenarios.\nLimitations\nWhile our study provides valuable insights into\nthe comparative strengths and weaknesses of Long\nContext (LC) and Retrieval-Augmented Generation\n(RAG) approaches, it is important to acknowledge\nthree limitations that may impact the generalizabil-\nity and comprehensiveness of the findings:\nOur analysis is limited to text-based long con-\ntexts, and neglecting other modalities such as audio,\nvideo, or multi-modal contexts. The applicability\nof these insights to non-textual long-context sce-\nnarios remains unexplored, which may limit the\nbroader applicability of the findings to multi-modal\napplications.\nOur work focuses on existing papers that com-\npare and combine RAG with long-context LLMs.\nTherefore, we mainly survey the retrievers and\nLLMs used in those papers, rather than all available\nretrievers and long-context LLMs.\nOur experiments rely on existing LC and RAG\nimplementations, including specific retrieval meth-\nods and strong long-context models. As the field\ncontinues to evolve, newer models or retrieval\nstrategies may alter the comparative outcomes.\nHowever, our evaluation framework is still applica-\nble to future evaluation.\nEthical Considerations\nAdvanced Long Context LLMs equipped with\nstrong RAG capabilities could be misused to gen-\nerate misleading or harmful content, such as fake\nnews or propaganda. Their long-context capability\ncould amplify the scale and believability of such\ncontent. Researchers should prioritize safety and\ntransparency in model usage to mitigate the risk.\nReferences\nChenxin An, Shansan Gong, Ming Zhong, Xingjian\nZhao, Mukai Li, Jun Zhang, Lingpeng Kong, and\nXipeng Qiu. 2024. L-eval: Instituting standardized\nevaluation for long context language models. In Pro-\nceedings of the 62nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 14388–14411, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2024. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May\n7-11, 2024. OpenReview.net.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,\nJiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao\nLiu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,\nand Juanzi Li. 2024a. LongBench: A bilingual, mul-\ntitask benchmark for long context understanding. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 3119–3137, Bangkok, Thailand.\nAssociation for Computational Linguistics.\nYushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xi-\naozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei\nHou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024b.\nLongbench v2: Towards deeper understanding and\nreasoning on realistic long-context multitasks. CoRR,\nabs/2412.15204.\nVictoria Basmova, Yoav Goldberg, and Reut Tsarfaty.\n2024. Llms’ reading comprehension is affected by\nparametric knowledge and struggles with hypotheti-\ncal statements. CoRR, abs/2404.06283.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, and Christopher Hesse et al. 2020.\nLanguage models are few-shot learners. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nZheng Cai, Maosong Cao, Haojiong Chen, Kai Chen,\nKeyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi\nChen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan,\nZhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe\nGu, and Tao Gui et al. 2024. Internlm2 technical\nreport. CoRR, abs/2403.17297.\nClaudio Carpineto and Giovanni Romano. 2012. A\nsurvey of automatic query expansion in information\nretrieval. ACM Comput. Surv., 44(1):1:1–1:50.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and\nYuandong Tian. 2023. Extending context window of\nlarge language models via positional interpolation.\nCoRR, abs/2306.15595.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A. Smith, and Matt Gardner. 2021. A dataset\nof information-seeking questions and answers an-\nchored in research papers. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4599–4610, On-\nline. Association for Computational Linguistics.\nDeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingx-\nuan Wang, Bo Liu, Chenggang Zhao, Chengqi Deng,\nChong Ruan, Damai Dai, Daya Guo, Dejian Yang,\nDeli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli\nLuo, Guangbo Hao, Guanting Chen, and Guowei Li\net al. 2024. Deepseek-v2: A strong, economical, and\nefficient mixture-of-experts language model. CoRR,\nabs/2405.04434.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, and An-\ngela Fan et al. 2024. The llama 3 herd of models.\nCoRR, abs/2407.21783.\nWeizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai,\nLei Deng, and Wei Han. 2024. Extending context\nwindow of large language models via semantic com-\npression. In Findings of the Association for Compu-\ntational Linguistics, ACL 2024, Bangkok, Thailand\nand virtual meeting, August 11-16, 2024, pages 5169–\n5181. Association for Computational Linguistics.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,\nMeng Wang, and Haofen Wang. 2023. Retrieval-\naugmented generation for large language models: A\nsurvey. CoRR, abs/2312.10997.\nShweta Gupta, Sunita Yadav, and Rajesh Prasad. 2018.\nDocument retrieval using efficient indexing tech-\nniques: A review. Information Retrieval and Man-\nagement: Concepts, Methodologies, Tools, and Ap-\nplications, pages 1745–1764.\nDonna Harman. 1992. Relevance feedback revisited. In\nProceedings of the 15th Annual International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval. Copenhagen, Denmark, June\n21-24, 1992, pages 1–10. ACM.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing a multi-\nhop QA dataset for comprehensive evaluation of\nreasoning steps. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 6609–6625, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nPranab Islam, Anand Kannappan, Douwe Kiela, Re-\nbecca Qian, Nino Scherrer, and Bertie Vidgen. 2023.\nFinancebench: A new benchmark for financial ques-\ntion answering. CoRR, abs/2311.11944.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2022. Unsupervised dense in-\nformation retrieval with contrastive learning. Trans.\nMach. Learn. Res., 2022.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv., 55(12):248:1–248:38.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris Bam-\nford, Devendra Singh Chaplot, Diego de Las Casas,\nEmma Bou Hanna, Florian Bressand, Gianna\nLengyel, Guillaume Bour, Guillaume Lample,\nLélio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian,\nSophia Yang, Szymon Antoniak, Teven Le Scao,\nThéophile Gervet, Thibaut Lavril, Thomas Wang,\nTimothée Lacroix, and William El Sayed. 2024a.\nMixtral of experts. CoRR, abs/2401.04088.\nZhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun,\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023. Active retrieval\naugmented generation. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 7969–7992, Singapore. As-\nsociation for Computational Linguistics.\nZiyan Jiang, Xueguang Ma, and Wenhu Chen. 2024b.\nLongrag: Enhancing retrieval-augmented generation\nwith long-context llms. CoRR, abs/2406.15319.\nBowen Jin, Jinsung Yoon, Jiawei Han, and Sercan Ö.\nArik. 2024. Long-context llms meet RAG: over-\ncoming challenges for long inputs in RAG. CoRR,\nabs/2410.05983.\nTomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris\nDyer, Karl Moritz Hermann, Gábor Melis, and Ed-\nward Grefenstette. 2018. The NarrativeQA reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317–328.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nQuinn Leng, Jacob Portes, Sam Havens, Matei Za-\nharia, and Michael Carbin. 2024. Long context\nrag performance of large language models. CoRR,\nabs/2411.03538.\nZhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei,\nand Michael Bendersky. 2024. Retrieval augmented\ngeneration or long-context llms? A comprehensive\nstudy and hybrid approach. In Proceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing: EMNLP 2024 - Industry Track,\nMiami, Florida, USA, November 12-16, 2024, pages\n881–893. Association for Computational Linguistics.\nSheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,\nJimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun\nChen. 2023. How to train your dragon: Diverse aug-\nmentation towards generalizable dense retrieval. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2023, pages 6385–6400, Singapore.\nAssociation for Computational Linguistics.\nJerry Liu. 2022. LlamaIndex. CoRR.\nChristopher D. Manning, Prabhakar Raghavan, and Hin-\nrich Schütze. 2008. Introduction to information re-\ntrieval. Cambridge University Press.\nThomas Mesnard, Cassidy Hardin, Robert Dadashi,\nSurya Bhupatiraju, Shreya Pathak, Laurent Sifre,\nMorgane Rivière, Mihir Sanjay Kale, Juliette Love,\nand Pouya Tafti et al. 2024. Gemma: Open models\nbased on gemini research and technology. CoRR,\nabs/2403.08295.\nErik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang,\nCongying Xia, Chen Xing, Jesse Vig, Semih\nYavuz, Philippe Laban, Ben Krause, Senthil Purush-\nwalkam, Tong Niu, Wojciech Kryscinski, Lidiya\nMurakhovs’ka, Prafulla Kumar Choubey, Alex Fab-\nbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat,\nChien-Sheng Wu, Silvio Savarese, Yingbo Zhou,\nShafiq Rayhan Joty, and Caiming Xiong. 2023. Xgen-\n7b technical report. CoRR, abs/2309.03450.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-\nman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\ning Bao, Mohammad Bavarian, and Jeff Belgum\net al. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,\nNikita Nangia, Jason Phang, Angelica Chen, Vishakh\nPadmakumar, Johnny Ma, Jana Thompson, He He,\nand Samuel Bowman. 2022. QuALITY: Question\nanswering with long input texts, yes! In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5336–5358,\nSeattle, United States. Association for Computational\nLinguistics.\nQwen, :, An Yang, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin,\nJian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang,\nJiaxi Yang, Jingren Zhou, and Junyang Lin et al. 2024.\nQwen2.5 technical report. CoRR, abs/2412.15115.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-\nrat, and Julian Schrittwieser et al. 2024. Gemini 1.5:\nUnlocking multimodal understanding across millions\nof tokens of context. CoRR, abs/2403.05530.\nStephen E. Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Found. Trends Inf. Retr., 3(4):333–389.\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh\nKhanna, Anna Goldie, and Christopher D. Manning.\n2024. RAPTOR: recursive abstractive processing for\ntree-organized retrieval. In The Twelfth International\nConference on Learning Representations, ICLR 2024,\nVienna, Austria, May 7-11, 2024. OpenReview.net.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nWeihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu,\nand Yiqun Liu. 2024. DRAGIN: dynamic retrieval\naugmented generation based on the real-time informa-\ntion needs of large language models. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2024, Bangkok, Thailand, August 11-16, 2024,\npages 12991–13013. Association for Computational\nLinguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Es-\niobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, and\nBrian Fuller et al. 2023. Llama 2: Open foundation\nand fine-tuned chat models. CoRR, abs/2307.09288.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Musique: Multi-\nhop questions via single-hop question composition.\nTrans. Assoc. Comput. Linguistics, 10:539–554.\nCunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui\nWu, Qipeng Guo, Cheng Deng, Guangsheng Bao,\nQian Wang, and Yue Zhang. 2024a. Novelqa: A\nbenchmark for long-range novel question answering.\nCoRR, abs/2403.12766.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,\nRangan Majumder, and Furu Wei. 2024b. Improv-\ning text embeddings with large language models. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2024, Bangkok, Thailand, August\n11-16, 2024, pages 11897–11916. Association for\nComputational Linguistics.\nXindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu\nRen, Mehdi Rezagholizadeh, and Armaghan Eshaghi.\n2024c. Beyond the limits: A survey of techniques to\nextend the context length in large language models.\nIn Proceedings of the Thirty-Third International Joint\nConference on Artificial Intelligence, IJCAI 2024,\nJeju, South Korea, August 3-9, 2024 , pages 8299–\n8307. ijcai.org.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighoff. 2023. C-pack: Packaged resources\nto advance general chinese embedding. CoRR,\nabs/2309.07597.\nPeng Xu, Wei Ping, Xianchao Wu, Zihan Liu, Moham-\nmad Shoeybi, and Bryan Catanzaro. 2024a. Chatqa 2:\nBridging the gap to proprietary llms in long context\nand RAG capabilities. CoRR, abs/2407.14482.\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee,\nChen Zhu, Zihan Liu, Sandeep Subramanian, Evelina\nBakhturina, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2024b. Retrieval meets long context large lan-\nguage models. In The Twelfth International Con-\nference on Learning Representations, ICLR 2024,\nVienna, Austria, May 7-11, 2024. OpenReview.net.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-\nran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian\nYang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, and\nJianxin Yang et al. 2024. Qwen2 technical report.\nCoRR, abs/2407.10671.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nBERTserini. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 72–77, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nTan Yu, Anbang Xu, and Rama Akkiraju. 2024. In\ndefense of RAG in the era of long-context language\nmodels. CoRR, abs/2409.01666.\nAohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang,\nDa Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao,\nHanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun,\nJiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing\nZhang, Juanzi Li, and Lei Zhao et al. 2024. Chatglm:\nA family of large language models from GLM-130B\nto GLM-4 all tools. CoRR, abs/2406.12793.\nTianjun Zhang, Shishir G. Patil, Naman Jain, Sheng\nShen, Matei Zaharia, Ion Stoica, and Joseph E. Gon-\nzalez. 2024. RAFT: adapting language model to\ndomain specific RAG. CoRR, abs/2403.10131.\n0 250 500 750 1000 1250 1500 1750\nWord Count\nWikiPedia\nStory\nPaper/Report\nDialogue\n1026 434\n1251 549\n1055 783\n91 73 LC\nRAG\nFigure 6: Performance breakdown by knowledge source\nfor LC Better and RAG Better.\nA F1 Score Computation\nTo calculate the F1 score, we first convert both the\nprediction and the reference text into sets of unique\ntokens. Tokens appearing in both sets count as true\npositives (TP), tokens present only in the prediction\nare false positives (FP), and tokens missing from\nthe prediction but in the reference are false nega-\ntives (FN). Precision is defined as TP\nTP+FP , recall as\nTP\nTP+FN , and the F1 score is their harmonic mean:\nF1 = 2 × precision ×recall\nprecision +recall.\nExample:\n\"cat leaps table quickly\"(prediction)\n\"the cat leaps over the table\" (reference)\nThe corresponding sets are:\nprediction_set = {cat, leaps, table, quickly}\ngold_set = {the, cat, leaps, over, table}.\nHere, {cat,leaps,table} are TP = 3,\n{quickly} is FP = 1, and {the,over} are FN\n= 2. Hence:\nprecision = 3\n3 +1 = 0.75, recall = 3\n3 +2 = 0.60,\nF1 = 2 × 0.75 ×0.60\n0.75 +0.60 = 0.67.\nB In-detail Analysis on Loose Evaluation\nSettings\nAs a complement to § 5.3, we provide a detailed\ncomparison of the performance of LC and RAG\nunder the loose evaluation settings based on Exact\nMatch (EM) and F1 scores.\nAs shown in Figure 6, loose evaluation setting\nreveals similar trends to the strict setting in the\nperformance of LC and RAG on different knowl-\nedge sources. LC outperforms RAG for structured\n0 250 500 750 1000 1250 1500 1750 2000\nWord Count\nOther\nHow\nWhy\nWhat\nWhich\nWhere\nWhen\nWho\n91 226\n593 392\n243 103\n1469 689\n302 137\n200 76\n214 87\n321 133 LC\nRAG\nFigure 7: Performance breakdown by question type for\nLC Better and RAG Better.\nsources like Wikipedia, course websites, and pa-\npers/reports, where having complete context is ad-\nvantageous. This trend is consistent in both evalua-\ntion settings. However, RAG performs better with\ndialogue-based and story-based knowledge sources,\nwhere the information is fragmented. The loose\nevaluation, with the inclusion of F1 scores, shows\na slight improvement for RAG in these cases, as\npartial answers are rewarded more, but the overall\ntrend remains the same.\nFigure 7 highlights the performance of LC and\nRAG across different question types. For fact-\nbased questions (e.g., “Who”, “Where”, “Which”),\nLC continues to outperform RAG in both evalua-\ntion settings, as these questions benefit from having\ncomplete, uninterrupted context. For open-ended\nquestions (e.g., “How”, “Why”), RAG shows com-\nparable performance to LC in both settings. The\nloose evaluation, however, slightly favors RAG due\nto its ability to synthesize information from mul-\ntiple sources, as F1 scoring acknowledges partial\ncorrectness. In the case of \"Other\" questions (sim-\nple \"Yes\" or \"No\" questions), RAG significantly\noutperforms LC in both evaluation settings, but the\nadvantage is more pronounced in the loose eval-\nuation. The inclusion of F1 scores helps RAG\ncapture partial successes that would be penalized\nunder strict EM-only scoring.\nOverall, the figures illustrate that the perfor-\nmance patterns of LC and RAG remain largely\nconsistent across both strict and loose evaluation\nsettings. The key difference is that RAG gains a\nslight performance boost in the loose evaluation."
  }
]