# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bRSu_qJdK8WSsRRYRTU22qiGo-CMS3-M
"""

import json
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from google.colab import files
from pathlib import Path

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"using device - {device}")
if device == "cuda":
    print(f"GPU - {torch.cuda.get_device_name(0)}")

print("\nupload your 'extracted_data.json' file")
uploaded = files.upload()

input_filename = "extracted_data.json"
if input_filename in uploaded:
    documents = json.loads(uploaded[input_filename].decode("utf-8"))

print("loading REBEL model to the GPU")
tokenizer = AutoTokenizer.from_pretrained("Babelscape/rebel-large")
model = AutoModelForSeq2SeqLM.from_pretrained("Babelscape/rebel-large").to(device)
print("model loaded successfully")

def parse_rebel_output(text: str):
    triplets = []
    relation_chunks = text.strip().split("<triplet>")

    for chunk in relation_chunks:
        if not chunk:
            continue
        try:
            subject, relation, obj = chunk.split("<subj>")
            triplets.append({
                "source": subject.strip(),
                "relation": relation.strip().replace(" ", "_").upper(),
                "target": obj.strip()
            })
        except ValueError:
            continue

    return triplets

print("\nStarting knowledge extraction")
unique_nodes = set()
all_edges = []

for i, doc in enumerate(documents):
    content = doc['content']
    print(f"Processing document {i+1}/{len(documents)}: {doc['source']}")

    if not content:
        continue

    inputs = tokenizer(
        content,
        return_tensors="pt",
        max_length=512,
        truncation=True,
        padding=True
    ).to(device)

    generated_ids = model.generate(
        **inputs,
        max_length=256,
        num_beams=5,
        num_return_sequences=1
    )

    decoded_text = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=False)
    extracted_triplets = parse_rebel_output(decoded_text)

    if not extracted_triplets:
        print("no relations found in this document")
        continue

    print(f"  + found {len(extracted_triplets)} relations.")
    for triplet in extracted_triplets:
        unique_nodes.add(triplet['source'])
        unique_nodes.add(triplet['target'])
        all_edges.append(triplet)

knowledge_graph = {
    "nodes": [{"id": node, "type": "ENTITY"} for node in unique_nodes],
    "edges": all_edges
}

output_filename = "knowledge_graph.json"
with open(output_filename, 'w', encoding='utf-8') as f:
    json.dump(knowledge_graph, f, indent=4, ensure_ascii=False)

print(f"\nKnowledge graph created with {len(knowledge_graph['nodes'])} nodes and {len(knowledge_graph['edges'])} edges.")
print(f" Saved to '{output_filename}'.")
print("\nDownloading the knowledge graph file")
files.download(output_filename)